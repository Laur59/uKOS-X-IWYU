From 8a2899c5a7adc51feb3327966b567a281611cb1c Mon Sep 17 00:00:00 2001
From: Laur59 <148864407+Laur59@users.noreply.github.com>
Date: Wed, 14 May 2025 21:27:59 +0300
Subject: [PATCH 1/3] Merge remote-tracking branch 'origin/release/20.x
 (7b09d7b4)' into 20.1.5

---
 .../ClangTidyDiagnosticConsumer.cpp           |   36 +-
 .../clang-tidy/ClangTidyDiagnosticConsumer.h  |    4 +
 .../clang-tidy/ClangTidyOptions.cpp           |    4 +-
 .../clang-tidy/tool/run-clang-tidy.py         |    7 +-
 clang-tools-extra/docs/ReleaseNotes.rst       |    6 +
 .../inheritance/.clang-tidy                   |    1 +
 .../inheritance/foo.cpp                       |    3 +
 .../inheritance/foo.h                         |    1 +
 .../inheritance/subfolder/.clang-tidy         |    2 +
 .../inheritance/subfolder/bar.cpp             |    8 +
 .../inheritance/subfolder/bar.h               |    1 +
 .../simple/.clang-tidy                        |    1 +
 .../simple/foo.cpp                            |    3 +
 .../simple/foo.h                              |    1 +
 clang/docs/HIPSupport.rst                     |   20 +
 clang/docs/ReleaseNotes.rst                   |   34 +-
 clang/include/clang/AST/Expr.h                |   15 +-
 clang/include/clang/Driver/Distro.h           |    4 +-
 clang/include/clang/Interpreter/Interpreter.h |   16 +-
 clang/include/clang/Sema/Sema.h               |   15 +-
 .../StaticAnalyser/Core/AnalyserOptions.def   |   13 +
 .../Core/PathSensitive/FunctionSummary.h      |    4 -
 clang/include/clang/Support/Compiler.h        |    2 +-
 clang/lib/AST/Expr.cpp                        |   14 +-
 clang/lib/AST/ExprConstant.cpp                |   23 +-
 clang/lib/AST/MicrosoftMangle.cpp             |   16 +-
 clang/lib/AST/NestedNameSpecifier.cpp         |   17 +-
 clang/lib/AST/ParentMapContext.cpp            |   17 +-
 clang/lib/Analysis/LiveVariables.cpp          |   11 +-
 clang/lib/Basic/Targets/SystemZ.cpp           |    2 +-
 clang/lib/CodeGen/CGExprConstant.cpp          |    5 +-
 clang/lib/Driver/Distro.cpp                   |    6 +
 clang/lib/Driver/ToolChains/Arch/ARM.cpp      |    8 -
 clang/lib/Driver/ToolChains/Hexagon.cpp       |    5 +-
 clang/lib/Format/ContinuationIndenter.cpp     |    7 +
 clang/lib/Format/Format.cpp                   |   60 +-
 clang/lib/Format/FormatToken.cpp              |    2 +-
 clang/lib/Format/TokenAnnotator.cpp           |   16 +-
 clang/lib/Format/TokenAnnotator.h             |    4 +-
 clang/lib/Format/UnwrappedLineParser.cpp      |   95 +-
 clang/lib/Headers/__clang_cuda_intrinsics.h   |   16 +-
 clang/lib/Headers/amdgpuintrin.h              |    2 +-
 clang/lib/Headers/avx10_2_512convertintrin.h  |   46 +-
 clang/lib/Headers/avx10_2_512satcvtdsintrin.h |   52 +-
 clang/lib/Headers/avx10_2_512satcvtintrin.h   |  168 +-
 clang/lib/Headers/avx10_2convertintrin.h      |  112 +-
 clang/lib/Headers/avx10_2minmaxintrin.h       |   45 -
 clang/lib/Headers/avx10_2niintrin.h           | 1666 ------------
 clang/lib/Headers/avx10_2satcvtdsintrin.h     |  210 +-
 clang/lib/Headers/avx10_2satcvtintrin.h       |  288 +-
 clang/lib/Headers/nvptxintrin.h               |    3 +-
 clang/lib/Headers/vecintrin.h                 |    5 +
 clang/lib/Interpreter/DeviceOffload.cpp       |   49 +-
 clang/lib/Interpreter/DeviceOffload.h         |    6 +-
 clang/lib/Interpreter/IncrementalExecutor.h   |    2 +-
 clang/lib/Interpreter/Interpreter.cpp         |   76 +-
 clang/lib/Interpreter/Wasm.cpp                |   13 +
 clang/lib/Interpreter/Wasm.h                  |    3 +
 clang/lib/Parse/ParseExpr.cpp                 |    3 -
 clang/lib/Parse/ParseInit.cpp                 |    2 +-
 clang/lib/Sema/Sema.cpp                       |   43 +
 clang/lib/Sema/SemaAccess.cpp                 |    4 +-
 clang/lib/Sema/SemaAttr.cpp                   |    5 +
 clang/lib/Sema/SemaCUDA.cpp                   |   23 +-
 clang/lib/Sema/SemaCast.cpp                   |   23 +
 clang/lib/Sema/SemaConcept.cpp                |   69 -
 clang/lib/Sema/SemaDecl.cpp                   |   17 +-
 clang/lib/Sema/SemaDeclCXX.cpp                |    2 +-
 clang/lib/Sema/SemaExprCXX.cpp                |    1 +
 clang/lib/Sema/SemaExprMember.cpp             |    1 -
 clang/lib/Sema/SemaInit.cpp                   |    1 +
 clang/lib/Sema/SemaLambda.cpp                 |   68 +
 clang/lib/Sema/SemaTemplate.cpp               |   36 +-
 clang/lib/Sema/SemaTemplateDeduction.cpp      |   16 +-
 clang/lib/Sema/SemaTemplateDeductionGuide.cpp |   25 +-
 clang/lib/Sema/SemaTemplateInstantiate.cpp    |   46 +-
 .../lib/Sema/SemaTemplateInstantiateDecl.cpp  |    2 +-
 clang/lib/Sema/TreeTransform.h                |   84 +-
 clang/lib/Serialization/ASTReader.cpp         |    6 +-
 .../Checkers/BuiltinFunctionChecker.cpp       |   86 +-
 clang/lib/StaticAnalyser/Core/ExprEngine.cpp  |   60 +-
 clang/test/Analysis/analyzer-config.c         |    1 +
 clang/test/Analysis/builtin_overflow.c        |    6 +-
 clang/test/Analysis/builtin_overflow_notes.c  |   10 +-
 clang/test/Analysis/live-stmts.cpp            |    2 +
 .../Analysis/loop-based-inlining-prevention.c |  200 ++
 clang/test/Analysis/loop-unrolling.cpp        |   30 +-
 .../dcl.attr/dcl.attr.nodiscard/p2.cpp        |   10 +-
 .../CodeGen/SystemZ/builtins-systemz-bitop.c  |    4 +-
 .../SystemZ/builtins-systemz-vector5-error.c  |    2 +-
 .../SystemZ/builtins-systemz-vector5.c        |    2 +-
 .../SystemZ/builtins-systemz-zvector5-error.c |    2 +-
 .../SystemZ/builtins-systemz-zvector5.c       |    4 +-
 .../test/CodeGen/SystemZ/systemz-abi-vector.c |    2 +
 clang/test/CodeGen/SystemZ/systemz-abi.c      |    2 +
 .../CodeGen/X86/avx10_2_512convert-builtins.c |  108 +-
 .../CodeGen/X86/avx10_2_512minmax-error.c     |   11 -
 .../X86/avx10_2_512satcvt-builtins-error.c    |  198 --
 .../CodeGen/X86/avx10_2_512satcvt-builtins.c  |  360 +--
 .../X86/avx10_2_512satcvtds-builtins-x64.c    |   96 +-
 .../X86/avx10_2_512satcvtds-builtins.c        |   74 +-
 .../CodeGen/X86/avx10_2convert-builtins.c     |  234 +-
 .../test/CodeGen/X86/avx10_2minmax-builtins.c |   54 -
 clang/test/CodeGen/X86/avx10_2ni-builtins.c   | 2405 -----------------
 .../test/CodeGen/X86/avx10_2satcvt-builtins.c |  574 ++--
 .../X86/avx10_2satcvtds-builtins-errors.c     |   57 -
 .../X86/avx10_2satcvtds-builtins-x64.c        |  240 +-
 .../CodeGen/X86/avx10_2satcvtds-builtins.c    |  223 +-
 clang/test/CodeGen/attr-target-x86.c          |    8 +-
 clang/test/CodeGenCXX/cxx23-p2280r4.cpp       |   28 +
 clang/test/CodeGenCXX/mangle-ms-matrix.cpp    |   57 +
 clang/test/CodeGenCoroutines/pr134409.cpp     |   43 +
 clang/test/Driver/arm-mfpu.c                  |    6 +-
 clang/test/Driver/hexagon-cpu-default.c       |    4 +
 clang/test/Driver/hexagon-toolchain-elf.c     |    1 +
 clang/test/Driver/hexagon-toolchain-linux.c   |    1 +
 .../aarch64-fujitsu-monaka.c                  |    2 +
 clang/test/Driver/systemz-march.c             |    2 +
 .../Misc/target-invalid-cpu-note/systemz.c    |    1 +
 clang/test/Modules/MixedModulePrecompile.cpp  |   63 +
 clang/test/Parser/recovery.cpp                |   18 +
 clang/test/Preprocessor/arm-target-features.c |   27 -
 clang/test/Preprocessor/embed_constexpr.c     |   21 +
 .../Preprocessor/predefined-arch-macros.c     |    3 +
 clang/test/Sema/GH126231.cpp                  |   18 +
 clang/test/Sema/warn-cast-function-type-win.c |   36 +
 clang/test/SemaCUDA/dtor.cu                   |  104 +
 .../SemaCXX/builtin-object-size-cxx14.cpp     |   12 +
 .../SemaCXX/concept-crash-on-diagnostic.cpp   |   12 +
 .../SemaCXX/constant-expression-cxx11.cpp     |    4 +-
 .../SemaCXX/constant-expression-p2280r4.cpp   |   32 +-
 clang/test/SemaCXX/ctad.cpp                   |  132 +-
 clang/test/SemaCXX/cxx2b-deducing-this.cpp    |    7 +
 clang/test/SemaCXX/ms-property.cpp            |    2 +-
 clang/test/SemaTemplate/concepts-lambda.cpp   |   15 +
 clang/test/SemaTemplate/cwg2398.cpp           |   22 +-
 clang/test/SemaTemplate/deduction-guide.cpp   |   80 +
 .../SemaTemplate/temp_arg_template_p0522.cpp  |    3 +-
 clang/unittests/AST/DeclPrinterTest.cpp       |   16 +-
 clang/unittests/Format/ConfigParseTest.cpp    |   20 +
 clang/unittests/Format/FormatTest.cpp         |   17 +-
 clang/unittests/Format/TokenAnnotatorTest.cpp |   11 +
 clang/unittests/Tooling/QualTypeNamesTest.cpp |   96 +
 cmake/Modules/LLVMVersion.cmake               |    2 +-
 .../lib/rtsan/rtsan_interceptors_posix.cpp    |   28 +-
 .../tests/rtsan_test_interceptors_posix.cpp   |   21 +-
 .../lib/sanitizer_common/sanitizer_linux.cpp  |    4 +
 .../sanitizer_linux_libcdep.cpp               |   13 +-
 .../sanitizer_platform_limits_posix.cpp       |    2 +-
 .../sanitizer_stoptheworld_linux_libcdep.cpp  |    3 +-
 .../test/profile/instrprof-darwin-exports.c   |    6 +-
 .../TestCases/Darwin/malloc_zone.cpp          |    2 +
 flang/lib/Semantics/check-declarations.cpp    |    5 +-
 flang/test/Semantics/call10.f90               |    4 +
 libcxx/docs/ReleaseNotes/20.rst               |   14 +-
 libcxx/include/__configuration/availability.h |   24 +-
 libcxx/include/__configuration/platform.h     |    7 +
 libcxx/include/__locale                       |  153 +-
 libcxx/include/__locale_dir/locale_base_api.h |  112 +-
 libcxx/include/__vector/vector_bool.h         |    1 +
 libcxx/include/codecvt                        |   31 +-
 libcxx/include/fstream                        |   55 +-
 libcxx/include/istream                        |    4 +-
 libcxx/include/regex                          |  290 +-
 libcxx/include/strstream                      |   55 +-
 .../configs/armv7m-picolibc-libc++.cfg.in     |    4 -
 .../test/libcxx/system_reserved_names.gen.py  |    6 +
 .../vendor/apple/disable-availability.sh.cpp  |   49 +
 .../sized_delete_array.pass.cpp               |    3 +
 .../new.delete.single/sized_delete.pass.cpp   |    3 +
 .../test/std/numerics/c.math/signbit.pass.cpp |    2 +-
 .../meta/meta.rel/is_virtual_base_of.pass.cpp |    2 +-
 .../is_implicit_lifetime.pass.cpp             |    2 +-
 .../is_implicit_lifetime.verify.cpp           |    2 +-
 .../tools/clang_tidy_checks/CMakeLists.txt    |    7 +-
 lld/COFF/Driver.cpp                           |    6 +-
 lld/COFF/MinGW.cpp                            |    5 +
 lld/COFF/SymbolTable.cpp                      |   25 +-
 lld/COFF/SymbolTable.h                        |    5 +-
 lld/ELF/Arch/Hexagon.cpp                      |    2 +-
 lld/docs/ReleaseNotes.rst                     |    5 +
 .../COFF/imports-static-lib-indirect.test     |   26 +
 lld/test/COFF/imports-static-lib.test         |   33 +
 lld/test/COFF/undefined_lazy.test             |   26 -
 lld/test/ELF/emulation-hexagon.s              |    4 +-
 lld/test/ELF/hexagon-eflag.s                  |    5 +-
 lld/wasm/Config.h                             |  106 +
 lld/wasm/Driver.cpp                           |   60 +-
 lld/wasm/InputChunks.cpp                      |   10 +-
 lld/wasm/MarkLive.cpp                         |    6 +-
 lld/wasm/OutputSections.cpp                   |    4 +-
 lld/wasm/Symbols.cpp                          |   25 -
 lld/wasm/Symbols.h                            |   99 -
 lld/wasm/SyntheticSections.cpp                |   32 +-
 lld/wasm/Writer.cpp                           |  185 +-
 lldb/cmake/modules/FindCursesAndPanel.cmake   |   52 +-
 lldb/source/API/SBTarget.cpp                  |    6 +-
 .../Host/posix/ProcessLauncherPosixFork.cpp   |    8 +-
 ...NativeRegisterContextLinux_loongarch64.cpp |   17 +-
 .../target/read-instructions-flavor/Makefile  |    3 +
 .../TestTargetReadInstructionsFlavor.py       |   40 +
 .../target/read-instructions-flavor/main.c    |   21 +
 lldb/tools/lldb-server/lldb-platform.cpp      |   18 +-
 lldb/unittests/Host/HostTest.cpp              |   43 +-
 llvm/docs/LangRef.rst                         |    2 +
 llvm/docs/ReleaseNotes.md                     |    4 +
 llvm/include/llvm/BinaryFormat/ELF.h          |   10 +
 llvm/include/llvm/CodeGen/MachineBasicBlock.h |    9 +
 llvm/include/llvm/CodeGen/MachineInstr.h      |   13 +-
 llvm/include/llvm/Support/Compiler.h          |    2 +-
 llvm/lib/Analysis/BasicAliasAnalysis.cpp      |    8 +-
 llvm/lib/Analysis/LoopAccessAnalysis.cpp      |   12 +-
 llvm/lib/Analysis/ScalarEvolution.cpp         |   11 +-
 llvm/lib/Analysis/ValueTracking.cpp           |    5 +-
 .../lib/CodeGen/ComplexDeinterleavingPass.cpp |   11 +
 llvm/lib/CodeGen/GlobalMerge.cpp              |    3 +-
 llvm/lib/CodeGen/ModuloSchedule.cpp           |   56 +-
 llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp |    5 +-
 llvm/lib/CodeGen/TailDuplicator.cpp           |   12 +-
 llvm/lib/CodeGen/TargetLoweringBase.cpp       |    3 +
 llvm/lib/IR/RuntimeLibcalls.cpp               |    1 +
 llvm/lib/IR/User.cpp                          |    6 +
 llvm/lib/IR/Verifier.cpp                      |    3 +
 llvm/lib/MC/MCWinCOFFStreamer.cpp             |    4 +-
 llvm/lib/ObjCopy/MachO/MachOLayoutBuilder.cpp |   13 +-
 llvm/lib/Passes/PassBuilderPipelines.cpp      |   13 +
 .../Target/AArch64/AArch64ISelLowering.cpp    |   35 +-
 llvm/lib/Target/AArch64/AArch64Processors.td  |    3 +-
 llvm/lib/Target/AArch64/SMEABIPass.cpp        |   16 +-
 .../AArch64/Utils/AArch64SMEAttributes.cpp    |    2 +
 .../AArch64/Utils/AArch64SMEAttributes.h      |    8 +-
 llvm/lib/Target/AMDGPU/FLATInstructions.td    |    2 +-
 llvm/lib/Target/ARM/ARMConstantIslandPass.cpp |    3 +-
 llvm/lib/Target/ARM/ARMISelLowering.cpp       |   67 +-
 .../Target/BPF/BPFAbstractMemberAccess.cpp    |    5 +-
 llvm/lib/Target/Hexagon/HexagonCallingConv.td |   19 +
 .../Target/Hexagon/HexagonHardwareLoops.cpp   |   46 +-
 llvm/lib/Target/Hexagon/HexagonISelLowering.h |    1 +
 .../Target/Hexagon/HexagonISelLoweringHVX.cpp |   32 +-
 llvm/lib/Target/Hexagon/HexagonPatterns.td    |   43 +-
 .../MCTargetDesc/HexagonAsmBackend.cpp        |   18 +
 .../MCTargetDesc/HexagonMCTargetDesc.cpp      |    2 +-
 .../Target/LoongArch/LoongArchAsmPrinter.cpp  |   35 +-
 .../LoongArch/LoongArchFloatInstrFormats.td   |    4 +-
 .../LoongArch/LoongArchISelLowering.cpp       |   16 +-
 .../Target/LoongArch/LoongArchISelLowering.h  |    1 +
 .../LoongArch/LoongArchLASXInstrInfo.td       |    4 +-
 .../Target/LoongArch/LoongArchLSXInstrInfo.td |    4 +-
 .../MCTargetDesc/LoongArchMCExpr.cpp          |    1 +
 llvm/lib/Target/PowerPC/PPCISelLowering.cpp   |    6 +
 llvm/lib/Target/PowerPC/PPCInstrVSX.td        |    4 +
 .../Target/RISCV/RISCVTargetTransformInfo.cpp |    5 +-
 .../Target/SystemZ/SystemZISelLowering.cpp    |   23 +-
 llvm/lib/Target/SystemZ/SystemZInstrVector.td |    4 +-
 llvm/lib/Target/SystemZ/SystemZProcessors.td  |    3 +-
 llvm/lib/Target/SystemZ/SystemZSchedule.td    |    1 +
 llvm/lib/Target/SystemZ/SystemZScheduleZ16.td |   16 +-
 llvm/lib/Target/SystemZ/SystemZScheduleZ17.td | 1754 ++++++++++++
 .../SystemZ/SystemZTargetTransformInfo.cpp    |   48 +-
 llvm/lib/Target/X86/X86.td                    |    2 +-
 llvm/lib/Target/X86/X86ExpandPseudo.cpp       |   14 +-
 .../Target/X86/X86FixupVectorConstants.cpp    |   11 +-
 llvm/lib/Target/X86/X86ISelLowering.cpp       |    9 +-
 llvm/lib/Target/X86/X86InstrAVX10.td          |   20 +-
 llvm/lib/Target/X86/X86InstrSSE.td            |    4 +-
 llvm/lib/Target/X86/X86MCInstLower.cpp        |    6 +-
 llvm/lib/TargetParser/Host.cpp                |    2 +-
 llvm/lib/TargetParser/RISCVISAInfo.cpp        |   18 +-
 llvm/lib/TargetParser/X86TargetParser.cpp     |    3 +-
 .../ToolDrivers/llvm-dlltool/CMakeLists.txt   |    1 +
 .../llvm-dlltool/DlltoolDriver.cpp            |  146 +-
 llvm/lib/ToolDrivers/llvm-dlltool/Options.td  |    5 +
 llvm/lib/Transforms/IPO/GlobalOpt.cpp         |    4 +
 .../InstCombine/InstCombineAndOrXor.cpp       |    4 +-
 .../InstCombine/InstCombineSelect.cpp         |    9 +-
 .../InstCombine/InstCombineVectorOps.cpp      |   16 +-
 .../lib/Transforms/Scalar/MemCpyOptimizer.cpp |   43 +-
 llvm/lib/Transforms/Utils/SimplifyIndVar.cpp  |    3 +
 .../Transforms/Vectorize/LoopVectorize.cpp    |   15 +-
 .../Transforms/Vectorize/VectorCombine.cpp    |    5 +
 llvm/test/Analysis/BasicAA/size-overflow.ll   |   14 +
 .../Analysis/CostModel/RISCV/reduce-fadd.ll   |  167 +-
 .../Analysis/CostModel/SystemZ/divrem-reg.ll  |   86 +-
 .../CostModel/SystemZ/i128-cmp-ext-conv.ll    |    8 +-
 .../Analysis/CostModel/SystemZ/int-arith.ll   |   10 +-
 ...erlying-object-different-address-spaces.ll |   39 +
 .../test/Analysis/ScalarEvolution/pr135531.ll |   19 +
 .../trip-count-unknown-stride.ll              |   34 +-
 llvm/test/Analysis/ValueTracking/phi-self.ll  |   89 +
 .../16bit-float-promotion-with-nofp.ll        |   91 +
 .../AArch64/aarch64-swp-ws-live-intervals.mir |  103 +
 llvm/test/CodeGen/AArch64/arm64-popcnt.ll     |  159 ++
 .../complex-deinterleaving-unrolled-cdot.ll   |  191 ++
 llvm/test/CodeGen/AArch64/parity.ll           |    2 +-
 llvm/test/CodeGen/AArch64/popcount.ll         |  105 +-
 .../AArch64/sme-disable-gisel-fisel.ll        |    9 +-
 .../CodeGen/AArch64/sme-new-zt0-function.ll   |   14 +
 llvm/test/CodeGen/AArch64/sme-zt0-state.ll    |   94 +-
 ...e-streaming-mode-fixed-length-fcopysign.ll |  199 ++
 llvm/test/CodeGen/BPF/CORE/arena_bitcast.ll   |   80 +
 llvm/test/CodeGen/Hexagon/arg-copy-elison.ll  |   23 +-
 .../Hexagon/atomicrmw-cond-sub-clamp.ll       |    4 +-
 .../Hexagon/atomicrmw-uinc-udec-wrap.ll       |   12 +-
 .../test/CodeGen/Hexagon/autohvx/fp-to-int.ll |  406 ++-
 .../test/CodeGen/Hexagon/autohvx/int-to-fp.ll |  120 +-
 llvm/test/CodeGen/Hexagon/bank-conflict.mir   |    2 +-
 .../CodeGen/Hexagon/calloperand-v128i1.ll     |   39 +
 .../test/CodeGen/Hexagon/calloperand-v16i1.ll |   40 +
 llvm/test/CodeGen/Hexagon/calloperand-v2i1.ll |   14 +
 .../test/CodeGen/Hexagon/calloperand-v32i1.ll |   50 +
 llvm/test/CodeGen/Hexagon/calloperand-v4i1.ll |   39 +
 .../test/CodeGen/Hexagon/calloperand-v64i1.ll |   50 +
 llvm/test/CodeGen/Hexagon/calloperand-v8i1.ll |   39 +
 llvm/test/CodeGen/Hexagon/fcmp-nan.ll         |  189 ++
 .../CodeGen/Hexagon/fixed-spill-mutable.ll    |    5 +-
 llvm/test/CodeGen/Hexagon/fp16-promote.ll     |   44 +
 .../CodeGen/Hexagon/hwloop-dist-check.mir     |  277 ++
 llvm/test/CodeGen/Hexagon/isel-memory-vNi1.ll |   50 +-
 .../CodeGen/Hexagon/isel/extract-subvec.ll    |   34 +
 llvm/test/CodeGen/Hexagon/isel/logical.ll     |   52 +-
 llvm/test/CodeGen/Hexagon/isel/pfalse-v4i1.ll |   29 +
 llvm/test/CodeGen/Hexagon/isel/select-i1.ll   |   12 +-
 .../CodeGen/Hexagon/postinc-baseoffset.mir    |    4 +-
 llvm/test/CodeGen/Hexagon/setmemrefs.ll       |    2 +-
 llvm/test/CodeGen/Hexagon/swp-phi-start.ll    |    5 +-
 .../swp-ws-live-intervals-issue128714.mir     |  157 ++
 .../lasx/inline-asm-operand-modifier.ll       |   40 +
 llvm/test/CodeGen/LoongArch/prefetchi.ll      |   33 +
 llvm/test/CodeGen/PowerPC/f128-conv.ll        |  487 ++--
 llvm/test/CodeGen/PowerPC/fp128-libcalls.ll   |   17 +
 .../PowerPC/global-merge-llvm-metadata.ll     |    9 +
 llvm/test/CodeGen/SystemZ/args-12.ll          |    2 +-
 llvm/test/CodeGen/SystemZ/args-13.ll          |    2 +-
 llvm/test/CodeGen/SystemZ/bitop-intrinsics.ll |    2 +-
 llvm/test/CodeGen/SystemZ/int-abs-03.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-add-19.ll       |    2 +-
 llvm/test/CodeGen/SystemZ/int-cmp-64.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-conv-15.ll      |    4 +-
 llvm/test/CodeGen/SystemZ/int-div-08.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-max-02.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-min-02.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-mul-14.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-mul-15.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-mul-16.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-neg-04.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/int-sub-12.ll       |    2 +-
 llvm/test/CodeGen/SystemZ/llxa-01.ll          |    2 +-
 llvm/test/CodeGen/SystemZ/llxa-02.ll          |    2 +-
 llvm/test/CodeGen/SystemZ/llxa-03.ll          |    2 +-
 llvm/test/CodeGen/SystemZ/llxa-04.ll          |    2 +-
 llvm/test/CodeGen/SystemZ/llxa-05.ll          |    2 +-
 llvm/test/CodeGen/SystemZ/lxa-01.ll           |    2 +-
 llvm/test/CodeGen/SystemZ/lxa-02.ll           |    2 +-
 llvm/test/CodeGen/SystemZ/lxa-03.ll           |    2 +-
 llvm/test/CodeGen/SystemZ/lxa-04.ll           |    2 +-
 llvm/test/CodeGen/SystemZ/lxa-05.ll           |    2 +-
 llvm/test/CodeGen/SystemZ/scalar-ctlz-03.ll   |    2 +-
 llvm/test/CodeGen/SystemZ/scalar-ctlz-04.ll   |    2 +-
 llvm/test/CodeGen/SystemZ/scalar-cttz-03.ll   |    2 +-
 llvm/test/CodeGen/SystemZ/scalar-cttz-04.ll   |    4 +-
 llvm/test/CodeGen/SystemZ/vec-cmp-09.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/vec-div-03.ll       |    4 +-
 llvm/test/CodeGen/SystemZ/vec-eval.ll         |   58 +-
 .../test/CodeGen/SystemZ/vec-intrinsics-05.ll |    4 +-
 llvm/test/CodeGen/SystemZ/vec-mul-06.ll       |    4 +-
 .../Thumb2/constant-islands-no-split.mir      |  165 --
 .../CodeGen/X86/avx10_2_512bf16-intrinsics.ll |    4 +-
 .../CodeGen/X86/avx10_2_512ni-intrinsics.ll   |   11 +
 .../CodeGen/X86/avx10_2bf16-intrinsics.ll     |   20 +-
 .../CodeGen/X86/base-pointer-and-cmpxchg.ll   |   34 +
 llvm/test/CodeGen/X86/pr134607.ll             |   20 +
 llvm/test/CodeGen/X86/pr138982.ll             |   23 +
 .../CodeGen/X86/tail-dup-computed-goto.mir    |  366 +++
 llvm/test/CodeGen/X86/vselect-constants.ll    |   18 +
 llvm/test/CodeGen/X86/win32-eh.ll             |    9 +
 llvm/test/MC/AMDGPU/gfx950_asm_features.s     |    7 +-
 .../{insns-arch15.txt => insns-z17.txt}       |    4 +-
 .../MC/Disassembler/X86/avx10.2-bf16-32.txt   |   54 +-
 .../MC/Disassembler/X86/avx10.2-bf16-64.txt   |   54 +-
 .../MC/Disassembler/X86/avx10.2-com-ef-32.txt |   96 +-
 .../MC/Disassembler/X86/avx10.2-com-ef-64.txt |   96 +-
 llvm/test/MC/Hexagon/align-leb128.s           |   18 +
 llvm/test/MC/Hexagon/align.s                  |   13 +
 llvm/test/MC/Hexagon/arch-support.s           |    4 +
 llvm/test/MC/Hexagon/hexagon_attributes.s     |   12 +-
 .../Relocations/relocation-specifier.s        |   26 +
 .../MC/LoongArch/Relocations/relocations.s    |   30 +
 llvm/test/MC/LoongArch/lasx/invalid-imm.s     |   12 +-
 llvm/test/MC/LoongArch/lsx/invalid-imm.s      |   12 +-
 .../{insn-bad-arch15.s => insn-bad-z17.s}     |    4 +-
 .../{insn-good-arch15.s => insn-good-z17.s}   |    4 +-
 llvm/test/MC/X86/avx10.2-bf16-32-att.s        |   54 +-
 llvm/test/MC/X86/avx10.2-bf16-32-intel.s      |   54 +-
 llvm/test/MC/X86/avx10.2-bf16-64-att.s        |   54 +-
 llvm/test/MC/X86/avx10.2-bf16-64-intel.s      |   54 +-
 llvm/test/MC/X86/avx10.2-com-ef-32-att.s      |   96 +-
 llvm/test/MC/X86/avx10.2-com-ef-32-intel.s    |   96 +-
 llvm/test/MC/X86/avx10.2-com-ef-64-att.s      |   96 +-
 llvm/test/MC/X86/avx10.2-com-ef-64-intel.s    |   96 +-
 .../GlobalOpt/malloc-promote-atomic.ll        |   28 +
 .../Transforms/IndVarSimplify/pr135182.ll     |   27 +
 llvm/test/Transforms/InstCombine/and-fcmp.ll  |   28 +
 llvm/test/Transforms/InstCombine/fabs.ll      |   57 +-
 .../InstCombine/shufflevec-bitcast.ll         |   35 +
 llvm/test/Transforms/InstSimplify/fcmp.ll     |   17 +
 llvm/test/Transforms/LoopUnroll/pr131465.ll   |   43 +
 .../AArch64/epilog-iv-select-cmp.ll           |  166 ++
 .../LoopVectorize/X86/cost-model.ll           |  124 +
 .../LoopVectorize/epilog-iv-select-cmp.ll     |  231 +-
 llvm/test/Transforms/MemCpyOpt/fca2memcpy.ll  |   40 +-
 llvm/test/Transforms/MemCpyOpt/memcpy-tbaa.ll |   77 +
 llvm/test/Transforms/MemCpyOpt/stack-move.ll  |   10 +-
 .../X86/load-extractelement-scalarization.ll  |   12 +
 llvm/test/Verifier/sme-attributes.ll          |    3 +
 ...agon_generated_funcs.ll.generated.expected |   25 +-
 ...on_generated_funcs.ll.nogenerated.expected |   25 +-
 .../llvm-dlltool/Inputs/gnu_foo_lib_h.yaml    |  133 +
 .../Inputs/gnu_foo_lib_s00000.yaml            |  116 +
 .../llvm-dlltool/Inputs/gnu_foo_lib_t.yaml    |  119 +
 .../llvm-dlltool/Inputs/llvm_foo_dll_1.yaml   |   69 +
 .../llvm-dlltool/Inputs/llvm_foo_dll_2.yaml   |   18 +
 .../llvm-dlltool/Inputs/llvm_foo_dll_3.yaml   |   23 +
 llvm/test/tools/llvm-dlltool/identify.test    |   69 +
 .../MachO/strip-with-encryption-info.test     |   12 +-
 llvm/tools/opt-viewer/optrecord.py            |    6 +-
 .../Target/AArch64/SMEAttributesTest.cpp      |   30 +
 llvm/unittests/TargetParser/Host.cpp          |    2 +-
 .../TargetParser/RISCVISAInfoTest.cpp         |    8 +
 offload/DeviceRTL/include/Synchronization.h   |    4 +
 429 files changed, 12377 insertions(+), 9146 deletions(-)
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/.clang-tidy
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.cpp
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.h
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/.clang-tidy
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.cpp
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.h
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/.clang-tidy
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.cpp
 create mode 100644 clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.h
 create mode 100644 clang/test/Analysis/loop-based-inlining-prevention.c
 delete mode 100755 clang/test/CodeGen/X86/avx10_2_512satcvt-builtins-error.c
 delete mode 100644 clang/test/CodeGen/X86/avx10_2satcvtds-builtins-errors.c
 create mode 100644 clang/test/CodeGenCXX/cxx23-p2280r4.cpp
 create mode 100644 clang/test/CodeGenCXX/mangle-ms-matrix.cpp
 create mode 100644 clang/test/CodeGenCoroutines/pr134409.cpp
 create mode 100644 clang/test/Driver/hexagon-cpu-default.c
 create mode 100644 clang/test/Modules/MixedModulePrecompile.cpp
 create mode 100644 clang/test/Preprocessor/embed_constexpr.c
 create mode 100644 clang/test/Sema/GH126231.cpp
 create mode 100644 clang/test/Sema/warn-cast-function-type-win.c
 create mode 100644 clang/test/SemaCUDA/dtor.cu
 create mode 100644 libcxx/test/libcxx/vendor/apple/disable-availability.sh.cpp
 create mode 100644 lld/test/COFF/imports-static-lib-indirect.test
 create mode 100644 lld/test/COFF/imports-static-lib.test
 delete mode 100644 lld/test/COFF/undefined_lazy.test
 create mode 100644 lldb/test/API/python_api/target/read-instructions-flavor/Makefile
 create mode 100644 lldb/test/API/python_api/target/read-instructions-flavor/TestTargetReadInstructionsFlavor.py
 create mode 100644 lldb/test/API/python_api/target/read-instructions-flavor/main.c
 create mode 100644 llvm/lib/Target/SystemZ/SystemZScheduleZ17.td
 create mode 100644 llvm/test/Analysis/BasicAA/size-overflow.ll
 create mode 100644 llvm/test/Analysis/LoopAccessAnalysis/underlying-object-different-address-spaces.ll
 create mode 100644 llvm/test/Analysis/ScalarEvolution/pr135531.ll
 create mode 100644 llvm/test/Analysis/ValueTracking/phi-self.ll
 create mode 100644 llvm/test/CodeGen/AArch64/aarch64-swp-ws-live-intervals.mir
 create mode 100644 llvm/test/CodeGen/AArch64/complex-deinterleaving-unrolled-cdot.ll
 create mode 100644 llvm/test/CodeGen/AArch64/sme-new-zt0-function.ll
 create mode 100644 llvm/test/CodeGen/BPF/CORE/arena_bitcast.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/calloperand-v128i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/calloperand-v16i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/calloperand-v2i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/calloperand-v32i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/calloperand-v4i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/calloperand-v64i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/calloperand-v8i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/fcmp-nan.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/fp16-promote.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/hwloop-dist-check.mir
 create mode 100644 llvm/test/CodeGen/Hexagon/isel/extract-subvec.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/isel/pfalse-v4i1.ll
 create mode 100644 llvm/test/CodeGen/Hexagon/swp-ws-live-intervals-issue128714.mir
 create mode 100644 llvm/test/CodeGen/LoongArch/prefetchi.ll
 create mode 100644 llvm/test/CodeGen/PowerPC/global-merge-llvm-metadata.ll
 delete mode 100644 llvm/test/CodeGen/Thumb2/constant-islands-no-split.mir
 create mode 100644 llvm/test/CodeGen/X86/pr134607.ll
 create mode 100644 llvm/test/CodeGen/X86/pr138982.ll
 create mode 100644 llvm/test/CodeGen/X86/tail-dup-computed-goto.mir
 rename llvm/test/MC/Disassembler/SystemZ/{insns-arch15.txt => insns-z17.txt} (99%)
 create mode 100644 llvm/test/MC/Hexagon/align-leb128.s
 create mode 100644 llvm/test/MC/LoongArch/Relocations/relocation-specifier.s
 rename llvm/test/MC/SystemZ/{insn-bad-arch15.s => insn-bad-z17.s} (98%)
 rename llvm/test/MC/SystemZ/{insn-good-arch15.s => insn-good-z17.s} (99%)
 create mode 100644 llvm/test/Transforms/GlobalOpt/malloc-promote-atomic.ll
 create mode 100644 llvm/test/Transforms/IndVarSimplify/pr135182.ll
 create mode 100644 llvm/test/Transforms/LoopUnroll/pr131465.ll
 create mode 100644 llvm/test/Transforms/LoopVectorize/AArch64/epilog-iv-select-cmp.ll
 create mode 100644 llvm/test/Transforms/MemCpyOpt/memcpy-tbaa.ll
 create mode 100644 llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_h.yaml
 create mode 100644 llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_s00000.yaml
 create mode 100644 llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_t.yaml
 create mode 100644 llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_1.yaml
 create mode 100644 llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_2.yaml
 create mode 100644 llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_3.yaml
 create mode 100644 llvm/test/tools/llvm-dlltool/identify.test

diff --git a/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.cpp b/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.cpp
index 4c75b4227011..71e852545203 100644
--- a/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.cpp
+++ b/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.cpp
@@ -311,18 +311,7 @@ ClangTidyDiagnosticConsumer::ClangTidyDiagnosticConsumer(
     : Context(Ctx), ExternalDiagEngine(ExternalDiagEngine),
       RemoveIncompatibleErrors(RemoveIncompatibleErrors),
       GetFixesFromNotes(GetFixesFromNotes),
-      EnableNolintBlocks(EnableNolintBlocks) {
-
-  if (Context.getOptions().HeaderFilterRegex &&
-      !Context.getOptions().HeaderFilterRegex->empty())
-    HeaderFilter =
-        std::make_unique<llvm::Regex>(*Context.getOptions().HeaderFilterRegex);
-
-  if (Context.getOptions().ExcludeHeaderFilterRegex &&
-      !Context.getOptions().ExcludeHeaderFilterRegex->empty())
-    ExcludeHeaderFilter = std::make_unique<llvm::Regex>(
-        *Context.getOptions().ExcludeHeaderFilterRegex);
-}
+      EnableNolintBlocks(EnableNolintBlocks) {}
 
 void ClangTidyDiagnosticConsumer::finalizeLastError() {
   if (!Errors.empty()) {
@@ -571,17 +560,30 @@ void ClangTidyDiagnosticConsumer::checkFilters(SourceLocation Location,
   }
 
   StringRef FileName(File->getName());
-  LastErrorRelatesToUserCode =
-      LastErrorRelatesToUserCode || Sources.isInMainFile(Location) ||
-      (HeaderFilter &&
-       (HeaderFilter->match(FileName) &&
-        !(ExcludeHeaderFilter && ExcludeHeaderFilter->match(FileName))));
+  LastErrorRelatesToUserCode = LastErrorRelatesToUserCode ||
+                               Sources.isInMainFile(Location) ||
+                               (getHeaderFilter()->match(FileName) &&
+                                !getExcludeHeaderFilter()->match(FileName));
 
   unsigned LineNumber = Sources.getExpansionLineNumber(Location);
   LastErrorPassesLineFilter =
       LastErrorPassesLineFilter || passesLineFilter(FileName, LineNumber);
 }
 
+llvm::Regex *ClangTidyDiagnosticConsumer::getHeaderFilter() {
+  if (!HeaderFilter)
+    HeaderFilter =
+        std::make_unique<llvm::Regex>(*Context.getOptions().HeaderFilterRegex);
+  return HeaderFilter.get();
+}
+
+llvm::Regex *ClangTidyDiagnosticConsumer::getExcludeHeaderFilter() {
+  if (!ExcludeHeaderFilter)
+    ExcludeHeaderFilter = std::make_unique<llvm::Regex>(
+        *Context.getOptions().ExcludeHeaderFilterRegex);
+  return ExcludeHeaderFilter.get();
+}
+
 void ClangTidyDiagnosticConsumer::removeIncompatibleErrors() {
   // Each error is modelled as the set of intervals in which it applies
   // replacements. To detect overlapping replacements, we use a sweep line
diff --git a/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.h b/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.h
index ff42f96a0477..d6cf6a2b2731 100644
--- a/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.h
+++ b/clang-tools-extra/clang-tidy/ClangTidyDiagnosticConsumer.h
@@ -302,6 +302,10 @@ private:
   /// context.
   llvm::Regex *getHeaderFilter();
 
+  /// Returns the \c ExcludeHeaderFilter constructed for the options set in the
+  /// context.
+  llvm::Regex *getExcludeHeaderFilter();
+
   /// Updates \c LastErrorRelatesToUserCode and LastErrorPassesLineFilter
   /// according to the diagnostic \p Location.
   void checkFilters(SourceLocation Location, const SourceManager &Sources);
diff --git a/clang-tools-extra/clang-tidy/ClangTidyOptions.cpp b/clang-tools-extra/clang-tidy/ClangTidyOptions.cpp
index 8bac6f161fa0..dd1d86882f5d 100644
--- a/clang-tools-extra/clang-tidy/ClangTidyOptions.cpp
+++ b/clang-tools-extra/clang-tidy/ClangTidyOptions.cpp
@@ -194,8 +194,8 @@ ClangTidyOptions ClangTidyOptions::getDefaults() {
   Options.WarningsAsErrors = "";
   Options.HeaderFileExtensions = {"", "h", "hh", "hpp", "hxx"};
   Options.ImplementationFileExtensions = {"c", "cc", "cpp", "cxx"};
-  Options.HeaderFilterRegex = std::nullopt;
-  Options.ExcludeHeaderFilterRegex = std::nullopt;
+  Options.HeaderFilterRegex = "";
+  Options.ExcludeHeaderFilterRegex = "";
   Options.SystemHeaders = false;
   Options.FormatStyle = "none";
   Options.User = std::nullopt;
diff --git a/clang-tools-extra/clang-tidy/tool/run-clang-tidy.py b/clang-tools-extra/clang-tidy/tool/run-clang-tidy.py
index f1b934f7139e..8741147a4f8a 100755
--- a/clang-tools-extra/clang-tidy/tool/run-clang-tidy.py
+++ b/clang-tools-extra/clang-tidy/tool/run-clang-tidy.py
@@ -87,7 +87,7 @@ def find_compilation_database(path: str) -> str:
 
 
 def get_tidy_invocation(
-    f: str,
+    f: Optional[str],
     clang_tidy_binary: str,
     checks: str,
     tmpdir: Optional[str],
@@ -147,7 +147,8 @@ def get_tidy_invocation(
         start.append(f"--warnings-as-errors={warnings_as_errors}")
     if allow_no_checks:
         start.append("--allow-no-checks")
-    start.append(f)
+    if f:
+        start.append(f)
     return start
 
 
@@ -490,7 +491,7 @@ async def main() -> None:
 
     try:
         invocation = get_tidy_invocation(
-            "",
+            None,
             clang_tidy_binary,
             args.checks,
             None,
diff --git a/clang-tools-extra/docs/ReleaseNotes.rst b/clang-tools-extra/docs/ReleaseNotes.rst
index 316ac1743ccb..0b2e9c5fabc3 100644
--- a/clang-tools-extra/docs/ReleaseNotes.rst
+++ b/clang-tools-extra/docs/ReleaseNotes.rst
@@ -187,6 +187,12 @@ Improvements to clang-tidy
   :doc:`readability-redundant-access-specifiers <clang-tidy/checks/readability/redundant-access-specifiers>`, CheckFirstDeclaration
   :doc:`readability-redundant-casting <clang-tidy/checks/readability/redundant-casting>`, IgnoreTypeAliases
 
+- Fixed bug in :program:`clang-tidy` by which `HeaderFilterRegex` did not take
+  effect when passed via the `.clang-tidy` file.
+
+- Fixed bug in :program:`run_clang_tidy.py` where the program would not
+  correctly display the checks enabled by the top-level `.clang-tidy` file.
+
 New checks
 ^^^^^^^^^^
 
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/.clang-tidy b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/.clang-tidy
new file mode 100644
index 000000000000..f4210353f94d
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/.clang-tidy
@@ -0,0 +1 @@
+HeaderFilterRegex: '.*'
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.cpp b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.cpp
new file mode 100644
index 000000000000..5828c2cafaf7
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.cpp
@@ -0,0 +1,3 @@
+// RUN: clang-tidy -checks=-*,google-explicit-constructor %s 2>&1 | FileCheck %s
+#include "foo.h"
+// CHECK: foo.h:1:12: warning: single-argument constructors must be marked explicit
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.h b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.h
new file mode 100644
index 000000000000..f61d4c2923b5
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/foo.h
@@ -0,0 +1 @@
+struct X { X(int); };
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/.clang-tidy b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/.clang-tidy
new file mode 100644
index 000000000000..96706c142804
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/.clang-tidy
@@ -0,0 +1,2 @@
+InheritParentConfig: true
+HeaderFilterRegex: 'subfolder/.*'
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.cpp b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.cpp
new file mode 100644
index 000000000000..229ba52e2695
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.cpp
@@ -0,0 +1,8 @@
+// shell is required for the "dirname" command
+// REQUIRES: shell
+// RUN: clang-tidy -checks=-*,google-explicit-constructor %s -- -I "$(dirname %S)" 2>&1 | FileCheck %s
+#include "foo.h"
+// CHECK-NOT: foo.h:1:12: warning: single-argument constructors must be marked explicit
+
+#include "bar.h"
+// CHECK: bar.h:1:13: warning: single-argument constructors must be marked explicit
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.h b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.h
new file mode 100644
index 000000000000..ee12d00d334d
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/inheritance/subfolder/bar.h
@@ -0,0 +1 @@
+struct XX { XX(int); };
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/.clang-tidy b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/.clang-tidy
new file mode 100644
index 000000000000..f4210353f94d
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/.clang-tidy
@@ -0,0 +1 @@
+HeaderFilterRegex: '.*'
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.cpp b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.cpp
new file mode 100644
index 000000000000..5828c2cafaf7
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.cpp
@@ -0,0 +1,3 @@
+// RUN: clang-tidy -checks=-*,google-explicit-constructor %s 2>&1 | FileCheck %s
+#include "foo.h"
+// CHECK: foo.h:1:12: warning: single-argument constructors must be marked explicit
diff --git a/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.h b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.h
new file mode 100644
index 000000000000..f61d4c2923b5
--- /dev/null
+++ b/clang-tools-extra/test/clang-tidy/infrastructure/header-filter-from-config-file/simple/foo.h
@@ -0,0 +1 @@
+struct X { X(int); };
diff --git a/clang/docs/HIPSupport.rst b/clang/docs/HIPSupport.rst
index 481ed3923081..8f473c21e191 100644
--- a/clang/docs/HIPSupport.rst
+++ b/clang/docs/HIPSupport.rst
@@ -286,6 +286,26 @@ Example Usage
       basePtr->virtualFunction(); // Allowed since obj is constructed in device code
    }
 
+Host and Device Attributes of Default Destructors
+===================================================
+
+If a default destructor does not have explicit host or device attributes,
+clang infers these attributes based on the destructors of its data members
+and base classes. If any conflicts are detected among these destructors,
+clang diagnoses the issue. Otherwise, clang adds an implicit host or device
+attribute according to whether the data members's and base classes's
+destructors can execute on the host or device side.
+
+For explicit template classes with virtual destructors, which must be emitted,
+the inference adopts a conservative approach. In this case, implicit host or
+device attributes from member and base class destructors are ignored. This
+precaution is necessary because, although a constexpr destructor carries
+implicit host or device attributes, a constexpr function may call a
+non-constexpr function, which is by default a host function.
+
+Users can override the inferred host and device attributes of default
+destructors by adding explicit host and device attributes to them.
+
 C++ Standard Parallelism Offload Support: Compiler And Runtime
 ==============================================================
 
diff --git a/clang/docs/ReleaseNotes.rst b/clang/docs/ReleaseNotes.rst
index 57a567509a06..47ef2f80ac3f 100644
--- a/clang/docs/ReleaseNotes.rst
+++ b/clang/docs/ReleaseNotes.rst
@@ -545,6 +545,11 @@ New Compiler Flags
 - The ``-Warray-compare-cxx26`` warning has been added to warn about array comparison
   starting from C++26, this warning is enabled as an error by default.
 
+- The ``-Wnontrivial-memcall`` warning has been added to warn about
+  passing non-trivially-copyable destination parameter to ``memcpy``,
+  ``memset`` and similar functions for which it is a documented undefined
+  behavior. It is implied by ``-Wnontrivial-memaccess``
+
 - clang-cl and clang-dxc now support ``-fdiagnostics-color=[auto|never|always]``
   in addition to ``-f[no-]color-diagnostics``.
 
@@ -576,11 +581,6 @@ Modified Compiler Flags
   to utilize these vector libraries. The behavior for all other vector function
   libraries remains unchanged.
 
-- The ``-Wnontrivial-memcall`` warning has been added to warn about
-  passing non-trivially-copyable destination parameter to ``memcpy``,
-  ``memset`` and similar functions for which it is a documented undefined
-  behavior. It is implied by ``-Wnontrivial-memaccess``
-
 - Added ``-fmodules-reduced-bmi`` flag corresponding to
   ``-fexperimental-modules-reduced-bmi`` flag. The ``-fmodules-reduced-bmi`` flag
   is intended to be enabled by default in the future.
@@ -694,6 +694,16 @@ Improvements to Clang's diagnostics
   match a template template parameter, in terms of the C++17 relaxed matching rules
   instead of the old ones.
 
+- No longer diagnosing idiomatic function pointer casts on Windows under
+  ``-Wcast-function-type-mismatch`` (which is enabled by ``-Wextra``). Clang
+  would previously warn on this construct, but will no longer do so on Windows:
+
+  .. code-block:: c
+
+    typedef void (WINAPI *PGNSI)(LPSYSTEM_INFO);
+    HMODULE Lib = LoadLibrary("kernel32");
+    PGNSI FnPtr = (PGNSI)GetProcAddress(Lib, "GetNativeSystemInfo");
+
 - Don't emit duplicated dangling diagnostics. (#GH93386).
 
 - Improved diagnostic when trying to befriend a concept. (#GH45182).
@@ -1058,6 +1068,13 @@ Bug Fixes to C++ Support
 - Fixed a substitution bug in transforming CTAD aliases when the type alias contains a non-pack template argument
   corresponding to a pack parameter (#GH124715)
 - Clang is now better at keeping track of friend function template instance contexts. (#GH55509)
+- Fixes matching of nested template template parameters. (#GH130362)
+- Correctly diagnoses template template paramters which have a pack parameter
+  not in the last position.
+- Fixed an integer overflow bug in computing template parameter depths when synthesizing CTAD guides. (#GH128691)
+- Fixed an incorrect pointer access when checking access-control on concepts. (#GH131530)
+- Fixed various alias CTAD bugs involving variadic template arguments. (#GH123591), (#GH127539), (#GH129077),
+  (#GH129620), and (#GH129998).
 
 Bug Fixes to AST Handling
 ^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -1250,6 +1267,8 @@ RISC-V Support
 - The option ``-mcmodel=large`` for the large code model is supported.
 - Bump RVV intrinsic to version 1.0, the spec: https://github.com/riscv-non-isa/rvv-intrinsic-doc/releases/tag/v1.0.0-rc4
 
+- `Zicsr` / `Zifencei` are allowed to be duplicated in the presence of `g` in `-march`.
+
 CUDA/HIP Language Changes
 ^^^^^^^^^^^^^^^^^^^^^^^^^
 - Fixed a bug about overriding a constexpr pure-virtual member function with a non-constexpr virtual member function which causes compilation failure when including standard C++ header `format`.
@@ -1298,6 +1317,11 @@ AVR Support
 
 - Reject C/C++ compilation for avr1 devices which have no SRAM.
 
+BPF Support
+^^^^^^^^^^^
+
+- Make ``-mcpu=v3`` as the default.
+
 DWARF Support in Clang
 ----------------------
 
diff --git a/clang/include/clang/AST/Expr.h b/clang/include/clang/AST/Expr.h
index 7be402264932..06ac0f1704aa 100644
--- a/clang/include/clang/AST/Expr.h
+++ b/clang/include/clang/AST/Expr.h
@@ -1752,7 +1752,14 @@ enum class StringLiteralKind {
   UTF8,
   UTF16,
   UTF32,
-  Unevaluated
+  Unevaluated,
+  // Binary kind of string literal is used for the data coming via #embed
+  // directive. File's binary contents is transformed to a special kind of
+  // string literal that in some cases may be used directly as an initializer
+  // and some features of classic string literals are not applicable to this
+  // kind of a string literal, for example finding a particular byte's source
+  // location for better diagnosing.
+  Binary
 };
 
 /// StringLiteral - This represents a string literal expression, e.g. "foo"
@@ -1884,6 +1891,8 @@ public:
   int64_t getCodeUnitS(size_t I, uint64_t BitWidth) const {
     int64_t V = getCodeUnit(I);
     if (isOrdinary() || isWide()) {
+      // Ordinary and wide string literals have types that can be signed.
+      // It is important for checking C23 constexpr initializers.
       unsigned Width = getCharByteWidth() * BitWidth;
       llvm::APInt AInt(Width, (uint64_t)V);
       V = AInt.getSExtValue();
@@ -4965,9 +4974,9 @@ public:
       assert(EExpr && CurOffset != ULLONG_MAX &&
              "trying to dereference an invalid iterator");
       IntegerLiteral *N = EExpr->FakeChildNode;
-      StringRef DataRef = EExpr->Data->BinaryData->getBytes();
       N->setValue(*EExpr->Ctx,
-                  llvm::APInt(N->getValue().getBitWidth(), DataRef[CurOffset],
+                  llvm::APInt(N->getValue().getBitWidth(),
+                              EExpr->Data->BinaryData->getCodeUnit(CurOffset),
                               N->getType()->isSignedIntegerType()));
       // We want to return a reference to the fake child node in the
       // EmbedExpr, not the local variable N.
diff --git a/clang/include/clang/Driver/Distro.h b/clang/include/clang/Driver/Distro.h
index b4d485dac8a2..c544a8c00219 100644
--- a/clang/include/clang/Driver/Distro.h
+++ b/clang/include/clang/Driver/Distro.h
@@ -39,6 +39,8 @@ public:
     DebianBullseye,
     DebianBookworm,
     DebianTrixie,
+    DebianForky,
+    DebianDuke,
     Exherbo,
     RHEL5,
     RHEL6,
@@ -128,7 +130,7 @@ public:
   bool IsOpenSUSE() const { return DistroVal == OpenSUSE; }
 
   bool IsDebian() const {
-    return DistroVal >= DebianLenny && DistroVal <= DebianTrixie;
+    return DistroVal >= DebianLenny && DistroVal <= DebianDuke;
   }
 
   bool IsUbuntu() const {
diff --git a/clang/include/clang/Interpreter/Interpreter.h b/clang/include/clang/Interpreter/Interpreter.h
index b1b63aedf86a..f8663e3193a1 100644
--- a/clang/include/clang/Interpreter/Interpreter.h
+++ b/clang/include/clang/Interpreter/Interpreter.h
@@ -41,6 +41,7 @@ class CXXRecordDecl;
 class Decl;
 class IncrementalExecutor;
 class IncrementalParser;
+class IncrementalCUDADeviceParser;
 
 /// Create a pre-configured \c CompilerInstance for incremental processing.
 class IncrementalCompilerBuilder {
@@ -93,7 +94,10 @@ class Interpreter {
   std::unique_ptr<IncrementalExecutor> IncrExecutor;
 
   // An optional parser for CUDA offloading
-  std::unique_ptr<IncrementalParser> DeviceParser;
+  std::unique_ptr<IncrementalCUDADeviceParser> DeviceParser;
+
+  // An optional action for CUDA offloading
+  std::unique_ptr<IncrementalAction> DeviceAct;
 
   /// List containing information about each incrementally parsed piece of code.
   std::list<PartialTranslationUnit> PTUs;
@@ -112,6 +116,9 @@ class Interpreter {
   /// Compiler instance performing the incremental compilation.
   std::unique_ptr<CompilerInstance> CI;
 
+  /// An optional compiler instance for CUDA offloading
+  std::unique_ptr<CompilerInstance> DeviceCI;
+
 protected:
   // Derived classes can use an extended interface of the Interpreter.
   Interpreter(std::unique_ptr<CompilerInstance> Instance, llvm::Error &Err,
@@ -175,10 +182,11 @@ private:
   llvm::Expected<Expr *> ExtractValueFromExpr(Expr *E);
   llvm::Expected<llvm::orc::ExecutorAddr> CompileDtorCall(CXXRecordDecl *CXXRD);
 
-  CodeGenerator *getCodeGen() const;
-  std::unique_ptr<llvm::Module> GenModule();
+  CodeGenerator *getCodeGen(IncrementalAction *Action = nullptr) const;
+  std::unique_ptr<llvm::Module> GenModule(IncrementalAction *Action = nullptr);
   PartialTranslationUnit &RegisterPTU(TranslationUnitDecl *TU,
-                                      std::unique_ptr<llvm::Module> M = {});
+                                      std::unique_ptr<llvm::Module> M = {},
+                                      IncrementalAction *Action = nullptr);
 
   // A cache for the compiled destructors used to for de-allocation of managed
   // clang::Values.
diff --git a/clang/include/clang/Sema/Sema.h b/clang/include/clang/Sema/Sema.h
index a30a7076ea5d..d8cc0171c22c 100644
--- a/clang/include/clang/Sema/Sema.h
+++ b/clang/include/clang/Sema/Sema.h
@@ -4336,11 +4336,11 @@ public:
   // Whether the callee should be ignored in CUDA/HIP/OpenMP host/device check.
   bool shouldIgnoreInHostDeviceCheck(FunctionDecl *Callee);
 
-private:
   /// Function or variable declarations to be checked for whether the deferred
   /// diagnostics should be emitted.
   llvm::SmallSetVector<Decl *, 4> DeclsToCheckForDeferredDiags;
 
+private:
   /// Map of current shadowing declarations to shadowed declarations. Warn if
   /// it looks like the user is trying to modify the shadowing declaration.
   llvm::DenseMap<const NamedDecl *, const NamedDecl *> ShadowingDecls;
@@ -10671,9 +10671,8 @@ public:
                            SourceLocation EndLoc);
   void ActOnForEachDeclStmt(DeclGroupPtrTy Decl);
 
-  /// DiagnoseDiscardedExprMarkedNodiscard - Given an expression that is
-  /// semantically a discarded-value expression, diagnose if any [[nodiscard]]
-  /// value has been discarded.
+
+  // Unused, kept in Clang 20 for ABI stability.
   void DiagnoseDiscardedExprMarkedNodiscard(const Expr *E);
 
   /// DiagnoseUnusedExprResult - If the statement passed in is an expression
@@ -11280,14 +11279,16 @@ public:
 
   /// The context in which we are checking a template parameter list.
   enum TemplateParamListContext {
-    TPC_ClassTemplate,
-    TPC_VarTemplate,
+    // For this context, Class, Variable, TypeAlias, and non-pack Template
+    // Template Parameters are treated uniformly.
+    TPC_Other,
+
     TPC_FunctionTemplate,
     TPC_ClassTemplateMember,
     TPC_FriendClassTemplate,
     TPC_FriendFunctionTemplate,
     TPC_FriendFunctionTemplateDefinition,
-    TPC_TypeAliasTemplate
+    TPC_TemplateTemplateParameterPack,
   };
 
   /// Checks the validity of a template parameter list, possibly
diff --git a/clang/include/clang/StaticAnalyser/Core/AnalyserOptions.def b/clang/include/clang/StaticAnalyser/Core/AnalyserOptions.def
index 34bb7a809162..dbb8e832db5f 100644
--- a/clang/include/clang/StaticAnalyser/Core/AnalyserOptions.def
+++ b/clang/include/clang/StaticAnalyser/Core/AnalyserOptions.def
@@ -385,6 +385,19 @@ ANALYZER_OPTION(
     "flex\" won't be analyzed.",
     true)
 
+ANALYZER_OPTION(
+    bool, InlineFunctionsWithAmbiguousLoops, "inline-functions-with-ambiguous-loops",
+    "If disabled (the default), the analyzer puts functions on a \"do not "
+    "inline this\" list if it finds an execution path within that function "
+    "that may potentially perform 'analyzer-max-loop' (= 4 by default) "
+    "iterations in a loop. (Note that functions that _definitely_ reach the "
+    "loop limit on some execution path are currently marked as \"do not "
+    "inline\" even if this option is enabled.) Enabling this option "
+    "eliminates this (somewhat arbitrary) restriction from the analysis "
+    "scope, which increases the analysis runtime (on average by ~10%, but "
+    "a few translation units may see much larger slowdowns).",
+    false)
+
 //===----------------------------------------------------------------------===//
 // Unsigned analyzer options.
 //===----------------------------------------------------------------------===//
diff --git a/clang/include/clang/StaticAnalyser/Core/PathSensitive/FunctionSummary.h b/clang/include/clang/StaticAnalyser/Core/PathSensitive/FunctionSummary.h
index 3ee0d229cfc2..761395260a0c 100644
--- a/clang/include/clang/StaticAnalyser/Core/PathSensitive/FunctionSummary.h
+++ b/clang/include/clang/StaticAnalyser/Core/PathSensitive/FunctionSummary.h
@@ -81,10 +81,6 @@ public:
     I->second.MayInline = 0;
   }
 
-  void markReachedMaxBlockCount(const Decl *D) {
-    markShouldNotInline(D);
-  }
-
   std::optional<bool> mayInline(const Decl *D) {
     MapTy::const_iterator I = Map.find(D);
     if (I != Map.end() && I->second.InlineChecked)
diff --git a/clang/include/clang/Support/Compiler.h b/clang/include/clang/Support/Compiler.h
index 13582b899dc2..5a74f8e3b672 100644
--- a/clang/include/clang/Support/Compiler.h
+++ b/clang/include/clang/Support/Compiler.h
@@ -54,7 +54,7 @@
 #define CLANG_ABI LLVM_ATTRIBUTE_VISIBILITY_DEFAULT
 #define CLANG_TEMPLATE_ABI LLVM_ATTRIBUTE_VISIBILITY_DEFAULT
 #define CLANG_EXPORT_TEMPLATE
-#elif defined(__MACH__) || defined(__WASM__)
+#elif defined(__MACH__) || defined(__WASM__) || defined(__EMSCRIPTEN__)
 #define CLANG_ABI LLVM_ATTRIBUTE_VISIBILITY_DEFAULT
 #define CLANG_TEMPLATE_ABI
 #define CLANG_EXPORT_TEMPLATE
diff --git a/clang/lib/AST/Expr.cpp b/clang/lib/AST/Expr.cpp
index aa7e14329a21..a5b7ef8c4271 100644
--- a/clang/lib/AST/Expr.cpp
+++ b/clang/lib/AST/Expr.cpp
@@ -1104,6 +1104,7 @@ unsigned StringLiteral::mapCharByteWidth(TargetInfo const &Target,
   switch (SK) {
   case StringLiteralKind::Ordinary:
   case StringLiteralKind::UTF8:
+  case StringLiteralKind::Binary:
     CharByteWidth = Target.getCharWidth();
     break;
   case StringLiteralKind::Wide:
@@ -1216,6 +1217,7 @@ void StringLiteral::outputString(raw_ostream &OS) const {
   switch (getKind()) {
   case StringLiteralKind::Unevaluated:
   case StringLiteralKind::Ordinary:
+  case StringLiteralKind::Binary:
     break; // no prefix.
   case StringLiteralKind::Wide:
     OS << 'L';
@@ -1332,6 +1334,11 @@ StringLiteral::getLocationOfByte(unsigned ByteNo, const SourceManager &SM,
                                  const LangOptions &Features,
                                  const TargetInfo &Target, unsigned *StartToken,
                                  unsigned *StartTokenByteOffset) const {
+  // No source location of bytes for binary literals since they don't come from
+  // source.
+  if (getKind() == StringLiteralKind::Binary)
+    return getStrTokenLoc(0);
+
   assert((getKind() == StringLiteralKind::Ordinary ||
           getKind() == StringLiteralKind::UTF8 ||
           getKind() == StringLiteralKind::Unevaluated) &&
@@ -1658,8 +1665,11 @@ SourceLocation CallExpr::getBeginLoc() const {
         Method && Method->isExplicitObjectMemberFunction()) {
       bool HasFirstArg = getNumArgs() > 0 && getArg(0);
       assert(HasFirstArg);
-      if (HasFirstArg)
-        return getArg(0)->getBeginLoc();
+      if (HasFirstArg) {
+        if (auto FirstArgLoc = getArg(0)->getBeginLoc(); FirstArgLoc.isValid()) {
+          return FirstArgLoc;
+        }
+      }
     }
   }
 
diff --git a/clang/lib/AST/ExprConstant.cpp b/clang/lib/AST/ExprConstant.cpp
index 0e41e3dbc8a3..23602362eaa7 100644
--- a/clang/lib/AST/ExprConstant.cpp
+++ b/clang/lib/AST/ExprConstant.cpp
@@ -2419,6 +2419,16 @@ static bool CheckLValueConstantExpression(EvalInfo &Info, SourceLocation Loc,
           LVal.getLValueCallIndex() == 0) &&
          "have call index for global lvalue");
 
+  if (LVal.allowConstexprUnknown()) {
+    if (BaseVD) {
+      Info.FFDiag(Loc, diag::note_constexpr_var_init_non_constant, 1) << BaseVD;
+      NoteLValueLocation(Info, Base);
+    } else {
+      Info.FFDiag(Loc);
+    }
+    return false;
+  }
+
   if (Base.is<DynamicAllocLValue>()) {
     Info.FFDiag(Loc, diag::note_constexpr_dynamic_alloc)
         << IsReferenceType << !Designator.Entries.empty();
@@ -3597,7 +3607,8 @@ static bool evaluateVarDeclInit(EvalInfo &Info, const Expr *E,
   // expressions here; doing so would regress diagnostics for things like
   // reading from a volatile constexpr variable.
   if ((Info.getLangOpts().CPlusPlus && !VD->hasConstantInitialization() &&
-       VD->mightBeUsableInConstantExpressions(Info.Ctx)) ||
+       VD->mightBeUsableInConstantExpressions(Info.Ctx) &&
+       !AllowConstexprUnknown) ||
       ((Info.getLangOpts().CPlusPlus || Info.getLangOpts().OpenCL) &&
        !Info.getLangOpts().CPlusPlus11 && !VD->hasICEInitializer(Info.Ctx))) {
     if (Init) {
@@ -3628,8 +3639,6 @@ static bool evaluateVarDeclInit(EvalInfo &Info, const Expr *E,
   if (AllowConstexprUnknown) {
     if (!Result)
       Result = &Info.CurrentCall->createConstexprUnknownAPValues(VD, Base);
-    else
-      Result->setConstexprUnknown();
   }
   return true;
 }
@@ -12701,11 +12710,13 @@ static bool determineEndOffset(EvalInfo &Info, SourceLocation ExprLoc,
   bool DetermineForCompleteObject = refersToCompleteObject(LVal);
 
   auto CheckedHandleSizeof = [&](QualType Ty, CharUnits &Result) {
-    if (Ty.isNull() || Ty->isIncompleteType() || Ty->isFunctionType())
+    if (Ty.isNull())
       return false;
 
-    if (Ty->isReferenceType())
-      Ty = Ty.getNonReferenceType();
+    Ty = Ty.getNonReferenceType();
+
+    if (Ty->isIncompleteType() || Ty->isFunctionType())
+      return false;
 
     return HandleSizeof(Info, ExprLoc, Ty, Result);
   };
diff --git a/clang/lib/AST/MicrosoftMangle.cpp b/clang/lib/AST/MicrosoftMangle.cpp
index 42b735ccf4a2..cb35dbd61120 100644
--- a/clang/lib/AST/MicrosoftMangle.cpp
+++ b/clang/lib/AST/MicrosoftMangle.cpp
@@ -3552,7 +3552,21 @@ void MicrosoftCXXNameMangler::mangleType(const DependentSizedExtVectorType *T,
 
 void MicrosoftCXXNameMangler::mangleType(const ConstantMatrixType *T,
                                          Qualifiers quals, SourceRange Range) {
-  Error(Range.getBegin(), "matrix type") << Range;
+  QualType EltTy = T->getElementType();
+
+  llvm::SmallString<64> TemplateMangling;
+  llvm::raw_svector_ostream Stream(TemplateMangling);
+  MicrosoftCXXNameMangler Extra(Context, Stream);
+
+  Stream << "?$";
+
+  Extra.mangleSourceName("__matrix");
+  Extra.mangleType(EltTy, Range, QMM_Escape);
+
+  Extra.mangleIntegerLiteral(llvm::APSInt::getUnsigned(T->getNumRows()));
+  Extra.mangleIntegerLiteral(llvm::APSInt::getUnsigned(T->getNumColumns()));
+
+  mangleArtificialTagType(TagTypeKind::Struct, TemplateMangling, {"__clang"});
 }
 
 void MicrosoftCXXNameMangler::mangleType(const DependentSizedMatrixType *T,
diff --git a/clang/lib/AST/NestedNameSpecifier.cpp b/clang/lib/AST/NestedNameSpecifier.cpp
index 76c77569da9f..c043996f1ada 100644
--- a/clang/lib/AST/NestedNameSpecifier.cpp
+++ b/clang/lib/AST/NestedNameSpecifier.cpp
@@ -283,13 +283,16 @@ void NestedNameSpecifier::print(raw_ostream &OS, const PrintingPolicy &Policy,
   case TypeSpec: {
     const auto *Record =
             dyn_cast_or_null<ClassTemplateSpecializationDecl>(getAsRecordDecl());
-    if (ResolveTemplateArguments && Record) {
+    const TemplateParameterList *TPL = nullptr;
+    if (Record) {
+      TPL = Record->getSpecializedTemplate()->getTemplateParameters();
+      if (ResolveTemplateArguments) {
         // Print the type trait with resolved template parameters.
         Record->printName(OS, Policy);
-        printTemplateArgumentList(
-            OS, Record->getTemplateArgs().asArray(), Policy,
-            Record->getSpecializedTemplate()->getTemplateParameters());
+        printTemplateArgumentList(OS, Record->getTemplateArgs().asArray(),
+                                  Policy, TPL);
         break;
+      }
     }
     const Type *T = getAsType();
 
@@ -313,8 +316,8 @@ void NestedNameSpecifier::print(raw_ostream &OS, const PrintingPolicy &Policy,
                                         TemplateName::Qualified::None);
 
       // Print the template argument list.
-      printTemplateArgumentList(OS, SpecType->template_arguments(),
-                                InnerPolicy);
+      printTemplateArgumentList(OS, SpecType->template_arguments(), InnerPolicy,
+                                TPL);
     } else if (const auto *DepSpecType =
                    dyn_cast<DependentTemplateSpecializationType>(T)) {
       // Print the template name without its corresponding
@@ -322,7 +325,7 @@ void NestedNameSpecifier::print(raw_ostream &OS, const PrintingPolicy &Policy,
       OS << DepSpecType->getIdentifier()->getName();
       // Print the template argument list.
       printTemplateArgumentList(OS, DepSpecType->template_arguments(),
-                                InnerPolicy);
+                                InnerPolicy, TPL);
     } else {
       // Print the type normally
       QualType(T, 0).print(OS, InnerPolicy);
diff --git a/clang/lib/AST/ParentMapContext.cpp b/clang/lib/AST/ParentMapContext.cpp
index 7ff492443031..d8dd352c42d6 100644
--- a/clang/lib/AST/ParentMapContext.cpp
+++ b/clang/lib/AST/ParentMapContext.cpp
@@ -12,10 +12,11 @@
 //===----------------------------------------------------------------------===//
 
 #include "clang/AST/ParentMapContext.h"
-#include "clang/AST/RecursiveASTVisitor.h"
 #include "clang/AST/Decl.h"
 #include "clang/AST/Expr.h"
+#include "clang/AST/RecursiveASTVisitor.h"
 #include "clang/AST/TemplateBase.h"
+#include "llvm/ADT/SmallPtrSet.h"
 
 using namespace clang;
 
@@ -69,17 +70,21 @@ class ParentMapContext::ParentMap {
       for (; N > 0; --N)
         push_back(Value);
     }
-    bool contains(const DynTypedNode &Value) {
-      return Seen.contains(Value);
+    bool contains(const DynTypedNode &Value) const {
+      const void *Identity = Value.getMemoizationData();
+      assert(Identity);
+      return Dedup.contains(Identity);
     }
     void push_back(const DynTypedNode &Value) {
-      if (!Value.getMemoizationData() || Seen.insert(Value).second)
+      const void *Identity = Value.getMemoizationData();
+      if (!Identity || Dedup.insert(Identity).second) {
         Items.push_back(Value);
+      }
     }
     llvm::ArrayRef<DynTypedNode> view() const { return Items; }
   private:
-    llvm::SmallVector<DynTypedNode, 2> Items;
-    llvm::SmallDenseSet<DynTypedNode, 2> Seen;
+    llvm::SmallVector<DynTypedNode, 1> Items;
+    llvm::SmallPtrSet<const void *, 2> Dedup;
   };
 
   /// Maps from a node to its parents. This is used for nodes that have
diff --git a/clang/lib/Analysis/LiveVariables.cpp b/clang/lib/Analysis/LiveVariables.cpp
index 481932ee59c8..5fb5ee767a68 100644
--- a/clang/lib/Analysis/LiveVariables.cpp
+++ b/clang/lib/Analysis/LiveVariables.cpp
@@ -662,12 +662,19 @@ void LiveVariables::dumpExprLiveness(const SourceManager &M) {
 }
 
 void LiveVariablesImpl::dumpExprLiveness(const SourceManager &M) {
+  const ASTContext &Ctx = analysisContext.getASTContext();
+  auto ByIDs = [&Ctx](const Expr *L, const Expr *R) {
+    return L->getID(Ctx) < R->getID(Ctx);
+  };
+
   // Don't iterate over blockEndsToLiveness directly because it's not sorted.
   for (const CFGBlock *B : *analysisContext.getCFG()) {
-
     llvm::errs() << "\n[ B" << B->getBlockID()
                  << " (live expressions at block exit) ]\n";
-    for (const Expr *E : blocksEndToLiveness[B].liveExprs) {
+    std::vector<const Expr *> LiveExprs;
+    llvm::append_range(LiveExprs, blocksEndToLiveness[B].liveExprs);
+    llvm::sort(LiveExprs, ByIDs);
+    for (const Expr *E : LiveExprs) {
       llvm::errs() << "\n";
       E->dump();
     }
diff --git a/clang/lib/Basic/Targets/SystemZ.cpp b/clang/lib/Basic/Targets/SystemZ.cpp
index c836d110d26d..6326188b3bd1 100644
--- a/clang/lib/Basic/Targets/SystemZ.cpp
+++ b/clang/lib/Basic/Targets/SystemZ.cpp
@@ -105,7 +105,7 @@ static constexpr ISANameRevision ISARevisions[] = {
   {{"arch12"}, 12}, {{"z14"}, 12},
   {{"arch13"}, 13}, {{"z15"}, 13},
   {{"arch14"}, 14}, {{"z16"}, 14},
-  {{"arch15"}, 15},
+  {{"arch15"}, 15}, {{"z17"}, 15},
 };
 
 int SystemZTargetInfo::getISARevision(StringRef Name) const {
diff --git a/clang/lib/CodeGen/CGExprConstant.cpp b/clang/lib/CodeGen/CGExprConstant.cpp
index 655fc3dc954c..9abbe4b801d5 100644
--- a/clang/lib/CodeGen/CGExprConstant.cpp
+++ b/clang/lib/CodeGen/CGExprConstant.cpp
@@ -1881,8 +1881,11 @@ llvm::Constant *ConstantEmitter::tryEmitPrivateForVarInit(const VarDecl &D) {
 
   // Try to emit the initializer.  Note that this can allow some things that
   // are not allowed by tryEmitPrivateForMemory alone.
-  if (APValue *value = D.evaluateValue())
+  if (APValue *value = D.evaluateValue()) {
+    assert(!value->allowConstexprUnknown() &&
+           "Constexpr unknown values are not allowed in CodeGen");
     return tryEmitPrivateForMemory(*value, destType);
+  }
 
   return nullptr;
 }
diff --git a/clang/lib/Driver/Distro.cpp b/clang/lib/Driver/Distro.cpp
index 3cc79535de8d..71ba71fa1837 100644
--- a/clang/lib/Driver/Distro.cpp
+++ b/clang/lib/Driver/Distro.cpp
@@ -160,6 +160,10 @@ static Distro::DistroType DetectDistro(llvm::vfs::FileSystem &VFS) {
         return Distro::DebianBookworm;
       case 13:
         return Distro::DebianTrixie;
+      case 14:
+        return Distro::DebianForky;
+      case 15:
+        return Distro::DebianDuke;
       default:
         return Distro::UnknownDistro;
       }
@@ -173,6 +177,8 @@ static Distro::DistroType DetectDistro(llvm::vfs::FileSystem &VFS) {
         .Case("bullseye/sid", Distro::DebianBullseye)
         .Case("bookworm/sid", Distro::DebianBookworm)
         .Case("trixie/sid", Distro::DebianTrixie)
+        .Case("forky/sid", Distro::DebianForky)
+        .Case("duke/sid", Distro::DebianDuke)
         .Default(Distro::UnknownDistro);
   }
 
diff --git a/clang/lib/Driver/ToolChains/Arch/ARM.cpp b/clang/lib/Driver/ToolChains/Arch/ARM.cpp
index 3aee540d501b..ef2d0c93b5b0 100644
--- a/clang/lib/Driver/ToolChains/Arch/ARM.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/ARM.cpp
@@ -658,21 +658,13 @@ llvm::ARM::FPUKind arm::getARMTargetFeatures(const Driver &D,
         CPUArgFPUKind != llvm::ARM::FK_INVALID ? CPUArgFPUKind : ArchArgFPUKind;
     (void)llvm::ARM::getFPUFeatures(FPUKind, Features);
   } else {
-    bool Generic = true;
     if (!ForAS) {
       std::string CPU = arm::getARMTargetCPU(CPUName, ArchName, Triple);
-      if (CPU != "generic")
-        Generic = false;
       llvm::ARM::ArchKind ArchKind =
           arm::getLLVMArchKindForARM(CPU, ArchName, Triple);
       FPUKind = llvm::ARM::getDefaultFPU(CPU, ArchKind);
       (void)llvm::ARM::getFPUFeatures(FPUKind, Features);
     }
-    if (Generic && (Triple.isOSWindows() || Triple.isOSDarwin()) &&
-        getARMSubArchVersionNumber(Triple) >= 7) {
-      FPUKind = llvm::ARM::parseFPU("neon");
-      (void)llvm::ARM::getFPUFeatures(FPUKind, Features);
-    }
   }
 
   // Now we've finished accumulating features from arch, cpu and fpu,
diff --git a/clang/lib/Driver/ToolChains/Hexagon.cpp b/clang/lib/Driver/ToolChains/Hexagon.cpp
index 76cedf312d68..6ea701a7882d 100644
--- a/clang/lib/Driver/ToolChains/Hexagon.cpp
+++ b/clang/lib/Driver/ToolChains/Hexagon.cpp
@@ -313,6 +313,7 @@ constructHexagonLinkArgs(Compilation &C, const JobAction &JA,
                                      // handled somewhere else.
   Args.ClaimAllArgs(options::OPT_static_libgcc);
 
+  CmdArgs.push_back("--eh-frame-hdr");
   //----------------------------------------------------------------------------
   //
   //----------------------------------------------------------------------------
@@ -802,9 +803,7 @@ bool HexagonToolChain::isAutoHVXEnabled(const llvm::opt::ArgList &Args) {
 // Returns the default CPU for Hexagon. This is the default compilation target
 // if no Hexagon processor is selected at the command-line.
 //
-StringRef HexagonToolChain::GetDefaultCPU() {
-  return "hexagonv60";
-}
+StringRef HexagonToolChain::GetDefaultCPU() { return "hexagonv68"; }
 
 StringRef HexagonToolChain::GetTargetCPUVersion(const ArgList &Args) {
   Arg *CpuArg = nullptr;
diff --git a/clang/lib/Format/ContinuationIndenter.cpp b/clang/lib/Format/ContinuationIndenter.cpp
index c311deaa17bb..6f7d213c0b55 100644
--- a/clang/lib/Format/ContinuationIndenter.cpp
+++ b/clang/lib/Format/ContinuationIndenter.cpp
@@ -349,6 +349,13 @@ bool ContinuationIndenter::canBreak(const LineState &State) {
     }
   }
 
+  // Allow breaking before the right parens with block indentation if there was
+  // a break after the left parens, which is tracked by BreakBeforeClosingParen.
+  if (Style.AlignAfterOpenBracket == FormatStyle::BAS_BlockIndent &&
+      Current.is(tok::r_paren)) {
+    return CurrentState.BreakBeforeClosingParen;
+  }
+
   // Don't allow breaking before a closing brace of a block-indented braced list
   // initializer if there isn't already a break.
   if (Current.is(tok::r_brace) && Current.MatchingParen &&
diff --git a/clang/lib/Format/Format.cpp b/clang/lib/Format/Format.cpp
index 0bb854588444..b97d8928178b 100644
--- a/clang/lib/Format/Format.cpp
+++ b/clang/lib/Format/Format.cpp
@@ -2114,10 +2114,14 @@ std::error_code parseConfiguration(llvm::MemoryBufferRef Config,
   FormatStyle::FormatStyleSet StyleSet;
   bool LanguageFound = false;
   for (const FormatStyle &Style : llvm::reverse(Styles)) {
-    if (Style.Language != FormatStyle::LK_None)
+    const auto Lang = Style.Language;
+    if (Lang != FormatStyle::LK_None)
       StyleSet.Add(Style);
-    if (Style.Language == Language)
+    if (Lang == Language ||
+        // For backward compatibility.
+        (Lang == FormatStyle::LK_Cpp && Language == FormatStyle::LK_C)) {
       LanguageFound = true;
+    }
   }
   if (!LanguageFound) {
     if (Styles.empty() || Styles[0].Language != FormatStyle::LK_None)
@@ -2157,8 +2161,14 @@ FormatStyle::FormatStyleSet::Get(FormatStyle::LanguageKind Language) const {
   if (!Styles)
     return std::nullopt;
   auto It = Styles->find(Language);
-  if (It == Styles->end())
-    return std::nullopt;
+  if (It == Styles->end()) {
+    if (Language != FormatStyle::LK_C)
+      return std::nullopt;
+    // For backward compatibility.
+    It = Styles->find(FormatStyle::LK_Cpp);
+    if (It == Styles->end())
+      return std::nullopt;
+  }
   FormatStyle Style = It->second;
   Style.StyleSet = *this;
   return Style;
@@ -3936,34 +3946,42 @@ tooling::Replacements sortUsingDeclarations(const FormatStyle &Style,
 LangOptions getFormattingLangOpts(const FormatStyle &Style) {
   LangOptions LangOpts;
 
-  FormatStyle::LanguageStandard LexingStd = Style.Standard;
-  if (LexingStd == FormatStyle::LS_Auto)
-    LexingStd = FormatStyle::LS_Latest;
-  if (LexingStd == FormatStyle::LS_Latest)
+  auto LexingStd = Style.Standard;
+  if (LexingStd == FormatStyle::LS_Auto || LexingStd == FormatStyle::LS_Latest)
     LexingStd = FormatStyle::LS_Cpp20;
-  LangOpts.CPlusPlus = 1;
-  LangOpts.CPlusPlus11 = LexingStd >= FormatStyle::LS_Cpp11;
-  LangOpts.CPlusPlus14 = LexingStd >= FormatStyle::LS_Cpp14;
-  LangOpts.CPlusPlus17 = LexingStd >= FormatStyle::LS_Cpp17;
-  LangOpts.CPlusPlus20 = LexingStd >= FormatStyle::LS_Cpp20;
-  LangOpts.Char8 = LexingStd >= FormatStyle::LS_Cpp20;
+
+  const bool SinceCpp11 = LexingStd >= FormatStyle::LS_Cpp11;
+  const bool SinceCpp20 = LexingStd >= FormatStyle::LS_Cpp20;
+
+  switch (Style.Language) {
+  case FormatStyle::LK_C:
+    LangOpts.C11 = 1;
+    break;
+  case FormatStyle::LK_Cpp:
+  case FormatStyle::LK_ObjC:
+    LangOpts.CXXOperatorNames = 1;
+    LangOpts.CPlusPlus11 = SinceCpp11;
+    LangOpts.CPlusPlus14 = LexingStd >= FormatStyle::LS_Cpp14;
+    LangOpts.CPlusPlus17 = LexingStd >= FormatStyle::LS_Cpp17;
+    LangOpts.CPlusPlus20 = SinceCpp20;
+    [[fallthrough]];
+  default:
+    LangOpts.CPlusPlus = 1;
+  }
+
+  LangOpts.Char8 = SinceCpp20;
   // Turning on digraphs in standards before C++0x is error-prone, because e.g.
   // the sequence "<::" will be unconditionally treated as "[:".
   // Cf. Lexer::LexTokenInternal.
-  LangOpts.Digraphs = LexingStd >= FormatStyle::LS_Cpp11;
+  LangOpts.Digraphs = SinceCpp11;
 
   LangOpts.LineComment = 1;
-
-  const auto Language = Style.Language;
-  LangOpts.C17 = Language == FormatStyle::LK_C;
-  LangOpts.CXXOperatorNames =
-      Language == FormatStyle::LK_Cpp || Language == FormatStyle::LK_ObjC;
-
   LangOpts.Bool = 1;
   LangOpts.ObjC = 1;
   LangOpts.MicrosoftExt = 1;    // To get kw___try, kw___finally.
   LangOpts.DeclSpecKeyword = 1; // To get __declspec.
   LangOpts.C99 = 1; // To get kw_restrict for non-underscore-prefixed restrict.
+
   return LangOpts;
 }
 
diff --git a/clang/lib/Format/FormatToken.cpp b/clang/lib/Format/FormatToken.cpp
index 60e428123d26..a4e2acc922c0 100644
--- a/clang/lib/Format/FormatToken.cpp
+++ b/clang/lib/Format/FormatToken.cpp
@@ -44,7 +44,7 @@ static SmallVector<StringRef> CppNonKeywordTypes = {
 bool FormatToken::isTypeName(const LangOptions &LangOpts) const {
   if (is(TT_TypeName) || Tok.isSimpleTypeSpecifier(LangOpts))
     return true;
-  return (LangOpts.CXXOperatorNames || LangOpts.C17) && is(tok::identifier) &&
+  return (LangOpts.CXXOperatorNames || LangOpts.C11) && is(tok::identifier) &&
          std::binary_search(CppNonKeywordTypes.begin(),
                             CppNonKeywordTypes.end(), TokenText);
 }
diff --git a/clang/lib/Format/TokenAnnotator.cpp b/clang/lib/Format/TokenAnnotator.cpp
index 976c4d888e1f..11b941c5a041 100644
--- a/clang/lib/Format/TokenAnnotator.cpp
+++ b/clang/lib/Format/TokenAnnotator.cpp
@@ -129,7 +129,6 @@ public:
       : Style(Style), Line(Line), CurrentToken(Line.First), AutoFound(false),
         IsCpp(Style.isCpp()), LangOpts(getFormattingLangOpts(Style)),
         Keywords(Keywords), Scopes(Scopes), TemplateDeclarationDepth(0) {
-    assert(IsCpp == (LangOpts.CXXOperatorNames || LangOpts.C17));
     Contexts.push_back(Context(tok::unknown, 1, /*IsExpression=*/false));
     resetTokenMetadata();
   }
@@ -3820,7 +3819,7 @@ static bool isFunctionDeclarationName(const LangOptions &LangOpts,
   };
 
   const auto *Next = Current.Next;
-  const bool IsCpp = LangOpts.CXXOperatorNames || LangOpts.C17;
+  const bool IsCpp = LangOpts.CXXOperatorNames || LangOpts.C11;
 
   // Find parentheses of parameter list.
   if (Current.is(tok::kw_operator)) {
@@ -3962,8 +3961,10 @@ void TokenAnnotator::calculateFormattingInformation(AnnotatedLine &Line) const {
   FormatToken *AfterLastAttribute = nullptr;
   FormatToken *ClosingParen = nullptr;
 
-  for (auto *Tok = FirstNonComment ? FirstNonComment->Next : nullptr; Tok;
-       Tok = Tok->Next) {
+  for (auto *Tok = FirstNonComment && FirstNonComment->isNot(tok::kw_using)
+                       ? FirstNonComment->Next
+                       : nullptr;
+       Tok; Tok = Tok->Next) {
     if (Tok->is(TT_StartOfName))
       SeenName = true;
     if (Tok->Previous->EndsCppAttributeGroup)
@@ -5437,7 +5438,12 @@ bool TokenAnnotator::spaceRequiredBefore(const AnnotatedLine &Line,
     // handled.
     if (Left.is(tok::amp) && Right.is(tok::r_square))
       return Style.SpacesInSquareBrackets;
-    return Style.SpaceAfterLogicalNot && Left.is(tok::exclaim);
+    if (Left.isNot(tok::exclaim))
+      return false;
+    if (Left.TokenText == "!")
+      return Style.SpaceAfterLogicalNot;
+    assert(Left.TokenText == "not");
+    return Right.isOneOf(tok::coloncolon, TT_UnaryOperator);
   }
 
   // If the next token is a binary operator or a selector name, we have
diff --git a/clang/lib/Format/TokenAnnotator.h b/clang/lib/Format/TokenAnnotator.h
index c0c13941ef4f..e4b94431e68b 100644
--- a/clang/lib/Format/TokenAnnotator.h
+++ b/clang/lib/Format/TokenAnnotator.h
@@ -224,9 +224,7 @@ class TokenAnnotator {
 public:
   TokenAnnotator(const FormatStyle &Style, const AdditionalKeywords &Keywords)
       : Style(Style), IsCpp(Style.isCpp()),
-        LangOpts(getFormattingLangOpts(Style)), Keywords(Keywords) {
-    assert(IsCpp == (LangOpts.CXXOperatorNames || LangOpts.C17));
-  }
+        LangOpts(getFormattingLangOpts(Style)), Keywords(Keywords) {}
 
   /// Adapts the indent levels of comment lines to the indent of the
   /// subsequent line.
diff --git a/clang/lib/Format/UnwrappedLineParser.cpp b/clang/lib/Format/UnwrappedLineParser.cpp
index 9b4257fdd8c8..673b3e6c4b8c 100644
--- a/clang/lib/Format/UnwrappedLineParser.cpp
+++ b/clang/lib/Format/UnwrappedLineParser.cpp
@@ -167,9 +167,7 @@ UnwrappedLineParser::UnwrappedLineParser(
                        ? IG_Rejected
                        : IG_Inited),
       IncludeGuardToken(nullptr), FirstStartColumn(FirstStartColumn),
-      Macros(Style.Macros, SourceMgr, Style, Allocator, IdentTable) {
-  assert(IsCpp == (LangOpts.CXXOperatorNames || LangOpts.C17));
-}
+      Macros(Style.Macros, SourceMgr, Style, Allocator, IdentTable) {}
 
 void UnwrappedLineParser::reset() {
   PPBranchLevel = -1;
@@ -1839,8 +1837,8 @@ void UnwrappedLineParser::parseStructuralElement(
       nextToken();
       if (FormatTok->is(tok::l_paren)) {
         parseParens();
-        assert(FormatTok->Previous);
-        if (FormatTok->Previous->endsSequence(tok::r_paren, tok::kw_auto,
+        if (FormatTok->Previous &&
+            FormatTok->Previous->endsSequence(tok::r_paren, tok::kw_auto,
                                               tok::l_paren)) {
           Line->SeenDecltypeAuto = true;
         }
@@ -2562,12 +2560,12 @@ bool UnwrappedLineParser::parseBracedList(bool IsAngleBracket, bool IsEnum) {
 /// Returns whether there is a `=` token between the parentheses.
 bool UnwrappedLineParser::parseParens(TokenType AmpAmpTokenType) {
   assert(FormatTok->is(tok::l_paren) && "'(' expected.");
-  auto *LeftParen = FormatTok;
+  auto *LParen = FormatTok;
   bool SeenComma = false;
   bool SeenEqual = false;
   bool MightBeFoldExpr = false;
-  const bool MightBeStmtExpr = Tokens->peekNextToken()->is(tok::l_brace);
   nextToken();
+  const bool MightBeStmtExpr = FormatTok->is(tok::l_brace);
   do {
     switch (FormatTok->Tok.getKind()) {
     case tok::l_paren:
@@ -2577,44 +2575,61 @@ bool UnwrappedLineParser::parseParens(TokenType AmpAmpTokenType) {
         parseChildBlock();
       break;
     case tok::r_paren: {
-      auto *Prev = LeftParen->Previous;
-      if (!MightBeStmtExpr && !MightBeFoldExpr && !Line->InMacroBody &&
-          Style.RemoveParentheses > FormatStyle::RPS_Leave) {
-        const auto *Next = Tokens->peekNextToken();
-        const bool DoubleParens =
-            Prev && Prev->is(tok::l_paren) && Next && Next->is(tok::r_paren);
-        const bool CommaSeparated =
-            !DoubleParens && Prev && Prev->isOneOf(tok::l_paren, tok::comma) &&
-            Next && Next->isOneOf(tok::comma, tok::r_paren);
-        const auto *PrevPrev = Prev ? Prev->getPreviousNonComment() : nullptr;
-        const bool Excluded =
-            PrevPrev &&
-            (PrevPrev->isOneOf(tok::kw___attribute, tok::kw_decltype) ||
-             SeenComma ||
-             (SeenEqual &&
-              (PrevPrev->isOneOf(tok::kw_if, tok::kw_while) ||
-               PrevPrev->endsSequence(tok::kw_constexpr, tok::kw_if))));
-        const bool ReturnParens =
-            Style.RemoveParentheses == FormatStyle::RPS_ReturnStatement &&
-            ((NestedLambdas.empty() && !IsDecltypeAutoFunction) ||
-             (!NestedLambdas.empty() && !NestedLambdas.back())) &&
-            Prev && Prev->isOneOf(tok::kw_return, tok::kw_co_return) && Next &&
-            Next->is(tok::semi);
-        if ((DoubleParens && !Excluded) || (CommaSeparated && !SeenComma) ||
-            ReturnParens) {
-          LeftParen->Optional = true;
-          FormatTok->Optional = true;
-        }
-      }
+      auto *Prev = LParen->Previous;
+      auto *RParen = FormatTok;
+      nextToken();
       if (Prev) {
+        auto OptionalParens = [&] {
+          if (MightBeStmtExpr || MightBeFoldExpr || Line->InMacroBody ||
+              SeenComma || Style.RemoveParentheses == FormatStyle::RPS_Leave ||
+              RParen->getPreviousNonComment() == LParen) {
+            return false;
+          }
+          const bool DoubleParens =
+              Prev->is(tok::l_paren) && FormatTok->is(tok::r_paren);
+          if (DoubleParens) {
+            const auto *PrevPrev = Prev->getPreviousNonComment();
+            const bool Excluded =
+                PrevPrev &&
+                (PrevPrev->isOneOf(tok::kw___attribute, tok::kw_decltype) ||
+                 (SeenEqual &&
+                  (PrevPrev->isOneOf(tok::kw_if, tok::kw_while) ||
+                   PrevPrev->endsSequence(tok::kw_constexpr, tok::kw_if))));
+            if (!Excluded)
+              return true;
+          } else {
+            const bool CommaSeparated =
+                Prev->isOneOf(tok::l_paren, tok::comma) &&
+                FormatTok->isOneOf(tok::comma, tok::r_paren);
+            if (CommaSeparated &&
+                // LParen is not preceded by ellipsis, comma.
+                !Prev->endsSequence(tok::comma, tok::ellipsis) &&
+                // RParen is not followed by comma, ellipsis.
+                !(FormatTok->is(tok::comma) &&
+                  Tokens->peekNextToken()->is(tok::ellipsis))) {
+              return true;
+            }
+            const bool ReturnParens =
+                Style.RemoveParentheses == FormatStyle::RPS_ReturnStatement &&
+                ((NestedLambdas.empty() && !IsDecltypeAutoFunction) ||
+                 (!NestedLambdas.empty() && !NestedLambdas.back())) &&
+                Prev->isOneOf(tok::kw_return, tok::kw_co_return) &&
+                FormatTok->is(tok::semi);
+            if (ReturnParens)
+              return true;
+          }
+          return false;
+        };
         if (Prev->is(TT_TypenameMacro)) {
-          LeftParen->setFinalizedType(TT_TypeDeclarationParen);
-          FormatTok->setFinalizedType(TT_TypeDeclarationParen);
-        } else if (Prev->is(tok::greater) && FormatTok->Previous == LeftParen) {
+          LParen->setFinalizedType(TT_TypeDeclarationParen);
+          RParen->setFinalizedType(TT_TypeDeclarationParen);
+        } else if (Prev->is(tok::greater) && RParen->Previous == LParen) {
           Prev->setFinalizedType(TT_TemplateCloser);
+        } else if (OptionalParens()) {
+          LParen->Optional = true;
+          RParen->Optional = true;
         }
       }
-      nextToken();
       return SeenEqual;
     }
     case tok::r_brace:
diff --git a/clang/lib/Headers/__clang_cuda_intrinsics.h b/clang/lib/Headers/__clang_cuda_intrinsics.h
index a04e8b6de44d..8b230af6f664 100644
--- a/clang/lib/Headers/__clang_cuda_intrinsics.h
+++ b/clang/lib/Headers/__clang_cuda_intrinsics.h
@@ -515,32 +515,32 @@ __device__ inline cuuint32_t __nvvm_get_smem_pointer(void *__ptr) {
 #if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 800
 __device__ inline unsigned __reduce_add_sync(unsigned __mask,
                                              unsigned __value) {
-  return __nvvm_redux_sync_add(__mask, __value);
+  return __nvvm_redux_sync_add(__value, __mask);
 }
 __device__ inline unsigned __reduce_min_sync(unsigned __mask,
                                              unsigned __value) {
-  return __nvvm_redux_sync_umin(__mask, __value);
+  return __nvvm_redux_sync_umin(__value, __mask);
 }
 __device__ inline unsigned __reduce_max_sync(unsigned __mask,
                                              unsigned __value) {
-  return __nvvm_redux_sync_umax(__mask, __value);
+  return __nvvm_redux_sync_umax(__value, __mask);
 }
 __device__ inline int __reduce_min_sync(unsigned __mask, int __value) {
-  return __nvvm_redux_sync_min(__mask, __value);
+  return __nvvm_redux_sync_min(__value, __mask);
 }
 __device__ inline int __reduce_max_sync(unsigned __mask, int __value) {
-  return __nvvm_redux_sync_max(__mask, __value);
+  return __nvvm_redux_sync_max(__value, __mask);
 }
 __device__ inline unsigned __reduce_or_sync(unsigned __mask, unsigned __value) {
-  return __nvvm_redux_sync_or(__mask, __value);
+  return __nvvm_redux_sync_or(__value, __mask);
 }
 __device__ inline unsigned __reduce_and_sync(unsigned __mask,
                                              unsigned __value) {
-  return __nvvm_redux_sync_and(__mask, __value);
+  return __nvvm_redux_sync_and(__value, __mask);
 }
 __device__ inline unsigned __reduce_xor_sync(unsigned __mask,
                                              unsigned __value) {
-  return __nvvm_redux_sync_xor(__mask, __value);
+  return __nvvm_redux_sync_xor(__value, __mask);
 }
 
 __device__ inline void __nv_memcpy_async_shared_global_4(void *__dst,
diff --git a/clang/lib/Headers/amdgpuintrin.h b/clang/lib/Headers/amdgpuintrin.h
index 9dad99ffe943..d12c7e244c2b 100644
--- a/clang/lib/Headers/amdgpuintrin.h
+++ b/clang/lib/Headers/amdgpuintrin.h
@@ -121,7 +121,7 @@ __gpu_read_first_lane_u64(uint64_t __lane_mask, uint64_t __x) {
   uint32_t __hi = (uint32_t)(__x >> 32ull);
   uint32_t __lo = (uint32_t)(__x & 0xFFFFFFFF);
   return ((uint64_t)__builtin_amdgcn_readfirstlane(__hi) << 32ull) |
-         ((uint64_t)__builtin_amdgcn_readfirstlane(__lo));
+         ((uint64_t)__builtin_amdgcn_readfirstlane(__lo) & 0xFFFFFFFF);
 }
 
 // Returns a bitmask of threads in the current lane for which \p x is true.
diff --git a/clang/lib/Headers/avx10_2_512convertintrin.h b/clang/lib/Headers/avx10_2_512convertintrin.h
index 516ccc68672d..ee8cbf28ca41 100644
--- a/clang/lib/Headers/avx10_2_512convertintrin.h
+++ b/clang/lib/Headers/avx10_2_512convertintrin.h
@@ -78,20 +78,20 @@ _mm512_maskz_cvtbiasph_bf8(__mmask32 __U, __m512i __A, __m512h __B) {
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_cvtbiassph_bf8(__m512i __A, __m512h __B) {
+_mm512_cvts_biasph_bf8(__m512i __A, __m512h __B) {
   return (__m256i)__builtin_ia32_vcvtbiasph2bf8s_512_mask(
       (__v64qi)__A, (__v32hf)__B, (__v32qi)_mm256_undefined_si256(),
       (__mmask32)-1);
 }
 
-static __inline__ __m256i __DEFAULT_FN_ATTRS512 _mm512_mask_cvtbiassph_bf8(
+static __inline__ __m256i __DEFAULT_FN_ATTRS512 _mm512_mask_cvts_biasph_bf8(
     __m256i __W, __mmask32 __U, __m512i __A, __m512h __B) {
   return (__m256i)__builtin_ia32_vcvtbiasph2bf8s_512_mask(
       (__v64qi)__A, (__v32hf)__B, (__v32qi)(__m256i)__W, (__mmask32)__U);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_maskz_cvtbiassph_bf8(__mmask32 __U, __m512i __A, __m512h __B) {
+_mm512_maskz_cvts_biasph_bf8(__mmask32 __U, __m512i __A, __m512h __B) {
   return (__m256i)__builtin_ia32_vcvtbiasph2bf8s_512_mask(
       (__v64qi)__A, (__v32hf)__B, (__v32qi)(__m256i)_mm256_setzero_si256(),
       (__mmask32)__U);
@@ -118,20 +118,20 @@ _mm512_maskz_cvtbiasph_hf8(__mmask32 __U, __m512i __A, __m512h __B) {
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_cvtbiassph_hf8(__m512i __A, __m512h __B) {
+_mm512_cvts_biasph_hf8(__m512i __A, __m512h __B) {
   return (__m256i)__builtin_ia32_vcvtbiasph2hf8s_512_mask(
       (__v64qi)__A, (__v32hf)__B, (__v32qi)_mm256_undefined_si256(),
       (__mmask32)-1);
 }
 
-static __inline__ __m256i __DEFAULT_FN_ATTRS512 _mm512_mask_cvtbiassph_hf8(
+static __inline__ __m256i __DEFAULT_FN_ATTRS512 _mm512_mask_cvts_biasph_hf8(
     __m256i __W, __mmask32 __U, __m512i __A, __m512h __B) {
   return (__m256i)__builtin_ia32_vcvtbiasph2hf8s_512_mask(
       (__v64qi)__A, (__v32hf)__B, (__v32qi)(__m256i)__W, (__mmask32)__U);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_maskz_cvtbiassph_hf8(__mmask32 __U, __m512i __A, __m512h __B) {
+_mm512_maskz_cvts_biasph_hf8(__mmask32 __U, __m512i __A, __m512h __B) {
   return (__m256i)__builtin_ia32_vcvtbiasph2hf8s_512_mask(
       (__v64qi)__A, (__v32hf)__B, (__v32qi)(__m256i)_mm256_setzero_si256(),
       (__mmask32)__U);
@@ -157,21 +157,21 @@ _mm512_maskz_cvt2ph_bf8(__mmask64 __U, __m512h __A, __m512h __B) {
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS512
-_mm512_cvts2ph_bf8(__m512h __A, __m512h __B) {
+_mm512_cvts_2ph_bf8(__m512h __A, __m512h __B) {
   return (__m512i)__builtin_ia32_vcvt2ph2bf8s_512((__v32hf)(__A),
                                                   (__v32hf)(__B));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS512
-_mm512_mask_cvts2ph_bf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
+_mm512_mask_cvts_2ph_bf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
   return (__m512i)__builtin_ia32_selectb_512(
-      (__mmask64)__U, (__v64qi)_mm512_cvts2ph_bf8(__A, __B), (__v64qi)__W);
+      (__mmask64)__U, (__v64qi)_mm512_cvts_2ph_bf8(__A, __B), (__v64qi)__W);
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS512
-_mm512_maskz_cvts2ph_bf8(__mmask64 __U, __m512h __A, __m512h __B) {
+_mm512_maskz_cvts_2ph_bf8(__mmask64 __U, __m512h __A, __m512h __B) {
   return (__m512i)__builtin_ia32_selectb_512(
-      (__mmask64)__U, (__v64qi)_mm512_cvts2ph_bf8(__A, __B),
+      (__mmask64)__U, (__v64qi)_mm512_cvts_2ph_bf8(__A, __B),
       (__v64qi)(__m512i)_mm512_setzero_si512());
 }
 
@@ -195,21 +195,21 @@ _mm512_maskz_cvt2ph_hf8(__mmask64 __U, __m512h __A, __m512h __B) {
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS512
-_mm512_cvts2ph_hf8(__m512h __A, __m512h __B) {
+_mm512_cvts_2ph_hf8(__m512h __A, __m512h __B) {
   return (__m512i)__builtin_ia32_vcvt2ph2hf8s_512((__v32hf)(__A),
                                                   (__v32hf)(__B));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS512
-_mm512_mask_cvts2ph_hf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
+_mm512_mask_cvts_2ph_hf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
   return (__m512i)__builtin_ia32_selectb_512(
-      (__mmask64)__U, (__v64qi)_mm512_cvts2ph_hf8(__A, __B), (__v64qi)__W);
+      (__mmask64)__U, (__v64qi)_mm512_cvts_2ph_hf8(__A, __B), (__v64qi)__W);
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS512
-_mm512_maskz_cvts2ph_hf8(__mmask64 __U, __m512h __A, __m512h __B) {
+_mm512_maskz_cvts_2ph_hf8(__mmask64 __U, __m512h __A, __m512h __B) {
   return (__m512i)__builtin_ia32_selectb_512(
-      (__mmask64)__U, (__v64qi)_mm512_cvts2ph_hf8(__A, __B),
+      (__mmask64)__U, (__v64qi)_mm512_cvts_2ph_hf8(__A, __B),
       (__v64qi)(__m512i)_mm512_setzero_si512());
 }
 
@@ -247,19 +247,20 @@ _mm512_maskz_cvtph_bf8(__mmask32 __U, __m512h __A) {
       (__v32hf)__A, (__v32qi)(__m256i)_mm256_setzero_si256(), (__mmask32)__U);
 }
 
-static __inline__ __m256i __DEFAULT_FN_ATTRS512 _mm512_cvtsph_bf8(__m512h __A) {
+static __inline__ __m256i __DEFAULT_FN_ATTRS512
+_mm512_cvts_ph_bf8(__m512h __A) {
   return (__m256i)__builtin_ia32_vcvtph2bf8s_512_mask(
       (__v32hf)__A, (__v32qi)(__m256i)_mm256_undefined_si256(), (__mmask32)-1);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_mask_cvtsph_bf8(__m256i __W, __mmask32 __U, __m512h __A) {
+_mm512_mask_cvts_ph_bf8(__m256i __W, __mmask32 __U, __m512h __A) {
   return (__m256i)__builtin_ia32_vcvtph2bf8s_512_mask(
       (__v32hf)__A, (__v32qi)(__m256i)__W, (__mmask32)__U);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_maskz_cvtsph_bf8(__mmask32 __U, __m512h __A) {
+_mm512_maskz_cvts_ph_bf8(__mmask32 __U, __m512h __A) {
   return (__m256i)__builtin_ia32_vcvtph2bf8s_512_mask(
       (__v32hf)__A, (__v32qi)(__m256i)_mm256_setzero_si256(), (__mmask32)__U);
 }
@@ -281,19 +282,20 @@ _mm512_maskz_cvtph_hf8(__mmask32 __U, __m512h __A) {
       (__v32hf)__A, (__v32qi)(__m256i)_mm256_setzero_si256(), (__mmask32)__U);
 }
 
-static __inline__ __m256i __DEFAULT_FN_ATTRS512 _mm512_cvtsph_hf8(__m512h __A) {
+static __inline__ __m256i __DEFAULT_FN_ATTRS512
+_mm512_cvts_ph_hf8(__m512h __A) {
   return (__m256i)__builtin_ia32_vcvtph2hf8s_512_mask(
       (__v32hf)__A, (__v32qi)(__m256i)_mm256_undefined_si256(), (__mmask32)-1);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_mask_cvtsph_hf8(__m256i __W, __mmask32 __U, __m512h __A) {
+_mm512_mask_cvts_ph_hf8(__m256i __W, __mmask32 __U, __m512h __A) {
   return (__m256i)__builtin_ia32_vcvtph2hf8s_512_mask(
       (__v32hf)__A, (__v32qi)(__m256i)__W, (__mmask32)__U);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS512
-_mm512_maskz_cvtsph_hf8(__mmask32 __U, __m512h __A) {
+_mm512_maskz_cvts_ph_hf8(__mmask32 __U, __m512h __A) {
   return (__m256i)__builtin_ia32_vcvtph2hf8s_512_mask(
       (__v32hf)__A, (__v32qi)(__m256i)_mm256_setzero_si256(), (__mmask32)__U);
 }
diff --git a/clang/lib/Headers/avx10_2_512satcvtdsintrin.h b/clang/lib/Headers/avx10_2_512satcvtdsintrin.h
index 5970ab033144..012a6282b5b1 100644
--- a/clang/lib/Headers/avx10_2_512satcvtdsintrin.h
+++ b/clang/lib/Headers/avx10_2_512satcvtdsintrin.h
@@ -20,20 +20,21 @@
                  __min_vector_width__(512)))
 
 // 512 bit : Double -> Int
-static __inline__ __m256i __DEFAULT_FN_ATTRS _mm512_cvttspd_epi32(__m512d __A) {
+static __inline__ __m256i __DEFAULT_FN_ATTRS
+_mm512_cvtts_pd_epi32(__m512d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2dqs512_round_mask(
       (__v8df)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttspd_epi32(__m256i __W, __mmask8 __U, __m512d __A) {
+_mm512_mask_cvtts_pd_epi32(__m256i __W, __mmask8 __U, __m512d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2dqs512_round_mask(
       (__v8df)__A, (__v8si)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttspd_epi32(__mmask8 __U, __m512d __A) {
+_mm512_maskz_cvtts_pd_epi32(__mmask8 __U, __m512d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2dqs512_round_mask(
       (__v8df)__A, (__v8si)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
@@ -55,20 +56,21 @@ _mm512_maskz_cvttspd_epi32(__mmask8 __U, __m512d __A) {
       (const int)(__R)))
 
 // 512 bit : Double -> uInt
-static __inline__ __m256i __DEFAULT_FN_ATTRS _mm512_cvttspd_epu32(__m512d __A) {
+static __inline__ __m256i __DEFAULT_FN_ATTRS
+_mm512_cvtts_pd_epu32(__m512d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2udqs512_round_mask(
       (__v8df)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttspd_epu32(__m256i __W, __mmask8 __U, __m512d __A) {
+_mm512_mask_cvtts_pd_epu32(__m256i __W, __mmask8 __U, __m512d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2udqs512_round_mask(
       (__v8df)__A, (__v8si)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttspd_epu32(__mmask8 __U, __m512d __A) {
+_mm512_maskz_cvtts_pd_epu32(__mmask8 __U, __m512d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2udqs512_round_mask(
       (__v8df)__A, (__v8si)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
@@ -91,18 +93,19 @@ _mm512_maskz_cvttspd_epu32(__mmask8 __U, __m512d __A) {
 
 //  512 bit : Double -> Long
 
-static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvttspd_epi64(__m512d __A) {
+static __inline__ __m512i __DEFAULT_FN_ATTRS
+_mm512_cvtts_pd_epi64(__m512d __A) {
   return ((__m512i)__builtin_ia32_vcvttpd2qqs512_round_mask(
       (__v8df)__A, (__v8di)_mm512_undefined_epi32(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttspd_epi64(__m512i __W, __mmask8 __U, __m512d __A) {
+_mm512_mask_cvtts_pd_epi64(__m512i __W, __mmask8 __U, __m512d __A) {
   return ((__m512i)__builtin_ia32_vcvttpd2qqs512_round_mask(
       (__v8df)__A, (__v8di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttspd_epi64(__mmask8 __U, __m512d __A) {
+_mm512_maskz_cvtts_pd_epi64(__mmask8 __U, __m512d __A) {
   return ((__m512i)__builtin_ia32_vcvttpd2qqs512_round_mask(
       (__v8df)__A, (__v8di)_mm512_setzero_si512(), __U,
       _MM_FROUND_CUR_DIRECTION));
@@ -125,20 +128,21 @@ _mm512_maskz_cvttspd_epi64(__mmask8 __U, __m512d __A) {
 
 // 512 bit : Double -> ULong
 
-static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvttspd_epu64(__m512d __A) {
+static __inline__ __m512i __DEFAULT_FN_ATTRS
+_mm512_cvtts_pd_epu64(__m512d __A) {
   return ((__m512i)__builtin_ia32_vcvttpd2uqqs512_round_mask(
       (__v8df)__A, (__v8di)_mm512_undefined_epi32(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttspd_epu64(__m512i __W, __mmask8 __U, __m512d __A) {
+_mm512_mask_cvtts_pd_epu64(__m512i __W, __mmask8 __U, __m512d __A) {
   return ((__m512i)__builtin_ia32_vcvttpd2uqqs512_round_mask(
       (__v8df)__A, (__v8di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttspd_epu64(__mmask8 __U, __m512d __A) {
+_mm512_maskz_cvtts_pd_epu64(__mmask8 __U, __m512d __A) {
   return ((__m512i)__builtin_ia32_vcvttpd2uqqs512_round_mask(
       (__v8df)__A, (__v8di)_mm512_setzero_si512(), __U,
       _MM_FROUND_CUR_DIRECTION));
@@ -160,20 +164,20 @@ _mm512_maskz_cvttspd_epu64(__mmask8 __U, __m512d __A) {
       (const int)(__R)))
 
 // 512 bit: Float -> int
-static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvttsps_epi32(__m512 __A) {
+static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvtts_ps_epi32(__m512 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2dqs512_round_mask(
       (__v16sf)(__A), (__v16si)_mm512_undefined_epi32(), (__mmask16)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttsps_epi32(__m512i __W, __mmask16 __U, __m512 __A) {
+_mm512_mask_cvtts_ps_epi32(__m512i __W, __mmask16 __U, __m512 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2dqs512_round_mask(
       (__v16sf)(__A), (__v16si)(__W), __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttsps_epi32(__mmask16 __U, __m512 __A) {
+_mm512_maskz_cvtts_ps_epi32(__mmask16 __U, __m512 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2dqs512_round_mask(
       (__v16sf)(__A), (__v16si)_mm512_setzero_si512(), __U,
       _MM_FROUND_CUR_DIRECTION));
@@ -195,20 +199,20 @@ _mm512_maskz_cvttsps_epi32(__mmask16 __U, __m512 __A) {
       (__mmask16)(__U), (const int)(__R)))
 
 // 512 bit: Float -> uint
-static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvttsps_epu32(__m512 __A) {
+static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvtts_ps_epu32(__m512 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2udqs512_round_mask(
       (__v16sf)(__A), (__v16si)_mm512_undefined_epi32(), (__mmask16)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttsps_epu32(__m512i __W, __mmask16 __U, __m512 __A) {
+_mm512_mask_cvtts_ps_epu32(__m512i __W, __mmask16 __U, __m512 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2udqs512_round_mask(
       (__v16sf)(__A), (__v16si)(__W), __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttsps_epu32(__mmask16 __U, __m512 __A) {
+_mm512_maskz_cvtts_ps_epu32(__mmask16 __U, __m512 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2udqs512_round_mask(
       (__v16sf)(__A), (__v16si)_mm512_setzero_si512(), __U,
       _MM_FROUND_CUR_DIRECTION));
@@ -230,20 +234,20 @@ _mm512_maskz_cvttsps_epu32(__mmask16 __U, __m512 __A) {
       (__mmask16)(__U), (const int)(__R)))
 
 // 512 bit : float -> long
-static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvttsps_epi64(__m256 __A) {
+static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvtts_ps_epi64(__m256 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2qqs512_round_mask(
       (__v8sf)__A, (__v8di)_mm512_undefined_epi32(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttsps_epi64(__m512i __W, __mmask8 __U, __m256 __A) {
+_mm512_mask_cvtts_ps_epi64(__m512i __W, __mmask8 __U, __m256 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2qqs512_round_mask(
       (__v8sf)__A, (__v8di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttsps_epi64(__mmask8 __U, __m256 __A) {
+_mm512_maskz_cvtts_ps_epi64(__mmask8 __U, __m256 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2qqs512_round_mask(
       (__v8sf)__A, (__v8di)_mm512_setzero_si512(), __U,
       _MM_FROUND_CUR_DIRECTION));
@@ -265,20 +269,20 @@ _mm512_maskz_cvttsps_epi64(__mmask8 __U, __m256 __A) {
       (const int)(__R)))
 
 // 512 bit : float -> ulong
-static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvttsps_epu64(__m256 __A) {
+static __inline__ __m512i __DEFAULT_FN_ATTRS _mm512_cvtts_ps_epu64(__m256 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2uqqs512_round_mask(
       (__v8sf)__A, (__v8di)_mm512_undefined_epi32(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_mask_cvttsps_epu64(__m512i __W, __mmask8 __U, __m256 __A) {
+_mm512_mask_cvtts_ps_epu64(__m512i __W, __mmask8 __U, __m256 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2uqqs512_round_mask(
       (__v8sf)__A, (__v8di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m512i __DEFAULT_FN_ATTRS
-_mm512_maskz_cvttsps_epu64(__mmask8 __U, __m256 __A) {
+_mm512_maskz_cvtts_ps_epu64(__mmask8 __U, __m256 __A) {
   return ((__m512i)__builtin_ia32_vcvttps2uqqs512_round_mask(
       (__v8sf)__A, (__v8di)_mm512_setzero_si512(), __U,
       _MM_FROUND_CUR_DIRECTION));
diff --git a/clang/lib/Headers/avx10_2_512satcvtintrin.h b/clang/lib/Headers/avx10_2_512satcvtintrin.h
index 7f41deb5212c..b58e3db8956d 100644
--- a/clang/lib/Headers/avx10_2_512satcvtintrin.h
+++ b/clang/lib/Headers/avx10_2_512satcvtintrin.h
@@ -14,286 +14,286 @@
 #ifndef __AVX10_2_512SATCVTINTRIN_H
 #define __AVX10_2_512SATCVTINTRIN_H
 
-#define _mm512_ipcvtbf16_epi8(A)                                               \
+#define _mm512_ipcvts_bf16_epi8(A)                                             \
   ((__m512i)__builtin_ia32_vcvtbf162ibs512((__v32bf)(__m512bh)(A)))
 
-#define _mm512_mask_ipcvtbf16_epi8(W, U, A)                                    \
+#define _mm512_mask_ipcvts_bf16_epi8(W, U, A)                                  \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvtbf16_epi8(A),      \
+                                       (__v32hi)_mm512_ipcvts_bf16_epi8(A),    \
                                        (__v32hi)(__m512i)(W)))
 
-#define _mm512_maskz_ipcvtbf16_epi8(U, A)                                      \
+#define _mm512_maskz_ipcvts_bf16_epi8(U, A)                                    \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvtbf16_epi8(A),      \
+                                       (__v32hi)_mm512_ipcvts_bf16_epi8(A),    \
                                        (__v32hi)_mm512_setzero_si512()))
 
-#define _mm512_ipcvtbf16_epu8(A)                                               \
+#define _mm512_ipcvts_bf16_epu8(A)                                             \
   ((__m512i)__builtin_ia32_vcvtbf162iubs512((__v32bf)(__m512bh)(A)))
 
-#define _mm512_mask_ipcvtbf16_epu8(W, U, A)                                    \
+#define _mm512_mask_ipcvts_bf16_epu8(W, U, A)                                  \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvtbf16_epu8(A),      \
+                                       (__v32hi)_mm512_ipcvts_bf16_epu8(A),    \
                                        (__v32hi)(__m512i)(W)))
 
-#define _mm512_maskz_ipcvtbf16_epu8(U, A)                                      \
+#define _mm512_maskz_ipcvts_bf16_epu8(U, A)                                    \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvtbf16_epu8(A),      \
+                                       (__v32hi)_mm512_ipcvts_bf16_epu8(A),    \
                                        (__v32hi)_mm512_setzero_si512()))
 
-#define _mm512_ipcvttbf16_epi8(A)                                              \
+#define _mm512_ipcvtts_bf16_epi8(A)                                            \
   ((__m512i)__builtin_ia32_vcvttbf162ibs512((__v32bf)(__m512bh)(A)))
 
-#define _mm512_mask_ipcvttbf16_epi8(W, U, A)                                   \
+#define _mm512_mask_ipcvtts_bf16_epi8(W, U, A)                                 \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvttbf16_epi8(A),     \
+                                       (__v32hi)_mm512_ipcvtts_bf16_epi8(A),   \
                                        (__v32hi)(__m512i)(W)))
 
-#define _mm512_maskz_ipcvttbf16_epi8(U, A)                                     \
+#define _mm512_maskz_ipcvtts_bf16_epi8(U, A)                                   \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvttbf16_epi8(A),     \
+                                       (__v32hi)_mm512_ipcvtts_bf16_epi8(A),   \
                                        (__v32hi)_mm512_setzero_si512()))
 
-#define _mm512_ipcvttbf16_epu8(A)                                              \
+#define _mm512_ipcvtts_bf16_epu8(A)                                            \
   ((__m512i)__builtin_ia32_vcvttbf162iubs512((__v32bf)(__m512bh)(A)))
 
-#define _mm512_mask_ipcvttbf16_epu8(W, U, A)                                   \
+#define _mm512_mask_ipcvtts_bf16_epu8(W, U, A)                                 \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvttbf16_epu8(A),     \
+                                       (__v32hi)_mm512_ipcvtts_bf16_epu8(A),   \
                                        (__v32hi)(__m512i)(W)))
 
-#define _mm512_maskz_ipcvttbf16_epu8(U, A)                                     \
+#define _mm512_maskz_ipcvtts_bf16_epu8(U, A)                                   \
   ((__m512i)__builtin_ia32_selectw_512((__mmask32)(U),                         \
-                                       (__v32hi)_mm512_ipcvttbf16_epu8(A),     \
+                                       (__v32hi)_mm512_ipcvtts_bf16_epu8(A),   \
                                        (__v32hi)_mm512_setzero_si512()))
 
-#define _mm512_ipcvtph_epi8(A)                                                 \
+#define _mm512_ipcvts_ph_epi8(A)                                               \
   ((__m512i)__builtin_ia32_vcvtph2ibs512_mask(                                 \
-      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,   \
+      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32) - 1, \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvtph_epi8(W, U, A)                                      \
+#define _mm512_mask_ipcvts_ph_epi8(W, U, A)                                    \
   ((__m512i)__builtin_ia32_vcvtph2ibs512_mask((__v32hf)(__m512h)(A),           \
                                               (__v32hu)(W), (__mmask32)(U),    \
                                               _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvtph_epi8(U, A)                                        \
+#define _mm512_maskz_ipcvts_ph_epi8(U, A)                                      \
   ((__m512i)__builtin_ia32_vcvtph2ibs512_mask(                                 \
       (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)(U),  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvt_roundph_epi8(A, R)                                        \
+#define _mm512_ipcvts_roundph_epi8(A, R)                                       \
   ((__m512i)__builtin_ia32_vcvtph2ibs512_mask((__v32hf)(__m512h)(A),           \
                                               (__v32hu)_mm512_setzero_si512(), \
-                                              (__mmask32)-1, (const int)R))
+                                              (__mmask32) - 1, (const int)R))
 
-#define _mm512_mask_ipcvt_roundph_epi8(W, U, A, R)                             \
+#define _mm512_mask_ipcvts_roundph_epi8(W, U, A, R)                            \
   ((__m512i)__builtin_ia32_vcvtph2ibs512_mask(                                 \
       (__v32hf)(__m512h)(A), (__v32hu)(W), (__mmask32)(U), (const int)R))
 
-#define _mm512_maskz_ipcvt_roundph_epi8(U, A, R)                               \
+#define _mm512_maskz_ipcvts_roundph_epi8(U, A, R)                              \
   ((__m512i)__builtin_ia32_vcvtph2ibs512_mask((__v32hf)(__m512h)(A),           \
                                               (__v32hu)_mm512_setzero_si512(), \
                                               (__mmask32)(U), (const int)R))
 
-#define _mm512_ipcvtph_epu8(A)                                                 \
+#define _mm512_ipcvts_ph_epu8(A)                                               \
   ((__m512i)__builtin_ia32_vcvtph2iubs512_mask(                                \
-      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,   \
+      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32) - 1, \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvtph_epu8(W, U, A)                                      \
+#define _mm512_mask_ipcvts_ph_epu8(W, U, A)                                    \
   ((__m512i)__builtin_ia32_vcvtph2iubs512_mask((__v32hf)(__m512h)(A),          \
                                                (__v32hu)(W), (__mmask32)(U),   \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvtph_epu8(U, A)                                        \
+#define _mm512_maskz_ipcvts_ph_epu8(U, A)                                      \
   ((__m512i)__builtin_ia32_vcvtph2iubs512_mask(                                \
       (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)(U),  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvt_roundph_epu8(A, R)                                        \
+#define _mm512_ipcvts_roundph_epu8(A, R)                                       \
   ((__m512i)__builtin_ia32_vcvtph2iubs512_mask(                                \
-      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,   \
+      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32) - 1, \
       (const int)R))
 
-#define _mm512_mask_ipcvt_roundph_epu8(W, U, A, R)                             \
+#define _mm512_mask_ipcvts_roundph_epu8(W, U, A, R)                            \
   ((__m512i)__builtin_ia32_vcvtph2iubs512_mask(                                \
       (__v32hf)(__m512h)(A), (__v32hu)(W), (__mmask32)(U), (const int)R))
 
-#define _mm512_maskz_ipcvt_roundph_epu8(U, A, R)                               \
+#define _mm512_maskz_ipcvts_roundph_epu8(U, A, R)                              \
   ((__m512i)__builtin_ia32_vcvtph2iubs512_mask(                                \
       (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)(U),  \
       (const int)R))
 
-#define _mm512_ipcvtps_epi8(A)                                                 \
+#define _mm512_ipcvts_ps_epi8(A)                                               \
   ((__m512i)__builtin_ia32_vcvtps2ibs512_mask(                                 \
-      (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)-1,    \
+      (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16) - 1,  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvtps_epi8(W, U, A)                                      \
+#define _mm512_mask_ipcvts_ps_epi8(W, U, A)                                    \
   ((__m512i)__builtin_ia32_vcvtps2ibs512_mask((__v16sf)(__m512)(A),            \
                                               (__v16su)(W), (__mmask16)(U),    \
                                               _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvtps_epi8(U, A)                                        \
+#define _mm512_maskz_ipcvts_ps_epi8(U, A)                                      \
   ((__m512i)__builtin_ia32_vcvtps2ibs512_mask(                                 \
       (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)(U),   \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvt_roundps_epi8(A, R)                                        \
+#define _mm512_ipcvts_roundps_epi8(A, R)                                       \
   ((__m512i)__builtin_ia32_vcvtps2ibs512_mask((__v16sf)(__m512)(A),            \
                                               (__v16su)_mm512_setzero_si512(), \
-                                              (__mmask16)-1, (const int)R))
+                                              (__mmask16) - 1, (const int)R))
 
-#define _mm512_mask_ipcvt_roundps_epi8(W, U, A, R)                             \
+#define _mm512_mask_ipcvts_roundps_epi8(W, U, A, R)                            \
   ((__m512i)__builtin_ia32_vcvtps2ibs512_mask(                                 \
       (__v16sf)(__m512)(A), (__v16su)(W), (__mmask16)(U), (const int)R))
 
-#define _mm512_maskz_ipcvt_roundps_epi8(U, A, R)                               \
+#define _mm512_maskz_ipcvts_roundps_epi8(U, A, R)                              \
   ((__m512i)__builtin_ia32_vcvtps2ibs512_mask((__v16sf)(__m512)(A),            \
                                               (__v16su)_mm512_setzero_si512(), \
                                               (__mmask16)(U), (const int)R))
 
-#define _mm512_ipcvtps_epu8(A)                                                 \
+#define _mm512_ipcvts_ps_epu8(A)                                               \
   ((__m512i)__builtin_ia32_vcvtps2iubs512_mask(                                \
-      (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)-1,    \
+      (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16) - 1,  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvtps_epu8(W, U, A)                                      \
+#define _mm512_mask_ipcvts_ps_epu8(W, U, A)                                    \
   ((__m512i)__builtin_ia32_vcvtps2iubs512_mask((__v16sf)(__m512)(A),           \
                                                (__v16su)(W), (__mmask16)(U),   \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvtps_epu8(U, A)                                        \
+#define _mm512_maskz_ipcvts_ps_epu8(U, A)                                      \
   ((__m512i)__builtin_ia32_vcvtps2iubs512_mask(                                \
       (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)(U),   \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvt_roundps_epu8(A, R)                                        \
+#define _mm512_ipcvts_roundps_epu8(A, R)                                       \
   ((__m512i)__builtin_ia32_vcvtps2iubs512_mask(                                \
-      (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)-1,    \
+      (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16) - 1,  \
       (const int)R))
 
-#define _mm512_mask_ipcvt_roundps_epu8(W, U, A, R)                             \
+#define _mm512_mask_ipcvts_roundps_epu8(W, U, A, R)                            \
   ((__m512i)__builtin_ia32_vcvtps2iubs512_mask(                                \
       (__v16sf)(__m512)(A), (__v16su)(W), (__mmask16)(U), (const int)R))
 
-#define _mm512_maskz_ipcvt_roundps_epu8(U, A, R)                               \
+#define _mm512_maskz_ipcvts_roundps_epu8(U, A, R)                              \
   ((__m512i)__builtin_ia32_vcvtps2iubs512_mask(                                \
       (__v16sf)(__m512)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)(U),   \
       (const int)R))
 
-#define _mm512_ipcvttph_epi8(A)                                                \
+#define _mm512_ipcvtts_ph_epi8(A)                                              \
   ((__m512i)__builtin_ia32_vcvttph2ibs512_mask(                                \
-      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,   \
+      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32) - 1, \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvttph_epi8(W, U, A)                                     \
+#define _mm512_mask_ipcvtts_ph_epi8(W, U, A)                                   \
   ((__m512i)__builtin_ia32_vcvttph2ibs512_mask((__v32hf)(__m512h)(A),          \
                                                (__v32hu)(W), (__mmask32)(U),   \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvttph_epi8(U, A)                                       \
+#define _mm512_maskz_ipcvtts_ph_epi8(U, A)                                     \
   ((__m512i)__builtin_ia32_vcvttph2ibs512_mask(                                \
       (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)(U),  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvtt_roundph_epi8(A, S)                                       \
+#define _mm512_ipcvtts_roundph_epi8(A, S)                                      \
   ((__m512i)__builtin_ia32_vcvttph2ibs512_mask(                                \
-      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,   \
+      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32) - 1, \
       S))
 
-#define _mm512_mask_ipcvtt_roundph_epi8(W, U, A, S)                            \
+#define _mm512_mask_ipcvtts_roundph_epi8(W, U, A, S)                           \
   ((__m512i)__builtin_ia32_vcvttph2ibs512_mask(                                \
       (__v32hf)(__m512h)(A), (__v32hu)(W), (__mmask32)(U), S))
 
-#define _mm512_maskz_ipcvtt_roundph_epi8(U, A, S)                              \
+#define _mm512_maskz_ipcvtts_roundph_epi8(U, A, S)                             \
   ((__m512i)__builtin_ia32_vcvttph2ibs512_mask(                                \
       (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)(U),  \
       S))
 
-#define _mm512_ipcvttph_epu8(A)                                                \
+#define _mm512_ipcvtts_ph_epu8(A)                                              \
   ((__m512i)__builtin_ia32_vcvttph2iubs512_mask(                               \
-      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,   \
+      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32) - 1, \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvttph_epu8(W, U, A)                                     \
+#define _mm512_mask_ipcvtts_ph_epu8(W, U, A)                                   \
   ((__m512i)__builtin_ia32_vcvttph2iubs512_mask((__v32hf)(__m512h)(A),         \
                                                 (__v32hu)(W), (__mmask32)(U),  \
                                                 _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvttph_epu8(U, A)                                       \
+#define _mm512_maskz_ipcvtts_ph_epu8(U, A)                                     \
   ((__m512i)__builtin_ia32_vcvttph2iubs512_mask(                               \
       (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)(U),  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvtt_roundph_epu8(A, S)                                       \
+#define _mm512_ipcvtts_roundph_epu8(A, S)                                      \
   ((__m512i)__builtin_ia32_vcvttph2iubs512_mask(                               \
-      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,   \
+      (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32) - 1, \
       S))
 
-#define _mm512_mask_ipcvtt_roundph_epu8(W, U, A, S)                            \
+#define _mm512_mask_ipcvtts_roundph_epu8(W, U, A, S)                           \
   ((__m512i)__builtin_ia32_vcvttph2iubs512_mask(                               \
       (__v32hf)(__m512h)(A), (__v32hu)(W), (__mmask32)(U), S))
 
-#define _mm512_maskz_ipcvtt_roundph_epu8(U, A, S)                              \
+#define _mm512_maskz_ipcvtts_roundph_epu8(U, A, S)                             \
   ((__m512i)__builtin_ia32_vcvttph2iubs512_mask(                               \
       (__v32hf)(__m512h)(A), (__v32hu)_mm512_setzero_si512(), (__mmask32)(U),  \
       S))
 
-#define _mm512_ipcvttps_epi8(A)                                                \
+#define _mm512_ipcvtts_ps_epi8(A)                                              \
   ((__m512i)__builtin_ia32_vcvttps2ibs512_mask(                                \
-      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)-1,   \
+      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16) - 1, \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvttps_epi8(W, U, A)                                     \
+#define _mm512_mask_ipcvtts_ps_epi8(W, U, A)                                   \
   ((__m512i)__builtin_ia32_vcvttps2ibs512_mask((__v16sf)(__m512h)(A),          \
                                                (__v16su)(W), (__mmask16)(U),   \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvttps_epi8(U, A)                                       \
+#define _mm512_maskz_ipcvtts_ps_epi8(U, A)                                     \
   ((__m512i)__builtin_ia32_vcvttps2ibs512_mask(                                \
       (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)(U),  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvtt_roundps_epi8(A, S)                                       \
+#define _mm512_ipcvtts_roundps_epi8(A, S)                                      \
   ((__m512i)__builtin_ia32_vcvttps2ibs512_mask(                                \
-      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)-1,   \
+      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16) - 1, \
       S))
 
-#define _mm512_mask_ipcvtt_roundps_epi8(W, U, A, S)                            \
+#define _mm512_mask_ipcvtts_roundps_epi8(W, U, A, S)                           \
   ((__m512i)__builtin_ia32_vcvttps2ibs512_mask(                                \
       (__v16sf)(__m512h)(A), (__v16su)(W), (__mmask16)(U), S))
 
-#define _mm512_maskz_ipcvtt_roundps_epi8(U, A, S)                              \
+#define _mm512_maskz_ipcvtts_roundps_epi8(U, A, S)                             \
   ((__m512i)__builtin_ia32_vcvttps2ibs512_mask(                                \
       (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)(U),  \
       S))
 
-#define _mm512_ipcvttps_epu8(A)                                                \
+#define _mm512_ipcvtts_ps_epu8(A)                                              \
   ((__m512i)__builtin_ia32_vcvttps2iubs512_mask(                               \
-      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)-1,   \
+      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16) - 1, \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_mask_ipcvttps_epu8(W, U, A)                                     \
+#define _mm512_mask_ipcvtts_ps_epu8(W, U, A)                                   \
   ((__m512i)__builtin_ia32_vcvttps2iubs512_mask((__v16sf)(__m512h)(A),         \
                                                 (__v16su)(W), (__mmask16)(U),  \
                                                 _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_maskz_ipcvttps_epu8(U, A)                                       \
+#define _mm512_maskz_ipcvtts_ps_epu8(U, A)                                     \
   ((__m512i)__builtin_ia32_vcvttps2iubs512_mask(                               \
       (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)(U),  \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm512_ipcvtt_roundps_epu8(A, S)                                       \
+#define _mm512_ipcvtts_roundps_epu8(A, S)                                      \
   ((__m512i)__builtin_ia32_vcvttps2iubs512_mask(                               \
-      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)-1,   \
+      (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16) - 1, \
       S))
 
-#define _mm512_mask_ipcvtt_roundps_epu8(W, U, A, S)                            \
+#define _mm512_mask_ipcvtts_roundps_epu8(W, U, A, S)                           \
   ((__m512i)__builtin_ia32_vcvttps2iubs512_mask(                               \
       (__v16sf)(__m512h)(A), (__v16su)(W), (__mmask16)(U), S))
 
-#define _mm512_maskz_ipcvtt_roundps_epu8(U, A, S)                              \
+#define _mm512_maskz_ipcvtts_roundps_epu8(U, A, S)                             \
   ((__m512i)__builtin_ia32_vcvttps2iubs512_mask(                               \
       (__v16sf)(__m512h)(A), (__v16su)_mm512_setzero_si512(), (__mmask16)(U),  \
       S))
diff --git a/clang/lib/Headers/avx10_2convertintrin.h b/clang/lib/Headers/avx10_2convertintrin.h
index 07722090c30e..bd6ff6099d8a 100644
--- a/clang/lib/Headers/avx10_2convertintrin.h
+++ b/clang/lib/Headers/avx10_2convertintrin.h
@@ -63,22 +63,8 @@ _mm256_maskz_cvtx2ps_ph(__mmask16 __U, __m256 __A, __m256 __B) {
       _MM_FROUND_CUR_DIRECTION);
 }
 
-#define _mm256_cvtx_round2ps_ph(A, B, R)                                       \
-  ((__m256h)__builtin_ia32_vcvt2ps2phx256_mask(                                \
-      (__v8sf)(A), (__v8sf)(B), (__v16hf)_mm256_undefined_ph(),                \
-      (__mmask16)(-1), (const int)(R)))
-
-#define _mm256_mask_cvtx_round2ps_ph(W, U, A, B, R)                            \
-  ((__m256h)__builtin_ia32_vcvt2ps2phx256_mask(                                \
-      (__v8sf)(A), (__v8sf)(B), (__v16hf)(W), (__mmask16)(U), (const int)(R)))
-
-#define _mm256_maskz_cvtx_round2ps_ph(U, A, B, R)                              \
-  ((__m256h)__builtin_ia32_vcvt2ps2phx256_mask(                                \
-      (__v8sf)(A), (__v8sf)(B), (__v16hf)(_mm256_setzero_ph()),                \
-      (__mmask16)(U), (const int)(R)))
-
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvtbiasph_bf8(__m128i __A,
-                                                                  __m128h __B) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128
+_mm_cvtbiasph_bf8(__m128i __A, __m128h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2bf8_128_mask(
       (__v16qi)__A, (__v8hf)__B, (__v16qi)_mm_undefined_si128(), (__mmask8)-1);
 }
@@ -117,39 +103,39 @@ _mm256_maskz_cvtbiasph_bf8(__mmask16 __U, __m256i __A, __m256h __B) {
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_cvtbiassph_bf8(__m128i __A, __m128h __B) {
+_mm_cvts_biasph_bf8(__m128i __A, __m128h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2bf8s_128_mask(
       (__v16qi)__A, (__v8hf)__B, (__v16qi)_mm_undefined_si128(), (__mmask8)-1);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvtbiassph_bf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
+_mm_mask_cvts_biasph_bf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2bf8s_128_mask(
       (__v16qi)__A, (__v8hf)__B, (__v16qi)(__m128i)__W, (__mmask8)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvtbiassph_bf8(__mmask8 __U, __m128i __A, __m128h __B) {
+_mm_maskz_cvts_biasph_bf8(__mmask8 __U, __m128i __A, __m128h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2bf8s_128_mask(
       (__v16qi)__A, (__v8hf)__B, (__v16qi)(__m128i)_mm_setzero_si128(),
       (__mmask8)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_cvtbiassph_bf8(__m256i __A, __m256h __B) {
+_mm256_cvts_biasph_bf8(__m256i __A, __m256h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2bf8s_256_mask(
       (__v32qi)__A, (__v16hf)__B, (__v16qi)(__m128i)_mm_undefined_si128(),
       (__mmask16)-1);
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS256 _mm256_mask_cvtbiassph_bf8(
+static __inline__ __m128i __DEFAULT_FN_ATTRS256 _mm256_mask_cvts_biasph_bf8(
     __m128i __W, __mmask16 __U, __m256i __A, __m256h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2bf8s_256_mask(
       (__v32qi)__A, (__v16hf)__B, (__v16qi)(__m128i)__W, (__mmask16)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvtbiassph_bf8(__mmask16 __U, __m256i __A, __m256h __B) {
+_mm256_maskz_cvts_biasph_bf8(__mmask16 __U, __m256i __A, __m256h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2bf8s_256_mask(
       (__v32qi)__A, (__v16hf)__B, (__v16qi)(__m128i)_mm_setzero_si128(),
       (__mmask16)__U);
@@ -195,39 +181,39 @@ _mm256_maskz_cvtbiasph_hf8(__mmask16 __U, __m256i __A, __m256h __B) {
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_cvtbiassph_hf8(__m128i __A, __m128h __B) {
+_mm_cvts_biasph_hf8(__m128i __A, __m128h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2hf8s_128_mask(
       (__v16qi)__A, (__v8hf)__B, (__v16qi)_mm_undefined_si128(), (__mmask8)-1);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvtbiassph_hf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
+_mm_mask_cvts_biasph_hf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2hf8s_128_mask(
       (__v16qi)__A, (__v8hf)__B, (__v16qi)(__m128i)__W, (__mmask8)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvtbiassph_hf8(__mmask8 __U, __m128i __A, __m128h __B) {
+_mm_maskz_cvts_biasph_hf8(__mmask8 __U, __m128i __A, __m128h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2hf8s_128_mask(
       (__v16qi)__A, (__v8hf)__B, (__v16qi)(__m128i)_mm_setzero_si128(),
       (__mmask8)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_cvtbiassph_hf8(__m256i __A, __m256h __B) {
+_mm256_cvts_biasph_hf8(__m256i __A, __m256h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2hf8s_256_mask(
       (__v32qi)__A, (__v16hf)__B, (__v16qi)(__m128i)_mm_undefined_si128(),
       (__mmask16)-1);
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS256 _mm256_mask_cvtbiassph_hf8(
+static __inline__ __m128i __DEFAULT_FN_ATTRS256 _mm256_mask_cvts_biasph_hf8(
     __m128i __W, __mmask16 __U, __m256i __A, __m256h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2hf8s_256_mask(
       (__v32qi)__A, (__v16hf)__B, (__v16qi)(__m128i)__W, (__mmask16)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvtbiassph_hf8(__mmask16 __U, __m256i __A, __m256h __B) {
+_mm256_maskz_cvts_biasph_hf8(__mmask16 __U, __m256i __A, __m256h __B) {
   return (__m128i)__builtin_ia32_vcvtbiasph2hf8s_256_mask(
       (__v32qi)__A, (__v16hf)__B, (__v16qi)(__m128i)_mm_setzero_si128(),
       (__mmask16)__U);
@@ -270,40 +256,40 @@ _mm256_maskz_cvt2ph_bf8(__mmask32 __U, __m256h __A, __m256h __B) {
       (__v32qi)(__m256i)_mm256_setzero_si256());
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvts2ph_bf8(__m128h __A,
-                                                                __m128h __B) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvts_2ph_bf8(__m128h __A,
+                                                                 __m128h __B) {
   return (__m128i)__builtin_ia32_vcvt2ph2bf8s_128((__v8hf)(__A), (__v8hf)(__B));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvts2ph_bf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
+_mm_mask_cvts_2ph_bf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
   return (__m128i)__builtin_ia32_selectb_128(
-      (__mmask16)__U, (__v16qi)_mm_cvts2ph_bf8(__A, __B), (__v16qi)__W);
+      (__mmask16)__U, (__v16qi)_mm_cvts_2ph_bf8(__A, __B), (__v16qi)__W);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvts2ph_bf8(__mmask16 __U, __m128h __A, __m128h __B) {
+_mm_maskz_cvts_2ph_bf8(__mmask16 __U, __m128h __A, __m128h __B) {
   return (__m128i)__builtin_ia32_selectb_128(
-      (__mmask16)__U, (__v16qi)_mm_cvts2ph_bf8(__A, __B),
+      (__mmask16)__U, (__v16qi)_mm_cvts_2ph_bf8(__A, __B),
       (__v16qi)(__m128i)_mm_setzero_si128());
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvts2ph_bf8(__m256h __A, __m256h __B) {
+_mm256_cvts_2ph_bf8(__m256h __A, __m256h __B) {
   return (__m256i)__builtin_ia32_vcvt2ph2bf8s_256((__v16hf)(__A),
                                                   (__v16hf)(__B));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvts2ph_bf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
+_mm256_mask_cvts_2ph_bf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
   return (__m256i)__builtin_ia32_selectb_256(
-      (__mmask32)__U, (__v32qi)_mm256_cvts2ph_bf8(__A, __B), (__v32qi)__W);
+      (__mmask32)__U, (__v32qi)_mm256_cvts_2ph_bf8(__A, __B), (__v32qi)__W);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvts2ph_bf8(__mmask32 __U, __m256h __A, __m256h __B) {
+_mm256_maskz_cvts_2ph_bf8(__mmask32 __U, __m256h __A, __m256h __B) {
   return (__m256i)__builtin_ia32_selectb_256(
-      (__mmask32)__U, (__v32qi)_mm256_cvts2ph_bf8(__A, __B),
+      (__mmask32)__U, (__v32qi)_mm256_cvts_2ph_bf8(__A, __B),
       (__v32qi)(__m256i)_mm256_setzero_si256());
 }
 
@@ -344,40 +330,40 @@ _mm256_maskz_cvt2ph_hf8(__mmask32 __U, __m256h __A, __m256h __B) {
       (__v32qi)(__m256i)_mm256_setzero_si256());
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvts2ph_hf8(__m128h __A,
-                                                                __m128h __B) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvts_2ph_hf8(__m128h __A,
+                                                                 __m128h __B) {
   return (__m128i)__builtin_ia32_vcvt2ph2hf8s_128((__v8hf)(__A), (__v8hf)(__B));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvts2ph_hf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
+_mm_mask_cvts_2ph_hf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
   return (__m128i)__builtin_ia32_selectb_128(
-      (__mmask16)__U, (__v16qi)_mm_cvts2ph_hf8(__A, __B), (__v16qi)__W);
+      (__mmask16)__U, (__v16qi)_mm_cvts_2ph_hf8(__A, __B), (__v16qi)__W);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvts2ph_hf8(__mmask16 __U, __m128h __A, __m128h __B) {
+_mm_maskz_cvts_2ph_hf8(__mmask16 __U, __m128h __A, __m128h __B) {
   return (__m128i)__builtin_ia32_selectb_128(
-      (__mmask16)__U, (__v16qi)_mm_cvts2ph_hf8(__A, __B),
+      (__mmask16)__U, (__v16qi)_mm_cvts_2ph_hf8(__A, __B),
       (__v16qi)(__m128i)_mm_setzero_si128());
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvts2ph_hf8(__m256h __A, __m256h __B) {
+_mm256_cvts_2ph_hf8(__m256h __A, __m256h __B) {
   return (__m256i)__builtin_ia32_vcvt2ph2hf8s_256((__v16hf)(__A),
                                                   (__v16hf)(__B));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvts2ph_hf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
+_mm256_mask_cvts_2ph_hf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
   return (__m256i)__builtin_ia32_selectb_256(
-      (__mmask32)__U, (__v32qi)_mm256_cvts2ph_hf8(__A, __B), (__v32qi)__W);
+      (__mmask32)__U, (__v32qi)_mm256_cvts_2ph_hf8(__A, __B), (__v32qi)__W);
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvts2ph_hf8(__mmask32 __U, __m256h __A, __m256h __B) {
+_mm256_maskz_cvts_2ph_hf8(__mmask32 __U, __m256h __A, __m256h __B) {
   return (__m256i)__builtin_ia32_selectb_256(
-      (__mmask32)__U, (__v32qi)_mm256_cvts2ph_hf8(__A, __B),
+      (__mmask32)__U, (__v32qi)_mm256_cvts_2ph_hf8(__A, __B),
       (__v32qi)(__m256i)_mm256_setzero_si256());
 }
 
@@ -449,36 +435,37 @@ _mm256_maskz_cvtph_bf8(__mmask16 __U, __m256h __A) {
       (__v16hf)__A, (__v16qi)(__m128i)_mm_setzero_si128(), (__mmask16)__U);
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvtsph_bf8(__m128h __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvts_ph_bf8(__m128h __A) {
   return (__m128i)__builtin_ia32_vcvtph2bf8s_128_mask(
       (__v8hf)__A, (__v16qi)(__m128i)_mm_undefined_si128(), (__mmask8)-1);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvtsph_bf8(__m128i __W, __mmask8 __U, __m128h __A) {
+_mm_mask_cvts_ph_bf8(__m128i __W, __mmask8 __U, __m128h __A) {
   return (__m128i)__builtin_ia32_vcvtph2bf8s_128_mask(
       (__v8hf)__A, (__v16qi)(__m128i)__W, (__mmask8)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvtsph_bf8(__mmask8 __U, __m128h __A) {
+_mm_maskz_cvts_ph_bf8(__mmask8 __U, __m128h __A) {
   return (__m128i)__builtin_ia32_vcvtph2bf8s_128_mask(
       (__v8hf)__A, (__v16qi)(__m128i)_mm_setzero_si128(), (__mmask8)__U);
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS256 _mm256_cvtsph_bf8(__m256h __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS256
+_mm256_cvts_ph_bf8(__m256h __A) {
   return (__m128i)__builtin_ia32_vcvtph2bf8s_256_mask(
       (__v16hf)__A, (__v16qi)(__m128i)_mm_undefined_si128(), (__mmask16)-1);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvtsph_bf8(__m128i __W, __mmask16 __U, __m256h __A) {
+_mm256_mask_cvts_ph_bf8(__m128i __W, __mmask16 __U, __m256h __A) {
   return (__m128i)__builtin_ia32_vcvtph2bf8s_256_mask(
       (__v16hf)__A, (__v16qi)(__m128i)__W, (__mmask16)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvtsph_bf8(__mmask16 __U, __m256h __A) {
+_mm256_maskz_cvts_ph_bf8(__mmask16 __U, __m256h __A) {
   return (__m128i)__builtin_ia32_vcvtph2bf8s_256_mask(
       (__v16hf)__A, (__v16qi)(__m128i)_mm_setzero_si128(), (__mmask16)__U);
 }
@@ -517,36 +504,37 @@ _mm256_maskz_cvtph_hf8(__mmask16 __U, __m256h __A) {
       (__v16hf)__A, (__v16qi)(__m128i)_mm_setzero_si128(), (__mmask16)__U);
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvtsph_hf8(__m128h __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvts_ph_hf8(__m128h __A) {
   return (__m128i)__builtin_ia32_vcvtph2hf8s_128_mask(
       (__v8hf)__A, (__v16qi)(__m128i)_mm_undefined_si128(), (__mmask8)-1);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvtsph_hf8(__m128i __W, __mmask8 __U, __m128h __A) {
+_mm_mask_cvts_ph_hf8(__m128i __W, __mmask8 __U, __m128h __A) {
   return (__m128i)__builtin_ia32_vcvtph2hf8s_128_mask(
       (__v8hf)__A, (__v16qi)(__m128i)__W, (__mmask8)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvtsph_hf8(__mmask8 __U, __m128h __A) {
+_mm_maskz_cvts_ph_hf8(__mmask8 __U, __m128h __A) {
   return (__m128i)__builtin_ia32_vcvtph2hf8s_128_mask(
       (__v8hf)__A, (__v16qi)(__m128i)_mm_setzero_si128(), (__mmask8)__U);
 }
 
-static __inline__ __m128i __DEFAULT_FN_ATTRS256 _mm256_cvtsph_hf8(__m256h __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS256
+_mm256_cvts_ph_hf8(__m256h __A) {
   return (__m128i)__builtin_ia32_vcvtph2hf8s_256_mask(
       (__v16hf)__A, (__v16qi)(__m128i)_mm_undefined_si128(), (__mmask16)-1);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvtsph_hf8(__m128i __W, __mmask16 __U, __m256h __A) {
+_mm256_mask_cvts_ph_hf8(__m128i __W, __mmask16 __U, __m256h __A) {
   return (__m128i)__builtin_ia32_vcvtph2hf8s_256_mask(
       (__v16hf)__A, (__v16qi)(__m128i)__W, (__mmask16)__U);
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvtsph_hf8(__mmask16 __U, __m256h __A) {
+_mm256_maskz_cvts_ph_hf8(__mmask16 __U, __m256h __A) {
   return (__m128i)__builtin_ia32_vcvtph2hf8s_256_mask(
       (__v16hf)__A, (__v16qi)(__m128i)_mm_setzero_si128(), (__mmask16)__U);
 }
diff --git a/clang/lib/Headers/avx10_2minmaxintrin.h b/clang/lib/Headers/avx10_2minmaxintrin.h
index 8164d49d89f1..a59b74dbc54b 100644
--- a/clang/lib/Headers/avx10_2minmaxintrin.h
+++ b/clang/lib/Headers/avx10_2minmaxintrin.h
@@ -80,21 +80,6 @@
       (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(C),                    \
       (__v4df)_mm256_setzero_pd(), (__mmask8)(U), _MM_FROUND_NO_EXC))
 
-#define _mm256_minmax_round_pd(A, B, C, R)                                     \
-  ((__m256d)__builtin_ia32_vminmaxpd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(C),                    \
-      (__v4df)_mm256_undefined_pd(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_minmax_round_pd(W, U, A, B, C, R)                          \
-  ((__m256d)__builtin_ia32_vminmaxpd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(C),                    \
-      (__v4df)(__m256d)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_minmax_round_pd(U, A, B, C, R)                            \
-  ((__m256d)__builtin_ia32_vminmaxpd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(C),                    \
-      (__v4df)_mm256_setzero_pd(), (__mmask8)(U), (int)(R)))
-
 #define _mm_minmax_ph(A, B, C)                                                 \
   ((__m128h)__builtin_ia32_vminmaxph128_mask(                                  \
       (__v8hf)(__m128h)(A), (__v8hf)(__m128h)(B), (int)(C),                    \
@@ -125,21 +110,6 @@
       (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (int)(C),                  \
       (__v16hf)_mm256_setzero_ph(), (__mmask16)(U), _MM_FROUND_NO_EXC))
 
-#define _mm256_minmax_round_ph(A, B, C, R)                                     \
-  ((__m256h)__builtin_ia32_vminmaxph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (int)(C),                  \
-      (__v16hf)_mm256_undefined_ph(), (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_minmax_round_ph(W, U, A, B, C, R)                          \
-  ((__m256h)__builtin_ia32_vminmaxph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (C),                       \
-      (__v16hf)(__m256h)(W), (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_minmax_round_ph(U, A, B, C, R)                            \
-  ((__m256h)__builtin_ia32_vminmaxph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (int)(C),                  \
-      (__v16hf)_mm256_setzero_ph(), (__mmask16)(U), (int)(R)))
-
 #define _mm_minmax_ps(A, B, C)                                                 \
   ((__m128)__builtin_ia32_vminmaxps128_mask(                                   \
       (__v4sf)(__m128)(A), (__v4sf)(__m128)(B), (int)(C),                      \
@@ -170,21 +140,6 @@
       (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(C),                      \
       (__v8sf)_mm256_setzero_ps(), (__mmask8)(U), _MM_FROUND_NO_EXC))
 
-#define _mm256_minmax_round_ps(A, B, C, R)                                     \
-  ((__m256)__builtin_ia32_vminmaxps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(C),                      \
-      (__v8sf)_mm256_undefined_ps(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_minmax_round_ps(W, U, A, B, C, R)                          \
-  ((__m256)__builtin_ia32_vminmaxps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(C), (__v8sf)(__m256)(W), \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_minmax_round_ps(U, A, B, C, R)                            \
-  ((__m256)__builtin_ia32_vminmaxps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(C),                      \
-      (__v8sf)_mm256_setzero_ps(), (__mmask8)(U), (int)(R)))
-
 #define _mm_minmax_sd(A, B, C)                                                 \
   ((__m128d)__builtin_ia32_vminmaxsd_round_mask(                               \
       (__v2df)(__m128d)(A), (__v2df)(__m128d)(B), (int)(C),                    \
diff --git a/clang/lib/Headers/avx10_2niintrin.h b/clang/lib/Headers/avx10_2niintrin.h
index c91a7b57c752..992be18f7720 100644
--- a/clang/lib/Headers/avx10_2niintrin.h
+++ b/clang/lib/Headers/avx10_2niintrin.h
@@ -402,1672 +402,6 @@ static __inline__ __m256i __DEFAULT_FN_ATTRS256 _mm256_maskz_dpwuuds_epi32(
       (__v8si)_mm256_setzero_si256());
 }
 
-/* YMM Rounding */
-#define _mm256_add_round_pd(A, B, R)                                           \
-  ((__m256d)__builtin_ia32_vaddpd256_round((__v4df)(__m256d)(A),               \
-                                           (__v4df)(__m256d)(B), (int)(R)))
-
-#define _mm256_mask_add_round_pd(W, U, A, B, R)                                \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_add_round_pd((A), (B), (R)),               \
-      (__v4df)(__m256d)(W)))
-
-#define _mm256_maskz_add_round_pd(U, A, B, R)                                  \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_add_round_pd((A), (B), (R)),               \
-      (__v4df)_mm256_setzero_pd()))
-
-#define _mm256_add_round_ph(A, B, R)                                           \
-  ((__m256h)__builtin_ia32_vaddph256_round((__v16hf)(__m256h)(A),              \
-                                           (__v16hf)(__m256h)(B), (int)(R)))
-
-#define _mm256_mask_add_round_ph(W, U, A, B, R)                                \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_add_round_ph((A), (B), (R)),             \
-      (__v16hf)(__m256h)(W)))
-
-#define _mm256_maskz_add_round_ph(U, A, B, R)                                  \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_add_round_ph((A), (B), (R)),             \
-      (__v16hf)_mm256_setzero_ph()))
-
-#define _mm256_add_round_ps(A, B, R)                                           \
-  ((__m256)__builtin_ia32_vaddps256_round((__v8sf)(__m256)(A),                 \
-                                          (__v8sf)(__m256)(B), (int)(R)))
-
-#define _mm256_mask_add_round_ps(W, U, A, B, R)                                \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_add_round_ps((A), (B), (R)),               \
-      (__v8sf)(__m256)(W)))
-
-#define _mm256_maskz_add_round_ps(U, A, B, R)                                  \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_add_round_ps((A), (B), (R)),               \
-      (__v8sf)_mm256_setzero_ps()))
-
-#define _mm256_cmp_round_pd_mask(A, B, P, R)                                   \
-  ((__mmask8)__builtin_ia32_vcmppd256_round_mask(                              \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(P), (__mmask8)-1,      \
-      (int)(R)))
-
-#define _mm256_mask_cmp_round_pd_mask(U, A, B, P, R)                           \
-  ((__mmask8)__builtin_ia32_vcmppd256_round_mask(                              \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(P), (__mmask8)(U),     \
-      (int)(R)))
-
-#define _mm256_cmp_round_ph_mask(A, B, P, R)                                   \
-  ((__mmask16)__builtin_ia32_vcmpph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (int)(P), (__mmask16)-1,   \
-      (int)(R)))
-
-#define _mm256_mask_cmp_round_ph_mask(U, A, B, P, R)                           \
-  ((__mmask16)__builtin_ia32_vcmpph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (int)(P), (__mmask16)(U),  \
-      (int)(R)))
-
-#define _mm256_cmp_round_ps_mask(A, B, P, R)                                   \
-  ((__mmask8)__builtin_ia32_vcmpps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(P), (__mmask8)-1,        \
-      (int)(R)))
-
-#define _mm256_mask_cmp_round_ps_mask(U, A, B, P, R)                           \
-  ((__mmask8)__builtin_ia32_vcmpps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(P), (__mmask8)(U),       \
-      (int)(R)))
-
-#define _mm256_cvt_roundepi32_ph(A, R)                                         \
-  ((__m128h)__builtin_ia32_vcvtdq2ph256_round_mask(                            \
-      (__v8si)(A), (__v8hf)_mm_undefined_ph(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvt_roundepi32_ph(W, U, A, R)                              \
-  ((__m128h)__builtin_ia32_vcvtdq2ph256_round_mask((__v8si)(A), (__v8hf)(W),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepi32_ph(U, A, R)                                \
-  ((__m128h)__builtin_ia32_vcvtdq2ph256_round_mask(                            \
-      (__v8si)(A), (__v8hf)_mm_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundepi32_ps(A, R)                                         \
-  ((__m256)__builtin_ia32_vcvtdq2ps256_round_mask((__v8si)(__m256i)(A),        \
-                                                  (__v8sf)_mm256_setzero_ps(), \
-                                                  (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_cvt_roundepi32_ps(W, U, A, R)                              \
-  ((__m256)__builtin_ia32_vcvtdq2ps256_round_mask(                             \
-      (__v8si)(__m256i)(A), (__v8sf)(__m256)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepi32_ps(U, A, R)                                \
-  ((__m256)__builtin_ia32_vcvtdq2ps256_round_mask((__v8si)(__m256i)(A),        \
-                                                  (__v8sf)_mm256_setzero_ps(), \
-                                                  (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundpd_epi32(A, R)                                         \
-  ((__m128i)__builtin_ia32_vcvtpd2dq256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4si)_mm_setzero_si128(), (__mmask8)-1,         \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundpd_epi32(W, U, A, R)                              \
-  ((__m128i)__builtin_ia32_vcvtpd2dq256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4si)(__m128i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundpd_epi32(U, A, R)                                \
-  ((__m128i)__builtin_ia32_vcvtpd2dq256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4si)_mm_setzero_si128(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_cvt_roundpd_ph(A, R)                                            \
-  ((__m128h)__builtin_ia32_vcvtpd2ph256_round_mask(                            \
-      (__v4df)(A), (__v8hf)_mm_undefined_ph(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvt_roundpd_ph(W, U, A, R)                                 \
-  ((__m128h)__builtin_ia32_vcvtpd2ph256_round_mask((__v4df)(A), (__v8hf)(W),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundpd_ph(U, A, R)                                   \
-  ((__m128h)__builtin_ia32_vcvtpd2ph256_round_mask(                            \
-      (__v4df)(A), (__v8hf)_mm_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundpd_ps(A, R)                                            \
-  ((__m128)__builtin_ia32_vcvtpd2ps256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4sf)_mm_setzero_ps(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_cvt_roundpd_ps(W, U, A, R)                                 \
-  ((__m128)__builtin_ia32_vcvtpd2ps256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4sf)(__m128)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundpd_ps(U, A, R)                                   \
-  ((__m128)__builtin_ia32_vcvtpd2ps256_round_mask((__v4df)(__m256d)(A),        \
-                                                  (__v4sf)_mm_setzero_ps(),    \
-                                                  (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundpd_epi64(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtpd2qq256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)-1,      \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundpd_epi64(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtpd2qq256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4di)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundpd_epi64(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtpd2qq256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)(U),     \
-      (int)(R)))
-
-#define _mm256_cvt_roundpd_epu32(A, R)                                         \
-  ((__m128i)__builtin_ia32_vcvtpd2udq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4su)_mm_setzero_si128(), (__mmask8)-1,         \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundpd_epu32(W, U, A, R)                              \
-  ((__m128i)__builtin_ia32_vcvtpd2udq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4su)(__m128i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundpd_epu32(U, A, R)                                \
-  ((__m128i)__builtin_ia32_vcvtpd2udq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4su)_mm_setzero_si128(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_cvt_roundpd_epu64(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtpd2uqq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)-1,      \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundpd_epu64(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtpd2uqq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4du)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundpd_epu64(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtpd2uqq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)(U),     \
-      (int)(R)))
-
-#define _mm256_cvt_roundph_epi32(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtph2dq256_round_mask(                            \
-      (__v8hf)(A), (__v8si)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundph_epi32(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtph2dq256_round_mask((__v8hf)(A), (__v8si)(W),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundph_epi32(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtph2dq256_round_mask(                            \
-      (__v8hf)(A), (__v8si)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundph_pd(A, R)                                            \
-  ((__m256d)__builtin_ia32_vcvtph2pd256_round_mask(                            \
-      (__v8hf)(A), (__v4df)_mm256_undefined_pd(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvt_roundph_pd(W, U, A, R)                                 \
-  ((__m256d)__builtin_ia32_vcvtph2pd256_round_mask((__v8hf)(A), (__v4df)(W),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundph_pd(U, A, R)                                   \
-  ((__m256d)__builtin_ia32_vcvtph2pd256_round_mask(                            \
-      (__v8hf)(A), (__v4df)_mm256_setzero_pd(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvtx_roundph_ps(A, R)                                           \
-  ((__m256)__builtin_ia32_vcvtph2psx256_round_mask(                            \
-      (__v8hf)(A), (__v8sf)_mm256_undefined_ps(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvtx_roundph_ps(W, U, A, R)                                \
-  ((__m256)__builtin_ia32_vcvtph2psx256_round_mask((__v8hf)(A), (__v8sf)(W),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtx_roundph_ps(U, A, R)                                  \
-  ((__m256)__builtin_ia32_vcvtph2psx256_round_mask(                            \
-      (__v8hf)(A), (__v8sf)_mm256_setzero_ps(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundph_epi64(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtph2qq256_round_mask(                            \
-      (__v8hf)(A), (__v4di)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundph_epi64(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtph2qq256_round_mask((__v8hf)(A), (__v4di)(W),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundph_epi64(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtph2qq256_round_mask(                            \
-      (__v8hf)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundph_epu32(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtph2udq256_round_mask(                           \
-      (__v8hf)(A), (__v8su)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundph_epu32(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtph2udq256_round_mask((__v8hf)(A), (__v8su)(W),  \
-                                                    (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundph_epu32(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtph2udq256_round_mask(                           \
-      (__v8hf)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundph_epu64(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtph2uqq256_round_mask(                           \
-      (__v8hf)(A), (__v4du)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundph_epu64(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtph2uqq256_round_mask((__v8hf)(A), (__v4du)(W),  \
-                                                    (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundph_epu64(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtph2uqq256_round_mask(                           \
-      (__v8hf)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundph_epu16(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtph2uw256_round_mask(                            \
-      (__v16hf)(A), (__v16hu)_mm256_undefined_si256(), (__mmask16)(-1),        \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundph_epu16(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtph2uw256_round_mask((__v16hf)(A), (__v16hu)(W), \
-                                                   (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundph_epu16(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtph2uw256_round_mask(                            \
-      (__v16hf)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)(U),           \
-      (int)(R)))
-
-#define _mm256_cvt_roundph_epi16(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtph2w256_round_mask(                             \
-      (__v16hf)(A), (__v16hi)_mm256_undefined_si256(), (__mmask16)(-1),        \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundph_epi16(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtph2w256_round_mask((__v16hf)(A), (__v16hi)(W),  \
-                                                  (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundph_epi16(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtph2w256_round_mask(                             \
-      (__v16hf)(A), (__v16hi)_mm256_setzero_si256(), (__mmask16)(U),           \
-      (int)(R)))
-
-#define _mm256_cvt_roundps_epi32(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtps2dq256_round_mask(                            \
-      (__v8sf)(__m256)(A), (__v8si)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundps_epi32(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtps2dq256_round_mask(                            \
-      (__v8sf)(__m256)(A), (__v8si)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundps_epi32(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtps2dq256_round_mask(                            \
-      (__v8sf)(__m256)(A), (__v8si)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvt_roundps_pd(A, R)                                            \
-  ((__m256d)__builtin_ia32_vcvtps2pd256_round_mask(                            \
-      (__v4sf)(__m128)(A), (__v4df)_mm256_undefined_pd(), (__mmask8)-1,        \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundps_pd(W, U, A, R)                                 \
-  ((__m256d)__builtin_ia32_vcvtps2pd256_round_mask(                            \
-      (__v4sf)(__m128)(A), (__v4df)(__m256d)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundps_pd(U, A, R)                                   \
-  ((__m256d)__builtin_ia32_vcvtps2pd256_round_mask(                            \
-      (__v4sf)(__m128)(A), (__v4df)_mm256_setzero_pd(), (__mmask8)(U),         \
-      (int)(R)))
-
-#define _mm256_cvt_roundps_ph(A, I)                                            \
-  ((__m128i)__builtin_ia32_vcvtps2ph256_mask((__v8sf)(__m256)(A), (int)(I),    \
-                                             (__v8hi)_mm_undefined_si128(),    \
-                                             (__mmask8)-1))
-
-/* FIXME: We may use these way in future.
-#define _mm256_cvt_roundps_ph(A, I)                                            \
-  ((__m128i)__builtin_ia32_vcvtps2ph256_round_mask(                            \
-      (__v8sf)(__m256)(A), (int)(I), (__v8hi)_mm_undefined_si128(),            \
-      (__mmask8)-1))
-#define _mm256_mask_cvt_roundps_ph(U, W, A, I)                                 \
-  ((__m128i)__builtin_ia32_vcvtps2ph256_round_mask(                            \
-      (__v8sf)(__m256)(A), (int)(I), (__v8hi)(__m128i)(U), (__mmask8)(W)))
-#define _mm256_maskz_cvt_roundps_ph(W, A, I)                                   \
-  ((__m128i)__builtin_ia32_vcvtps2ph256_round_mask(                            \
-      (__v8sf)(__m256)(A), (int)(I), (__v8hi)_mm_setzero_si128(),              \
-      (__mmask8)(W))) */
-
-#define _mm256_cvtx_roundps_ph(A, R)                                           \
-  ((__m128h)__builtin_ia32_vcvtps2phx256_round_mask(                           \
-      (__v8sf)(A), (__v8hf)_mm_undefined_ph(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvtx_roundps_ph(W, U, A, R)                                \
-  ((__m128h)__builtin_ia32_vcvtps2phx256_round_mask((__v8sf)(A), (__v8hf)(W),  \
-                                                    (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtx_roundps_ph(U, A, R)                                  \
-  ((__m128h)__builtin_ia32_vcvtps2phx256_round_mask(                           \
-      (__v8sf)(A), (__v8hf)_mm_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundps_epi64(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtps2qq256_round_mask(                            \
-      (__v4sf)(__m128)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundps_epi64(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtps2qq256_round_mask(                            \
-      (__v4sf)(__m128)(A), (__v4di)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundps_epi64(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtps2qq256_round_mask(                            \
-      (__v4sf)(__m128)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvt_roundps_epu32(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtps2udq256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundps_epu32(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtps2udq256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8su)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundps_epu32(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtps2udq256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvt_roundps_epu64(A, R)                                         \
-  ((__m256i)__builtin_ia32_vcvtps2uqq256_round_mask(                           \
-      (__v4sf)(__m128)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundps_epu64(W, U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvtps2uqq256_round_mask(                           \
-      (__v4sf)(__m128)(A), (__v4du)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundps_epu64(U, A, R)                                \
-  ((__m256i)__builtin_ia32_vcvtps2uqq256_round_mask(                           \
-      (__v4sf)(__m128)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvt_roundepi64_pd(A, R)                                         \
-  ((__m256d)__builtin_ia32_vcvtqq2pd256_round_mask(                            \
-      (__v4di)(__m256i)(A), (__v4df)_mm256_setzero_pd(), (__mmask8)-1,         \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundepi64_pd(W, U, A, R)                              \
-  ((__m256d)__builtin_ia32_vcvtqq2pd256_round_mask(                            \
-      (__v4di)(__m256i)(A), (__v4df)(__m256d)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepi64_pd(U, A, R)                                \
-  ((__m256d)__builtin_ia32_vcvtqq2pd256_round_mask(                            \
-      (__v4di)(__m256i)(A), (__v4df)_mm256_setzero_pd(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_cvt_roundepi64_ph(A, R)                                         \
-  ((__m128h)__builtin_ia32_vcvtqq2ph256_round_mask(                            \
-      (__v4di)(A), (__v8hf)_mm_undefined_ph(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvt_roundepi64_ph(W, U, A, R)                              \
-  ((__m128h)__builtin_ia32_vcvtqq2ph256_round_mask((__v4di)(A), (__v8hf)(W),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepi64_ph(U, A, R)                                \
-  ((__m128h)__builtin_ia32_vcvtqq2ph256_round_mask(                            \
-      (__v4di)(A), (__v8hf)_mm_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundepi64_ps(A, R)                                         \
-  ((__m128)__builtin_ia32_vcvtqq2ps256_round_mask(                             \
-      (__v4di)(__m256i)(A), (__v4sf)_mm_setzero_ps(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_cvt_roundepi64_ps(W, U, A, R)                              \
-  ((__m128)__builtin_ia32_vcvtqq2ps256_round_mask(                             \
-      (__v4di)(__m256i)(A), (__v4sf)(__m128)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepi64_ps(U, A, R)                                \
-  ((__m128)__builtin_ia32_vcvtqq2ps256_round_mask((__v4di)(__m256i)(A),        \
-                                                  (__v4sf)_mm_setzero_ps(),    \
-                                                  (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvtt_roundpd_epi32(A, R)                                        \
-  ((__m128i)__builtin_ia32_vcvttpd2dq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4si)_mm_setzero_si128(), (__mmask8)-1,         \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundpd_epi32(W, U, A, R)                             \
-  ((__m128i)__builtin_ia32_vcvttpd2dq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4si)(__m128i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundpd_epi32(U, A, R)                               \
-  ((__m128i)__builtin_ia32_vcvttpd2dq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4si)_mm_setzero_si128(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_cvtt_roundpd_epi64(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttpd2qq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)-1,      \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundpd_epi64(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttpd2qq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4di)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundpd_epi64(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttpd2qq256_round_mask(                           \
-      (__v4df)(__m256d)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)(U),     \
-      (int)(R)))
-
-#define _mm256_cvtt_roundpd_epu32(A, R)                                        \
-  ((__m128i)__builtin_ia32_vcvttpd2udq256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4su)_mm_setzero_si128(), (__mmask8)-1,         \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundpd_epu32(W, U, A, R)                             \
-  ((__m128i)__builtin_ia32_vcvttpd2udq256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4su)(__m128i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundpd_epu32(U, A, R)                               \
-  ((__m128i)__builtin_ia32_vcvttpd2udq256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4su)_mm_setzero_si128(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_cvtt_roundpd_epu64(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttpd2uqq256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)-1,      \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundpd_epu64(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttpd2uqq256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4du)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundpd_epu64(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttpd2uqq256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)(U),     \
-      (int)(R)))
-
-#define _mm256_cvtt_roundph_epi32(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttph2dq256_round_mask(                           \
-      (__v8hf)(A), (__v8si)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundph_epi32(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttph2dq256_round_mask((__v8hf)(A), (__v8si)(W),  \
-                                                    (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundph_epi32(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttph2dq256_round_mask(                           \
-      (__v8hf)(A), (__v8si)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvtt_roundph_epi64(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttph2qq256_round_mask(                           \
-      (__v8hf)(A), (__v4di)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundph_epi64(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttph2qq256_round_mask((__v8hf)(A), (__v4di)(W),  \
-                                                    (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundph_epi64(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttph2qq256_round_mask(                           \
-      (__v8hf)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvtt_roundph_epu32(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttph2udq256_round_mask(                          \
-      (__v8hf)(A), (__v8su)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundph_epu32(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttph2udq256_round_mask((__v8hf)(A), (__v8su)(W), \
-                                                     (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundph_epu32(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttph2udq256_round_mask(                          \
-      (__v8hf)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvtt_roundph_epu64(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttph2uqq256_round_mask(                          \
-      (__v8hf)(A), (__v4du)_mm256_undefined_si256(), (__mmask8)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundph_epu64(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttph2uqq256_round_mask((__v8hf)(A), (__v4du)(W), \
-                                                     (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundph_epu64(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttph2uqq256_round_mask(                          \
-      (__v8hf)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvtt_roundph_epu16(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttph2uw256_round_mask(                           \
-      (__v16hf)(A), (__v16hu)_mm256_undefined_si256(), (__mmask16)(-1),        \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundph_epu16(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttph2uw256_round_mask(                           \
-      (__v16hf)(A), (__v16hu)(W), (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundph_epu16(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttph2uw256_round_mask(                           \
-      (__v16hf)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)(U),           \
-      (int)(R)))
-
-#define _mm256_cvtt_roundph_epi16(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttph2w256_round_mask(                            \
-      (__v16hf)(A), (__v16hi)_mm256_undefined_si256(), (__mmask16)(-1),        \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundph_epi16(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttph2w256_round_mask((__v16hf)(A), (__v16hi)(W), \
-                                                   (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundph_epi16(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttph2w256_round_mask(                            \
-      (__v16hf)(A), (__v16hi)_mm256_setzero_si256(), (__mmask16)(U),           \
-      (int)(R)))
-
-#define _mm256_cvtt_roundps_epi32(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttps2dq256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8si)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundps_epi32(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttps2dq256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8si)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundps_epi32(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttps2dq256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8si)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvtt_roundps_epi64(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttps2qq256_round_mask(                           \
-      (__v4sf)(__m128)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundps_epi64(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttps2qq256_round_mask(                           \
-      (__v4sf)(__m128)(A), (__v4di)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundps_epi64(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttps2qq256_round_mask(                           \
-      (__v4sf)(__m128)(A), (__v4di)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvtt_roundps_epu32(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttps2udq256_round_mask(                          \
-      (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundps_epu32(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttps2udq256_round_mask(                          \
-      (__v8sf)(__m256)(A), (__v8su)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundps_epu32(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttps2udq256_round_mask(                          \
-      (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvtt_roundps_epu64(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvttps2uqq256_round_mask(                          \
-      (__v4sf)(__m128)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_cvtt_roundps_epu64(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvttps2uqq256_round_mask(                          \
-      (__v4sf)(__m128)(A), (__v4du)(__m256i)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvtt_roundps_epu64(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvttps2uqq256_round_mask(                          \
-      (__v4sf)(__m128)(A), (__v4du)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (int)(R)))
-
-#define _mm256_cvt_roundepu32_ph(A, R)                                         \
-  ((__m128h)__builtin_ia32_vcvtudq2ph256_round_mask(                           \
-      (__v8su)(A), (__v8hf)_mm_undefined_ph(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvt_roundepu32_ph(W, U, A, R)                              \
-  ((__m128h)__builtin_ia32_vcvtudq2ph256_round_mask((__v8su)(A), (__v8hf)(W),  \
-                                                    (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepu32_ph(U, A, R)                                \
-  ((__m128h)__builtin_ia32_vcvtudq2ph256_round_mask(                           \
-      (__v8su)(A), (__v8hf)_mm_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundepu32_ps(A, R)                                         \
-  ((__m256)__builtin_ia32_vcvtudq2ps256_round_mask(                            \
-      (__v8su)(__m256i)(A), (__v8sf)_mm256_setzero_ps(), (__mmask8)-1,         \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundepu32_ps(W, U, A, R)                              \
-  ((__m256)__builtin_ia32_vcvtudq2ps256_round_mask(                            \
-      (__v8su)(__m256i)(A), (__v8sf)(__m256)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepu32_ps(U, A, R)                                \
-  ((__m256)__builtin_ia32_vcvtudq2ps256_round_mask(                            \
-      (__v8su)(__m256i)(A), (__v8sf)_mm256_setzero_ps(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_cvt_roundepu64_pd(A, R)                                         \
-  ((__m256d)__builtin_ia32_vcvtuqq2pd256_round_mask(                           \
-      (__v4du)(__m256i)(A), (__v4df)_mm256_setzero_pd(), (__mmask8)-1,         \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundepu64_pd(W, U, A, R)                              \
-  ((__m256d)__builtin_ia32_vcvtuqq2pd256_round_mask(                           \
-      (__v4du)(__m256i)(A), (__v4df)(__m256d)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepu64_pd(U, A, R)                                \
-  ((__m256d)__builtin_ia32_vcvtuqq2pd256_round_mask(                           \
-      (__v4du)(__m256i)(A), (__v4df)_mm256_setzero_pd(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_cvt_roundepu64_ph(A, R)                                         \
-  ((__m128h)__builtin_ia32_vcvtuqq2ph256_round_mask(                           \
-      (__v4du)(A), (__v8hf)_mm_undefined_ph(), (__mmask8)(-1), (int)(R)))
-
-#define _mm256_mask_cvt_roundepu64_ph(W, U, A, R)                              \
-  ((__m128h)__builtin_ia32_vcvtuqq2ph256_round_mask((__v4du)(A), (__v8hf)(W),  \
-                                                    (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepu64_ph(U, A, R)                                \
-  ((__m128h)__builtin_ia32_vcvtuqq2ph256_round_mask(                           \
-      (__v4du)(A), (__v8hf)_mm_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundepu64_ps(A, R)                                         \
-  ((__m128)__builtin_ia32_vcvtuqq2ps256_round_mask(                            \
-      (__v4du)(__m256i)(A), (__v4sf)_mm_setzero_ps(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_cvt_roundepu64_ps(W, U, A, R)                              \
-  ((__m128)__builtin_ia32_vcvtuqq2ps256_round_mask(                            \
-      (__v4du)(__m256i)(A), (__v4sf)(__m128)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepu64_ps(U, A, R)                                \
-  ((__m128)__builtin_ia32_vcvtuqq2ps256_round_mask((__v4du)(__m256i)(A),       \
-                                                   (__v4sf)_mm_setzero_ps(),   \
-                                                   (__mmask8)(U), (int)(R)))
-
-#define _mm256_cvt_roundepu16_ph(A, R)                                         \
-  ((__m256h)__builtin_ia32_vcvtuw2ph256_round_mask(                            \
-      (__v16hu)(A), (__v16hf)_mm256_undefined_ph(), (__mmask16)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundepu16_ph(W, U, A, R)                              \
-  ((__m256h)__builtin_ia32_vcvtuw2ph256_round_mask((__v16hu)(A), (__v16hf)(W), \
-                                                   (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepu16_ph(U, A, R)                                \
-  ((__m256h)__builtin_ia32_vcvtuw2ph256_round_mask(                            \
-      (__v16hu)(A), (__v16hf)_mm256_setzero_ph(), (__mmask16)(U), (int)(R)))
-
-#define _mm256_cvt_roundepi16_ph(A, R)                                         \
-  ((__m256h)__builtin_ia32_vcvtw2ph256_round_mask(                             \
-      (__v16hi)(A), (__v16hf)_mm256_undefined_ph(), (__mmask16)(-1),           \
-      (int)(R)))
-
-#define _mm256_mask_cvt_roundepi16_ph(W, U, A, R)                              \
-  ((__m256h)__builtin_ia32_vcvtw2ph256_round_mask((__v16hi)(A), (__v16hf)(W),  \
-                                                  (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_cvt_roundepi16_ph(U, A, R)                                \
-  ((__m256h)__builtin_ia32_vcvtw2ph256_round_mask(                             \
-      (__v16hi)(A), (__v16hf)_mm256_setzero_ph(), (__mmask16)(U), (int)(R)))
-
-#define _mm256_div_round_pd(A, B, R)                                           \
-  ((__m256d)__builtin_ia32_vdivpd256_round((__v4df)(__m256d)(A),               \
-                                           (__v4df)(__m256d)(B), (int)(R)))
-
-#define _mm256_mask_div_round_pd(W, U, A, B, R)                                \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_div_round_pd((A), (B), (R)),               \
-      (__v4df)(__m256d)(W)))
-
-#define _mm256_maskz_div_round_pd(U, A, B, R)                                  \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_div_round_pd((A), (B), (R)),               \
-      (__v4df)_mm256_setzero_pd()))
-
-#define _mm256_div_round_ph(A, B, R)                                           \
-  ((__m256h)__builtin_ia32_vdivph256_round((__v16hf)(__m256h)(A),              \
-                                           (__v16hf)(__m256h)(B), (int)(R)))
-
-#define _mm256_mask_div_round_ph(W, U, A, B, R)                                \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_div_round_ph((A), (B), (R)),             \
-      (__v16hf)(__m256h)(W)))
-
-#define _mm256_maskz_div_round_ph(U, A, B, R)                                  \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_div_round_ph((A), (B), (R)),             \
-      (__v16hf)_mm256_setzero_ph()))
-
-#define _mm256_div_round_ps(A, B, R)                                           \
-  ((__m256)__builtin_ia32_vdivps256_round((__v8sf)(__m256)(A),                 \
-                                          (__v8sf)(__m256)(B), (int)(R)))
-
-#define _mm256_mask_div_round_ps(W, U, A, B, R)                                \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_div_round_ps((A), (B), (R)),               \
-      (__v8sf)(__m256)(W)))
-
-#define _mm256_maskz_div_round_ps(U, A, B, R)                                  \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_div_round_ps((A), (B), (R)),               \
-      (__v8sf)_mm256_setzero_ps()))
-
-#define _mm256_fcmadd_round_pch(A, B, C, R)                                    \
-  ((__m256h)__builtin_ia32_vfcmaddcph256_round_mask3(                          \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fcmadd_round_pch(A, U, B, C, R)                            \
-  ((__m256h)__builtin_ia32_vfcmaddcph256_round_mask(                           \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fcmadd_round_pch(A, B, C, U, R)                           \
-  ((__m256h)__builtin_ia32_vfcmaddcph256_round_mask3(                          \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fcmadd_round_pch(U, A, B, C, R)                           \
-  ((__m256h)__builtin_ia32_vfcmaddcph256_round_maskz(                          \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_cmul_round_pch(A, B, R)                                         \
-  ((__m256h)__builtin_ia32_vfcmulcph256_round_mask(                            \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B),                              \
-      (__v8sf)(__m256h)_mm256_undefined_ph(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_cmul_round_pch(W, U, A, B, R)                              \
-  ((__m256h)__builtin_ia32_vfcmulcph256_round_mask(                            \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(W),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_cmul_round_pch(U, A, B, R)                                \
-  ((__m256h)__builtin_ia32_vfcmulcph256_round_mask(                            \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B),                              \
-      (__v8sf)(__m256h)_mm256_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_fixupimm_round_pd(A, B, C, imm, R)                              \
-  ((__m256d)__builtin_ia32_vfixupimmpd256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4di)(__m256i)(C),        \
-      (int)(imm), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fixupimm_round_pd(A, U, B, C, imm, R)                      \
-  ((__m256d)__builtin_ia32_vfixupimmpd256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4di)(__m256i)(C),        \
-      (int)(imm), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fixupimm_round_pd(U, A, B, C, imm, R)                     \
-  ((__m256d)__builtin_ia32_vfixupimmpd256_round_maskz(                         \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4di)(__m256i)(C),        \
-      (int)(imm), (__mmask8)(U), (int)(R)))
-
-#define _mm256_fixupimm_round_ps(A, B, C, imm, R)                              \
-  ((__m256)__builtin_ia32_vfixupimmps256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8si)(__m256i)(C),          \
-      (int)(imm), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fixupimm_round_ps(A, U, B, C, imm, R)                      \
-  ((__m256)__builtin_ia32_vfixupimmps256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8si)(__m256i)(C),          \
-      (int)(imm), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fixupimm_round_ps(U, A, B, C, imm, R)                     \
-  ((__m256)__builtin_ia32_vfixupimmps256_round_maskz(                          \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8si)(__m256i)(C),          \
-      (int)(imm), (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmadd_round_pd(A, B, C, R)                                      \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmadd_round_pd(A, U, B, C, R)                              \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmadd_round_pd(A, B, C, U, R)                             \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask3(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmadd_round_pd(U, A, B, C, R)                             \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_maskz(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmsub_round_pd(A, B, C, R)                                      \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),       \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmsub_round_pd(A, U, B, C, R)                              \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmsub_round_pd(U, A, B, C, R)                             \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_maskz(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fnmadd_round_pd(A, B, C, R)                                     \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      -(__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),       \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask3_fnmadd_round_pd(A, B, C, U, R)                            \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask3(                            \
-      -(__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fnmadd_round_pd(U, A, B, C, R)                            \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_maskz(                            \
-      -(__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fnmsub_round_pd(A, B, C, R)                                     \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      -(__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),      \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_maskz_fnmsub_round_pd(U, A, B, C, R)                            \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_maskz(                            \
-      -(__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),      \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmadd_round_ph(A, B, C, R)                                      \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_fmadd_round_ph(A, U, B, C, R)                              \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_mask3_fmadd_round_ph(A, B, C, U, R)                             \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask3(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_fmadd_round_ph(U, A, B, C, R)                             \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_maskz(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_fmsub_round_ph(A, B, C, R)                                      \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),    \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_fmsub_round_ph(A, U, B, C, R)                              \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_fmsub_round_ph(U, A, B, C, R)                             \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_maskz(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_fnmadd_round_ph(A, B, C, R)                                     \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), -(__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),    \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask3_fnmadd_round_ph(A, B, C, U, R)                            \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask3(                            \
-      -(__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_fnmadd_round_ph(U, A, B, C, R)                            \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_maskz(                            \
-      -(__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_fnmsub_round_ph(A, B, C, R)                                     \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), -(__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),   \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_maskz_fnmsub_round_ph(U, A, B, C, R)                            \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_maskz(                            \
-      -(__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),   \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_fmadd_round_ps(A, B, C, R)                                      \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmadd_round_ps(A, U, B, C, R)                              \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmadd_round_ps(A, B, C, U, R)                             \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask3(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmadd_round_ps(U, A, B, C, R)                             \
-  ((__m256)__builtin_ia32_vfmaddps256_round_maskz(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmsub_round_ps(A, B, C, R)                                      \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),          \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmsub_round_ps(A, U, B, C, R)                              \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmsub_round_ps(U, A, B, C, R)                             \
-  ((__m256)__builtin_ia32_vfmaddps256_round_maskz(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fnmadd_round_ps(A, B, C, R)                                     \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), -(__v8sf)(__m256)(B), (__v8sf)(__m256)(C),          \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask3_fnmadd_round_ps(A, B, C, U, R)                            \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask3(                             \
-      -(__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fnmadd_round_ps(U, A, B, C, R)                            \
-  ((__m256)__builtin_ia32_vfmaddps256_round_maskz(                             \
-      -(__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fnmsub_round_ps(A, B, C, R)                                     \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), -(__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),         \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_maskz_fnmsub_round_ps(U, A, B, C, R)                            \
-  ((__m256)__builtin_ia32_vfmaddps256_round_maskz(                             \
-      -(__v8sf)(__m256)(A), (__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),         \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmadd_round_pch(A, B, C, R)                                     \
-  ((__m256h)__builtin_ia32_vfmaddcph256_round_mask3(                           \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmadd_round_pch(A, U, B, C, R)                             \
-  ((__m256h)__builtin_ia32_vfmaddcph256_round_mask(                            \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmadd_round_pch(A, B, C, U, R)                            \
-  ((__m256h)__builtin_ia32_vfmaddcph256_round_mask3(                           \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmadd_round_pch(U, A, B, C, R)                            \
-  ((__m256h)__builtin_ia32_vfmaddcph256_round_maskz(                           \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmaddsub_round_pd(A, B, C, R)                                   \
-  ((__m256d)__builtin_ia32_vfmaddsubpd256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmaddsub_round_pd(A, U, B, C, R)                           \
-  ((__m256d)__builtin_ia32_vfmaddsubpd256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmaddsub_round_pd(A, B, C, U, R)                          \
-  ((__m256d)__builtin_ia32_vfmaddsubpd256_round_mask3(                         \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmaddsub_round_pd(U, A, B, C, R)                          \
-  ((__m256d)__builtin_ia32_vfmaddsubpd256_round_maskz(                         \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmsubadd_round_pd(A, B, C, R)                                   \
-  ((__m256d)__builtin_ia32_vfmaddsubpd256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),       \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmsubadd_round_pd(A, U, B, C, R)                           \
-  ((__m256d)__builtin_ia32_vfmaddsubpd256_round_mask(                          \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmsubadd_round_pd(U, A, B, C, R)                          \
-  ((__m256d)__builtin_ia32_vfmaddsubpd256_round_maskz(                         \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmaddsub_round_ph(A, B, C, R)                                   \
-  ((__m256h)__builtin_ia32_vfmaddsubph256_round_mask(                          \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_fmaddsub_round_ph(A, U, B, C, R)                           \
-  ((__m256h)__builtin_ia32_vfmaddsubph256_round_mask(                          \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_mask3_fmaddsub_round_ph(A, B, C, U, R)                          \
-  ((__m256h)__builtin_ia32_vfmaddsubph256_round_mask3(                         \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_fmaddsub_round_ph(U, A, B, C, R)                          \
-  ((__m256h)__builtin_ia32_vfmaddsubph256_round_maskz(                         \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_fmsubadd_round_ph(A, B, C, R)                                   \
-  ((__m256h)__builtin_ia32_vfmaddsubph256_round_mask(                          \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),    \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_fmsubadd_round_ph(A, U, B, C, R)                           \
-  ((__m256h)__builtin_ia32_vfmaddsubph256_round_mask(                          \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_fmsubadd_round_ph(U, A, B, C, R)                          \
-  ((__m256h)__builtin_ia32_vfmaddsubph256_round_maskz(                         \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_fmaddsub_round_ps(A, B, C, R)                                   \
-  ((__m256)__builtin_ia32_vfmaddsubps256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmaddsub_round_ps(A, U, B, C, R)                           \
-  ((__m256)__builtin_ia32_vfmaddsubps256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmaddsub_round_ps(A, B, C, U, R)                          \
-  ((__m256)__builtin_ia32_vfmaddsubps256_round_mask3(                          \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmaddsub_round_ps(U, A, B, C, R)                          \
-  ((__m256)__builtin_ia32_vfmaddsubps256_round_maskz(                          \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_fmsubadd_round_ps(A, B, C, R)                                   \
-  ((__m256)__builtin_ia32_vfmaddsubps256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),          \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_fmsubadd_round_ps(A, U, B, C, R)                           \
-  ((__m256)__builtin_ia32_vfmaddsubps256_round_mask(                           \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_fmsubadd_round_ps(U, A, B, C, R)                          \
-  ((__m256)__builtin_ia32_vfmaddsubps256_round_maskz(                          \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-#define _mm256_mask3_fmsub_round_pd(A, B, C, U, R)                             \
-  ((__m256d)__builtin_ia32_vfmsubpd256_round_mask3(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmsubadd_round_pd(A, B, C, U, R)                          \
-  ((__m256d)__builtin_ia32_vfmsubaddpd256_round_mask3(                         \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask_fnmadd_round_pd(A, U, B, C, R)                             \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      (__v4df)(__m256d)(A), -(__v4df)(__m256d)(B), (__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask_fnmsub_round_pd(A, U, B, C, R)                             \
-  ((__m256d)__builtin_ia32_vfmaddpd256_round_mask(                             \
-      (__v4df)(__m256d)(A), -(__v4df)(__m256d)(B), -(__v4df)(__m256d)(C),      \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fnmsub_round_pd(A, B, C, U, R)                            \
-  ((__m256d)__builtin_ia32_vfmsubpd256_round_mask3(                            \
-      -(__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(C),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmsub_round_ph(A, B, C, U, R)                             \
-  ((__m256h)__builtin_ia32_vfmsubph256_round_mask3(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_mask3_fmsubadd_round_ph(A, B, C, U, R)                          \
-  ((__m256h)__builtin_ia32_vfmsubaddph256_round_mask3(                         \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_mask_fnmadd_round_ph(A, U, B, C, R)                             \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), -(__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_mask_fnmsub_round_ph(A, U, B, C, R)                             \
-  ((__m256h)__builtin_ia32_vfmaddph256_round_mask(                             \
-      (__v16hf)(__m256h)(A), -(__v16hf)(__m256h)(B), -(__v16hf)(__m256h)(C),   \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_mask3_fnmsub_round_ph(A, B, C, U, R)                            \
-  ((__m256h)__builtin_ia32_vfmsubph256_round_mask3(                            \
-      -(__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(C),    \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_mask3_fmsub_round_ps(A, B, C, U, R)                             \
-  ((__m256)__builtin_ia32_vfmsubps256_round_mask3(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fmsubadd_round_ps(A, B, C, U, R)                          \
-  ((__m256)__builtin_ia32_vfmsubaddps256_round_mask3(                          \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask_fnmadd_round_ps(A, U, B, C, R)                             \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), -(__v8sf)(__m256)(B), (__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask_fnmsub_round_ps(A, U, B, C, R)                             \
-  ((__m256)__builtin_ia32_vfmaddps256_round_mask(                              \
-      (__v8sf)(__m256)(A), -(__v8sf)(__m256)(B), -(__v8sf)(__m256)(C),         \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask3_fnmsub_round_ps(A, B, C, U, R)                            \
-  ((__m256)__builtin_ia32_vfmsubps256_round_mask3(                             \
-      -(__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(C),          \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mul_round_pch(A, B, R)                                          \
-  ((__m256h)__builtin_ia32_vfmulcph256_round_mask(                             \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B),                              \
-      (__v8sf)(__m256h)_mm256_undefined_ph(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_mul_round_pch(W, U, A, B, R)                               \
-  ((__m256h)__builtin_ia32_vfmulcph256_round_mask(                             \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B), (__v8sf)(__m256h)(W),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_mul_round_pch(U, A, B, R)                                 \
-  ((__m256h)__builtin_ia32_vfmulcph256_round_mask(                             \
-      (__v8sf)(__m256h)(A), (__v8sf)(__m256h)(B),                              \
-      (__v8sf)(__m256h)_mm256_setzero_ph(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_getexp_round_pd(A, R)                                           \
-  ((__m256d)__builtin_ia32_vgetexppd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)_mm256_undefined_pd(), (__mmask8)-1,       \
-      (int)(R)))
-
-#define _mm256_mask_getexp_round_pd(W, U, A, R)                                \
-  ((__m256d)__builtin_ia32_vgetexppd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_getexp_round_pd(U, A, R)                                  \
-  ((__m256d)__builtin_ia32_vgetexppd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)_mm256_setzero_pd(), (__mmask8)(U),        \
-      (int)(R)))
-
-#define _mm256_getexp_round_ph(A, R)                                           \
-  ((__m256h)__builtin_ia32_vgetexpph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)_mm256_undefined_ph(), (__mmask16)-1,    \
-      (int)(R)))
-
-#define _mm256_mask_getexp_round_ph(W, U, A, R)                                \
-  ((__m256h)__builtin_ia32_vgetexpph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(W), (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_getexp_round_ph(U, A, R)                                  \
-  ((__m256h)__builtin_ia32_vgetexpph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)_mm256_setzero_ph(), (__mmask16)(U),     \
-      (int)(R)))
-
-#define _mm256_getexp_round_ps(A, R)                                           \
-  ((__m256)__builtin_ia32_vgetexpps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)_mm256_undefined_ps(), (__mmask8)-1,        \
-      (int)(R)))
-
-#define _mm256_mask_getexp_round_ps(W, U, A, R)                                \
-  ((__m256)__builtin_ia32_vgetexpps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_getexp_round_ps(U, A, R)                                  \
-  ((__m256)__builtin_ia32_vgetexpps256_round_mask((__v8sf)(__m256)(A),         \
-                                                  (__v8sf)_mm256_setzero_ps(), \
-                                                  (__mmask8)(U), (int)(R)))
-
-#define _mm256_getmant_round_pd(A, B, C, R)                                    \
-  ((__m256d)__builtin_ia32_vgetmantpd256_round_mask(                           \
-      (__v4df)(__m256d)(A), (int)(((C) << 2) | (B)),                           \
-      (__v4df)_mm256_undefined_pd(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_getmant_round_pd(W, U, A, B, C, R)                         \
-  ((__m256d)__builtin_ia32_vgetmantpd256_round_mask(                           \
-      (__v4df)(__m256d)(A), (int)(((C) << 2) | (B)), (__v4df)(__m256d)(W),     \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_getmant_round_pd(U, A, B, C, R)                           \
-  ((__m256d)__builtin_ia32_vgetmantpd256_round_mask(                           \
-      (__v4df)(__m256d)(A), (int)(((C) << 2) | (B)),                           \
-      (__v4df)_mm256_setzero_pd(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_getmant_round_ph(A, B, C, R)                                    \
-  ((__m256h)__builtin_ia32_vgetmantph256_round_mask(                           \
-      (__v16hf)(__m256h)(A), (int)(((C) << 2) | (B)),                          \
-      (__v16hf)_mm256_undefined_ph(), (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_getmant_round_ph(W, U, A, B, C, R)                         \
-  ((__m256h)__builtin_ia32_vgetmantph256_round_mask(                           \
-      (__v16hf)(__m256h)(A), (int)(((C) << 2) | (B)), (__v16hf)(__m256h)(W),   \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_getmant_round_ph(U, A, B, C, R)                           \
-  ((__m256h)__builtin_ia32_vgetmantph256_round_mask(                           \
-      (__v16hf)(__m256h)(A), (int)(((C) << 2) | (B)),                          \
-      (__v16hf)_mm256_setzero_ph(), (__mmask16)(U), (int)(R)))
-
-#define _mm256_getmant_round_ps(A, B, C, R)                                    \
-  ((__m256)__builtin_ia32_vgetmantps256_round_mask(                            \
-      (__v8sf)(__m256)(A), (int)(((C) << 2) | (B)),                            \
-      (__v8sf)_mm256_undefined_ps(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_getmant_round_ps(W, U, A, B, C, R)                         \
-  ((__m256)__builtin_ia32_vgetmantps256_round_mask(                            \
-      (__v8sf)(__m256)(A), (int)(((C) << 2) | (B)), (__v8sf)(__m256)(W),       \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_getmant_round_ps(U, A, B, C, R)                           \
-  ((__m256)__builtin_ia32_vgetmantps256_round_mask(                            \
-      (__v8sf)(__m256)(A), (int)(((C) << 2) | (B)),                            \
-      (__v8sf)_mm256_setzero_ps(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_max_round_pd(A, B, R)                                           \
-  ((__m256d)__builtin_ia32_vmaxpd256_round((__v4df)(__m256d)(A),               \
-                                           (__v4df)(__m256d)(B), (int)(R)))
-
-#define _mm256_mask_max_round_pd(W, U, A, B, R)                                \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_max_round_pd((A), (B), (R)),               \
-      (__v4df)(__m256d)(W)))
-
-#define _mm256_maskz_max_round_pd(U, A, B, R)                                  \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_max_round_pd((A), (B), (R)),               \
-      (__v4df)_mm256_setzero_pd()))
-
-#define _mm256_max_round_ph(A, B, R)                                           \
-  ((__m256h)__builtin_ia32_vmaxph256_round((__v16hf)(__m256h)(A),              \
-                                           (__v16hf)(__m256h)(B), (int)(R)))
-
-#define _mm256_mask_max_round_ph(W, U, A, B, R)                                \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_max_round_ph((A), (B), (R)),             \
-      (__v16hf)(__m256h)(W)))
-
-#define _mm256_maskz_max_round_ph(U, A, B, R)                                  \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_max_round_ph((A), (B), (R)),             \
-      (__v16hf)_mm256_setzero_ph()))
-
-#define _mm256_max_round_ps(A, B, R)                                           \
-  ((__m256)__builtin_ia32_vmaxps256_round((__v8sf)(__m256)(A),                 \
-                                          (__v8sf)(__m256)(B), (int)(R)))
-
-#define _mm256_mask_max_round_ps(W, U, A, B, R)                                \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_max_round_ps((A), (B), (R)),               \
-      (__v8sf)(__m256)(W)))
-
-#define _mm256_maskz_max_round_ps(U, A, B, R)                                  \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_max_round_ps((A), (B), (R)),               \
-      (__v8sf)_mm256_setzero_ps()))
-
-#define _mm256_min_round_pd(A, B, R)                                           \
-  ((__m256d)__builtin_ia32_vminpd256_round((__v4df)(__m256d)(A),               \
-                                           (__v4df)(__m256d)(B), (int)(R)))
-
-#define _mm256_mask_min_round_pd(W, U, A, B, R)                                \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_min_round_pd((A), (B), (R)),               \
-      (__v4df)(__m256d)(W)))
-
-#define _mm256_maskz_min_round_pd(U, A, B, R)                                  \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_min_round_pd((A), (B), (R)),               \
-      (__v4df)_mm256_setzero_pd()))
-
-#define _mm256_min_round_ph(A, B, R)                                           \
-  ((__m256h)__builtin_ia32_vminph256_round((__v16hf)(__m256h)(A),              \
-                                           (__v16hf)(__m256h)(B), (int)(R)))
-
-#define _mm256_mask_min_round_ph(W, U, A, B, R)                                \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_min_round_ph((A), (B), (R)),             \
-      (__v16hf)(__m256h)(W)))
-
-#define _mm256_maskz_min_round_ph(U, A, B, R)                                  \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_min_round_ph((A), (B), (R)),             \
-      (__v16hf)_mm256_setzero_ph()))
-
-#define _mm256_min_round_ps(A, B, R)                                           \
-  ((__m256)__builtin_ia32_vminps256_round((__v8sf)(__m256)(A),                 \
-                                          (__v8sf)(__m256)(B), (int)(R)))
-
-#define _mm256_mask_min_round_ps(W, U, A, B, R)                                \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_min_round_ps((A), (B), (R)),               \
-      (__v8sf)(__m256)(W)))
-
-#define _mm256_maskz_min_round_ps(U, A, B, R)                                  \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_min_round_ps((A), (B), (R)),               \
-      (__v8sf)_mm256_setzero_ps()))
-
-#define _mm256_mul_round_pd(A, B, R)                                           \
-  ((__m256d)__builtin_ia32_vmulpd256_round((__v4df)(__m256d)(A),               \
-                                           (__v4df)(__m256d)(B), (int)(R)))
-
-#define _mm256_mask_mul_round_pd(W, U, A, B, R)                                \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_mul_round_pd((A), (B), (R)),               \
-      (__v4df)(__m256d)(W)))
-
-#define _mm256_maskz_mul_round_pd(U, A, B, R)                                  \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_mul_round_pd((A), (B), (R)),               \
-      (__v4df)_mm256_setzero_pd()))
-
-#define _mm256_mul_round_ph(A, B, R)                                           \
-  ((__m256h)__builtin_ia32_vmulph256_round((__v16hf)(__m256h)(A),              \
-                                           (__v16hf)(__m256h)(B), (int)(R)))
-
-#define _mm256_mask_mul_round_ph(W, U, A, B, R)                                \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_mul_round_ph((A), (B), (R)),             \
-      (__v16hf)(__m256h)(W)))
-
-#define _mm256_maskz_mul_round_ph(U, A, B, R)                                  \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_mul_round_ph((A), (B), (R)),             \
-      (__v16hf)_mm256_setzero_ph()))
-
-#define _mm256_mul_round_ps(A, B, R)                                           \
-  ((__m256)__builtin_ia32_vmulps256_round((__v8sf)(__m256)(A),                 \
-                                          (__v8sf)(__m256)(B), (int)(R)))
-
-#define _mm256_mask_mul_round_ps(W, U, A, B, R)                                \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_mul_round_ps((A), (B), (R)),               \
-      (__v8sf)(__m256)(W)))
-
-#define _mm256_maskz_mul_round_ps(U, A, B, R)                                  \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_mul_round_ps((A), (B), (R)),               \
-      (__v8sf)_mm256_setzero_ps()))
-
-#define _mm256_range_round_pd(A, B, C, R)                                      \
-  ((__m256d)__builtin_ia32_vrangepd256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(C),                    \
-      (__v4df)_mm256_setzero_pd(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_range_round_pd(W, U, A, B, C, R)                           \
-  ((__m256d)__builtin_ia32_vrangepd256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(C),                    \
-      (__v4df)(__m256d)(W), (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_range_round_pd(U, A, B, C, R)                             \
-  ((__m256d)__builtin_ia32_vrangepd256_round_mask(                             \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (int)(C),                    \
-      (__v4df)_mm256_setzero_pd(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_range_round_ps(A, B, C, R)                                      \
-  ((__m256)__builtin_ia32_vrangeps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(C),                      \
-      (__v8sf)_mm256_setzero_ps(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_range_round_ps(W, U, A, B, C, R)                           \
-  ((__m256)__builtin_ia32_vrangeps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(C), (__v8sf)(__m256)(W), \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_range_round_ps(U, A, B, C, R)                             \
-  ((__m256)__builtin_ia32_vrangeps256_round_mask(                              \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (int)(C),                      \
-      (__v8sf)_mm256_setzero_ps(), (__mmask8)(U), (int)(R)))
-
-#define _mm256_reduce_round_pd(A, B, R)                                        \
-  ((__m256d)__builtin_ia32_vreducepd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (int)(B), (__v4df)_mm256_setzero_pd(),             \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_reduce_round_pd(W, U, A, B, R)                             \
-  ((__m256d)__builtin_ia32_vreducepd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (int)(B), (__v4df)(__m256d)(W), (__mmask8)(U),     \
-      (int)(R)))
-
-#define _mm256_maskz_reduce_round_pd(U, A, B, R)                               \
-  ((__m256d)__builtin_ia32_vreducepd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (int)(B), (__v4df)_mm256_setzero_pd(),             \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_mask_reduce_round_ph(W, U, A, imm, R)                           \
-  ((__m256h)__builtin_ia32_vreduceph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (int)(imm), (__v16hf)(__m256h)(W),                \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_reduce_round_ph(U, A, imm, R)                             \
-  ((__m256h)__builtin_ia32_vreduceph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (int)(imm), (__v16hf)_mm256_setzero_ph(),         \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_reduce_round_ph(A, imm, R)                                      \
-  ((__m256h)__builtin_ia32_vreduceph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (int)(imm), (__v16hf)_mm256_undefined_ph(),       \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_reduce_round_ps(A, B, R)                                        \
-  ((__m256)__builtin_ia32_vreduceps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (int)(B), (__v8sf)_mm256_setzero_ps(),              \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_reduce_round_ps(W, U, A, B, R)                             \
-  ((__m256)__builtin_ia32_vreduceps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (int)(B), (__v8sf)(__m256)(W), (__mmask8)(U),       \
-      (int)(R)))
-
-#define _mm256_maskz_reduce_round_ps(U, A, B, R)                               \
-  ((__m256)__builtin_ia32_vreduceps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (int)(B), (__v8sf)_mm256_setzero_ps(),              \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_roundscale_round_pd(A, imm, R)                                  \
-  ((__m256d)__builtin_ia32_vrndscalepd256_round_mask(                          \
-      (__v4df)(__m256d)(A), (int)(imm), (__v4df)_mm256_undefined_pd(),         \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_roundscale_round_pd(A, B, C, imm, R)                       \
-  ((__m256d)__builtin_ia32_vrndscalepd256_round_mask(                          \
-      (__v4df)(__m256d)(C), (int)(imm), (__v4df)(__m256d)(A), (__mmask8)(B),   \
-      (int)(R)))
-
-#define _mm256_maskz_roundscale_round_pd(A, B, imm, R)                         \
-  ((__m256d)__builtin_ia32_vrndscalepd256_round_mask(                          \
-      (__v4df)(__m256d)(B), (int)(imm), (__v4df)_mm256_setzero_pd(),           \
-      (__mmask8)(A), (int)(R)))
-
-#define _mm256_roundscale_round_ph(A, imm, R)                                  \
-  ((__m256h)__builtin_ia32_vrndscaleph256_round_mask(                          \
-      (__v16hf)(__m256h)(A), (int)(imm), (__v16hf)_mm256_undefined_ph(),       \
-      (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_roundscale_round_ph(A, B, C, imm, R)                       \
-  ((__m256h)__builtin_ia32_vrndscaleph256_round_mask(                          \
-      (__v16hf)(__m256h)(C), (int)(imm), (__v16hf)(__m256h)(A),                \
-      (__mmask16)(B), (int)(R)))
-
-#define _mm256_maskz_roundscale_round_ph(A, B, imm, R)                         \
-  ((__m256h)__builtin_ia32_vrndscaleph256_round_mask(                          \
-      (__v16hf)(__m256h)(B), (int)(imm), (__v16hf)_mm256_setzero_ph(),         \
-      (__mmask16)(A), (int)(R)))
-
-#define _mm256_roundscale_round_ps(A, imm, R)                                  \
-  ((__m256)__builtin_ia32_vrndscaleps256_round_mask(                           \
-      (__v8sf)(__m256)(A), (int)(imm), (__v8sf)_mm256_undefined_ps(),          \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_roundscale_round_ps(A, B, C, imm, R)                       \
-  ((__m256)__builtin_ia32_vrndscaleps256_round_mask(                           \
-      (__v8sf)(__m256)(C), (int)(imm), (__v8sf)(__m256)(A), (__mmask8)(B),     \
-      (int)(R)))
-
-#define _mm256_maskz_roundscale_round_ps(A, B, imm, R)                         \
-  ((__m256)__builtin_ia32_vrndscaleps256_round_mask(                           \
-      (__v8sf)(__m256)(B), (int)(imm), (__v8sf)_mm256_setzero_ps(),            \
-      (__mmask8)(A), (int)(R)))
-
-#define _mm256_scalef_round_pd(A, B, R)                                        \
-  ((__m256d)__builtin_ia32_vscalefpd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B),                              \
-      (__v4df)_mm256_undefined_pd(), (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_scalef_round_pd(W, U, A, B, R)                             \
-  ((__m256d)__builtin_ia32_vscalefpd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)(__m256d)(W),        \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_scalef_round_pd(U, A, B, R)                               \
-  ((__m256d)__builtin_ia32_vscalefpd256_round_mask(                            \
-      (__v4df)(__m256d)(A), (__v4df)(__m256d)(B), (__v4df)_mm256_setzero_pd(), \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_scalef_round_ph(A, B, R)                                        \
-  ((__m256h)__builtin_ia32_vscalefph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B),                            \
-      (__v16hf)_mm256_undefined_ph(), (__mmask16)-1, (int)(R)))
-
-#define _mm256_mask_scalef_round_ph(W, U, A, B, R)                             \
-  ((__m256h)__builtin_ia32_vscalefph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B), (__v16hf)(__m256h)(W),     \
-      (__mmask16)(U), (int)(R)))
-
-#define _mm256_maskz_scalef_round_ph(U, A, B, R)                               \
-  ((__m256h)__builtin_ia32_vscalefph256_round_mask(                            \
-      (__v16hf)(__m256h)(A), (__v16hf)(__m256h)(B),                            \
-      (__v16hf)_mm256_setzero_ph(), (__mmask16)(U), (int)(R)))
-
-#define _mm256_scalef_round_ps(A, B, R)                                        \
-  ((__m256)__builtin_ia32_vscalefps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)_mm256_undefined_ps(), \
-      (__mmask8)-1, (int)(R)))
-
-#define _mm256_mask_scalef_round_ps(W, U, A, B, R)                             \
-  ((__m256)__builtin_ia32_vscalefps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)(__m256)(W),           \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_maskz_scalef_round_ps(U, A, B, R)                               \
-  ((__m256)__builtin_ia32_vscalefps256_round_mask(                             \
-      (__v8sf)(__m256)(A), (__v8sf)(__m256)(B), (__v8sf)_mm256_setzero_ps(),   \
-      (__mmask8)(U), (int)(R)))
-
-#define _mm256_sqrt_round_pd(A, R)                                             \
-  ((__m256d)__builtin_ia32_vsqrtpd256_round((__v4df)(__m256d)(A), (int)(R)))
-
-#define _mm256_mask_sqrt_round_pd(W, U, A, R)                                  \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_sqrt_round_pd((A), (R)),                   \
-      (__v4df)(__m256d)(W)))
-
-#define _mm256_maskz_sqrt_round_pd(U, A, R)                                    \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_sqrt_round_pd((A), (R)),                   \
-      (__v4df)_mm256_setzero_pd()))
-
-#define _mm256_sqrt_round_ph(A, R)                                             \
-  ((__m256h)__builtin_ia32_vsqrtph256_round((__v16hf)(__m256h)(A), (int)(R)))
-
-#define _mm256_mask_sqrt_round_ph(W, U, A, R)                                  \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_sqrt_round_ph((A), (R)),                 \
-      (__v16hf)(__m256h)(W)))
-
-#define _mm256_maskz_sqrt_round_ph(U, A, R)                                    \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_sqrt_round_ph((A), (R)),                 \
-      (__v16hf)_mm256_setzero_ph()))
-
-#define _mm256_sqrt_round_ps(A, R)                                             \
-  ((__m256)__builtin_ia32_vsqrtps256_round((__v8sf)(__m256)(A), (int)(R)))
-
-#define _mm256_mask_sqrt_round_ps(W, U, A, R)                                  \
-  ((__m256)__builtin_ia32_selectps_256((__mmask8)(U),                          \
-                                       (__v8sf)_mm256_sqrt_round_ps((A), (R)), \
-                                       (__v8sf)(__m256)(W)))
-
-#define _mm256_maskz_sqrt_round_ps(U, A, R)                                    \
-  ((__m256)__builtin_ia32_selectps_256((__mmask8)(U),                          \
-                                       (__v8sf)_mm256_sqrt_round_ps((A), (R)), \
-                                       (__v8sf)_mm256_setzero_ps()))
-
-#define _mm256_sub_round_pd(A, B, R)                                           \
-  ((__m256d)__builtin_ia32_vsubpd256_round((__v4df)(__m256d)(A),               \
-                                           (__v4df)(__m256d)(B), (int)(R)))
-
-#define _mm256_mask_sub_round_pd(W, U, A, B, R)                                \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_sub_round_pd((A), (B), (R)),               \
-      (__v4df)(__m256d)(W)))
-
-#define _mm256_maskz_sub_round_pd(U, A, B, R)                                  \
-  ((__m256d)__builtin_ia32_selectpd_256(                                       \
-      (__mmask8)(U), (__v4df)_mm256_sub_round_pd((A), (B), (R)),               \
-      (__v4df)_mm256_setzero_pd()))
-
-#define _mm256_sub_round_ph(A, B, R)                                           \
-  ((__m256h)__builtin_ia32_vsubph256_round((__v16hf)(__m256h)(A),              \
-                                           (__v16hf)(__m256h)(B), (int)(R)))
-
-#define _mm256_mask_sub_round_ph(W, U, A, B, R)                                \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_sub_round_ph((A), (B), (R)),             \
-      (__v16hf)(__m256h)(W)))
-
-#define _mm256_maskz_sub_round_ph(U, A, B, R)                                  \
-  ((__m256h)__builtin_ia32_selectph_256(                                       \
-      (__mmask16)(U), (__v16hf)_mm256_sub_round_ph((A), (B), (R)),             \
-      (__v16hf)_mm256_setzero_ph()))
-
-#define _mm256_sub_round_ps(A, B, R)                                           \
-  ((__m256)__builtin_ia32_vsubps256_round((__v8sf)(__m256)(A),                 \
-                                          (__v8sf)(__m256)(B), (int)(R)))
-
-#define _mm256_mask_sub_round_ps(W, U, A, B, R)                                \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_sub_round_ps((A), (B), (R)),               \
-      (__v8sf)(__m256)(W)))
-
-#define _mm256_maskz_sub_round_ps(U, A, B, R)                                  \
-  ((__m256)__builtin_ia32_selectps_256(                                        \
-      (__mmask8)(U), (__v8sf)_mm256_sub_round_ps((A), (B), (R)),               \
-      (__v8sf)_mm256_setzero_ps()))
-
 #undef __DEFAULT_FN_ATTRS256
 #undef __DEFAULT_FN_ATTRS128
 
diff --git a/clang/lib/Headers/avx10_2satcvtdsintrin.h b/clang/lib/Headers/avx10_2satcvtdsintrin.h
index 59028436311e..912428748721 100644
--- a/clang/lib/Headers/avx10_2satcvtdsintrin.h
+++ b/clang/lib/Headers/avx10_2satcvtdsintrin.h
@@ -71,175 +71,138 @@
 #endif /* __x86_64__ */
 
 // 128 Bit : Double -> int
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttspd_epi32(__m128d __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128
+_mm_cvtts_pd_epi32(__m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2dqs128_mask(
       (__v2df)__A, (__v4si)(__m128i)_mm_undefined_si128(), (__mmask8)(-1)));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttspd_epi32(__m128i __W, __mmask8 __U, __m128d __A) {
+_mm_mask_cvtts_pd_epi32(__m128i __W, __mmask8 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2dqs128_mask((__v2df)__A, (__v4si)__W,
                                                       __U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttspd_epi32(__mmask16 __U, __m128d __A) {
+_mm_maskz_cvtts_pd_epi32(__mmask16 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2dqs128_mask(
       (__v2df)__A, (__v4si)(__m128i)_mm_setzero_si128(), __U));
 }
 
 // 256 Bit : Double -> int
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_cvttspd_epi32(__m256d __A) {
+_mm256_cvtts_pd_epi32(__m256d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2dqs256_round_mask(
       (__v4df)__A, (__v4si)_mm_undefined_si128(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttspd_epi32(__m128i __W, __mmask8 __U, __m256d __A) {
+_mm256_mask_cvtts_pd_epi32(__m128i __W, __mmask8 __U, __m256d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2dqs256_round_mask(
       (__v4df)__A, (__v4si)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttspd_epi32(__mmask8 __U, __m256d __A) {
+_mm256_maskz_cvtts_pd_epi32(__mmask8 __U, __m256d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2dqs256_round_mask(
       (__v4df)__A, (__v4si)_mm_setzero_si128(), __U, _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundpd_epi32(__A, __R)                                   \
-  ((__m128i)__builtin_ia32_vcvttpd2dqs256_round_mask(                          \
-      (__v4df)(__m256d)__A, (__v4si)(__m128i)_mm_undefined_si128(),            \
-      (__mmask8) - 1, (int)(__R)))
-
-#define _mm256_mask_cvtts_roundpd_epi32(__W, __U, __A, __R)                    \
-  ((__m128i)__builtin_ia32_vcvttpd2dqs256_round_mask(                          \
-      (__v4df)(__m256d)__A, (__v4si)(__m128i)__W, (__mmask8)__U, (int)(__R)))
-
-#define _mm256_maskz_cvtts_roundpd_epi32(__U, __A, __R)                        \
-  ((__m128i)__builtin_ia32_vcvttpd2dqs256_round_mask(                          \
-      (__v4df)(__m256d)__A, (__v4si)(__m128i)_mm_setzero_si128(),              \
-      (__mmask8)__U, (int)(__R)))
-
 // 128 Bit : Double -> uint
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttspd_epu32(__m128d __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128
+_mm_cvtts_pd_epu32(__m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2udqs128_mask(
       (__v2df)__A, (__v4si)(__m128i)_mm_undefined_si128(), (__mmask8)(-1)));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttspd_epu32(__m128i __W, __mmask8 __U, __m128d __A) {
+_mm_mask_cvtts_pd_epu32(__m128i __W, __mmask8 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2udqs128_mask(
       (__v2df)__A, (__v4si)(__m128i)__W, (__mmask8)__U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttspd_epu32(__mmask8 __U, __m128d __A) {
+_mm_maskz_cvtts_pd_epu32(__mmask8 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2udqs128_mask(
       (__v2df)__A, (__v4si)(__m128i)_mm_setzero_si128(), __U));
 }
 
 // 256 Bit : Double -> uint
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_cvttspd_epu32(__m256d __A) {
+_mm256_cvtts_pd_epu32(__m256d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2udqs256_round_mask(
       (__v4df)__A, (__v4si)_mm_undefined_si128(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttspd_epu32(__m128i __W, __mmask8 __U, __m256d __A) {
+_mm256_mask_cvtts_pd_epu32(__m128i __W, __mmask8 __U, __m256d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2udqs256_round_mask(
       (__v4df)__A, (__v4si)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttspd_epu32(__mmask8 __U, __m256d __A) {
+_mm256_maskz_cvtts_pd_epu32(__mmask8 __U, __m256d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2udqs256_round_mask(
       (__v4df)__A, (__v4si)_mm_setzero_si128(), __U, _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundpd_epu32(__A, __R)                                   \
-  ((__m128i)__builtin_ia32_vcvttpd2udqs256_round_mask(                         \
-      (__v4df)(__m256d)__A, (__v4si)(__m128i)_mm_undefined_si128(),            \
-      (__mmask8) - 1, (int)(__R)))
-
-#define _mm256_mask_cvtts_roundpd_epu32(__W, __U, __A, __R)                    \
-  ((__m128i)__builtin_ia32_vcvttpd2udqs256_round_mask(                         \
-      (__v4df)(__m256d)__A, (__v4si)(__m128i)__W, (__mmask8)__U, (int)(__R)))
-
-#define _mm256_maskz_cvtts_roundpd_epu32(__U, __A, __R)                        \
-  ((__m128i)__builtin_ia32_vcvttpd2udqs256_round_mask(                         \
-      (__v4df)(__m256d)__A, (__v4si)(__m128i)_mm_setzero_si128(),              \
-      (__mmask8)__U, (int)(__R)))
-
 // 128 Bit : Double -> long
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttspd_epi64(__m128d __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128
+_mm_cvtts_pd_epi64(__m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2qqs128_mask(
       (__v2df)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttspd_epi64(__m128i __W, __mmask8 __U, __m128d __A) {
+_mm_mask_cvtts_pd_epi64(__m128i __W, __mmask8 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2qqs128_mask((__v2df)__A, (__v2di)__W,
                                                       (__mmask8)__U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttspd_epi64(__mmask8 __U, __m128d __A) {
+_mm_maskz_cvtts_pd_epi64(__mmask8 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2qqs128_mask(
       (__v2df)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U));
 }
 
 // 256 Bit : Double -> long
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvttspd_epi64(__m256d __A) {
+_mm256_cvtts_pd_epi64(__m256d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2qqs256_round_mask(
       (__v4df)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttspd_epi64(__m256i __W, __mmask8 __U, __m256d __A) {
+_mm256_mask_cvtts_pd_epi64(__m256i __W, __mmask8 __U, __m256d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2qqs256_round_mask(
       (__v4df)__A, (__v4di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttspd_epi64(__mmask8 __U, __m256d __A) {
+_mm256_maskz_cvtts_pd_epi64(__mmask8 __U, __m256d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2qqs256_round_mask(
       (__v4df)__A, (__v4di)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundpd_epi64(__A, __R)                                   \
-  ((__m256i)__builtin_ia32_vcvttpd2qqs256_round_mask(                          \
-      (__v4df)__A, (__v4di)_mm256_undefined_si256(), (__mmask8) - 1,           \
-      (int)__R))
-
-#define _mm256_mask_cvtts_roundpd_epi64(__W, __U, __A, __R)                    \
-  ((__m256i)__builtin_ia32_vcvttpd2qqs256_round_mask((__v4df)__A, (__v4di)__W, \
-                                                     (__mmask8)__U, (int)__R))
-
-#define _mm256_maskz_cvtts_roundpd_epi64(__U, __A, __R)                        \
-  ((__m256i)__builtin_ia32_vcvttpd2qqs256_round_mask(                          \
-      (__v4df)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U, (int)__R))
-
 // 128 Bit : Double -> ulong
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttspd_epu64(__m128d __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128
+_mm_cvtts_pd_epu64(__m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2uqqs128_mask(
       (__v2df)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttspd_epu64(__m128i __W, __mmask8 __U, __m128d __A) {
+_mm_mask_cvtts_pd_epu64(__m128i __W, __mmask8 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2uqqs128_mask((__v2df)__A, (__v2di)__W,
                                                        (__mmask8)__U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttspd_epu64(__mmask8 __U, __m128d __A) {
+_mm_maskz_cvtts_pd_epu64(__mmask8 __U, __m128d __A) {
   return ((__m128i)__builtin_ia32_vcvttpd2uqqs128_mask(
       (__v2df)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U));
 }
@@ -247,105 +210,78 @@ _mm_maskz_cvttspd_epu64(__mmask8 __U, __m128d __A) {
 // 256 Bit : Double -> ulong
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvttspd_epu64(__m256d __A) {
+_mm256_cvtts_pd_epu64(__m256d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2uqqs256_round_mask(
       (__v4df)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttspd_epu64(__m256i __W, __mmask8 __U, __m256d __A) {
+_mm256_mask_cvtts_pd_epu64(__m256i __W, __mmask8 __U, __m256d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2uqqs256_round_mask(
       (__v4df)__A, (__v4di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttspd_epu64(__mmask8 __U, __m256d __A) {
+_mm256_maskz_cvtts_pd_epu64(__mmask8 __U, __m256d __A) {
   return ((__m256i)__builtin_ia32_vcvttpd2uqqs256_round_mask(
       (__v4df)__A, (__v4di)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundpd_epu64(__A, __R)                                   \
-  ((__m256i)__builtin_ia32_vcvttpd2uqqs256_round_mask(                         \
-      (__v4df)__A, (__v4di)_mm256_undefined_si256(), (__mmask8) - 1,           \
-      (int)__R))
-
-#define _mm256_mask_cvtts_roundpd_epu64(__W, __U, __A, __R)                    \
-  ((__m256i)__builtin_ia32_vcvttpd2uqqs256_round_mask(                         \
-      (__v4df)__A, (__v4di)__W, (__mmask8)__U, (int)__R))
-
-#define _mm256_maskz_cvtts_roundpd_epu64(__U, __A, __R)                        \
-  ((__m256i)__builtin_ia32_vcvttpd2uqqs256_round_mask(                         \
-      (__v4df)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U, (int)__R))
-
 // 128 Bit : float -> int
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttsps_epi32(__m128 __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvtts_ps_epi32(__m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2dqs128_mask(
       (__v4sf)__A, (__v4si)(__m128i)_mm_undefined_si128(), (__mmask8)(-1)));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttsps_epi32(__m128i __W, __mmask8 __U, __m128 __A) {
+_mm_mask_cvtts_ps_epi32(__m128i __W, __mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2dqs128_mask((__v4sf)__A, (__v4si)__W,
                                                       (__mmask8)__U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttsps_epi32(__mmask8 __U, __m128 __A) {
+_mm_maskz_cvtts_ps_epi32(__mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2dqs128_mask(
       (__v4sf)__A, (__v4si)(__m128i)_mm_setzero_si128(), (__mmask8)__U));
 }
 
 // 256 Bit : float -> int
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvttsps_epi32(__m256 __A) {
+_mm256_cvtts_ps_epi32(__m256 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2dqs256_round_mask(
       (__v8sf)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttsps_epi32(__m256i __W, __mmask8 __U, __m256 __A) {
+_mm256_mask_cvtts_ps_epi32(__m256i __W, __mmask8 __U, __m256 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2dqs256_round_mask(
       (__v8sf)__A, (__v8si)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttsps_epi32(__mmask8 __U, __m256 __A) {
+_mm256_maskz_cvtts_ps_epi32(__mmask8 __U, __m256 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2dqs256_round_mask(
       (__v8sf)__A, (__v8si)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundps_epi32(__A, __R)                                   \
-  ((__m256i)__builtin_ia32_vcvttps2dqs256_round_mask(                          \
-      (__v8sf)(__m256)__A, (__v8si)(__m256i)_mm256_undefined_si256(),          \
-      (__mmask8) - 1, (int)(__R)))
-
-#define _mm256_mask_cvtts_roundps_epi32(__W, __U, __A, __R)                    \
-  ((__m256i)__builtin_ia32_vcvttps2dqs256_round_mask(                          \
-      (__v8sf)(__m256)__A, (__v8si)(__m256i)__W, (__mmask8)__U, (int)(__R)))
-
-#define _mm256_maskz_cvtts_roundps_epi32(__U, __A, __R)                        \
-  ((__m256i)__builtin_ia32_vcvttps2dqs256_round_mask(                          \
-      (__v8sf)(__m256)__A, (__v8si)(__m256i)_mm256_setzero_si256(),            \
-      (__mmask8)__U, (int)(__R)))
-
 // 128 Bit : float -> uint
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttsps_epu32(__m128 __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvtts_ps_epu32(__m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2udqs128_mask(
       (__v4sf)__A, (__v4si)(__m128i)_mm_undefined_si128(), (__mmask8)(-1)));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttsps_epu32(__m128i __W, __mmask8 __U, __m128 __A) {
+_mm_mask_cvtts_ps_epu32(__m128i __W, __mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2udqs128_mask((__v4sf)__A, (__v4si)__W,
                                                        (__mmask8)__U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttsps_epu32(__mmask8 __U, __m128 __A) {
+_mm_maskz_cvtts_ps_epu32(__mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2udqs128_mask(
       (__v4sf)__A, (__v4si)_mm_setzero_si128(), (__mmask8)__U));
 }
@@ -353,144 +289,102 @@ _mm_maskz_cvttsps_epu32(__mmask8 __U, __m128 __A) {
 // 256 Bit : float -> uint
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvttsps_epu32(__m256 __A) {
+_mm256_cvtts_ps_epu32(__m256 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2udqs256_round_mask(
       (__v8sf)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttsps_epu32(__m256i __W, __mmask8 __U, __m256 __A) {
+_mm256_mask_cvtts_ps_epu32(__m256i __W, __mmask8 __U, __m256 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2udqs256_round_mask(
       (__v8sf)__A, (__v8si)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttsps_epu32(__mmask8 __U, __m256 __A) {
+_mm256_maskz_cvtts_ps_epu32(__mmask8 __U, __m256 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2udqs256_round_mask(
       (__v8sf)__A, (__v8si)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundps_epu32(__A, __R)                                   \
-  ((__m256i)__builtin_ia32_vcvttps2udqs256_round_mask(                         \
-      (__v8sf)(__m256)__A, (__v8si)(__m256i)_mm256_undefined_si256(),          \
-      (__mmask8) - 1, (int)(__R)))
-
-#define _mm256_mask_cvtts_roundps_epu32(__W, __U, __A, __R)                    \
-  ((__m256i)__builtin_ia32_vcvttps2udqs256_round_mask(                         \
-      (__v8sf)(__m256)__A, (__v8si)(__m256i)__W, (__mmask8)__U, (int)(__R)))
-
-#define _mm256_maskz_cvtts_roundps_epu32(__U, __A, __R)                        \
-  ((__m256i)__builtin_ia32_vcvttps2udqs256_round_mask(                         \
-      (__v8sf)(__m256)__A, (__v8si)(__m256i)_mm256_setzero_si256(),            \
-      (__mmask8)__U, (int)(__R)))
-
 // 128 bit : float -> long
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttsps_epi64(__m128 __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvtts_ps_epi64(__m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2qqs128_mask(
       (__v4sf)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttsps_epi64(__m128i __W, __mmask8 __U, __m128 __A) {
+_mm_mask_cvtts_ps_epi64(__m128i __W, __mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2qqs128_mask(
       (__v4sf)__A, (__v2di)(__m128i)__W, (__mmask8)__U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttsps_epi64(__mmask8 __U, __m128 __A) {
+_mm_maskz_cvtts_ps_epi64(__mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2qqs128_mask(
       (__v4sf)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U));
 }
 // 256 bit : float -> long
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvttsps_epi64(__m128 __A) {
+_mm256_cvtts_ps_epi64(__m128 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2qqs256_round_mask(
       (__v4sf)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttsps_epi64(__m256i __W, __mmask8 __U, __m128 __A) {
+_mm256_mask_cvtts_ps_epi64(__m256i __W, __mmask8 __U, __m128 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2qqs256_round_mask(
       (__v4sf)__A, (__v4di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttsps_epi64(__mmask8 __U, __m128 __A) {
+_mm256_maskz_cvtts_ps_epi64(__mmask8 __U, __m128 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2qqs256_round_mask(
       (__v4sf)__A, (__v4di)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundps_epi64(__A, __R)                                   \
-  ((__m256i)__builtin_ia32_vcvttps2qqs256_round_mask(                          \
-      (__v4sf)(__m128)__A, (__v4di)_mm256_undefined_si256(), (__mmask8) - 1,   \
-      (int)__R))
-
-#define _mm256_mask_cvtts_roundps_epi64(__W, __U, __A, __R)                    \
-  ((__m256i)__builtin_ia32_vcvttps2qqs256_round_mask(                          \
-      (__v4sf)(__m128)__A, (__v4di)__W, (__mmask8)__U, (int)__R))
-
-#define _mm256_maskz_cvtts_roundps_epi64(__U, __A, __R)                        \
-  ((__m256i)__builtin_ia32_vcvttps2qqs256_round_mask(                          \
-      (__v4sf)(__m128)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U,      \
-      (int)__R))
-
 // 128 bit : float -> ulong
-static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvttsps_epu64(__m128 __A) {
+static __inline__ __m128i __DEFAULT_FN_ATTRS128 _mm_cvtts_ps_epu64(__m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2uqqs128_mask(
       (__v4sf)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_mask_cvttsps_epu64(__m128i __W, __mmask8 __U, __m128 __A) {
+_mm_mask_cvtts_ps_epu64(__m128i __W, __mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2uqqs128_mask(
       (__v4sf)__A, (__v2di)(__m128i)__W, (__mmask8)__U));
 }
 
 static __inline__ __m128i __DEFAULT_FN_ATTRS128
-_mm_maskz_cvttsps_epu64(__mmask8 __U, __m128 __A) {
+_mm_maskz_cvtts_ps_epu64(__mmask8 __U, __m128 __A) {
   return ((__m128i)__builtin_ia32_vcvttps2uqqs128_mask(
       (__v4sf)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U));
 }
 // 256 bit : float -> ulong
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_cvttsps_epu64(__m128 __A) {
+_mm256_cvtts_ps_epu64(__m128 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2uqqs256_round_mask(
       (__v4sf)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1,
       _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_mask_cvttsps_epu64(__m256i __W, __mmask8 __U, __m128 __A) {
+_mm256_mask_cvtts_ps_epu64(__m256i __W, __mmask8 __U, __m128 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2uqqs256_round_mask(
       (__v4sf)__A, (__v4di)__W, __U, _MM_FROUND_CUR_DIRECTION));
 }
 
 static __inline__ __m256i __DEFAULT_FN_ATTRS256
-_mm256_maskz_cvttsps_epu64(__mmask8 __U, __m128 __A) {
+_mm256_maskz_cvtts_ps_epu64(__mmask8 __U, __m128 __A) {
   return ((__m256i)__builtin_ia32_vcvttps2uqqs256_round_mask(
       (__v4sf)__A, (__v4di)_mm256_setzero_si256(), __U,
       _MM_FROUND_CUR_DIRECTION));
 }
 
-#define _mm256_cvtts_roundps_epu64(__A, __R)                                   \
-  ((__m256i)__builtin_ia32_vcvttps2uqqs256_round_mask(                         \
-      (__v4sf)(__m128)__A, (__v4di)_mm256_undefined_si256(), (__mmask8) - 1,   \
-      (int)__R))
-
-#define _mm256_mask_cvtts_roundps_epu64(__W, __U, __A, __R)                    \
-  ((__m256i)__builtin_ia32_vcvttps2uqqs256_round_mask(                         \
-      (__v4sf)(__m128)__A, (__v4di)__W, (__mmask8)__U, (int)__R))
-
-#define _mm256_maskz_cvtts_roundps_epu64(__U, __A, __R)                        \
-  ((__m256i)__builtin_ia32_vcvttps2uqqs256_round_mask(                         \
-      (__v4sf)(__m128)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U,      \
-      (int)__R))
-
 #undef __DEFAULT_FN_ATTRS128
 #undef __DEFAULT_FN_ATTRS256
 #endif // __AVX10_2SATCVTDSINTRIN_H
diff --git a/clang/lib/Headers/avx10_2satcvtintrin.h b/clang/lib/Headers/avx10_2satcvtintrin.h
index d16c60e6382d..cfa5b02fc7d4 100644
--- a/clang/lib/Headers/avx10_2satcvtintrin.h
+++ b/clang/lib/Headers/avx10_2satcvtintrin.h
@@ -14,431 +14,319 @@
 #ifndef __AVX10_2SATCVTINTRIN_H
 #define __AVX10_2SATCVTINTRIN_H
 
-#define _mm_ipcvtbf16_epi8(A)                                                  \
+#define _mm_ipcvts_bf16_epi8(A)                                                \
   ((__m128i)__builtin_ia32_vcvtbf162ibs128((__v8bf)(__m128bh)(A)))
 
-#define _mm_mask_ipcvtbf16_epi8(W, U, A)                                       \
+#define _mm_mask_ipcvts_bf16_epi8(W, U, A)                                     \
   ((__m128i)__builtin_ia32_selectw_128(                                        \
-      (__mmask8)(U), (__v8hi)_mm_ipcvtbf16_epi8(A), (__v8hi)(__m128i)(W)))
+      (__mmask8)(U), (__v8hi)_mm_ipcvts_bf16_epi8(A), (__v8hi)(__m128i)(W)))
 
-#define _mm_maskz_ipcvtbf16_epi8(U, A)                                         \
+#define _mm_maskz_ipcvts_bf16_epi8(U, A)                                       \
   ((__m128i)__builtin_ia32_selectw_128((__mmask8)(U),                          \
-                                       (__v8hi)_mm_ipcvtbf16_epi8(A),          \
+                                       (__v8hi)_mm_ipcvts_bf16_epi8(A),        \
                                        (__v8hi)_mm_setzero_si128()))
 
-#define _mm256_ipcvtbf16_epi8(A)                                               \
+#define _mm256_ipcvts_bf16_epi8(A)                                             \
   ((__m256i)__builtin_ia32_vcvtbf162ibs256((__v16bf)(__m256bh)(A)))
 
-#define _mm256_mask_ipcvtbf16_epi8(W, U, A)                                    \
+#define _mm256_mask_ipcvts_bf16_epi8(W, U, A)                                  \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvtbf16_epi8(A),      \
+                                       (__v16hi)_mm256_ipcvts_bf16_epi8(A),    \
                                        (__v16hi)(__m256i)(W)))
 
-#define _mm256_maskz_ipcvtbf16_epi8(U, A)                                      \
+#define _mm256_maskz_ipcvts_bf16_epi8(U, A)                                    \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvtbf16_epi8(A),      \
+                                       (__v16hi)_mm256_ipcvts_bf16_epi8(A),    \
                                        (__v16hi)_mm256_setzero_si256()))
 
-#define _mm_ipcvtbf16_epu8(A)                                                  \
+#define _mm_ipcvts_bf16_epu8(A)                                                \
   ((__m128i)__builtin_ia32_vcvtbf162iubs128((__v8bf)(__m128bh)(A)))
 
-#define _mm_mask_ipcvtbf16_epu8(W, U, A)                                       \
+#define _mm_mask_ipcvts_bf16_epu8(W, U, A)                                     \
   ((__m128i)__builtin_ia32_selectw_128(                                        \
-      (__mmask8)(U), (__v8hi)_mm_ipcvtbf16_epu8(A), (__v8hi)(__m128i)(W)))
+      (__mmask8)(U), (__v8hi)_mm_ipcvts_bf16_epu8(A), (__v8hi)(__m128i)(W)))
 
-#define _mm_maskz_ipcvtbf16_epu8(U, A)                                         \
+#define _mm_maskz_ipcvts_bf16_epu8(U, A)                                       \
   ((__m128i)__builtin_ia32_selectw_128((__mmask8)(U),                          \
-                                       (__v8hi)_mm_ipcvtbf16_epu8(A),          \
+                                       (__v8hi)_mm_ipcvts_bf16_epu8(A),        \
                                        (__v8hi)_mm_setzero_si128()))
 
-#define _mm256_ipcvtbf16_epu8(A)                                               \
+#define _mm256_ipcvts_bf16_epu8(A)                                             \
   ((__m256i)__builtin_ia32_vcvtbf162iubs256((__v16bf)(__m256bh)(A)))
 
-#define _mm256_mask_ipcvtbf16_epu8(W, U, A)                                    \
+#define _mm256_mask_ipcvts_bf16_epu8(W, U, A)                                  \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvtbf16_epu8(A),      \
+                                       (__v16hi)_mm256_ipcvts_bf16_epu8(A),    \
                                        (__v16hi)(__m256i)(W)))
 
-#define _mm256_maskz_ipcvtbf16_epu8(U, A)                                      \
+#define _mm256_maskz_ipcvts_bf16_epu8(U, A)                                    \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvtbf16_epu8(A),      \
+                                       (__v16hi)_mm256_ipcvts_bf16_epu8(A),    \
                                        (__v16hi)_mm256_setzero_si256()))
 
-#define _mm_ipcvtph_epi8(A)                                                    \
+#define _mm_ipcvts_ph_epi8(A)                                                  \
   ((__m128i)__builtin_ia32_vcvtph2ibs128_mask(                                 \
       (__v8hf)(__m128h)(A), (__v8hu)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvtph_epi8(W, U, A)                                         \
+#define _mm_mask_ipcvts_ph_epi8(W, U, A)                                       \
   ((__m128i)__builtin_ia32_vcvtph2ibs128_mask((__v8hf)(__m128h)(A),            \
                                               (__v8hu)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvtph_epi8(U, A)                                           \
+#define _mm_maskz_ipcvts_ph_epi8(U, A)                                         \
   ((__m128i)__builtin_ia32_vcvtph2ibs128_mask(                                 \
       (__v8hf)(__m128h)(A), (__v8hu)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvtph_epi8(A)                                                 \
+#define _mm256_ipcvts_ph_epi8(A)                                               \
   ((__m256i)__builtin_ia32_vcvtph2ibs256_mask(                                 \
       (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)-1,   \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvtph_epi8(W, U, A)                                      \
+#define _mm256_mask_ipcvts_ph_epi8(W, U, A)                                    \
   ((__m256i)__builtin_ia32_vcvtph2ibs256_mask((__v16hf)(__m256h)(A),           \
                                               (__v16hu)(W), (__mmask16)(U),    \
                                               _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvtph_epi8(U, A)                                        \
+#define _mm256_maskz_ipcvts_ph_epi8(U, A)                                      \
   ((__m256i)__builtin_ia32_vcvtph2ibs256_mask(                                 \
       (__v16hf)(__m256h)(A), (__v16hu)(_mm256_setzero_si256()),                \
       (__mmask16)(U), _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_ipcvt_roundph_epi8(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvtph2ibs256_mask((__v16hf)(__m256h)(A),           \
-                                              (__v16hu)_mm256_setzero_si256(), \
-                                              (__mmask16)-1, (const int)R))
-
-#define _mm256_mask_ipcvt_roundph_epi8(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvtph2ibs256_mask(                                 \
-      (__v16hf)(__m256h)(A), (__v16hu)(W), (__mmask16)(U), (const int)R))
-
-#define _mm256_maskz_ipcvt_roundph_epi8(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvtph2ibs256_mask((__v16hf)(__m256h)(A),           \
-                                              (__v16hu)_mm256_setzero_si256(), \
-                                              (__mmask16)(U), (const int)R))
-
-#define _mm_ipcvtph_epu8(A)                                                    \
+#define _mm_ipcvts_ph_epu8(A)                                                  \
   ((__m128i)__builtin_ia32_vcvtph2iubs128_mask(                                \
       (__v8hf)(__m128h)(A), (__v8hu)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvtph_epu8(W, U, A)                                         \
+#define _mm_mask_ipcvts_ph_epu8(W, U, A)                                       \
   ((__m128i)__builtin_ia32_vcvtph2iubs128_mask((__v8hf)(__m128h)(A),           \
                                                (__v8hu)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvtph_epu8(U, A)                                           \
+#define _mm_maskz_ipcvts_ph_epu8(U, A)                                         \
   ((__m128i)__builtin_ia32_vcvtph2iubs128_mask(                                \
       (__v8hf)(__m128h)(A), (__v8hu)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvtph_epu8(A)                                                 \
+#define _mm256_ipcvts_ph_epu8(A)                                               \
   ((__m256i)__builtin_ia32_vcvtph2iubs256_mask(                                \
       (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)-1,   \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvtph_epu8(W, U, A)                                      \
+#define _mm256_mask_ipcvts_ph_epu8(W, U, A)                                    \
   ((__m256i)__builtin_ia32_vcvtph2iubs256_mask((__v16hf)(__m256h)(A),          \
                                                (__v16hu)(W), (__mmask16)(U),   \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvtph_epu8(U, A)                                        \
+#define _mm256_maskz_ipcvts_ph_epu8(U, A)                                      \
   ((__m256i)__builtin_ia32_vcvtph2iubs256_mask(                                \
       (__v16hf)(__m256h)(A), (__v16hu)(_mm256_setzero_si256()),                \
       (__mmask16)(U), _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_ipcvt_roundph_epu8(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvtph2iubs256_mask(                                \
-      (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)-1,   \
-      (const int)R))
-
-#define _mm256_mask_ipcvt_roundph_epu8(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvtph2iubs256_mask(                                \
-      (__v16hf)(__m256h)(A), (__v16hu)(W), (__mmask16)(U), (const int)R))
-
-#define _mm256_maskz_ipcvt_roundph_epu8(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvtph2iubs256_mask(                                \
-      (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)(U),  \
-      (const int)R))
-
-#define _mm_ipcvtps_epi8(A)                                                    \
+#define _mm_ipcvts_ps_epi8(A)                                                  \
   ((__m128i)__builtin_ia32_vcvtps2ibs128_mask(                                 \
       (__v4sf)(__m128)(A), (__v4su)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvtps_epi8(W, U, A)                                         \
+#define _mm_mask_ipcvts_ps_epi8(W, U, A)                                       \
   ((__m128i)__builtin_ia32_vcvtps2ibs128_mask((__v4sf)(__m128)(A),             \
                                               (__v4su)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvtps_epi8(U, A)                                           \
+#define _mm_maskz_ipcvts_ps_epi8(U, A)                                         \
   ((__m128i)__builtin_ia32_vcvtps2ibs128_mask(                                 \
       (__v4sf)(__m128)(A), (__v4su)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvtps_epi8(A)                                                 \
+#define _mm256_ipcvts_ps_epi8(A)                                               \
   ((__m256i)__builtin_ia32_vcvtps2ibs256_mask(                                 \
       (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)-1,       \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvtps_epi8(W, U, A)                                      \
+#define _mm256_mask_ipcvts_ps_epi8(W, U, A)                                    \
   ((__m256i)__builtin_ia32_vcvtps2ibs256_mask((__v8sf)(__m256)(A),             \
                                               (__v8su)(W), (__mmask8)(U),      \
                                               _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvtps_epi8(U, A)                                        \
+#define _mm256_maskz_ipcvts_ps_epi8(U, A)                                      \
   ((__m256i)__builtin_ia32_vcvtps2ibs256_mask(                                 \
       (__v8sf)(__m256)(A), (__v8su)(_mm256_setzero_si256()), (__mmask8)(U),    \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_ipcvt_roundps_epi8(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvtps2ibs256_mask((__v8sf)(__m256)(A),             \
-                                              (__v8su)_mm256_setzero_si256(),  \
-                                              (__mmask8)-1, (const int)R))
-
-#define _mm256_mask_ipcvt_roundps_epi8(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvtps2ibs256_mask(                                 \
-      (__v8sf)(__m256)(A), (__v8su)(W), (__mmask8)(U), (const int)R))
-
-#define _mm256_maskz_ipcvt_roundps_epi8(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvtps2ibs256_mask((__v8sf)(__m256)(A),             \
-                                              (__v8su)_mm256_setzero_si256(),  \
-                                              (__mmask8)(U), (const int)R))
-
-#define _mm_ipcvtps_epu8(A)                                                    \
+#define _mm_ipcvts_ps_epu8(A)                                                  \
   ((__m128i)__builtin_ia32_vcvtps2iubs128_mask(                                \
       (__v4sf)(__m128)(A), (__v4su)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvtps_epu8(W, U, A)                                         \
+#define _mm_mask_ipcvts_ps_epu8(W, U, A)                                       \
   ((__m128i)__builtin_ia32_vcvtps2iubs128_mask((__v4sf)(__m128)(A),            \
                                                (__v4su)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvtps_epu8(U, A)                                           \
+#define _mm_maskz_ipcvts_ps_epu8(U, A)                                         \
   ((__m128i)__builtin_ia32_vcvtps2iubs128_mask(                                \
       (__v4sf)(__m128)(A), (__v4su)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvtps_epu8(A)                                                 \
+#define _mm256_ipcvts_ps_epu8(A)                                               \
   ((__m256i)__builtin_ia32_vcvtps2iubs256_mask(                                \
       (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)-1,       \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvtps_epu8(W, U, A)                                      \
+#define _mm256_mask_ipcvts_ps_epu8(W, U, A)                                    \
   ((__m256i)__builtin_ia32_vcvtps2iubs256_mask((__v8sf)(__m256)(A),            \
                                                (__v8su)(W), (__mmask8)(U),     \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvtps_epu8(U, A)                                        \
+#define _mm256_maskz_ipcvts_ps_epu8(U, A)                                      \
   ((__m256i)__builtin_ia32_vcvtps2iubs256_mask(                                \
       (__v8sf)(__m256)(A), (__v8su)(_mm256_setzero_si256()), (__mmask8)(U),    \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_ipcvt_roundps_epu8(A, R)                                        \
-  ((__m256i)__builtin_ia32_vcvtps2iubs256_mask((__v8sf)(__m256)(A),            \
-                                               (__v8su)_mm256_setzero_si256(), \
-                                               (__mmask8)-1, (const int)R))
-
-#define _mm256_mask_ipcvt_roundps_epu8(W, U, A, R)                             \
-  ((__m256i)__builtin_ia32_vcvtps2iubs256_mask(                                \
-      (__v8sf)(__m256)(A), (__v8su)(W), (__mmask8)(U), (const int)R))
-
-#define _mm256_maskz_ipcvt_roundps_epu8(U, A, R)                               \
-  ((__m256i)__builtin_ia32_vcvtps2iubs256_mask((__v8sf)(__m256)(A),            \
-                                               (__v8su)_mm256_setzero_si256(), \
-                                               (__mmask8)(U), (const int)R))
-
-#define _mm_ipcvttbf16_epi8(A)                                                 \
+#define _mm_ipcvtts_bf16_epi8(A)                                               \
   ((__m128i)__builtin_ia32_vcvttbf162ibs128((__v8bf)(__m128bh)(A)))
 
-#define _mm_mask_ipcvttbf16_epi8(W, U, A)                                      \
+#define _mm_mask_ipcvtts_bf16_epi8(W, U, A)                                    \
   ((__m128i)__builtin_ia32_selectw_128(                                        \
-      (__mmask8)(U), (__v8hi)_mm_ipcvttbf16_epi8(A), (__v8hi)(__m128i)(W)))
+      (__mmask8)(U), (__v8hi)_mm_ipcvtts_bf16_epi8(A), (__v8hi)(__m128i)(W)))
 
-#define _mm_maskz_ipcvttbf16_epi8(U, A)                                        \
+#define _mm_maskz_ipcvtts_bf16_epi8(U, A)                                      \
   ((__m128i)__builtin_ia32_selectw_128((__mmask8)(U),                          \
-                                       (__v8hi)_mm_ipcvttbf16_epi8(A),         \
+                                       (__v8hi)_mm_ipcvtts_bf16_epi8(A),       \
                                        (__v8hi)_mm_setzero_si128()))
 
-#define _mm256_ipcvttbf16_epi8(A)                                              \
+#define _mm256_ipcvtts_bf16_epi8(A)                                            \
   ((__m256i)__builtin_ia32_vcvttbf162ibs256((__v16bf)(__m256bh)(A)))
 
-#define _mm256_mask_ipcvttbf16_epi8(W, U, A)                                   \
+#define _mm256_mask_ipcvtts_bf16_epi8(W, U, A)                                 \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvttbf16_epi8(A),     \
+                                       (__v16hi)_mm256_ipcvtts_bf16_epi8(A),   \
                                        (__v16hi)(__m256i)(W)))
 
-#define _mm256_maskz_ipcvttbf16_epi8(U, A)                                     \
+#define _mm256_maskz_ipcvtts_bf16_epi8(U, A)                                   \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvttbf16_epi8(A),     \
+                                       (__v16hi)_mm256_ipcvtts_bf16_epi8(A),   \
                                        (__v16hi)_mm256_setzero_si256()))
 
-#define _mm_ipcvttbf16_epu8(A)                                                 \
+#define _mm_ipcvtts_bf16_epu8(A)                                               \
   ((__m128i)__builtin_ia32_vcvttbf162iubs128((__v8bf)(__m128bh)(A)))
 
-#define _mm_mask_ipcvttbf16_epu8(W, U, A)                                      \
+#define _mm_mask_ipcvtts_bf16_epu8(W, U, A)                                    \
   ((__m128i)__builtin_ia32_selectw_128(                                        \
-      (__mmask8)(U), (__v8hi)_mm_ipcvttbf16_epu8(A), (__v8hi)(__m128i)(W)))
+      (__mmask8)(U), (__v8hi)_mm_ipcvtts_bf16_epu8(A), (__v8hi)(__m128i)(W)))
 
-#define _mm_maskz_ipcvttbf16_epu8(U, A)                                        \
+#define _mm_maskz_ipcvtts_bf16_epu8(U, A)                                      \
   ((__m128i)__builtin_ia32_selectw_128((__mmask8)(U),                          \
-                                       (__v8hi)_mm_ipcvttbf16_epu8(A),         \
+                                       (__v8hi)_mm_ipcvtts_bf16_epu8(A),       \
                                        (__v8hi)_mm_setzero_si128()))
 
-#define _mm256_ipcvttbf16_epu8(A)                                              \
+#define _mm256_ipcvtts_bf16_epu8(A)                                            \
   ((__m256i)__builtin_ia32_vcvttbf162iubs256((__v16bf)(__m256bh)(A)))
 
-#define _mm256_mask_ipcvttbf16_epu8(W, U, A)                                   \
+#define _mm256_mask_ipcvtts_bf16_epu8(W, U, A)                                 \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvttbf16_epu8(A),     \
+                                       (__v16hi)_mm256_ipcvtts_bf16_epu8(A),   \
                                        (__v16hi)(__m256i)(W)))
 
-#define _mm256_maskz_ipcvttbf16_epu8(U, A)                                     \
+#define _mm256_maskz_ipcvtts_bf16_epu8(U, A)                                   \
   ((__m256i)__builtin_ia32_selectw_256((__mmask16)(U),                         \
-                                       (__v16hi)_mm256_ipcvttbf16_epu8(A),     \
+                                       (__v16hi)_mm256_ipcvtts_bf16_epu8(A),   \
                                        (__v16hi)_mm256_setzero_si256()))
 
-#define _mm_ipcvttph_epi8(A)                                                   \
+#define _mm_ipcvtts_ph_epi8(A)                                                 \
   ((__m128i)__builtin_ia32_vcvttph2ibs128_mask(                                \
       (__v8hf)(__m128h)(A), (__v8hu)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvttph_epi8(W, U, A)                                        \
+#define _mm_mask_ipcvtts_ph_epi8(W, U, A)                                      \
   ((__m128i)__builtin_ia32_vcvttph2ibs128_mask((__v8hf)(__m128h)(A),           \
                                                (__v8hu)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvttph_epi8(U, A)                                          \
+#define _mm_maskz_ipcvtts_ph_epi8(U, A)                                        \
   ((__m128i)__builtin_ia32_vcvttph2ibs128_mask(                                \
       (__v8hf)(__m128h)(A), (__v8hu)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvttph_epi8(A)                                                \
+#define _mm256_ipcvtts_ph_epi8(A)                                              \
   ((__m256i)__builtin_ia32_vcvttph2ibs256_mask(                                \
       (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)-1,   \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvttph_epi8(W, U, A)                                     \
+#define _mm256_mask_ipcvtts_ph_epi8(W, U, A)                                   \
   ((__m256i)__builtin_ia32_vcvttph2ibs256_mask((__v16hf)(__m256h)(A),          \
                                                (__v16hu)(W), (__mmask16)(U),   \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvttph_epi8(U, A)                                       \
+#define _mm256_maskz_ipcvtts_ph_epi8(U, A)                                     \
   ((__m256i)__builtin_ia32_vcvttph2ibs256_mask(                                \
       (__v16hf)(__m256h)(A), (__v16hu)(_mm256_setzero_si256()),                \
       (__mmask16)(U), _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_ipcvtt_roundph_epi8(A, R)                                       \
-  ((__m256i)__builtin_ia32_vcvttph2ibs256_mask(                                \
-      (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)-1,   \
-      (const int)R))
-
-#define _mm256_mask_ipcvtt_roundph_epi8(W, U, A, R)                            \
-  ((__m256i)__builtin_ia32_vcvttph2ibs256_mask(                                \
-      (__v16hf)(__m256h)(A), (__v16hu)(W), (__mmask16)(U), (const int)R))
-
-#define _mm256_maskz_ipcvtt_roundph_epi8(U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvttph2ibs256_mask(                                \
-      (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)(U),  \
-      (const int)R))
-
-#define _mm_ipcvttph_epu8(A)                                                   \
+#define _mm_ipcvtts_ph_epu8(A)                                                 \
   ((__m128i)__builtin_ia32_vcvttph2iubs128_mask(                               \
       (__v8hf)(__m128h)(A), (__v8hu)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvttph_epu8(W, U, A)                                        \
+#define _mm_mask_ipcvtts_ph_epu8(W, U, A)                                      \
   ((__m128i)__builtin_ia32_vcvttph2iubs128_mask((__v8hf)(__m128h)(A),          \
                                                 (__v8hu)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvttph_epu8(U, A)                                          \
+#define _mm_maskz_ipcvtts_ph_epu8(U, A)                                        \
   ((__m128i)__builtin_ia32_vcvttph2iubs128_mask(                               \
       (__v8hf)(__m128h)(A), (__v8hu)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvttph_epu8(A)                                                \
+#define _mm256_ipcvtts_ph_epu8(A)                                              \
   ((__m256i)__builtin_ia32_vcvttph2iubs256_mask(                               \
       (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)-1,   \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvttph_epu8(W, U, A)                                     \
+#define _mm256_mask_ipcvtts_ph_epu8(W, U, A)                                   \
   ((__m256i)__builtin_ia32_vcvttph2iubs256_mask((__v16hf)(__m256h)(A),         \
                                                 (__v16hu)(W), (__mmask16)(U),  \
                                                 _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvttph_epu8(U, A)                                       \
+#define _mm256_maskz_ipcvtts_ph_epu8(U, A)                                     \
   ((__m256i)__builtin_ia32_vcvttph2iubs256_mask(                               \
       (__v16hf)(__m256h)(A), (__v16hu)(_mm256_setzero_si256()),                \
       (__mmask16)(U), _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_ipcvtt_roundph_epu8(A, R)                                       \
-  ((__m256i)__builtin_ia32_vcvttph2iubs256_mask(                               \
-      (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)-1,   \
-      (const int)R))
-
-#define _mm256_mask_ipcvtt_roundph_epu8(W, U, A, R)                            \
-  ((__m256i)__builtin_ia32_vcvttph2iubs256_mask(                               \
-      (__v16hf)(__m256h)(A), (__v16hu)(W), (__mmask16)(U), (const int)R))
-
-#define _mm256_maskz_ipcvtt_roundph_epu8(U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvttph2iubs256_mask(                               \
-      (__v16hf)(__m256h)(A), (__v16hu)_mm256_setzero_si256(), (__mmask16)(U),  \
-      (const int)R))
-
-#define _mm_ipcvttps_epi8(A)                                                   \
+#define _mm_ipcvtts_ps_epi8(A)                                                 \
   ((__m128i)__builtin_ia32_vcvttps2ibs128_mask(                                \
       (__v4sf)(__m128)(A), (__v4su)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvttps_epi8(W, U, A)                                        \
+#define _mm_mask_ipcvtts_ps_epi8(W, U, A)                                      \
   ((__m128i)__builtin_ia32_vcvttps2ibs128_mask((__v4sf)(__m128)(A),            \
                                                (__v4su)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvttps_epi8(U, A)                                          \
+#define _mm_maskz_ipcvtts_ps_epi8(U, A)                                        \
   ((__m128i)__builtin_ia32_vcvttps2ibs128_mask(                                \
       (__v4sf)(__m128)(A), (__v4su)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvttps_epi8(A)                                                \
+#define _mm256_ipcvtts_ps_epi8(A)                                              \
   ((__m256i)__builtin_ia32_vcvttps2ibs256_mask(                                \
       (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)-1,       \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvttps_epi8(W, U, A)                                     \
+#define _mm256_mask_ipcvtts_ps_epi8(W, U, A)                                   \
   ((__m256i)__builtin_ia32_vcvttps2ibs256_mask((__v8sf)(__m256)(A),            \
                                                (__v8su)(W), (__mmask8)(U),     \
                                                _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvttps_epi8(U, A)                                       \
+#define _mm256_maskz_ipcvtts_ps_epi8(U, A)                                     \
   ((__m256i)__builtin_ia32_vcvttps2ibs256_mask(                                \
       (__v8sf)(__m256)(A), (__v8su)(_mm256_setzero_si256()), (__mmask8)(U),    \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_ipcvtt_roundps_epi8(A, R)                                       \
-  ((__m256i)__builtin_ia32_vcvttps2ibs256_mask((__v8sf)(__m256)(A),            \
-                                               (__v8su)_mm256_setzero_si256(), \
-                                               (__mmask8)-1, (const int)R))
-
-#define _mm256_mask_ipcvtt_roundps_epi8(W, U, A, R)                            \
-  ((__m256i)__builtin_ia32_vcvttps2ibs256_mask(                                \
-      (__v8sf)(__m256)(A), (__v8su)(W), (__mmask8)(U), (const int)R))
-
-#define _mm256_maskz_ipcvtt_roundps_epi8(U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvttps2ibs256_mask((__v8sf)(__m256)(A),            \
-                                               (__v8su)_mm256_setzero_si256(), \
-                                               (__mmask8)(U), (const int)R))
-
-#define _mm_ipcvttps_epu8(A)                                                   \
+#define _mm_ipcvtts_ps_epu8(A)                                                 \
   ((__m128i)__builtin_ia32_vcvttps2iubs128_mask(                               \
       (__v4sf)(__m128)(A), (__v4su)_mm_setzero_si128(), (__mmask8)-1))
 
-#define _mm_mask_ipcvttps_epu8(W, U, A)                                        \
+#define _mm_mask_ipcvtts_ps_epu8(W, U, A)                                      \
   ((__m128i)__builtin_ia32_vcvttps2iubs128_mask((__v4sf)(__m128)(A),           \
                                                 (__v4su)(W), (__mmask8)(U)))
 
-#define _mm_maskz_ipcvttps_epu8(U, A)                                          \
+#define _mm_maskz_ipcvtts_ps_epu8(U, A)                                        \
   ((__m128i)__builtin_ia32_vcvttps2iubs128_mask(                               \
       (__v4sf)(__m128)(A), (__v4su)(_mm_setzero_si128()), (__mmask8)(U)))
 
-#define _mm256_ipcvttps_epu8(A)                                                \
+#define _mm256_ipcvtts_ps_epu8(A)                                              \
   ((__m256i)__builtin_ia32_vcvttps2iubs256_mask(                               \
       (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)-1,       \
       _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_mask_ipcvttps_epu8(W, U, A)                                     \
+#define _mm256_mask_ipcvtts_ps_epu8(W, U, A)                                   \
   ((__m256i)__builtin_ia32_vcvttps2iubs256_mask((__v8sf)(__m256)(A),           \
                                                 (__v8su)(W), (__mmask8)(U),    \
                                                 _MM_FROUND_CUR_DIRECTION))
 
-#define _mm256_maskz_ipcvttps_epu8(U, A)                                       \
+#define _mm256_maskz_ipcvtts_ps_epu8(U, A)                                     \
   ((__m256i)__builtin_ia32_vcvttps2iubs256_mask(                               \
       (__v8sf)(__m256)(A), (__v8su)(_mm256_setzero_si256()), (__mmask8)(U),    \
       _MM_FROUND_CUR_DIRECTION))
-
-#define _mm256_ipcvtt_roundps_epu8(A, R)                                       \
-  ((__m256i)__builtin_ia32_vcvttps2iubs256_mask(                               \
-      (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)-1,       \
-      (const int)R))
-
-#define _mm256_mask_ipcvtt_roundps_epu8(W, U, A, R)                            \
-  ((__m256i)__builtin_ia32_vcvttps2iubs256_mask(                               \
-      (__v8sf)(__m256)(A), (__v8su)(W), (__mmask8)(U), (const int)R))
-
-#define _mm256_maskz_ipcvtt_roundps_epu8(U, A, R)                              \
-  ((__m256i)__builtin_ia32_vcvttps2iubs256_mask(                               \
-      (__v8sf)(__m256)(A), (__v8su)_mm256_setzero_si256(), (__mmask8)(U),      \
-      (const int)R))
 #endif // __AVX10_2SATCVTINTRIN_H
diff --git a/clang/lib/Headers/nvptxintrin.h b/clang/lib/Headers/nvptxintrin.h
index 0afcb1c5ff0f..ea21359840ed 100644
--- a/clang/lib/Headers/nvptxintrin.h
+++ b/clang/lib/Headers/nvptxintrin.h
@@ -127,7 +127,8 @@ __gpu_read_first_lane_u64(uint64_t __lane_mask, uint64_t __x) {
                                              __gpu_num_lanes() - 1)
           << 32ull) |
          ((uint64_t)__nvvm_shfl_sync_idx_i32(__mask, __lo, __id,
-                                             __gpu_num_lanes() - 1));
+                                             __gpu_num_lanes() - 1) &
+          0xFFFFFFFF);
 }
 
 // Returns a bitmask of threads in the current lane for which \p x is true.
diff --git a/clang/lib/Headers/vecintrin.h b/clang/lib/Headers/vecintrin.h
index a14c39f9f731..338ea51ce886 100644
--- a/clang/lib/Headers/vecintrin.h
+++ b/clang/lib/Headers/vecintrin.h
@@ -7,6 +7,9 @@
  *===-----------------------------------------------------------------------===
  */
 
+#ifndef _VECINTRIN_H
+#define _VECINTRIN_H
+
 #if defined(__s390x__) && defined(__VEC__)
 
 #define __ATTRS_ai __attribute__((__always_inline__))
@@ -12861,3 +12864,5 @@ vec_search_string_until_zero_cc(__vector unsigned int __a,
 #error "Use -fzvector to enable vector extensions"
 
 #endif
+
+#endif /* _VECINTRIN_H */
diff --git a/clang/lib/Interpreter/DeviceOffload.cpp b/clang/lib/Interpreter/DeviceOffload.cpp
index 1999d63d1aa0..05625ddedb72 100644
--- a/clang/lib/Interpreter/DeviceOffload.cpp
+++ b/clang/lib/Interpreter/DeviceOffload.cpp
@@ -25,16 +25,14 @@
 namespace clang {
 
 IncrementalCUDADeviceParser::IncrementalCUDADeviceParser(
-    std::unique_ptr<CompilerInstance> DeviceInstance,
-    CompilerInstance &HostInstance,
+    CompilerInstance &DeviceInstance, CompilerInstance &HostInstance,
     llvm::IntrusiveRefCntPtr<llvm::vfs::InMemoryFileSystem> FS,
     llvm::Error &Err, const std::list<PartialTranslationUnit> &PTUs)
-    : IncrementalParser(*DeviceInstance, Err), PTUs(PTUs), VFS(FS),
+    : IncrementalParser(DeviceInstance, Err), PTUs(PTUs), VFS(FS),
       CodeGenOpts(HostInstance.getCodeGenOpts()),
-      TargetOpts(HostInstance.getTargetOpts()) {
+      TargetOpts(DeviceInstance.getTargetOpts()) {
   if (Err)
     return;
-  DeviceCI = std::move(DeviceInstance);
   StringRef Arch = TargetOpts.CPU;
   if (!Arch.starts_with("sm_") || Arch.substr(3).getAsInteger(10, SMVersion)) {
     Err = llvm::joinErrors(std::move(Err), llvm::make_error<llvm::StringError>(
@@ -44,34 +42,6 @@ IncrementalCUDADeviceParser::IncrementalCUDADeviceParser(
   }
 }
 
-llvm::Expected<TranslationUnitDecl *>
-IncrementalCUDADeviceParser::Parse(llvm::StringRef Input) {
-  auto PTU = IncrementalParser::Parse(Input);
-  if (!PTU)
-    return PTU.takeError();
-
-  auto PTX = GeneratePTX();
-  if (!PTX)
-    return PTX.takeError();
-
-  auto Err = GenerateFatbinary();
-  if (Err)
-    return std::move(Err);
-
-  std::string FatbinFileName =
-      "/incr_module_" + std::to_string(PTUs.size()) + ".fatbin";
-  VFS->addFile(FatbinFileName, 0,
-               llvm::MemoryBuffer::getMemBuffer(
-                   llvm::StringRef(FatbinContent.data(), FatbinContent.size()),
-                   "", false));
-
-  CodeGenOpts.CudaGpuBinaryFileName = FatbinFileName;
-
-  FatbinContent.clear();
-
-  return PTU;
-}
-
 llvm::Expected<llvm::StringRef> IncrementalCUDADeviceParser::GeneratePTX() {
   auto &PTU = PTUs.back();
   std::string Error;
@@ -172,6 +142,19 @@ llvm::Error IncrementalCUDADeviceParser::GenerateFatbinary() {
 
   FatbinContent.append(PTXCode.begin(), PTXCode.end());
 
+  const PartialTranslationUnit &PTU = PTUs.back();
+
+  std::string FatbinFileName = "/" + PTU.TheModule->getName().str() + ".fatbin";
+
+  VFS->addFile(FatbinFileName, 0,
+               llvm::MemoryBuffer::getMemBuffer(
+                   llvm::StringRef(FatbinContent.data(), FatbinContent.size()),
+                   "", false));
+
+  CodeGenOpts.CudaGpuBinaryFileName = FatbinFileName;
+
+  FatbinContent.clear();
+
   return llvm::Error::success();
 }
 
diff --git a/clang/lib/Interpreter/DeviceOffload.h b/clang/lib/Interpreter/DeviceOffload.h
index b9a1acab004c..0b903e31c679 100644
--- a/clang/lib/Interpreter/DeviceOffload.h
+++ b/clang/lib/Interpreter/DeviceOffload.h
@@ -28,13 +28,10 @@ class IncrementalCUDADeviceParser : public IncrementalParser {
 
 public:
   IncrementalCUDADeviceParser(
-      std::unique_ptr<CompilerInstance> DeviceInstance,
-      CompilerInstance &HostInstance,
+      CompilerInstance &DeviceInstance, CompilerInstance &HostInstance,
       llvm::IntrusiveRefCntPtr<llvm::vfs::InMemoryFileSystem> VFS,
       llvm::Error &Err, const std::list<PartialTranslationUnit> &PTUs);
 
-  llvm::Expected<TranslationUnitDecl *> Parse(llvm::StringRef Input) override;
-
   // Generate PTX for the last PTU.
   llvm::Expected<llvm::StringRef> GeneratePTX();
 
@@ -44,7 +41,6 @@ public:
   ~IncrementalCUDADeviceParser();
 
 protected:
-  std::unique_ptr<CompilerInstance> DeviceCI;
   int SMVersion;
   llvm::SmallString<1024> PTXCode;
   llvm::SmallVector<char, 1024> FatbinContent;
diff --git a/clang/lib/Interpreter/IncrementalExecutor.h b/clang/lib/Interpreter/IncrementalExecutor.h
index dbd61f0b8b1e..71d71bc3883e 100644
--- a/clang/lib/Interpreter/IncrementalExecutor.h
+++ b/clang/lib/Interpreter/IncrementalExecutor.h
@@ -57,7 +57,7 @@ public:
   virtual llvm::Error removeModule(PartialTranslationUnit &PTU);
   virtual llvm::Error runCtors() const;
   virtual llvm::Error cleanUp();
-  llvm::Expected<llvm::orc::ExecutorAddr>
+  virtual llvm::Expected<llvm::orc::ExecutorAddr>
   getSymbolAddress(llvm::StringRef Name, SymbolNameKind NameKind) const;
 
   llvm::orc::LLJIT &GetExecutionEngine() { return *Jit; }
diff --git a/clang/lib/Interpreter/Interpreter.cpp b/clang/lib/Interpreter/Interpreter.cpp
index fa4c1439c926..3b81f9d701b4 100644
--- a/clang/lib/Interpreter/Interpreter.cpp
+++ b/clang/lib/Interpreter/Interpreter.cpp
@@ -18,6 +18,7 @@
 #include "llvm/Support/VirtualFileSystem.h"
 #ifdef __EMSCRIPTEN__
 #include "Wasm.h"
+#include <dlfcn.h>
 #endif // __EMSCRIPTEN__
 
 #include "clang/AST/ASTConsumer.h"
@@ -415,6 +416,10 @@ Interpreter::Interpreter(std::unique_ptr<CompilerInstance> Instance,
 Interpreter::~Interpreter() {
   IncrParser.reset();
   Act->FinalizeAction();
+  if (DeviceParser)
+    DeviceParser.reset();
+  if (DeviceAct)
+    DeviceAct->FinalizeAction();
   if (IncrExecutor) {
     if (llvm::Error Err = IncrExecutor->cleanUp())
       llvm::report_fatal_error(
@@ -480,20 +485,37 @@ Interpreter::createWithCUDA(std::unique_ptr<CompilerInstance> CI,
   OverlayVFS->pushOverlay(IMVFS);
   CI->createFileManager(OverlayVFS);
 
-  auto Interp = Interpreter::create(std::move(CI));
-  if (auto E = Interp.takeError())
-    return std::move(E);
+  llvm::Expected<std::unique_ptr<Interpreter>> InterpOrErr =
+      Interpreter::create(std::move(CI));
+  if (!InterpOrErr)
+    return InterpOrErr;
+
+  std::unique_ptr<Interpreter> Interp = std::move(*InterpOrErr);
 
   llvm::Error Err = llvm::Error::success();
-  auto DeviceParser = std::make_unique<IncrementalCUDADeviceParser>(
-      std::move(DCI), *(*Interp)->getCompilerInstance(), IMVFS, Err,
-      (*Interp)->PTUs);
+  llvm::LLVMContext &LLVMCtx = *Interp->TSCtx->getContext();
+
+  auto DeviceAct =
+      std::make_unique<IncrementalAction>(*DCI, LLVMCtx, Err, *Interp);
+
   if (Err)
     return std::move(Err);
 
-  (*Interp)->DeviceParser = std::move(DeviceParser);
+  Interp->DeviceAct = std::move(DeviceAct);
+
+  DCI->ExecuteAction(*Interp->DeviceAct);
+
+  Interp->DeviceCI = std::move(DCI);
+
+  auto DeviceParser = std::make_unique<IncrementalCUDADeviceParser>(
+      *Interp->DeviceCI, *Interp->getCompilerInstance(), IMVFS, Err,
+      Interp->PTUs);
+
+  if (Err)
+    return std::move(Err);
 
-  return Interp;
+  Interp->DeviceParser = std::move(DeviceParser);
+  return std::move(Interp);
 }
 
 const CompilerInstance *Interpreter::getCompilerInstance() const {
@@ -531,15 +553,17 @@ size_t Interpreter::getEffectivePTUSize() const {
 
 PartialTranslationUnit &
 Interpreter::RegisterPTU(TranslationUnitDecl *TU,
-                         std::unique_ptr<llvm::Module> M /*={}*/) {
+                         std::unique_ptr<llvm::Module> M /*={}*/,
+                         IncrementalAction *Action) {
   PTUs.emplace_back(PartialTranslationUnit());
   PartialTranslationUnit &LastPTU = PTUs.back();
   LastPTU.TUPart = TU;
 
   if (!M)
-    M = GenModule();
+    M = GenModule(Action);
 
-  assert((!getCodeGen() || M) && "Must have a llvm::Module at this point");
+  assert((!getCodeGen(Action) || M) &&
+         "Must have a llvm::Module at this point");
 
   LastPTU.TheModule = std::move(M);
   LLVM_DEBUG(llvm::dbgs() << "compile-ptu " << PTUs.size() - 1
@@ -559,6 +583,16 @@ Interpreter::Parse(llvm::StringRef Code) {
     llvm::Expected<TranslationUnitDecl *> DeviceTU = DeviceParser->Parse(Code);
     if (auto E = DeviceTU.takeError())
       return std::move(E);
+
+    RegisterPTU(*DeviceTU, nullptr, DeviceAct.get());
+
+    llvm::Expected<llvm::StringRef> PTX = DeviceParser->GeneratePTX();
+    if (!PTX)
+      return PTX.takeError();
+
+    llvm::Error Err = DeviceParser->GenerateFatbinary();
+    if (Err)
+      return std::move(Err);
   }
 
   // Tell the interpreter sliently ignore unused expressions since value
@@ -711,6 +745,14 @@ llvm::Error Interpreter::Undo(unsigned N) {
 }
 
 llvm::Error Interpreter::LoadDynamicLibrary(const char *name) {
+#ifdef __EMSCRIPTEN__
+  void *handle = dlopen(name, RTLD_NOW | RTLD_GLOBAL);
+  if (!handle) {
+    llvm::errs() << dlerror() << '\n';
+    return llvm::make_error<llvm::StringError>("Failed to load dynamic library",
+                                               llvm::inconvertibleErrorCode());
+  }
+#else
   auto EE = getExecutionEngine();
   if (!EE)
     return EE.takeError();
@@ -722,13 +764,15 @@ llvm::Error Interpreter::LoadDynamicLibrary(const char *name) {
     EE->getMainJITDylib().addGenerator(std::move(*DLSG));
   else
     return DLSG.takeError();
+#endif
 
   return llvm::Error::success();
 }
 
-std::unique_ptr<llvm::Module> Interpreter::GenModule() {
+std::unique_ptr<llvm::Module>
+Interpreter::GenModule(IncrementalAction *Action) {
   static unsigned ID = 0;
-  if (CodeGenerator *CG = getCodeGen()) {
+  if (CodeGenerator *CG = getCodeGen(Action)) {
     // Clang's CodeGen is designed to work with a single llvm::Module. In many
     // cases for convenience various CodeGen parts have a reference to the
     // llvm::Module (TheModule or Module) which does not change when a new
@@ -750,8 +794,10 @@ std::unique_ptr<llvm::Module> Interpreter::GenModule() {
   return nullptr;
 }
 
-CodeGenerator *Interpreter::getCodeGen() const {
-  FrontendAction *WrappedAct = Act->getWrapped();
+CodeGenerator *Interpreter::getCodeGen(IncrementalAction *Action) const {
+  if (!Action)
+    Action = Act.get();
+  FrontendAction *WrappedAct = Action->getWrapped();
   if (!WrappedAct->hasIRSupport())
     return nullptr;
   return static_cast<CodeGenAction *>(WrappedAct)->getCodeGenerator();
diff --git a/clang/lib/Interpreter/Wasm.cpp b/clang/lib/Interpreter/Wasm.cpp
index aa10b160ccf8..74c83169ced6 100644
--- a/clang/lib/Interpreter/Wasm.cpp
+++ b/clang/lib/Interpreter/Wasm.cpp
@@ -144,6 +144,19 @@ llvm::Error WasmIncrementalExecutor::cleanUp() {
   return llvm::Error::success();
 }
 
+llvm::Expected<llvm::orc::ExecutorAddr>
+WasmIncrementalExecutor::getSymbolAddress(llvm::StringRef Name,
+                                          SymbolNameKind NameKind) const {
+  void *Sym = dlsym(RTLD_DEFAULT, Name.str().c_str());
+  if (!Sym) {
+    return llvm::make_error<llvm::StringError>("dlsym failed for symbol: " +
+                                                   Name.str(),
+                                               llvm::inconvertibleErrorCode());
+  }
+
+  return llvm::orc::ExecutorAddr::fromPtr(Sym);
+}
+
 WasmIncrementalExecutor::~WasmIncrementalExecutor() = default;
 
 } // namespace clang
\ No newline at end of file
diff --git a/clang/lib/Interpreter/Wasm.h b/clang/lib/Interpreter/Wasm.h
index 4632613326d3..9a752934e318 100644
--- a/clang/lib/Interpreter/Wasm.h
+++ b/clang/lib/Interpreter/Wasm.h
@@ -29,6 +29,9 @@ public:
   llvm::Error removeModule(PartialTranslationUnit &PTU) override;
   llvm::Error runCtors() const override;
   llvm::Error cleanUp() override;
+  llvm::Expected<llvm::orc::ExecutorAddr>
+  getSymbolAddress(llvm::StringRef Name,
+                   SymbolNameKind NameKind) const override;
 
   ~WasmIncrementalExecutor() override;
 };
diff --git a/clang/lib/Parse/ParseExpr.cpp b/clang/lib/Parse/ParseExpr.cpp
index 0cadede51a9b..2fab1dfed4a0 100644
--- a/clang/lib/Parse/ParseExpr.cpp
+++ b/clang/lib/Parse/ParseExpr.cpp
@@ -2237,8 +2237,6 @@ Parser::ParsePostfixExpressionSuffix(ExprResult LHS) {
             if (PP.isCodeCompletionReached() && !CalledSignatureHelp)
               RunSignatureHelp();
             LHS = ExprError();
-          } else if (!HasError && HasTrailingComma) {
-            Diag(Tok, diag::err_expected_expression);
           } else if (LHS.isInvalid()) {
             for (auto &E : ArgExprs)
               Actions.CorrectDelayedTyposInExpr(E);
@@ -3738,7 +3736,6 @@ bool Parser::ParseExpressionList(SmallVectorImpl<Expr *> &Exprs,
     if (Tok.is(tok::r_paren)) {
       if (HasTrailingComma)
         *HasTrailingComma = true;
-      break;
     }
   }
   if (SawError) {
diff --git a/clang/lib/Parse/ParseInit.cpp b/clang/lib/Parse/ParseInit.cpp
index 63b1d7bd9db5..471b3eaf2828 100644
--- a/clang/lib/Parse/ParseInit.cpp
+++ b/clang/lib/Parse/ParseInit.cpp
@@ -445,7 +445,7 @@ ExprResult Parser::createEmbedExpr() {
           Context.MakeIntValue(Str.size(), Context.getSizeType());
       QualType ArrayTy = Context.getConstantArrayType(
           Ty, ArraySize, nullptr, ArraySizeModifier::Normal, 0);
-      return StringLiteral::Create(Context, Str, StringLiteralKind::Ordinary,
+      return StringLiteral::Create(Context, Str, StringLiteralKind::Binary,
                                    false, ArrayTy, StartLoc);
     };
 
diff --git a/clang/lib/Sema/Sema.cpp b/clang/lib/Sema/Sema.cpp
index 9507d7602aa4..e0eac690e6e6 100644
--- a/clang/lib/Sema/Sema.cpp
+++ b/clang/lib/Sema/Sema.cpp
@@ -1789,6 +1789,47 @@ public:
       Inherited::visitUsedDecl(Loc, D);
   }
 
+  // Visitor member and parent dtors called by this dtor.
+  void VisitCalledDestructors(CXXDestructorDecl *DD) {
+    const CXXRecordDecl *RD = DD->getParent();
+
+    // Visit the dtors of all members
+    for (const FieldDecl *FD : RD->fields()) {
+      QualType FT = FD->getType();
+      if (const auto *RT = FT->getAs<RecordType>())
+        if (const auto *ClassDecl = dyn_cast<CXXRecordDecl>(RT->getDecl()))
+          if (ClassDecl->hasDefinition())
+            if (CXXDestructorDecl *MemberDtor = ClassDecl->getDestructor())
+              asImpl().visitUsedDecl(MemberDtor->getLocation(), MemberDtor);
+    }
+
+    // Also visit base class dtors
+    for (const auto &Base : RD->bases()) {
+      QualType BaseType = Base.getType();
+      if (const auto *RT = BaseType->getAs<RecordType>())
+        if (const auto *BaseDecl = dyn_cast<CXXRecordDecl>(RT->getDecl()))
+          if (BaseDecl->hasDefinition())
+            if (CXXDestructorDecl *BaseDtor = BaseDecl->getDestructor())
+              asImpl().visitUsedDecl(BaseDtor->getLocation(), BaseDtor);
+    }
+  }
+
+  void VisitDeclStmt(DeclStmt *DS) {
+    // Visit dtors called by variables that need destruction
+    for (auto *D : DS->decls())
+      if (auto *VD = dyn_cast<VarDecl>(D))
+        if (VD->isThisDeclarationADefinition() &&
+            VD->needsDestruction(S.Context)) {
+          QualType VT = VD->getType();
+          if (const auto *RT = VT->getAs<RecordType>())
+            if (const auto *ClassDecl = dyn_cast<CXXRecordDecl>(RT->getDecl()))
+              if (ClassDecl->hasDefinition())
+                if (CXXDestructorDecl *Dtor = ClassDecl->getDestructor())
+                  asImpl().visitUsedDecl(Dtor->getLocation(), Dtor);
+        }
+
+    Inherited::VisitDeclStmt(DS);
+  }
   void checkVar(VarDecl *VD) {
     assert(VD->isFileVarDecl() &&
            "Should only check file-scope variables");
@@ -1830,6 +1871,8 @@ public:
     if (auto *S = FD->getBody()) {
       this->Visit(S);
     }
+    if (CXXDestructorDecl *Dtor = dyn_cast<CXXDestructorDecl>(FD))
+      asImpl().VisitCalledDestructors(Dtor);
     UsePath.pop_back();
     InUsePath.erase(FD);
   }
diff --git a/clang/lib/Sema/SemaAccess.cpp b/clang/lib/Sema/SemaAccess.cpp
index f79d9a758e7a..6813786df3fc 100644
--- a/clang/lib/Sema/SemaAccess.cpp
+++ b/clang/lib/Sema/SemaAccess.cpp
@@ -1518,8 +1518,8 @@ void Sema::HandleDelayedAccessCheck(DelayedDiagnostic &DD, Decl *D) {
   } else if (FunctionDecl *FN = dyn_cast<FunctionDecl>(D)) {
     DC = FN;
   } else if (TemplateDecl *TD = dyn_cast<TemplateDecl>(D)) {
-    if (isa<DeclContext>(TD->getTemplatedDecl()))
-      DC = cast<DeclContext>(TD->getTemplatedDecl());
+    if (auto *D = dyn_cast_if_present<DeclContext>(TD->getTemplatedDecl()))
+      DC = D;
   } else if (auto *RD = dyn_cast<RequiresExprBodyDecl>(D)) {
     DC = RD;
   }
diff --git a/clang/lib/Sema/SemaAttr.cpp b/clang/lib/Sema/SemaAttr.cpp
index 6907fa91e28c..27b5eb5f2c77 100644
--- a/clang/lib/Sema/SemaAttr.cpp
+++ b/clang/lib/Sema/SemaAttr.cpp
@@ -14,6 +14,7 @@
 #include "CheckExprLifetime.h"
 #include "clang/AST/ASTConsumer.h"
 #include "clang/AST/Attr.h"
+#include "clang/AST/DeclCXX.h"
 #include "clang/AST/Expr.h"
 #include "clang/Basic/TargetInfo.h"
 #include "clang/Lex/Preprocessor.h"
@@ -219,6 +220,10 @@ void Sema::inferGslOwnerPointerAttribute(CXXRecordDecl *Record) {
 void Sema::inferLifetimeBoundAttribute(FunctionDecl *FD) {
   if (FD->getNumParams() == 0)
     return;
+  // Skip void returning functions (except constructors). This can occur in
+  // cases like 'as_const'.
+  if (!isa<CXXConstructorDecl>(FD) && FD->getReturnType()->isVoidType())
+    return;
 
   if (unsigned BuiltinID = FD->getBuiltinID()) {
     // Add lifetime attribute to std::move, std::fowrard et al.
diff --git a/clang/lib/Sema/SemaCUDA.cpp b/clang/lib/Sema/SemaCUDA.cpp
index 0e1bf727d72d..0e5fc5e1a40b 100644
--- a/clang/lib/Sema/SemaCUDA.cpp
+++ b/clang/lib/Sema/SemaCUDA.cpp
@@ -372,6 +372,21 @@ bool SemaCUDA::inferTargetForImplicitSpecialMember(CXXRecordDecl *ClassDecl,
                                                    CXXMethodDecl *MemberDecl,
                                                    bool ConstRHS,
                                                    bool Diagnose) {
+  // If MemberDecl is virtual destructor of an explicit template class
+  // instantiation, it must be emitted, therefore it needs to be inferred
+  // conservatively by ignoring implicit host/device attrs of member and parent
+  // dtors called by it. Also, it needs to be checed by deferred diag visitor.
+  bool IsExpVDtor = false;
+  if (isa<CXXDestructorDecl>(MemberDecl) && MemberDecl->isVirtual()) {
+    if (auto *Spec = dyn_cast<ClassTemplateSpecializationDecl>(ClassDecl)) {
+      TemplateSpecializationKind TSK = Spec->getTemplateSpecializationKind();
+      IsExpVDtor = TSK == TSK_ExplicitInstantiationDeclaration ||
+                   TSK == TSK_ExplicitInstantiationDefinition;
+    }
+  }
+  if (IsExpVDtor)
+    SemaRef.DeclsToCheckForDeferredDiags.insert(MemberDecl);
+
   // If the defaulted special member is defined lexically outside of its
   // owning class, or the special member already has explicit device or host
   // attributes, do not infer.
@@ -422,7 +437,9 @@ bool SemaCUDA::inferTargetForImplicitSpecialMember(CXXRecordDecl *ClassDecl,
     if (!SMOR.getMethod())
       continue;
 
-    CUDAFunctionTarget BaseMethodTarget = IdentifyTarget(SMOR.getMethod());
+    CUDAFunctionTarget BaseMethodTarget =
+        IdentifyTarget(SMOR.getMethod(), IsExpVDtor);
+
     if (!InferredTarget) {
       InferredTarget = BaseMethodTarget;
     } else {
@@ -466,7 +483,9 @@ bool SemaCUDA::inferTargetForImplicitSpecialMember(CXXRecordDecl *ClassDecl,
     if (!SMOR.getMethod())
       continue;
 
-    CUDAFunctionTarget FieldMethodTarget = IdentifyTarget(SMOR.getMethod());
+    CUDAFunctionTarget FieldMethodTarget =
+        IdentifyTarget(SMOR.getMethod(), IsExpVDtor);
+
     if (!InferredTarget) {
       InferredTarget = FieldMethodTarget;
     } else {
diff --git a/clang/lib/Sema/SemaCast.cpp b/clang/lib/Sema/SemaCast.cpp
index 54bc52fa2ac4..d0d44e889913 100644
--- a/clang/lib/Sema/SemaCast.cpp
+++ b/clang/lib/Sema/SemaCast.cpp
@@ -1151,10 +1151,33 @@ static unsigned int checkCastFunctionType(Sema &Self, const ExprResult &SrcExpr,
     return false;
   };
 
+  auto IsFarProc = [](const FunctionType *T) {
+    // The definition of FARPROC depends on the platform in terms of its return
+    // type, which could be int, or long long, etc. We'll look for a source
+    // signature for: <integer type> (*)() and call that "close enough" to
+    // FARPROC to be sufficient to silence the diagnostic. This is similar to
+    // how we allow casts between function pointers and void * for supporting
+    // dlsym.
+    // Note: we could check for __stdcall on the function pointer as well, but
+    // that seems like splitting hairs.
+    if (!T->getReturnType()->isIntegerType())
+      return false;
+    if (const auto *PT = T->getAs<FunctionProtoType>())
+      return !PT->isVariadic() && PT->getNumParams() == 0;
+    return true;
+  };
+
   // Skip if either function type is void(*)(void)
   if (IsVoidVoid(SrcFTy) || IsVoidVoid(DstFTy))
     return 0;
 
+  // On Windows, GetProcAddress() returns a FARPROC, which is a typedef for a
+  // function pointer type (with no prototype, in C). We don't want to diagnose
+  // this case so we don't diagnose idiomatic code on Windows.
+  if (Self.getASTContext().getTargetInfo().getTriple().isOSWindows() &&
+      IsFarProc(SrcFTy))
+    return 0;
+
   // Check return type.
   if (!argTypeIsABIEquivalent(SrcFTy->getReturnType(), DstFTy->getReturnType(),
                               Self.Context))
diff --git a/clang/lib/Sema/SemaConcept.cpp b/clang/lib/Sema/SemaConcept.cpp
index a7b609f7f3ce..8adebccde042 100644
--- a/clang/lib/Sema/SemaConcept.cpp
+++ b/clang/lib/Sema/SemaConcept.cpp
@@ -702,75 +702,6 @@ bool Sema::CheckConstraintSatisfaction(const Expr *ConstraintExpr,
       .isInvalid();
 }
 
-bool Sema::addInstantiatedCapturesToScope(
-    FunctionDecl *Function, const FunctionDecl *PatternDecl,
-    LocalInstantiationScope &Scope,
-    const MultiLevelTemplateArgumentList &TemplateArgs) {
-  const auto *LambdaClass = cast<CXXMethodDecl>(Function)->getParent();
-  const auto *LambdaPattern = cast<CXXMethodDecl>(PatternDecl)->getParent();
-
-  unsigned Instantiated = 0;
-
-  // FIXME: This is a workaround for not having deferred lambda body
-  // instantiation.
-  // When transforming a lambda's body, if we encounter another call to a
-  // nested lambda that contains a constraint expression, we add all of the
-  // outer lambda's instantiated captures to the current instantiation scope to
-  // facilitate constraint evaluation. However, these captures don't appear in
-  // the CXXRecordDecl until after the lambda expression is rebuilt, so we
-  // pull them out from the corresponding LSI.
-  LambdaScopeInfo *InstantiatingScope = nullptr;
-  if (LambdaPattern->capture_size() && !LambdaClass->capture_size()) {
-    for (FunctionScopeInfo *Scope : llvm::reverse(FunctionScopes)) {
-      auto *LSI = dyn_cast<LambdaScopeInfo>(Scope);
-      if (!LSI ||
-          LSI->CallOperator->getTemplateInstantiationPattern() != PatternDecl)
-        continue;
-      InstantiatingScope = LSI;
-      break;
-    }
-    assert(InstantiatingScope);
-  }
-
-  auto AddSingleCapture = [&](const ValueDecl *CapturedPattern,
-                              unsigned Index) {
-    ValueDecl *CapturedVar =
-        InstantiatingScope ? InstantiatingScope->Captures[Index].getVariable()
-                           : LambdaClass->getCapture(Index)->getCapturedVar();
-    assert(CapturedVar->isInitCapture());
-    Scope.InstantiatedLocal(CapturedPattern, CapturedVar);
-  };
-
-  for (const LambdaCapture &CapturePattern : LambdaPattern->captures()) {
-    if (!CapturePattern.capturesVariable()) {
-      Instantiated++;
-      continue;
-    }
-    ValueDecl *CapturedPattern = CapturePattern.getCapturedVar();
-
-    if (!CapturedPattern->isInitCapture()) {
-      Instantiated++;
-      continue;
-    }
-
-    if (!CapturedPattern->isParameterPack()) {
-      AddSingleCapture(CapturedPattern, Instantiated++);
-    } else {
-      Scope.MakeInstantiatedLocalArgPack(CapturedPattern);
-      SmallVector<UnexpandedParameterPack, 2> Unexpanded;
-      SemaRef.collectUnexpandedParameterPacks(
-          dyn_cast<VarDecl>(CapturedPattern)->getInit(), Unexpanded);
-      auto NumArgumentsInExpansion =
-          getNumArgumentsInExpansionFromUnexpanded(Unexpanded, TemplateArgs);
-      if (!NumArgumentsInExpansion)
-        continue;
-      for (unsigned Arg = 0; Arg < *NumArgumentsInExpansion; ++Arg)
-        AddSingleCapture(CapturedPattern, Instantiated++);
-    }
-  }
-  return false;
-}
-
 bool Sema::SetupConstraintScope(
     FunctionDecl *FD, std::optional<ArrayRef<TemplateArgument>> TemplateArgs,
     const MultiLevelTemplateArgumentList &MLTAL,
diff --git a/clang/lib/Sema/SemaDecl.cpp b/clang/lib/Sema/SemaDecl.cpp
index 01f09aba8c2a..41d5f9f2f342 100644
--- a/clang/lib/Sema/SemaDecl.cpp
+++ b/clang/lib/Sema/SemaDecl.cpp
@@ -8145,7 +8145,7 @@ NamedDecl *Sema::ActOnVariableDeclarator(
               (D.getCXXScopeSpec().isSet() && DC && DC->isRecord() &&
                DC->isDependentContext())
                   ? TPC_ClassTemplateMember
-                  : TPC_VarTemplate))
+                  : TPC_Other))
         NewVD->setInvalidDecl();
 
       // If we are providing an explicit specialization of a static variable
@@ -20388,6 +20388,21 @@ Sema::FunctionEmissionStatus Sema::getEmissionStatus(const FunctionDecl *FD,
 
     if (IsEmittedForExternalSymbol())
       return FunctionEmissionStatus::Emitted;
+
+    // If FD is a virtual destructor of an explicit instantiation
+    // of a template class, return Emitted.
+    if (auto *Destructor = dyn_cast<CXXDestructorDecl>(FD)) {
+      if (Destructor->isVirtual()) {
+        if (auto *Spec = dyn_cast<ClassTemplateSpecializationDecl>(
+                Destructor->getParent())) {
+          TemplateSpecializationKind TSK =
+              Spec->getTemplateSpecializationKind();
+          if (TSK == TSK_ExplicitInstantiationDeclaration ||
+              TSK == TSK_ExplicitInstantiationDefinition)
+            return FunctionEmissionStatus::Emitted;
+        }
+      }
+    }
   }
 
   // Otherwise, the function is known-emitted if it's in our set of
diff --git a/clang/lib/Sema/SemaDeclCXX.cpp b/clang/lib/Sema/SemaDeclCXX.cpp
index e4e3bbad1f52..85de46c9adab 100644
--- a/clang/lib/Sema/SemaDeclCXX.cpp
+++ b/clang/lib/Sema/SemaDeclCXX.cpp
@@ -13533,7 +13533,7 @@ Decl *Sema::ActOnAliasDeclaration(Scope *S, AccessSpecifier AS,
     // Merge any previous default template arguments into our parameters,
     // and check the parameter list.
     if (CheckTemplateParameterList(TemplateParams, OldTemplateParams,
-                                   TPC_TypeAliasTemplate))
+                                   TPC_Other))
       return nullptr;
 
     TypeAliasTemplateDecl *NewDecl =
diff --git a/clang/lib/Sema/SemaExprCXX.cpp b/clang/lib/Sema/SemaExprCXX.cpp
index 1e39d69e8b23..c6621402adfc 100644
--- a/clang/lib/Sema/SemaExprCXX.cpp
+++ b/clang/lib/Sema/SemaExprCXX.cpp
@@ -4143,6 +4143,7 @@ Sema::IsStringLiteralToNonConstPointerConversion(Expr *From, QualType ToType) {
             // We don't allow UTF literals to be implicitly converted
             break;
           case StringLiteralKind::Ordinary:
+          case StringLiteralKind::Binary:
             return (ToPointeeType->getKind() == BuiltinType::Char_U ||
                     ToPointeeType->getKind() == BuiltinType::Char_S);
           case StringLiteralKind::Wide:
diff --git a/clang/lib/Sema/SemaExprMember.cpp b/clang/lib/Sema/SemaExprMember.cpp
index d130e8b86bc5..adb8e3cc90c0 100644
--- a/clang/lib/Sema/SemaExprMember.cpp
+++ b/clang/lib/Sema/SemaExprMember.cpp
@@ -1136,7 +1136,6 @@ Sema::BuildMemberReferenceExpr(Expr *BaseExpr, QualType BaseExprType,
     if (Converted.isInvalid())
       return true;
     BaseExpr = Converted.get();
-    DiagnoseDiscardedExprMarkedNodiscard(BaseExpr);
     return false;
   };
   auto ConvertBaseExprToGLValue = [&] {
diff --git a/clang/lib/Sema/SemaInit.cpp b/clang/lib/Sema/SemaInit.cpp
index 37796758960c..6e9ed875b50c 100644
--- a/clang/lib/Sema/SemaInit.cpp
+++ b/clang/lib/Sema/SemaInit.cpp
@@ -105,6 +105,7 @@ static StringInitFailureKind IsStringInit(Expr *Init, const ArrayType *AT,
       return SIF_None;
     [[fallthrough]];
   case StringLiteralKind::Ordinary:
+  case StringLiteralKind::Binary:
     // char array can be initialized with a narrow string.
     // Only allow char x[] = "foo";  not char x[] = L"foo";
     if (ElemTy->isCharType())
diff --git a/clang/lib/Sema/SemaLambda.cpp b/clang/lib/Sema/SemaLambda.cpp
index ceb32ee15dfa..981856fbf25a 100644
--- a/clang/lib/Sema/SemaLambda.cpp
+++ b/clang/lib/Sema/SemaLambda.cpp
@@ -2389,6 +2389,74 @@ static FunctionDecl *getPatternFunctionDecl(FunctionDecl *FD) {
   return FTD->getTemplatedDecl();
 }
 
+bool Sema::addInstantiatedCapturesToScope(
+    FunctionDecl *Function, const FunctionDecl *PatternDecl,
+    LocalInstantiationScope &Scope,
+    const MultiLevelTemplateArgumentList &TemplateArgs) {
+  const auto *LambdaClass = cast<CXXMethodDecl>(Function)->getParent();
+  const auto *LambdaPattern = cast<CXXMethodDecl>(PatternDecl)->getParent();
+
+  unsigned Instantiated = 0;
+
+  // FIXME: This is a workaround for not having deferred lambda body
+  // instantiation.
+  // When transforming a lambda's body, if we encounter another call to a
+  // nested lambda that contains a constraint expression, we add all of the
+  // outer lambda's instantiated captures to the current instantiation scope to
+  // facilitate constraint evaluation. However, these captures don't appear in
+  // the CXXRecordDecl until after the lambda expression is rebuilt, so we
+  // pull them out from the corresponding LSI.
+  LambdaScopeInfo *InstantiatingScope = nullptr;
+  if (LambdaPattern->capture_size() && !LambdaClass->capture_size()) {
+    for (FunctionScopeInfo *Scope : llvm::reverse(FunctionScopes)) {
+      auto *LSI = dyn_cast<LambdaScopeInfo>(Scope);
+      if (!LSI || getPatternFunctionDecl(LSI->CallOperator) != PatternDecl)
+        continue;
+      InstantiatingScope = LSI;
+      break;
+    }
+    assert(InstantiatingScope);
+  }
+
+  auto AddSingleCapture = [&](const ValueDecl *CapturedPattern,
+                              unsigned Index) {
+    ValueDecl *CapturedVar =
+        InstantiatingScope ? InstantiatingScope->Captures[Index].getVariable()
+                           : LambdaClass->getCapture(Index)->getCapturedVar();
+    assert(CapturedVar->isInitCapture());
+    Scope.InstantiatedLocal(CapturedPattern, CapturedVar);
+  };
+
+  for (const LambdaCapture &CapturePattern : LambdaPattern->captures()) {
+    if (!CapturePattern.capturesVariable()) {
+      Instantiated++;
+      continue;
+    }
+    ValueDecl *CapturedPattern = CapturePattern.getCapturedVar();
+
+    if (!CapturedPattern->isInitCapture()) {
+      Instantiated++;
+      continue;
+    }
+
+    if (!CapturedPattern->isParameterPack()) {
+      AddSingleCapture(CapturedPattern, Instantiated++);
+    } else {
+      Scope.MakeInstantiatedLocalArgPack(CapturedPattern);
+      SmallVector<UnexpandedParameterPack, 2> Unexpanded;
+      SemaRef.collectUnexpandedParameterPacks(
+          dyn_cast<VarDecl>(CapturedPattern)->getInit(), Unexpanded);
+      auto NumArgumentsInExpansion =
+          getNumArgumentsInExpansionFromUnexpanded(Unexpanded, TemplateArgs);
+      if (!NumArgumentsInExpansion)
+        continue;
+      for (unsigned Arg = 0; Arg < *NumArgumentsInExpansion; ++Arg)
+        AddSingleCapture(CapturedPattern, Instantiated++);
+    }
+  }
+  return false;
+}
+
 Sema::LambdaScopeForCallOperatorInstantiationRAII::
     LambdaScopeForCallOperatorInstantiationRAII(
         Sema &SemaRef, FunctionDecl *FD, MultiLevelTemplateArgumentList MLTAL,
diff --git a/clang/lib/Sema/SemaTemplate.cpp b/clang/lib/Sema/SemaTemplate.cpp
index 938671055333..1c555b38277b 100644
--- a/clang/lib/Sema/SemaTemplate.cpp
+++ b/clang/lib/Sema/SemaTemplate.cpp
@@ -1591,8 +1591,16 @@ NamedDecl *Sema::ActOnTemplateTemplateParameter(
   assert(S->isTemplateParamScope() &&
          "Template template parameter not in template parameter scope!");
 
-  // Construct the parameter object.
   bool IsParameterPack = EllipsisLoc.isValid();
+
+  bool Invalid = false;
+  if (CheckTemplateParameterList(
+          Params,
+          /*OldParams=*/nullptr,
+          IsParameterPack ? TPC_TemplateTemplateParameterPack : TPC_Other))
+    Invalid = true;
+
+  // Construct the parameter object.
   TemplateTemplateParmDecl *Param = TemplateTemplateParmDecl::Create(
       Context, Context.getTranslationUnitDecl(),
       NameLoc.isInvalid() ? TmpLoc : NameLoc, Depth, Position, IsParameterPack,
@@ -1615,9 +1623,12 @@ NamedDecl *Sema::ActOnTemplateTemplateParameter(
   if (Params->size() == 0) {
     Diag(Param->getLocation(), diag::err_template_template_parm_no_parms)
     << SourceRange(Params->getLAngleLoc(), Params->getRAngleLoc());
-    Param->setInvalidDecl();
+    Invalid = true;
   }
 
+  if (Invalid)
+    Param->setInvalidDecl();
+
   // C++0x [temp.param]p9:
   //   A default template-argument may be specified for any kind of
   //   template-parameter that is not a template parameter pack.
@@ -2066,7 +2077,7 @@ DeclResult Sema::CheckClassTemplate(
            SemanticContext->isDependentContext())
               ? TPC_ClassTemplateMember
           : TUK == TagUseKind::Friend ? TPC_FriendClassTemplate
-                                      : TPC_ClassTemplate,
+                                      : TPC_Other,
           SkipBody))
     Invalid = true;
 
@@ -2208,9 +2219,8 @@ static bool DiagnoseDefaultTemplateArgument(Sema &S,
                                             SourceLocation ParamLoc,
                                             SourceRange DefArgRange) {
   switch (TPC) {
-  case Sema::TPC_ClassTemplate:
-  case Sema::TPC_VarTemplate:
-  case Sema::TPC_TypeAliasTemplate:
+  case Sema::TPC_Other:
+  case Sema::TPC_TemplateTemplateParameterPack:
     return false;
 
   case Sema::TPC_FunctionTemplate:
@@ -2383,8 +2393,11 @@ bool Sema::CheckTemplateParameterList(TemplateParameterList *NewParams,
         MissingDefaultArg = true;
     } else if (NonTypeTemplateParmDecl *NewNonTypeParm
                = dyn_cast<NonTypeTemplateParmDecl>(*NewParam)) {
-      // Check for unexpanded parameter packs.
-      if (!NewNonTypeParm->isParameterPack() &&
+      // Check for unexpanded parameter packs, except in a template template
+      // parameter pack, as in those any unexpanded packs should be expanded
+      // along with the parameter itself.
+      if (TPC != TPC_TemplateTemplateParameterPack &&
+          !NewNonTypeParm->isParameterPack() &&
           DiagnoseUnexpandedParameterPack(NewNonTypeParm->getLocation(),
                                           NewNonTypeParm->getTypeSourceInfo(),
                                           UPPC_NonTypeTemplateParameterType)) {
@@ -2492,8 +2505,7 @@ bool Sema::CheckTemplateParameterList(TemplateParameterList *NewParams,
     //   If a template parameter of a primary class template or alias template
     //   is a template parameter pack, it shall be the last template parameter.
     if (SawParameterPack && (NewParam + 1) != NewParamEnd &&
-        (TPC == TPC_ClassTemplate || TPC == TPC_VarTemplate ||
-         TPC == TPC_TypeAliasTemplate)) {
+        (TPC == TPC_Other || TPC == TPC_TemplateTemplateParameterPack)) {
       Diag((*NewParam)->getLocation(),
            diag::err_template_param_pack_must_be_last_template_parameter);
       Invalid = true;
@@ -2526,8 +2538,8 @@ bool Sema::CheckTemplateParameterList(TemplateParameterList *NewParams,
           << PrevModuleName;
       Invalid = true;
     } else if (MissingDefaultArg &&
-               (TPC == TPC_ClassTemplate || TPC == TPC_FriendClassTemplate ||
-                TPC == TPC_VarTemplate || TPC == TPC_TypeAliasTemplate)) {
+               (TPC == TPC_Other || TPC == TPC_TemplateTemplateParameterPack ||
+                TPC == TPC_FriendClassTemplate)) {
       // C++ 23[temp.param]p14:
       // If a template-parameter of a class template, variable template, or
       // alias template has a default template argument, each subsequent
diff --git a/clang/lib/Sema/SemaTemplateDeduction.cpp b/clang/lib/Sema/SemaTemplateDeduction.cpp
index 5304b5a2155b..7a880505a53f 100644
--- a/clang/lib/Sema/SemaTemplateDeduction.cpp
+++ b/clang/lib/Sema/SemaTemplateDeduction.cpp
@@ -3427,9 +3427,9 @@ static TemplateDeductionResult FinishTemplateArgumentDeduction(
       if (!P.isPackExpansion() && !A.isPackExpansion()) {
         Info.Param =
             makeTemplateParameter(Template->getTemplateParameters()->getParam(
-                (PsStack.empty() ? TemplateArgs.end()
-                                 : PsStack.front().begin()) -
-                TemplateArgs.begin()));
+                (AsStack.empty() ? CTAI.CanonicalConverted.end()
+                                 : AsStack.front().begin()) -
+                1 - CTAI.CanonicalConverted.begin()));
         Info.FirstArg = P;
         Info.SecondArg = A;
         return TemplateDeductionResult::NonDeducedMismatch;
@@ -6625,17 +6625,19 @@ bool Sema::isTemplateTemplateParameterAtLeastAsSpecializedAs(
 
   TemplateDeductionResult TDK;
   runWithSufficientStackSpace(Info.getLocation(), [&] {
-    TDK = ::FinishTemplateArgumentDeduction(
-        *this, AArg, /*IsPartialOrdering=*/true, PArgs, Deduced, Info);
+    TDK = ::FinishTemplateArgumentDeduction(*this, AArg, PartialOrdering, PArgs,
+                                            Deduced, Info);
   });
   switch (TDK) {
   case TemplateDeductionResult::Success:
     return true;
 
   // It doesn't seem possible to get a non-deduced mismatch when partial
-  // ordering TTPs.
+  // ordering TTPs, except with an invalid template parameter list which has
+  // a parameter after a pack.
   case TemplateDeductionResult::NonDeducedMismatch:
-    llvm_unreachable("Unexpected NonDeducedMismatch");
+    assert(PArg->isInvalidDecl() && "Unexpected NonDeducedMismatch");
+    return false;
 
   // Substitution failures should have already been diagnosed.
   case TemplateDeductionResult::AlreadyDiagnosed:
diff --git a/clang/lib/Sema/SemaTemplateDeductionGuide.cpp b/clang/lib/Sema/SemaTemplateDeductionGuide.cpp
index 00c5dfd3d7a4..6728857edc6d 100644
--- a/clang/lib/Sema/SemaTemplateDeductionGuide.cpp
+++ b/clang/lib/Sema/SemaTemplateDeductionGuide.cpp
@@ -377,8 +377,13 @@ struct ConvertConstructorToDeductionGuideTransform {
         if (NestedPattern)
           Args.addOuterRetainedLevels(NestedPattern->getTemplateDepth());
         auto [Depth, Index] = getDepthAndIndex(Param);
+        // Depth can be 0 if FTD belongs to a non-template class/a class
+        // template specialization with an empty template parameter list. In
+        // that case, we don't want the NewDepth to overflow, and it should
+        // remain 0.
         NamedDecl *NewParam = transformTemplateParameter(
-            SemaRef, DC, Param, Args, Index + Depth1IndexAdjustment, Depth - 1);
+            SemaRef, DC, Param, Args, Index + Depth1IndexAdjustment,
+            Depth ? Depth - 1 : 0);
         if (!NewParam)
           return nullptr;
         // Constraints require that we substitute depth-1 arguments
@@ -982,6 +987,19 @@ getRHSTemplateDeclAndArgs(Sema &SemaRef, TypeAliasTemplateDecl *AliasTemplate) {
   return {Template, AliasRhsTemplateArgs};
 }
 
+bool IsNonDeducedArgument(const TemplateArgument &TA) {
+  // The following cases indicate the template argument is non-deducible:
+  //   1. The result is null. E.g. When it comes from a default template
+  //   argument that doesn't appear in the alias declaration.
+  //   2. The template parameter is a pack and that cannot be deduced from
+  //   the arguments within the alias declaration.
+  // Non-deducible template parameters will persist in the transformed
+  // deduction guide.
+  return TA.isNull() ||
+         (TA.getKind() == TemplateArgument::Pack &&
+          llvm::any_of(TA.pack_elements(), IsNonDeducedArgument));
+}
+
 // Build deduction guides for a type alias template from the given underlying
 // deduction guide F.
 FunctionTemplateDecl *
@@ -1050,7 +1068,8 @@ BuildDeductionGuideForTypeAlias(Sema &SemaRef,
   // !!NOTE: DeduceResults respects the sequence of template parameters of
   // the deduction guide f.
   for (unsigned Index = 0; Index < DeduceResults.size(); ++Index) {
-    if (const auto &D = DeduceResults[Index]; !D.isNull()) // Deduced
+    const auto &D = DeduceResults[Index];
+    if (!IsNonDeducedArgument(D))
       DeducedArgs.push_back(D);
     else
       NonDeducedTemplateParamsInFIndex.push_back(Index);
@@ -1114,7 +1133,7 @@ BuildDeductionGuideForTypeAlias(Sema &SemaRef,
   Args.addOuterTemplateArguments(TransformedDeducedAliasArgs);
   for (unsigned Index = 0; Index < DeduceResults.size(); ++Index) {
     const auto &D = DeduceResults[Index];
-    if (D.isNull()) {
+    if (IsNonDeducedArgument(D)) {
       // 2): Non-deduced template parameters would be substituted later.
       continue;
     }
diff --git a/clang/lib/Sema/SemaTemplateInstantiate.cpp b/clang/lib/Sema/SemaTemplateInstantiate.cpp
index cf29d8a101b4..73567f3be814 100644
--- a/clang/lib/Sema/SemaTemplateInstantiate.cpp
+++ b/clang/lib/Sema/SemaTemplateInstantiate.cpp
@@ -1347,6 +1347,16 @@ std::optional<TemplateDeductionInfo *> Sema::isSFINAEContext() const {
   return std::nullopt;
 }
 
+static TemplateArgument
+getPackSubstitutedTemplateArgument(Sema &S, TemplateArgument Arg) {
+  assert(S.ArgumentPackSubstitutionIndex >= 0);
+  assert(S.ArgumentPackSubstitutionIndex < (int)Arg.pack_size());
+  Arg = Arg.pack_begin()[S.ArgumentPackSubstitutionIndex];
+  if (Arg.isPackExpansion())
+    Arg = Arg.getPackExpansionPattern();
+  return Arg;
+}
+
 //===----------------------------------------------------------------------===/
 // Template Instantiation for Types
 //===----------------------------------------------------------------------===/
@@ -1466,11 +1476,13 @@ namespace {
       }
     }
 
-    static TemplateArgument
+    TemplateArgument
     getTemplateArgumentPackPatternForRewrite(const TemplateArgument &TA) {
       if (TA.getKind() != TemplateArgument::Pack)
         return TA;
-      assert(TA.pack_size() == 1 &&
+      if (SemaRef.ArgumentPackSubstitutionIndex != -1)
+        return getPackSubstitutedTemplateArgument(SemaRef, TA);
+      assert(TA.pack_size() == 1 && TA.pack_begin()->isPackExpansion() &&
              "unexpected pack arguments in template rewrite");
       TemplateArgument Arg = *TA.pack_begin();
       if (Arg.isPackExpansion())
@@ -1629,6 +1641,9 @@ namespace {
       std::vector<TemplateArgument> TArgs;
       switch (Arg.getKind()) {
       case TemplateArgument::Pack:
+        assert(SemaRef.CodeSynthesisContexts.empty() ||
+               SemaRef.CodeSynthesisContexts.back().Kind ==
+                   Sema::CodeSynthesisContext::BuildingDeductionGuides);
         // Literally rewrite the template argument pack, instead of unpacking
         // it.
         for (auto &pack : Arg.getPackAsArray()) {
@@ -1649,6 +1664,23 @@ namespace {
       return inherited::TransformTemplateArgument(Input, Output, Uneval);
     }
 
+    std::optional<unsigned> ComputeSizeOfPackExprWithoutSubstitution(
+        ArrayRef<TemplateArgument> PackArgs) {
+      // Don't do this when rewriting template parameters for CTAD:
+      //   1) The heuristic needs the unpacked Subst* nodes to figure out the
+      //   expanded size, but this never applies since Subst* nodes are not
+      //   created in rewrite scenarios.
+      //
+      //   2) The heuristic substitutes into the pattern with pack expansion
+      //   suppressed, which does not meet the requirements for argument
+      //   rewriting when template arguments include a non-pack matching against
+      //   a pack, particularly when rewriting an alias CTAD.
+      if (TemplateArgs.isRewrite())
+        return std::nullopt;
+
+      return inherited::ComputeSizeOfPackExprWithoutSubstitution(PackArgs);
+    }
+
     template<typename Fn>
     QualType TransformFunctionProtoType(TypeLocBuilder &TLB,
                                         FunctionProtoTypeLoc TL,
@@ -1867,16 +1899,6 @@ bool TemplateInstantiator::AlreadyTransformed(QualType T) {
   return true;
 }
 
-static TemplateArgument
-getPackSubstitutedTemplateArgument(Sema &S, TemplateArgument Arg) {
-  assert(S.ArgumentPackSubstitutionIndex >= 0);
-  assert(S.ArgumentPackSubstitutionIndex < (int)Arg.pack_size());
-  Arg = Arg.pack_begin()[S.ArgumentPackSubstitutionIndex];
-  if (Arg.isPackExpansion())
-    Arg = Arg.getPackExpansionPattern();
-  return Arg;
-}
-
 Decl *TemplateInstantiator::TransformDecl(SourceLocation Loc, Decl *D) {
   if (!D)
     return nullptr;
diff --git a/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp b/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp
index 89ad2a0a9b7b..0c25b87439a9 100644
--- a/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp
+++ b/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp
@@ -1827,7 +1827,7 @@ Decl *TemplateDeclInstantiator::VisitClassTemplateDecl(ClassTemplateDecl *D) {
       // Do some additional validation, then merge default arguments
       // from the existing declarations.
       if (SemaRef.CheckTemplateParameterList(InstParams, PrevParams,
-                                             Sema::TPC_ClassTemplate))
+                                             Sema::TPC_Other))
         return nullptr;
 
       Inst->setAccess(PrevClassTemplate->getAccess());
diff --git a/clang/lib/Sema/TreeTransform.h b/clang/lib/Sema/TreeTransform.h
index 2a5e354ff716..3e8f0ec485e9 100644
--- a/clang/lib/Sema/TreeTransform.h
+++ b/clang/lib/Sema/TreeTransform.h
@@ -3660,6 +3660,9 @@ public:
     return SemaRef.BuildCXXNoexceptExpr(Range.getBegin(), Arg, Range.getEnd());
   }
 
+  std::optional<unsigned>
+  ComputeSizeOfPackExprWithoutSubstitution(ArrayRef<TemplateArgument> PackArgs);
+
   /// Build a new expression to compute the length of a parameter pack.
   ExprResult RebuildSizeOfPackExpr(SourceLocation OperatorLoc, NamedDecl *Pack,
                                    SourceLocation PackLoc,
@@ -15877,6 +15880,49 @@ TreeTransform<Derived>::TransformPackExpansionExpr(PackExpansionExpr *E) {
                                            E->getNumExpansions());
 }
 
+template <typename Derived>
+std::optional<unsigned>
+TreeTransform<Derived>::ComputeSizeOfPackExprWithoutSubstitution(
+    ArrayRef<TemplateArgument> PackArgs) {
+  std::optional<unsigned> Result = 0;
+  for (const TemplateArgument &Arg : PackArgs) {
+    if (!Arg.isPackExpansion()) {
+      Result = *Result + 1;
+      continue;
+    }
+
+    TemplateArgumentLoc ArgLoc;
+    InventTemplateArgumentLoc(Arg, ArgLoc);
+
+    // Find the pattern of the pack expansion.
+    SourceLocation Ellipsis;
+    std::optional<unsigned> OrigNumExpansions;
+    TemplateArgumentLoc Pattern =
+        getSema().getTemplateArgumentPackExpansionPattern(ArgLoc, Ellipsis,
+                                                          OrigNumExpansions);
+
+    // Substitute under the pack expansion. Do not expand the pack (yet).
+    TemplateArgumentLoc OutPattern;
+    Sema::ArgumentPackSubstitutionIndexRAII SubstIndex(getSema(), -1);
+    if (getDerived().TransformTemplateArgument(Pattern, OutPattern,
+                                               /*Uneval*/ true))
+      return true;
+
+    // See if we can determine the number of arguments from the result.
+    std::optional<unsigned> NumExpansions =
+        getSema().getFullyPackExpandedSize(OutPattern.getArgument());
+    if (!NumExpansions) {
+      // No: we must be in an alias template expansion, and we're going to
+      // need to actually expand the packs.
+      Result = std::nullopt;
+      break;
+    }
+
+    Result = *Result + *NumExpansions;
+  }
+  return Result;
+}
+
 template<typename Derived>
 ExprResult
 TreeTransform<Derived>::TransformSizeOfPackExpr(SizeOfPackExpr *E) {
@@ -15942,42 +15988,8 @@ TreeTransform<Derived>::TransformSizeOfPackExpr(SizeOfPackExpr *E) {
   }
 
   // Try to compute the result without performing a partial substitution.
-  std::optional<unsigned> Result = 0;
-  for (const TemplateArgument &Arg : PackArgs) {
-    if (!Arg.isPackExpansion()) {
-      Result = *Result + 1;
-      continue;
-    }
-
-    TemplateArgumentLoc ArgLoc;
-    InventTemplateArgumentLoc(Arg, ArgLoc);
-
-    // Find the pattern of the pack expansion.
-    SourceLocation Ellipsis;
-    std::optional<unsigned> OrigNumExpansions;
-    TemplateArgumentLoc Pattern =
-        getSema().getTemplateArgumentPackExpansionPattern(ArgLoc, Ellipsis,
-                                                          OrigNumExpansions);
-
-    // Substitute under the pack expansion. Do not expand the pack (yet).
-    TemplateArgumentLoc OutPattern;
-    Sema::ArgumentPackSubstitutionIndexRAII SubstIndex(getSema(), -1);
-    if (getDerived().TransformTemplateArgument(Pattern, OutPattern,
-                                               /*Uneval*/ true))
-      return true;
-
-    // See if we can determine the number of arguments from the result.
-    std::optional<unsigned> NumExpansions =
-        getSema().getFullyPackExpandedSize(OutPattern.getArgument());
-    if (!NumExpansions) {
-      // No: we must be in an alias template expansion, and we're going to need
-      // to actually expand the packs.
-      Result = std::nullopt;
-      break;
-    }
-
-    Result = *Result + *NumExpansions;
-  }
+  std::optional<unsigned> Result =
+      getDerived().ComputeSizeOfPackExprWithoutSubstitution(PackArgs);
 
   // Common case: we could determine the number of expansions without
   // substituting.
diff --git a/clang/lib/Serialization/ASTReader.cpp b/clang/lib/Serialization/ASTReader.cpp
index f524251c48dd..427b3c82c473 100644
--- a/clang/lib/Serialization/ASTReader.cpp
+++ b/clang/lib/Serialization/ASTReader.cpp
@@ -9616,9 +9616,9 @@ ModuleFile *ASTReader::getLocalModuleFile(ModuleFile &M, unsigned ID) const {
     return I == GlobalSubmoduleMap.end() ? nullptr : I->second;
   } else {
     // It's a prefix (preamble, PCH, ...). Look it up by index.
-    unsigned IndexFromEnd = ID >> 1;
+   int IndexFromEnd = static_cast<int>(ID >> 1);
     assert(IndexFromEnd && "got reference to unknown module file");
-    return getModuleManager().pch_modules().end()[-IndexFromEnd];
+    return getModuleManager().pch_modules().end()[-static_cast<int>(IndexFromEnd)];
   }
 }
 
@@ -9636,7 +9636,7 @@ unsigned ASTReader::getModuleFileID(ModuleFile *M) {
   auto PCHModules = getModuleManager().pch_modules();
   auto I = llvm::find(PCHModules, M);
   assert(I != PCHModules.end() && "emitting reference to unknown file");
-  return (I - PCHModules.end()) << 1;
+  return std::distance(I, PCHModules.end()) << 1;
 }
 
 std::optional<ASTSourceDescriptor> ASTReader::getSourceDescriptor(unsigned ID) {
diff --git a/clang/lib/StaticAnalyser/Checkers/BuiltinFunctionChecker.cpp b/clang/lib/StaticAnalyser/Checkers/BuiltinFunctionChecker.cpp
index cfdd3c9faa36..bcc4ca77f588 100644
--- a/clang/lib/StaticAnalyser/Checkers/BuiltinFunctionChecker.cpp
+++ b/clang/lib/StaticAnalyser/Checkers/BuiltinFunctionChecker.cpp
@@ -97,10 +97,14 @@ public:
   void handleOverflowBuiltin(const CallEvent &Call, CheckerContext &C,
                              BinaryOperator::Opcode Op,
                              QualType ResultType) const;
-  const NoteTag *createBuiltinNoOverflowNoteTag(CheckerContext &C,
-                                                bool BothFeasible, SVal Arg1,
-                                                SVal Arg2, SVal Result) const;
-  const NoteTag *createBuiltinOverflowNoteTag(CheckerContext &C) const;
+  const NoteTag *createBuiltinOverflowNoteTag(CheckerContext &C,
+                                              bool BothFeasible, SVal Arg1,
+                                              SVal Arg2, SVal Result) const;
+  ProgramStateRef initStateAftetBuiltinOverflow(CheckerContext &C,
+                                                ProgramStateRef State,
+                                                const CallEvent &Call,
+                                                SVal RetCal,
+                                                bool IsOverflow) const;
   std::pair<bool, bool> checkOverflow(CheckerContext &C, SVal RetVal,
                                       QualType Res) const;
 
@@ -122,30 +126,24 @@ private:
 
 } // namespace
 
-const NoteTag *BuiltinFunctionChecker::createBuiltinNoOverflowNoteTag(
-    CheckerContext &C, bool BothFeasible, SVal Arg1, SVal Arg2,
-    SVal Result) const {
-  return C.getNoteTag([Result, Arg1, Arg2, BothFeasible](
-                          PathSensitiveBugReport &BR, llvm::raw_ostream &OS) {
+const NoteTag *BuiltinFunctionChecker::createBuiltinOverflowNoteTag(
+    CheckerContext &C, bool overflow, SVal Arg1, SVal Arg2, SVal Result) const {
+  return C.getNoteTag([Result, Arg1, Arg2, overflow](PathSensitiveBugReport &BR,
+                                                     llvm::raw_ostream &OS) {
     if (!BR.isInteresting(Result))
       return;
 
-    // Propagate interestingness to input argumets if result is interesting.
+    // Propagate interestingness to input arguments if result is interesting.
     BR.markInteresting(Arg1);
     BR.markInteresting(Arg2);
 
-    if (BothFeasible)
+    if (overflow)
+      OS << "Assuming overflow";
+    else
       OS << "Assuming no overflow";
   });
 }
 
-const NoteTag *
-BuiltinFunctionChecker::createBuiltinOverflowNoteTag(CheckerContext &C) const {
-  return C.getNoteTag([](PathSensitiveBugReport &BR,
-                         llvm::raw_ostream &OS) { OS << "Assuming overflow"; },
-                      /*isPrunable=*/true);
-}
-
 std::pair<bool, bool>
 BuiltinFunctionChecker::checkOverflow(CheckerContext &C, SVal RetVal,
                                       QualType Res) const {
@@ -175,6 +173,29 @@ BuiltinFunctionChecker::checkOverflow(CheckerContext &C, SVal RetVal,
   return {MayOverflow || MayUnderflow, MayNotOverflow && MayNotUnderflow};
 }
 
+ProgramStateRef BuiltinFunctionChecker::initStateAftetBuiltinOverflow(
+    CheckerContext &C, ProgramStateRef State, const CallEvent &Call,
+    SVal RetVal, bool IsOverflow) const {
+  SValBuilder &SVB = C.getSValBuilder();
+  SVal Arg1 = Call.getArgSVal(0);
+  SVal Arg2 = Call.getArgSVal(1);
+  auto BoolTy = C.getASTContext().BoolTy;
+
+  ProgramStateRef NewState =
+      State->BindExpr(Call.getOriginExpr(), C.getLocationContext(),
+                      SVB.makeTruthVal(IsOverflow, BoolTy));
+
+  if (auto L = Call.getArgSVal(2).getAs<Loc>()) {
+    NewState = NewState->bindLoc(*L, RetVal, C.getLocationContext());
+
+    // Propagate taint if any of the arguments were tainted
+    if (isTainted(State, Arg1) || isTainted(State, Arg2))
+      NewState = addTaint(NewState, *L);
+  }
+
+  return NewState;
+}
+
 void BuiltinFunctionChecker::handleOverflowBuiltin(const CallEvent &Call,
                                                    CheckerContext &C,
                                                    BinaryOperator::Opcode Op,
@@ -184,8 +205,6 @@ void BuiltinFunctionChecker::handleOverflowBuiltin(const CallEvent &Call,
 
   ProgramStateRef State = C.getState();
   SValBuilder &SVB = C.getSValBuilder();
-  const Expr *CE = Call.getOriginExpr();
-  auto BoolTy = C.getASTContext().BoolTy;
 
   SVal Arg1 = Call.getArgSVal(0);
   SVal Arg2 = Call.getArgSVal(1);
@@ -195,29 +214,20 @@ void BuiltinFunctionChecker::handleOverflowBuiltin(const CallEvent &Call,
   SVal RetVal = SVB.evalBinOp(State, Op, Arg1, Arg2, ResultType);
 
   auto [Overflow, NotOverflow] = checkOverflow(C, RetValMax, ResultType);
-  if (NotOverflow) {
-    ProgramStateRef StateNoOverflow = State->BindExpr(
-        CE, C.getLocationContext(), SVB.makeTruthVal(false, BoolTy));
-
-    if (auto L = Call.getArgSVal(2).getAs<Loc>()) {
-      StateNoOverflow =
-          StateNoOverflow->bindLoc(*L, RetVal, C.getLocationContext());
 
-      // Propagate taint if any of the argumets were tainted
-      if (isTainted(State, Arg1) || isTainted(State, Arg2))
-        StateNoOverflow = addTaint(StateNoOverflow, *L);
-    }
+  if (NotOverflow) {
+    auto NewState =
+        initStateAftetBuiltinOverflow(C, State, Call, RetVal, false);
 
-    C.addTransition(
-        StateNoOverflow,
-        createBuiltinNoOverflowNoteTag(
-            C, /*BothFeasible=*/NotOverflow && Overflow, Arg1, Arg2, RetVal));
+    C.addTransition(NewState, createBuiltinOverflowNoteTag(
+                                  C, /*overflow=*/false, Arg1, Arg2, RetVal));
   }
 
   if (Overflow) {
-    C.addTransition(State->BindExpr(CE, C.getLocationContext(),
-                                    SVB.makeTruthVal(true, BoolTy)),
-                    createBuiltinOverflowNoteTag(C));
+    auto NewState = initStateAftetBuiltinOverflow(C, State, Call, RetVal, true);
+
+    C.addTransition(NewState, createBuiltinOverflowNoteTag(C, /*overflow=*/true,
+                                                           Arg1, Arg2, RetVal));
   }
 }
 
diff --git a/clang/lib/StaticAnalyser/Core/ExprEngine.cpp b/clang/lib/StaticAnalyser/Core/ExprEngine.cpp
index 140c77790496..cfb8be2e7f0f 100644
--- a/clang/lib/StaticAnalyser/Core/ExprEngine.cpp
+++ b/clang/lib/StaticAnalyser/Core/ExprEngine.cpp
@@ -2510,6 +2510,20 @@ bool ExprEngine::replayWithoutInlining(ExplodedNode *N,
   return true;
 }
 
+/// Return the innermost location context which is inlined at `Node`, unless
+/// it's the top-level (entry point) location context.
+static const LocationContext *getInlinedLocationContext(ExplodedNode *Node,
+                                                        ExplodedGraph &G) {
+  const LocationContext *CalleeLC = Node->getLocation().getLocationContext();
+  const LocationContext *RootLC =
+      (*G.roots_begin())->getLocation().getLocationContext();
+
+  if (CalleeLC->getStackFrame() == RootLC->getStackFrame())
+    return nullptr;
+
+  return CalleeLC;
+}
+
 /// Block entrance.  (Update counters).
 void ExprEngine::processCFGBlockEntrance(const BlockEdge &L,
                                          NodeBuilderWithSinks &nodeBuilder,
@@ -2557,21 +2571,24 @@ void ExprEngine::processCFGBlockEntrance(const BlockEdge &L,
     const ExplodedNode *Sink =
                    nodeBuilder.generateSink(Pred->getState(), Pred, &tag);
 
-    // Check if we stopped at the top level function or not.
-    // Root node should have the location context of the top most function.
-    const LocationContext *CalleeLC = Pred->getLocation().getLocationContext();
-    const LocationContext *CalleeSF = CalleeLC->getStackFrame();
-    const LocationContext *RootLC =
-                        (*G.roots_begin())->getLocation().getLocationContext();
-    if (RootLC->getStackFrame() != CalleeSF) {
-      Engine.FunctionSummaries->markReachedMaxBlockCount(CalleeSF->getDecl());
+    if (const LocationContext *LC = getInlinedLocationContext(Pred, G)) {
+      // FIXME: This will unconditionally prevent inlining this function (even
+      // from other entry points), which is not a reasonable heuristic: even if
+      // we reached max block count on this particular execution path, there
+      // may be other execution paths (especially with other parametrizations)
+      // where the analyzer can reach the end of the function (so there is no
+      // natural reason to avoid inlining it). However, disabling this would
+      // significantly increase the analysis time (because more entry points
+      // would exhaust their allocated budget), so it must be compensated by a
+      // different (more reasonable) reduction of analysis scope.
+      Engine.FunctionSummaries->markShouldNotInline(
+          LC->getStackFrame()->getDecl());
 
       // Re-run the call evaluation without inlining it, by storing the
       // no-inlining policy in the state and enqueuing the new work item on
       // the list. Replay should almost never fail. Use the stats to catch it
       // if it does.
-      if ((!AMgr.options.NoRetryExhausted &&
-           replayWithoutInlining(Pred, CalleeLC)))
+      if ((!AMgr.options.NoRetryExhausted && replayWithoutInlining(Pred, LC)))
         return;
       NumMaxBlockCountReachedInInlined++;
     } else
@@ -2835,8 +2852,29 @@ void ExprEngine::processBranch(
       // conflicts with the widen-loop analysis option (which is off by
       // default). If we intend to support and stabilize the loop widening,
       // we must ensure that it 'plays nicely' with this logic.
-      if (!SkipTrueBranch || AMgr.options.ShouldWidenLoops)
+      if (!SkipTrueBranch || AMgr.options.ShouldWidenLoops) {
         Builder.generateNode(StTrue, true, PredN);
+      } else if (!AMgr.options.InlineFunctionsWithAmbiguousLoops) {
+        // FIXME: There is an ancient and arbitrary heuristic in
+        // `ExprEngine::processCFGBlockEntrance` which prevents all further
+        // inlining of a function if it finds an execution path within that
+        // function which reaches the `MaxBlockVisitOnPath` limit (a/k/a
+        // `analyzer-max-loop`, by default four iterations in a loop). Adding
+        // this "don't assume third iteration" logic significantly increased
+        // the analysis runtime on some inputs because less functions were
+        // arbitrarily excluded from being inlined, so more entry points used
+        // up their full allocated budget. As a hacky compensation for this,
+        // here we apply the "should not inline" mark in cases when the loop
+        // could potentially reach the `MaxBlockVisitOnPath` limit without the
+        // "don't assume third iteration" logic. This slightly overcompensates
+        // (activates if the third iteration can be entered, and will not
+        // recognize cases where the fourth iteration would't be completed), but
+        // should be good enough for practical purposes.
+        if (const LocationContext *LC = getInlinedLocationContext(Pred, G)) {
+          Engine.FunctionSummaries->markShouldNotInline(
+              LC->getStackFrame()->getDecl());
+        }
+      }
     }
 
     if (StFalse)
diff --git a/clang/test/Analysis/analyzer-config.c b/clang/test/Analysis/analyzer-config.c
index d5eb790b82f2..b47ca59e7982 100644
--- a/clang/test/Analysis/analyzer-config.c
+++ b/clang/test/Analysis/analyzer-config.c
@@ -88,6 +88,7 @@
 // CHECK-NEXT: graph-trim-interval = 1000
 // CHECK-NEXT: ignore-bison-generated-files = true
 // CHECK-NEXT: ignore-flex-generated-files = true
+// CHECK-NEXT: inline-functions-with-ambiguous-loops = false
 // CHECK-NEXT: inline-lambdas = true
 // CHECK-NEXT: ipa = dynamic-bifurcate
 // CHECK-NEXT: ipa-always-inline-size = 3
diff --git a/clang/test/Analysis/builtin_overflow.c b/clang/test/Analysis/builtin_overflow.c
index 9d98ce7a1af4..d290333071dc 100644
--- a/clang/test/Analysis/builtin_overflow.c
+++ b/clang/test/Analysis/builtin_overflow.c
@@ -26,7 +26,7 @@ void test_add_overflow(void)
    int res;
 
    if (__builtin_add_overflow(__INT_MAX__, 1, &res)) {
-     clang_analyzer_dump_int(res); //expected-warning{{1st function call argument is an uninitialized value}}
+     clang_analyzer_dump_int(res); //expected-warning{{-2147483648 S32b}}
      return;
    }
 
@@ -38,7 +38,7 @@ void test_add_underoverflow(void)
    int res;
 
    if (__builtin_add_overflow(__INT_MIN__, -1, &res)) {
-     clang_analyzer_dump_int(res); //expected-warning{{1st function call argument is an uninitialized value}}
+     clang_analyzer_dump_int(res); //expected-warning{{2147483647 S32b}}
      return;
    }
 
@@ -160,7 +160,7 @@ void test_bool_assign(void)
 {
     int res;
 
-    // Reproduce issue from GH#111147. __builtin_*_overflow funcions
+    // Reproduce issue from GH#111147. __builtin_*_overflow functions
     // should return _Bool, but not int.
     _Bool ret = __builtin_mul_overflow(10, 20, &res); // no crash
 }
diff --git a/clang/test/Analysis/builtin_overflow_notes.c b/clang/test/Analysis/builtin_overflow_notes.c
index 20f333a4a6cc..94c79b5ed334 100644
--- a/clang/test/Analysis/builtin_overflow_notes.c
+++ b/clang/test/Analysis/builtin_overflow_notes.c
@@ -19,12 +19,16 @@ void test_no_overflow_note(int a, int b)
 
 void test_overflow_note(int a, int b)
 {
-   int res; // expected-note{{'res' declared without an initial value}}
+   int res;
 
    if (__builtin_add_overflow(a, b, &res)) { // expected-note {{Assuming overflow}}
                                              // expected-note@-1 {{Taking true branch}}
-     int var = res; // expected-warning{{Assigned value is garbage or undefined}}
-                    // expected-note@-1 {{Assigned value is garbage or undefined}}
+     if (res) { // expected-note {{Assuming 'res' is not equal to 0}}
+                // expected-note@-1 {{Taking true branch}}
+        int *ptr = 0; // expected-note {{'ptr' initialized to a null pointer value}}
+        int var = *(int *) ptr; //expected-warning {{Dereference of null pointer}}
+                                //expected-note@-1 {{Dereference of null pointer}}
+     }
      return;
    }
 }
diff --git a/clang/test/Analysis/live-stmts.cpp b/clang/test/Analysis/live-stmts.cpp
index c60f522588e3..ca2ff6da8b13 100644
--- a/clang/test/Analysis/live-stmts.cpp
+++ b/clang/test/Analysis/live-stmts.cpp
@@ -44,6 +44,8 @@ int testThatDumperWorks(int x, int y, int z) {
 // CHECK-NEXT: ImplicitCastExpr {{.*}} <IntegralToBoolean>
 // CHECK-NEXT: `-ImplicitCastExpr {{.*}} <LValueToRValue>
 // CHECK-NEXT:   `-DeclRefExpr {{.*}} 'x' 'int'
+// CHECK-EMPTY:
+// CHECK-EMPTY:
 // CHECK: [ B4 (live expressions at block exit) ]
 // CHECK-EMPTY:
 // CHECK-NEXT: DeclRefExpr {{.*}} 'y' 'int'
diff --git a/clang/test/Analysis/loop-based-inlining-prevention.c b/clang/test/Analysis/loop-based-inlining-prevention.c
new file mode 100644
index 000000000000..73627112e2d3
--- /dev/null
+++ b/clang/test/Analysis/loop-based-inlining-prevention.c
@@ -0,0 +1,200 @@
+// RUN: %clang_analyze_cc1 -analyzer-checker=core,debug.ExprInspection -verify=expected,default %s
+// RUN: %clang_analyze_cc1 -analyzer-checker=core,debug.ExprInspection -analyzer-config inline-functions-with-ambiguous-loops=true -verify=expected,enabled %s
+
+// This file tests some heuristics in the engine that put functions on a
+// "do not inline" list if their analyisis reaches the `analyzer-max-loop`
+// limit (by default 4 iterations) in a loop. This was almost surely intended
+// as memoization optimization for the "retry without inlining" fallback (if we
+// had to retry once, next time don't even try inlining), but aggressively
+// oversteps the "natural" scope: reaching 4 iterations on _one particular_
+// execution path does not imply that each path would need "retry without
+// inlining" especially if a different call receives different arguments.
+//
+// This heuristic significantly affects the scope/depth of the analysis (and
+// therefore the execution time) because without this limitation on the
+// inlining significantly more entry points would be able to exhaust their
+// `max-nodes` quota. (Trivial thin wrappers around big complex functions are
+// common in many projects.)
+//
+// Unfortunately, this arbitrary heuristic strongly relies on the current loop
+// handling model and its many limitations, so improvements in loop handling
+// can cause surprising slowdowns by reducing the "do not inline" blacklist.
+// In the tests "FIXME-BUT-NEEDED" comments mark "problematic" (aka buggy)
+// analyzer behavior which cannot be fixed without also improving the
+// heuristics for (not) inlining large functions.
+
+  int getNum(void); // Get an unknown symbolic number.
+
+void clang_analyzer_dump(int arg);
+
+//-----------------------------------------------------------------------------
+// Simple case: inlined function never reaches `analyzer-max-loop`, so it is
+// always inlined.
+
+int inner_simple(int callIdx) {
+  clang_analyzer_dump(callIdx); // expected-warning {{1 S32}}
+                                // expected-warning@-1 {{2 S32}}
+  return 42;
+}
+
+int outer_simple(void) {
+  int x = inner_simple(1);
+  int y = inner_simple(2);
+  return 53 / (x - y); // expected-warning {{Division by zero}}
+}
+
+//-----------------------------------------------------------------------------
+// Inlined function always reaches `analyzer-max-loop`, which stops the
+// analysis on that path and puts the function on the "do not inline" list.
+
+int inner_fixed_loop_1(int callIdx) {
+  int i;
+  clang_analyzer_dump(callIdx); // expected-warning {{1 S32}}
+  for (i = 0; i < 10; i++); // FIXME-BUT-NEEDED: This stops the analysis.
+  clang_analyzer_dump(callIdx); // no-warning
+  return 42;
+}
+
+int outer_fixed_loop_1(void) {
+  int x = inner_fixed_loop_1(1);
+  int y = inner_fixed_loop_1(2);
+
+ // FIXME-BUT-NEEDED: The analysis doesn't reach this zero division.
+  return 53 / (x - y); // no-warning
+}
+
+//-----------------------------------------------------------------------------
+// Inlined function always reaches `analyzer-max-loop`; inlining is prevented
+// even for different entry points.
+// NOTE: the analyzer happens to analyze the entry points in a reversed order,
+// so `outer_2_fixed_loop_2` is analyzed first and it will be the one which is
+// able to inline the inner function.
+
+int inner_fixed_loop_2(int callIdx) {
+  // Identical copy of inner_fixed_loop_1.
+  int i;
+  clang_analyzer_dump(callIdx); // expected-warning {{2 S32}}
+  for (i = 0; i < 10; i++); // FIXME-BUT-NEEDED: This stops the analysis.
+  clang_analyzer_dump(callIdx); // no-warning
+  return 42;
+}
+
+int outer_1_fixed_loop_2(void) {
+  return inner_fixed_loop_2(1);
+}
+
+int outer_2_fixed_loop_2(void) {
+  return inner_fixed_loop_2(2);
+}
+
+//-----------------------------------------------------------------------------
+// Inlined function reaches `analyzer-max-loop` only in its second call. The
+// function is inlined twice but the second call doesn't finish and ends up
+// being conservatively evaluated.
+
+int inner_parametrized_loop_1(int count) {
+  int i;
+  clang_analyzer_dump(count); // expected-warning {{2 S32}}
+                              // expected-warning@-1 {{10 S32}}
+  for (i = 0; i < count; i++);
+      // FIXME-BUT-NEEDED: This loop stops the analysis when count >=4.
+  clang_analyzer_dump(count); // expected-warning {{2 S32}}
+  return 42;
+}
+
+int outer_parametrized_loop_1(void) {
+  int x = inner_parametrized_loop_1(2);
+  int y = inner_parametrized_loop_1(10);
+
+ // FIXME-BUT-NEEDED: The analysis doesn't reach this zero division.
+  return 53 / (x - y); // no-warning
+}
+
+//-----------------------------------------------------------------------------
+// Inlined function reaches `analyzer-max-loop` on its first call, so the
+// second call isn't inlined (although it could be fully evaluated).
+
+int inner_parametrized_loop_2(int count) {
+  // Identical copy of inner_parametrized_loop_1.
+  int i;
+  clang_analyzer_dump(count); // expected-warning {{10 S32}}
+  for (i = 0; i < count; i++);
+      // FIXME-BUT-NEEDED: This loop stops the analysis when count >=4.
+  clang_analyzer_dump(count); // no-warning
+  return 42;
+}
+
+int outer_parametrized_loop_2(void) {
+  int y = inner_parametrized_loop_2(10);
+  int x = inner_parametrized_loop_2(2);
+
+ // FIXME-BUT-NEEDED: The analysis doesn't reach this zero division.
+  return 53 / (x - y); // no-warning
+}
+
+//-----------------------------------------------------------------------------
+// Inlined function may or may not reach `analyzer-max-loop` depending on an
+// ambiguous check before the loop. This is very similar to the "fixed loop"
+// cases: the function is placed on the "don't inline" list when any execution
+// path reaches `analyzer-max-loop` (even if other execution paths reach the
+// end of the function).
+// NOTE: This is tested with two separate entry points to ensure that one
+// inlined call is fully evaluated before we try to inline the other call.
+// NOTE: the analyzer happens to analyze the entry points in a reversed order,
+// so `outer_2_conditional_loop` is analyzed first and it will be the one which
+// is able to inline the inner function.
+
+int inner_conditional_loop(int callIdx) {
+  int i;
+  clang_analyzer_dump(callIdx); // expected-warning {{2 S32}}
+  if (getNum() == 777) {
+    for (i = 0; i < 10; i++);
+  }
+  clang_analyzer_dump(callIdx); // expected-warning {{2 S32}}
+  return 42;
+}
+
+int outer_1_conditional_loop(void) {
+  return inner_conditional_loop(1);
+}
+
+int outer_2_conditional_loop(void) {
+  return inner_conditional_loop(2);
+}
+
+//-----------------------------------------------------------------------------
+// Inlined function executes an ambiguous loop that may or may not reach
+// `analyzer-max-loop`. Historically, before the "don't assume third iteration"
+// commit (bb27d5e5c6b194a1440b8ac4e5ace68d0ee2a849) this worked like the
+// `conditional_loop` cases: the analyzer was able to find a path reaching
+// `analyzer-max-loop` so inlining was disabled. After that commit the analyzer
+// does not _assume_ a third (or later) iteration (i.e. does not enter those
+// iterations if the loop condition is an unknown value), so e.g. this test
+// function does not reach `analyzer-max-loop` iterations and the inlining is
+// not disabled.
+// Unfortunately this change significantly increased the workload and
+// runtime of the analyzer (more entry points used up their budget), so the
+// option `inline-functions-with-ambiguous-loops` was introduced and disabled
+// by default to suppress the inlining in situations where the "don't assume
+// third iteration" logic activates.
+// NOTE: This is tested with two separate entry points to ensure that one
+// inlined call is fully evaluated before we try to inline the other call.
+// NOTE: the analyzer happens to analyze the entry points in a reversed order,
+// so `outer_2_ambiguous_loop` is analyzed first and it will be the one which
+// is able to inline the inner function.
+
+int inner_ambiguous_loop(int callIdx) {
+  int i;
+  clang_analyzer_dump(callIdx); // default-warning {{2 S32}}
+                                // enabled-warning@-1 {{1 S32}}
+                                // enabled-warning@-2 {{2 S32}}
+  for (i = 0; i < getNum(); i++);
+  return i;
+}
+
+int outer_1_ambiguous_loop(void) {
+  return inner_ambiguous_loop(1);
+}
+int outer_2_ambiguous_loop(void) {
+  return inner_ambiguous_loop(2);
+}
diff --git a/clang/test/Analysis/loop-unrolling.cpp b/clang/test/Analysis/loop-unrolling.cpp
index bf05a7739ce4..ebae81e000c7 100644
--- a/clang/test/Analysis/loop-unrolling.cpp
+++ b/clang/test/Analysis/loop-unrolling.cpp
@@ -1,5 +1,5 @@
-// RUN: %clang_analyze_cc1 -analyzer-checker=core,debug.ExprInspection -analyzer-config unroll-loops=true,cfg-loopexit=true -verify -std=c++14 -analyzer-config exploration_strategy=unexplored_first_queue %s
-// RUN: %clang_analyze_cc1 -analyzer-checker=core,debug.ExprInspection -analyzer-config unroll-loops=true,cfg-loopexit=true,exploration_strategy=dfs -verify -std=c++14 -DDFS=1 %s
+// RUN: %clang_analyze_cc1 -analyzer-checker=core,debug.ExprInspection -analyzer-config unroll-loops=true,cfg-loopexit=true -verify=expected,default -std=c++14 -analyzer-config exploration_strategy=unexplored_first_queue %s
+// RUN: %clang_analyze_cc1 -analyzer-checker=core,debug.ExprInspection -analyzer-config unroll-loops=true,cfg-loopexit=true,exploration_strategy=dfs -verify=expected,dfs -std=c++14 %s
 
 void clang_analyzer_numTimesReached();
 void clang_analyzer_warnIfReached();
@@ -337,6 +337,7 @@ int nested_both_unrolled() {
 }
 
 int simple_known_bound_loop() {
+  // Iteration count visible: can be unrolled and fully executed.
   for (int i = 2; i < 12; i++) {
     // This function is inlined in nested_inlined_unroll1()
     clang_analyzer_numTimesReached(); // expected-warning {{90}}
@@ -345,27 +346,42 @@ int simple_known_bound_loop() {
 }
 
 int simple_unknown_bound_loop() {
+  // Iteration count unknown: unrolling won't happen and the execution will be
+  // split two times:
+  // (1) split between skipped loop (immediate exit) and entering the loop
+  // (2) split between exit after 1 iteration and entering the second iteration
+  // After these there is no third state split because the "don't assume third
+  // iteration" logic in `ExprEngine::processBranch` prevents it; but the
+  // `legacy-inlining-prevention` logic will put this function onto the list of
+  // functions that may not be inlined in the future.
+  // The exploration strategy apparently influences the number of times this
+  // function can be inlined before it's placed on the "don't inline" list.
   for (int i = 2; i < getNum(); i++) {
-    clang_analyzer_numTimesReached(); // expected-warning {{8}}
+    clang_analyzer_numTimesReached(); // default-warning {{4}} dfs-warning {{8}}
   }
   return 0;
 }
 
 int nested_inlined_unroll1() {
+  // Here the analyzer can unroll and fully execute both the outer loop and the
+  // inner loop within simple_known_bound_loop().
   int k;
   for (int i = 0; i < 9; i++) {
     clang_analyzer_numTimesReached(); // expected-warning {{9}}
-    k = simple_known_bound_loop();    // no reevaluation without inlining
+    k = simple_known_bound_loop();
   }
   int a = 22 / k; // expected-warning {{Division by zero}}
   return 0;
 }
 
 int nested_inlined_no_unroll1() {
+  // Here no unrolling happens and we only run `analyzer-max-loop` (= 4)
+  // iterations of the loop within this function, but some state splits happen
+  // in `simple_unknown_bound_loop()` calls.
   int k;
-  for (int i = 0; i < 9; i++) {
-    clang_analyzer_numTimesReached(); // expected-warning {{10}}
-    k = simple_unknown_bound_loop();  // reevaluation without inlining, splits the state as well
+  for (int i = 0; i < 40; i++) {
+    clang_analyzer_numTimesReached(); // default-warning {{9}} dfs-warning {{12}}
+    k = simple_unknown_bound_loop(); 
   }
   int a = 22 / k; // no-warning
   return 0;
diff --git a/clang/test/CXX/dcl.dcl/dcl.attr/dcl.attr.nodiscard/p2.cpp b/clang/test/CXX/dcl.dcl/dcl.attr/dcl.attr.nodiscard/p2.cpp
index 18f4bd5e9c0f..0012ab976baa 100644
--- a/clang/test/CXX/dcl.dcl/dcl.attr/dcl.attr.nodiscard/p2.cpp
+++ b/clang/test/CXX/dcl.dcl/dcl.attr/dcl.attr.nodiscard/p2.cpp
@@ -164,19 +164,21 @@ struct X {
 
 [[nodiscard]] X get_X();
 // cxx11-warning@-1 {{use of the 'nodiscard' attribute is a C++17 extension}}
+[[nodiscard]] X* get_Ptr();
+// cxx11-warning@-1 {{use of the 'nodiscard' attribute is a C++17 extension}}
 void f() {
+  get_X(); // expected-warning{{ignoring return value of function declared with 'nodiscard' attribute}}
+  (void) get_X();
   (void) get_X().variant_member;
   (void) get_X().anonymous_struct_member;
   (void) get_X().data_member;
   (void) get_X().static_data_member;
-  // expected-warning@-1 {{ignoring return value of function declared with 'nodiscard' attribute}}
   (void) get_X().unscoped_enum;
-  // expected-warning@-1 {{ignoring return value of function declared with 'nodiscard' attribute}}
   (void) get_X().scoped_enum;
-  // expected-warning@-1 {{ignoring return value of function declared with 'nodiscard' attribute}}
   (void) get_X().implicit_object_member_function();
   (void) get_X().static_member_function();
-  // expected-warning@-1 {{ignoring return value of function declared with 'nodiscard' attribute}}
+  (void) get_Ptr()->implicit_object_member_function();
+  (void) get_Ptr()->static_member_function();
 #if __cplusplus >= 202302L
   (void) get_X().explicit_object_member_function();
 #endif
diff --git a/clang/test/CodeGen/SystemZ/builtins-systemz-bitop.c b/clang/test/CodeGen/SystemZ/builtins-systemz-bitop.c
index 5b4051c8d6f1..717a7d7ab49e 100644
--- a/clang/test/CodeGen/SystemZ/builtins-systemz-bitop.c
+++ b/clang/test/CodeGen/SystemZ/builtins-systemz-bitop.c
@@ -1,6 +1,6 @@
 // REQUIRES: systemz-registered-target
-// RUN: %clang_cc1 -target-cpu arch15 -triple s390x-ibm-linux -Wall -Wno-unused -Werror -emit-llvm %s -o - | FileCheck %s
-// RUN: %clang_cc1 -target-cpu arch15 -triple s390x-ibm-linux -Wall -Wno-unused -Werror -emit-llvm -x c++ %s -o - | FileCheck %s
+// RUN: %clang_cc1 -target-cpu z17 -triple s390x-ibm-linux -Wall -Wno-unused -Werror -emit-llvm %s -o - | FileCheck %s
+// RUN: %clang_cc1 -target-cpu z17 -triple s390x-ibm-linux -Wall -Wno-unused -Werror -emit-llvm -x c++ %s -o - | FileCheck %s
 
 unsigned long test_bdepg(unsigned long a, unsigned long b) {
 // CHECK-LABEL: test_bdepg
diff --git a/clang/test/CodeGen/SystemZ/builtins-systemz-vector5-error.c b/clang/test/CodeGen/SystemZ/builtins-systemz-vector5-error.c
index 3943a15af9d2..8275b9ddb88a 100644
--- a/clang/test/CodeGen/SystemZ/builtins-systemz-vector5-error.c
+++ b/clang/test/CodeGen/SystemZ/builtins-systemz-vector5-error.c
@@ -1,5 +1,5 @@
 // REQUIRES: systemz-registered-target
-// RUN: %clang_cc1 -target-cpu arch15 -triple s390x-unknown-unknown \
+// RUN: %clang_cc1 -target-cpu z17 -triple s390x-unknown-unknown \
 // RUN: -Wall -Wno-unused -Werror -fsyntax-only -verify %s
 
 typedef __attribute__((vector_size(16))) signed char vec_schar;
diff --git a/clang/test/CodeGen/SystemZ/builtins-systemz-vector5.c b/clang/test/CodeGen/SystemZ/builtins-systemz-vector5.c
index c3621819e71f..b765fa64b33d 100644
--- a/clang/test/CodeGen/SystemZ/builtins-systemz-vector5.c
+++ b/clang/test/CodeGen/SystemZ/builtins-systemz-vector5.c
@@ -1,5 +1,5 @@
 // REQUIRES: systemz-registered-target
-// RUN: %clang_cc1 -target-cpu arch15 -triple s390x-ibm-linux -flax-vector-conversions=none \
+// RUN: %clang_cc1 -target-cpu z17 -triple s390x-ibm-linux -flax-vector-conversions=none \
 // RUN: -Wall -Wno-unused -Werror -emit-llvm %s -o - | FileCheck %s
 
 typedef __attribute__((vector_size(16))) signed char vec_schar;
diff --git a/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5-error.c b/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5-error.c
index 9f4844efd631..79041b923068 100644
--- a/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5-error.c
+++ b/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5-error.c
@@ -1,5 +1,5 @@
 // REQUIRES: systemz-registered-target
-// RUN: %clang_cc1 -target-cpu arch15 -triple s390x-linux-gnu \
+// RUN: %clang_cc1 -target-cpu z17 -triple s390x-linux-gnu \
 // RUN: -fzvector -flax-vector-conversions=none \
 // RUN: -Wall -Wno-unused -Werror -fsyntax-only -verify %s
 
diff --git a/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5.c b/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5.c
index 7a29dbf552e0..6ee9e1ee3a11 100644
--- a/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5.c
+++ b/clang/test/CodeGen/SystemZ/builtins-systemz-zvector5.c
@@ -1,8 +1,8 @@
 // REQUIRES: systemz-registered-target
-// RUN: %clang_cc1 -target-cpu arch15 -triple s390x-linux-gnu \
+// RUN: %clang_cc1 -target-cpu z17 -triple s390x-linux-gnu \
 // RUN: -O2 -fzvector -flax-vector-conversions=none \
 // RUN: -Wall -Wno-unused -Werror -emit-llvm %s -o - | FileCheck %s
-// RUN: %clang_cc1 -target-cpu arch15 -triple s390x-linux-gnu \
+// RUN: %clang_cc1 -target-cpu z17 -triple s390x-linux-gnu \
 // RUN: -O2 -fzvector -flax-vector-conversions=none \
 // RUN: -Wall -Wno-unused -Werror -S %s -o - | FileCheck %s --check-prefix=CHECK-ASM
 
diff --git a/clang/test/CodeGen/SystemZ/systemz-abi-vector.c b/clang/test/CodeGen/SystemZ/systemz-abi-vector.c
index 1e1926678ec3..e5704709a3a3 100644
--- a/clang/test/CodeGen/SystemZ/systemz-abi-vector.c
+++ b/clang/test/CodeGen/SystemZ/systemz-abi-vector.c
@@ -18,6 +18,8 @@
 // RUN:   -emit-llvm -o - %s | FileCheck --check-prefix=CHECK-VECTOR %s
 // RUN: %clang_cc1 -no-enable-noundef-analysis -triple s390x-linux-gnu -target-cpu arch14 \
 // RUN:   -emit-llvm -o - %s | FileCheck --check-prefix=CHECK-VECTOR %s
+// RUN: %clang_cc1 -no-enable-noundef-analysis -triple s390x-linux-gnu -target-cpu z17 \
+// RUN:   -emit-llvm -o - %s | FileCheck --check-prefix=CHECK-VECTOR %s
 // RUN: %clang_cc1 -no-enable-noundef-analysis -triple s390x-linux-gnu -target-cpu arch15 \
 // RUN:   -emit-llvm -o - %s | FileCheck --check-prefix=CHECK-VECTOR %s
 
diff --git a/clang/test/CodeGen/SystemZ/systemz-abi.c b/clang/test/CodeGen/SystemZ/systemz-abi.c
index 58081bdc6cc2..7de425950e9f 100644
--- a/clang/test/CodeGen/SystemZ/systemz-abi.c
+++ b/clang/test/CodeGen/SystemZ/systemz-abi.c
@@ -24,6 +24,8 @@
 // RUN: %clang_cc1 -no-enable-noundef-analysis -triple s390x-linux-gnu -target-cpu arch14 \
 // RUN:   -emit-llvm -o - %s -mfloat-abi soft | FileCheck %s \
 // RUN:   --check-prefixes=CHECK,SOFT-FLOAT
+// RUN: %clang_cc1 -no-enable-noundef-analysis -triple s390x-linux-gnu -target-cpu z17 \
+// RUN:   -emit-llvm -o - %s | FileCheck %s --check-prefixes=CHECK,HARD-FLOAT
 // RUN: %clang_cc1 -no-enable-noundef-analysis -triple s390x-linux-gnu -target-cpu arch15 \
 // RUN:   -emit-llvm -o - %s | FileCheck %s --check-prefixes=CHECK,HARD-FLOAT
 // RUN: %clang_cc1 -no-enable-noundef-analysis -triple s390x-linux-gnu -target-cpu arch15 \
diff --git a/clang/test/CodeGen/X86/avx10_2_512convert-builtins.c b/clang/test/CodeGen/X86/avx10_2_512convert-builtins.c
index dcf7bbc005a7..3ac7c2cc8716 100644
--- a/clang/test/CodeGen/X86/avx10_2_512convert-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2_512convert-builtins.c
@@ -59,22 +59,22 @@ __m256i test_mm512_maskz_cvtbiasph_bf8(__mmask32 __U, __m512i __A, __m512h __B)
   return _mm512_maskz_cvtbiasph_bf8(__U, __A, __B);
 }
 
-__m256i test_mm512_cvtbiassph_bf8(__m512i __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_cvtbiassph_bf8(
+__m256i test_mm512_cvts_biasph_bf8(__m512i __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_cvts_biasph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s512(
-  return _mm512_cvtbiassph_bf8(__A, __B);
+  return _mm512_cvts_biasph_bf8(__A, __B);
 }
 
-__m256i test_mm512_mask_cvtbiassph_bf8(__m256i __W, __mmask32 __U, __m512i __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_cvtbiassph_bf8(
+__m256i test_mm512_mask_cvts_biasph_bf8(__m256i __W, __mmask32 __U, __m512i __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_cvts_biasph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s512(
-  return _mm512_mask_cvtbiassph_bf8(__W, __U, __A, __B);
+  return _mm512_mask_cvts_biasph_bf8(__W, __U, __A, __B);
 }
 
-__m256i test_mm512_maskz_cvtbiassph_bf8(__mmask32 __U, __m512i __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_cvtbiassph_bf8(
+__m256i test_mm512_maskz_cvts_biasph_bf8(__mmask32 __U, __m512i __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_cvts_biasph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s512(
-  return _mm512_maskz_cvtbiassph_bf8(__U, __A, __B);
+  return _mm512_maskz_cvts_biasph_bf8(__U, __A, __B);
 }
 
 __m256i test_mm512_cvtbiasph_hf8(__m512i __A, __m512h __B) {
@@ -95,22 +95,22 @@ __m256i test_mm512_maskz_cvtbiasph_hf8(__mmask32 __U, __m512i __A, __m512h __B)
   return _mm512_maskz_cvtbiasph_hf8(__U, __A, __B);
 }
 
-__m256i test_mm512_cvtbiassph_hf8(__m512i __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_cvtbiassph_hf8(
+__m256i test_mm512_cvts_biasph_hf8(__m512i __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_cvts_biasph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s512(
-  return _mm512_cvtbiassph_hf8(__A, __B);
+  return _mm512_cvts_biasph_hf8(__A, __B);
 }
 
-__m256i test_mm512_mask_cvtbiassph_hf8(__m256i __W, __mmask32 __U, __m512i __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_cvtbiassph_hf8(
+__m256i test_mm512_mask_cvts_biasph_hf8(__m256i __W, __mmask32 __U, __m512i __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_cvts_biasph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s512(
-  return _mm512_mask_cvtbiassph_hf8(__W, __U, __A, __B);
+  return _mm512_mask_cvts_biasph_hf8(__W, __U, __A, __B);
 }
 
-__m256i test_mm512_maskz_cvtbiassph_hf8(__mmask32 __U, __m512i __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_cvtbiassph_hf8(
+__m256i test_mm512_maskz_cvts_biasph_hf8(__mmask32 __U, __m512i __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_cvts_biasph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s512(
-  return _mm512_maskz_cvtbiassph_hf8(__U, __A, __B);
+  return _mm512_maskz_cvts_biasph_hf8(__U, __A, __B);
 }
 
 __m512i test_mm512_cvt2ph_bf8(__m512h __A, __m512h __B) {
@@ -135,26 +135,26 @@ __m512i test_mm512_maskz_cvt2ph_bf8(__mmask32 __U, __m512h __A, __m512h __B) {
   return _mm512_maskz_cvt2ph_bf8(__U, __A, __B);
 }
 
-__m512i test_mm512_cvts2ph_bf8(__m512h __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_cvts2ph_bf8(
+__m512i test_mm512_cvts_2ph_bf8(__m512h __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_cvts_2ph_bf8(
   // CHECK: call <64 x i8> @llvm.x86.avx10.vcvt2ph2bf8s512(
-  return _mm512_cvts2ph_bf8(__A, __B);
+  return _mm512_cvts_2ph_bf8(__A, __B);
 }
 
-__m512i test_mm512_mask_cvts2ph_bf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_cvts2ph_bf8(
+__m512i test_mm512_mask_cvts_2ph_bf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_cvts_2ph_bf8(
   // CHECK: call <64 x i8> @llvm.x86.avx10.vcvt2ph2bf8s512(
   // CHECK: select <64 x i1> %{{.*}}, <64 x i8> %{{.*}}, <64 x i8> %{{.*}}
   // CHECK: ret <8 x i64> %{{.*}}
-  return _mm512_mask_cvts2ph_bf8(__W, __U, __A, __B);
+  return _mm512_mask_cvts_2ph_bf8(__W, __U, __A, __B);
 }
 
-__m512i test_mm512_maskz_cvts2ph_bf8(__mmask64 __U, __m512h __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_cvts2ph_bf8(
+__m512i test_mm512_maskz_cvts_2ph_bf8(__mmask64 __U, __m512h __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_cvts_2ph_bf8(
   // CHECK: call <64 x i8> @llvm.x86.avx10.vcvt2ph2bf8s512(
   // CHECK: zeroinitializer
   // CHECK: select <64 x i1> %{{.*}}, <64 x i8> %{{.*}}, <64 x i8> %{{.*}}
-  return _mm512_maskz_cvts2ph_bf8(__U, __A, __B);
+  return _mm512_maskz_cvts_2ph_bf8(__U, __A, __B);
 }
 
 __m512i test_mm512_cvt2ph_hf8(__m512h __A, __m512h __B) {
@@ -179,26 +179,26 @@ __m512i test_mm512_maskz_cvt2ph_hf8(__mmask64 __U, __m512h __A, __m512h __B) {
   return _mm512_maskz_cvt2ph_hf8(__U, __A, __B);
 }
 
-__m512i test_mm512_cvts2ph_hf8(__m512h __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_cvts2ph_hf8(
+__m512i test_mm512_cvts_2ph_hf8(__m512h __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_cvts_2ph_hf8(
   // CHECK: call <64 x i8> @llvm.x86.avx10.vcvt2ph2hf8s512(
-  return _mm512_cvts2ph_hf8(__A, __B);
+  return _mm512_cvts_2ph_hf8(__A, __B);
 }
 
-__m512i test_mm512_mask_cvts2ph_hf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_cvts2ph_hf8(
+__m512i test_mm512_mask_cvts_2ph_hf8(__m512i __W, __mmask64 __U, __m512h __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_cvts_2ph_hf8(
   // CHECK: call <64 x i8> @llvm.x86.avx10.vcvt2ph2hf8s512(
   // CHECK: select <64 x i1> %{{.*}}, <64 x i8> %{{.*}}, <64 x i8> %{{.*}}
   // CHECK: ret <8 x i64> %{{.*}}
-  return _mm512_mask_cvts2ph_hf8(__W, __U, __A, __B);
+  return _mm512_mask_cvts_2ph_hf8(__W, __U, __A, __B);
 }
 
-__m512i test_mm512_maskz_cvts2ph_hf8(__mmask64 __U, __m512h __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_cvts2ph_hf8(
+__m512i test_mm512_maskz_cvts_2ph_hf8(__mmask64 __U, __m512h __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_cvts_2ph_hf8(
   // CHECK: call <64 x i8> @llvm.x86.avx10.vcvt2ph2hf8s512(
   // CHECK: zeroinitializer
   // CHECK: select <64 x i1> %{{.*}}, <64 x i8> %{{.*}}, <64 x i8> %{{.*}}
-  return _mm512_maskz_cvts2ph_hf8(__U, __A, __B);
+  return _mm512_maskz_cvts_2ph_hf8(__U, __A, __B);
 }
 
 __m512h test_mm512_cvthf8_ph(__m256i __A) {
@@ -237,22 +237,22 @@ __m256i test_mm512_maskz_cvtph_bf8(__mmask32 __A, __m512h __B) {
   return _mm512_maskz_cvtph_bf8(__A, __B);
 }
 
-__m256i test_mm512_cvtsph_bf8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_cvtsph_bf8(
+__m256i test_mm512_cvts_ph_bf8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_cvts_ph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s512(
-  return _mm512_cvtsph_bf8(__A);
+  return _mm512_cvts_ph_bf8(__A);
 }
 
-__m256i test_mm512_mask_cvtsph_bf8(__m256i __A, __mmask32 __B, __m512h __C) {
-  // CHECK-LABEL: @test_mm512_mask_cvtsph_bf8(
+__m256i test_mm512_mask_cvts_ph_bf8(__m256i __A, __mmask32 __B, __m512h __C) {
+  // CHECK-LABEL: @test_mm512_mask_cvts_ph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s512(
-  return _mm512_mask_cvtsph_bf8(__A, __B, __C);
+  return _mm512_mask_cvts_ph_bf8(__A, __B, __C);
 }
 
-__m256i test_mm512_maskz_cvtsph_bf8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_cvtsph_bf8(
+__m256i test_mm512_maskz_cvts_ph_bf8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_cvts_ph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s512(
-  return _mm512_maskz_cvtsph_bf8(__A, __B);
+  return _mm512_maskz_cvts_ph_bf8(__A, __B);
 }
 
 __m256i test_mm512_cvtph_hf8(__m512h __A) {
@@ -273,22 +273,22 @@ __m256i test_mm512_maskz_cvtph_hf8(__mmask32 __A, __m512h __B) {
   return _mm512_maskz_cvtph_hf8(__A, __B);
 }
 
-__m256i test_mm512_cvtsph_hf8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_cvtsph_hf8(
+__m256i test_mm512_cvts_ph_hf8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_cvts_ph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s512(
-  return _mm512_cvtsph_hf8(__A);
+  return _mm512_cvts_ph_hf8(__A);
 }
 
-__m256i test_mm512_mask_cvtsph_hf8(__m256i __A, __mmask32 __B, __m512h __C) {
-  // CHECK-LABEL: @test_mm512_mask_cvtsph_hf8(
+__m256i test_mm512_mask_cvts_ph_hf8(__m256i __A, __mmask32 __B, __m512h __C) {
+  // CHECK-LABEL: @test_mm512_mask_cvts_ph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s512(
-  return _mm512_mask_cvtsph_hf8(__A, __B, __C);
+  return _mm512_mask_cvts_ph_hf8(__A, __B, __C);
 }
 
-__m256i test_mm512_maskz_cvtsph_hf8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_cvtsph_hf8(
+__m256i test_mm512_maskz_cvts_ph_hf8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_cvts_ph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s512(
-  return _mm512_maskz_cvtsph_hf8(__A, __B);
+  return _mm512_maskz_cvts_ph_hf8(__A, __B);
 }
 
 __m512h test_mm512_cvtbf8_ph(__m256i A) {
diff --git a/clang/test/CodeGen/X86/avx10_2_512minmax-error.c b/clang/test/CodeGen/X86/avx10_2_512minmax-error.c
index 6db7801eb004..2ee496d317a5 100644
--- a/clang/test/CodeGen/X86/avx10_2_512minmax-error.c
+++ b/clang/test/CodeGen/X86/avx10_2_512minmax-error.c
@@ -113,17 +113,6 @@ __m512 test_mm512_minmax_round_ps(__m512 __A, __m512 __B) {
   return _mm512_minmax_round_ps(__A, __B, 127, 11); // expected-error {{invalid rounding argument}}
 }
 
-__m256d test_mm256_minmax_round_pd(__m256d __A, __m256d __B) {
-  return _mm256_minmax_round_pd(__A, __B, 127, 11); // expected-error {{invalid rounding argument}}
-}
-
-__m256h test_mm256_minmax_round_ph(__m256h __A, __m256h __B) {
-  return _mm256_minmax_round_ph(__A, __B, 127, 11); // expected-error {{invalid rounding argument}}
-}
-
-__m256 test_mm256_minmax_round_ps(__m256 __A, __m256 __B) {
-  return _mm256_minmax_round_ps(__A, __B, 127, 11); // expected-error {{invalid rounding argument}}
-}
 __m128d test_mm_minmax_round_sd(__m128d __A, __m128d __B) {
   return _mm_minmax_round_sd(__A, __B, 127, 11); // expected-error {{invalid rounding argument}}
 }
diff --git a/clang/test/CodeGen/X86/avx10_2_512satcvt-builtins-error.c b/clang/test/CodeGen/X86/avx10_2_512satcvt-builtins-error.c
deleted file mode 100755
index 81bf59153e67..000000000000
--- a/clang/test/CodeGen/X86/avx10_2_512satcvt-builtins-error.c
+++ /dev/null
@@ -1,198 +0,0 @@
-// RUN: %clang_cc1 %s -flax-vector-conversions=none -ffreestanding -triple=x86_64 -target-feature +avx10.2-512 \
-// RUN: -Wall -Werror -verify
-// RUN: %clang_cc1 %s -flax-vector-conversions=none -ffreestanding -triple=i386 -target-feature +avx10.2-512 \
-// RUN: -Wall -Werror -verify
-
-#include <immintrin.h>
-
-__m512i test_mm512_ipcvt_roundph_epi8(__m512h __A) {
-  return _mm512_ipcvt_roundph_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvt_roundph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
-  return _mm512_mask_ipcvt_roundph_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvt_roundph_epi8(__mmask32 __A, __m512h __B) {
-  return _mm512_maskz_ipcvt_roundph_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_ipcvt_roundph_epu8(__m512h __A) {
-  return _mm512_ipcvt_roundph_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvt_roundph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
-  return _mm512_mask_ipcvt_roundph_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvt_roundph_epu8(__mmask32 __A, __m512h __B) {
-  return _mm512_maskz_ipcvt_roundph_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_ipcvt_roundps_epi8(__m512 __A) {
-  return _mm512_ipcvt_roundps_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvt_roundps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
-  return _mm512_mask_ipcvt_roundps_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvt_roundps_epi8(__mmask16 __A, __m512 __B) {
-  return _mm512_maskz_ipcvt_roundps_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_ipcvt_roundps_epu8(__m512 __A) {
-  return _mm512_ipcvt_roundps_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvt_roundps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
-  return _mm512_mask_ipcvt_roundps_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvt_roundps_epu8(__mmask16 __A, __m512 __B) {
-  return _mm512_maskz_ipcvt_roundps_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_ipcvtt_roundph_epi8(__m512h __A) {
-  return _mm512_ipcvtt_roundph_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvtt_roundph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
-  return _mm512_mask_ipcvtt_roundph_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvtt_roundph_epi8(__mmask32 __A, __m512h __B) {
-  return _mm512_maskz_ipcvtt_roundph_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_ipcvtt_roundph_epu8(__m512h __A) {
-  return _mm512_ipcvtt_roundph_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvtt_roundph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
-  return _mm512_mask_ipcvtt_roundph_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvtt_roundph_epu8(__mmask32 __A, __m512h __B) {
-  return _mm512_maskz_ipcvtt_roundph_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_ipcvtt_roundps_epi8(__m512 __A) {
-  return _mm512_ipcvtt_roundps_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvtt_roundps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
-  return _mm512_mask_ipcvtt_roundps_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvtt_roundps_epi8(__mmask16 __A, __m512 __B) {
-  return _mm512_maskz_ipcvtt_roundps_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_ipcvtt_roundps_epu8(__m512 __A) {
-  return _mm512_ipcvtt_roundps_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_mask_ipcvtt_roundps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
-  return _mm512_mask_ipcvtt_roundps_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m512i test_mm512_maskz_ipcvtt_roundps_epu8(__mmask16 __A, __m512 __B) {
-  return _mm512_maskz_ipcvtt_roundps_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvt_roundph_epi8(__m256h __A) {
-  return _mm256_ipcvt_roundph_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvt_roundph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
-  return _mm256_mask_ipcvt_roundph_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvt_roundph_epi8(__mmask16 __A, __m256h __B) {
-  return _mm256_maskz_ipcvt_roundph_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvt_roundph_epu8(__m256h __A) {
-  return _mm256_ipcvt_roundph_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvt_roundph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
-  return _mm256_mask_ipcvt_roundph_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvt_roundph_epu8(__mmask16 __A, __m256h __B) {
-  return _mm256_maskz_ipcvt_roundph_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvt_roundps_epi8(__m256 __A) {
-  return _mm256_ipcvt_roundps_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvt_roundps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
-  return _mm256_mask_ipcvt_roundps_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvt_roundps_epi8(__mmask8 __A, __m256 __B) {
-  return _mm256_maskz_ipcvt_roundps_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvt_roundps_epu8(__m256 __A) {
-  return _mm256_ipcvt_roundps_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvt_roundps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
-  return _mm256_mask_ipcvt_roundps_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvt_roundps_epu8(__mmask8 __A, __m256 __B) {
-  return _mm256_maskz_ipcvt_roundps_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvtt_roundph_epi8(__m256h __A) {
-  return _mm256_ipcvtt_roundph_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvtt_roundph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
-  return _mm256_mask_ipcvtt_roundph_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvtt_roundph_epi8(__mmask16 __A, __m256h __B) {
-  return _mm256_maskz_ipcvtt_roundph_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvtt_roundph_epu8(__m256h __A) {
-  return _mm256_ipcvtt_roundph_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvtt_roundph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
-  return _mm256_mask_ipcvtt_roundph_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvtt_roundph_epu8(__mmask16 __A, __m256h __B) {
-  return _mm256_maskz_ipcvtt_roundph_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvtt_roundps_epi8(__m256 __A) {
-  return _mm256_ipcvtt_roundps_epi8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvtt_roundps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
-  return _mm256_mask_ipcvtt_roundps_epi8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvtt_roundps_epi8(__mmask8 __A, __m256 __B) {
-  return _mm256_maskz_ipcvtt_roundps_epi8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_ipcvtt_roundps_epu8(__m256 __A) {
-  return _mm256_ipcvtt_roundps_epu8(__A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_ipcvtt_roundps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
-  return _mm256_mask_ipcvtt_roundps_epu8(__S, __A, __B, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_ipcvtt_roundps_epu8(__mmask8 __A, __m256 __B) {
-  return _mm256_maskz_ipcvtt_roundps_epu8(__A, __B, 22); // expected-error {{invalid rounding argument}}
-}
diff --git a/clang/test/CodeGen/X86/avx10_2_512satcvt-builtins.c b/clang/test/CodeGen/X86/avx10_2_512satcvt-builtins.c
index 0d3b0c278b44..0a1c32914439 100755
--- a/clang/test/CodeGen/X86/avx10_2_512satcvt-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2_512satcvt-builtins.c
@@ -5,375 +5,375 @@
 
 #include <immintrin.h>
 
-__m512i test_mm512_ipcvtbf16_epi8(__m512bh __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtbf16_epi8(
+__m512i test_mm512_ipcvts_bf16_epi8(__m512bh __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs512
-  return _mm512_ipcvtbf16_epi8(__A);
+  return _mm512_ipcvts_bf16_epi8(__A);
 }
 
-__m512i test_mm512_mask_ipcvtbf16_epi8(__m512i __S, __mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtbf16_epi8(
+__m512i test_mm512_mask_ipcvts_bf16_epi8(__m512i __S, __mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs512
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_mask_ipcvtbf16_epi8(__S, __A, __B);
+  return _mm512_mask_ipcvts_bf16_epi8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvtbf16_epi8(__mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtbf16_epi8
+__m512i test_mm512_maskz_ipcvts_bf16_epi8(__mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_bf16_epi8
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs512
   // CHECK: zeroinitializer
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_maskz_ipcvtbf16_epi8(__A, __B);
+  return _mm512_maskz_ipcvts_bf16_epi8(__A, __B);
 }
 
-__m512i test_mm512_ipcvtbf16_epu8(__m512bh __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtbf16_epu8(
+__m512i test_mm512_ipcvts_bf16_epu8(__m512bh __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs512
-  return _mm512_ipcvtbf16_epu8(__A);
+  return _mm512_ipcvts_bf16_epu8(__A);
 }
 
-__m512i test_mm512_mask_ipcvtbf16_epu8(__m512i __S, __mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtbf16_epu8(
+__m512i test_mm512_mask_ipcvts_bf16_epu8(__m512i __S, __mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs512
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_mask_ipcvtbf16_epu8(__S, __A, __B);
+  return _mm512_mask_ipcvts_bf16_epu8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvtbf16_epu8(__mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtbf16_epu8
+__m512i test_mm512_maskz_ipcvts_bf16_epu8(__mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_bf16_epu8
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs512
   // CHECK: zeroinitializer
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_maskz_ipcvtbf16_epu8(__A, __B);
+  return _mm512_maskz_ipcvts_bf16_epu8(__A, __B);
 }
 
-__m512i test_mm512_ipcvtph_epi8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtph_epi8(
+__m512i test_mm512_ipcvts_ph_epi8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs512
-  return _mm512_ipcvtph_epi8(__A);
+  return _mm512_ipcvts_ph_epi8(__A);
 }
 
-__m512i test_mm512_mask_ipcvtph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtph_epi8(
+__m512i test_mm512_mask_ipcvts_ph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs512
-  return _mm512_mask_ipcvtph_epi8(__S, __A, __B);
+  return _mm512_mask_ipcvts_ph_epi8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvtph_epi8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtph_epi8(
+__m512i test_mm512_maskz_ipcvts_ph_epi8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs512
-  return _mm512_maskz_ipcvtph_epi8(__A, __B);
+  return _mm512_maskz_ipcvts_ph_epi8(__A, __B);
 }
 
-__m512i test_mm512_ipcvt_roundph_epi8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvt_roundph_epi8(
+__m512i test_mm512_ipcvts_roundph_epi8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_roundph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs512
-  return _mm512_ipcvt_roundph_epi8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_ipcvts_roundph_epi8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvt_roundph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvt_roundph_epi8
+__m512i test_mm512_mask_ipcvts_roundph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_roundph_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs512
-  return _mm512_mask_ipcvt_roundph_epi8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvts_roundph_epi8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_maskz_ipcvt_roundph_epi8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvt_roundph_epi8
+__m512i test_mm512_maskz_ipcvts_roundph_epi8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_roundph_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs512
-  return _mm512_maskz_ipcvt_roundph_epi8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvts_roundph_epi8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_ipcvtph_epu8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtph_epu8(
+__m512i test_mm512_ipcvts_ph_epu8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs512
-  return _mm512_ipcvtph_epu8(__A);
+  return _mm512_ipcvts_ph_epu8(__A);
 }
 
-__m512i test_mm512_mask_ipcvtph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtph_epu8(
+__m512i test_mm512_mask_ipcvts_ph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs512
-  return _mm512_mask_ipcvtph_epu8(__S, __A, __B);
+  return _mm512_mask_ipcvts_ph_epu8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvtph_epu8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtph_epu8(
+__m512i test_mm512_maskz_ipcvts_ph_epu8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs512
-  return _mm512_maskz_ipcvtph_epu8(__A, __B);
+  return _mm512_maskz_ipcvts_ph_epu8(__A, __B);
 }
 
-__m512i test_mm512_ipcvt_roundph_epu8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvt_roundph_epu8(
+__m512i test_mm512_ipcvts_roundph_epu8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_roundph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs512
-  return _mm512_ipcvt_roundph_epu8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_ipcvts_roundph_epu8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvt_roundph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvt_roundph_epu8
+__m512i test_mm512_mask_ipcvts_roundph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_roundph_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs512
-  return _mm512_mask_ipcvt_roundph_epu8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvts_roundph_epu8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_maskz_ipcvt_roundph_epu8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvt_roundph_epu8
+__m512i test_mm512_maskz_ipcvts_roundph_epu8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_roundph_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs512
-  return _mm512_maskz_ipcvt_roundph_epu8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvts_roundph_epu8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_ipcvtps_epi8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtps_epi8(
+__m512i test_mm512_ipcvts_ps_epi8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs512
-  return _mm512_ipcvtps_epi8(__A);
+  return _mm512_ipcvts_ps_epi8(__A);
 }
 
-__m512i test_mm512_mask_ipcvtps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtps_epi8(
+__m512i test_mm512_mask_ipcvts_ps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs512
-  return _mm512_mask_ipcvtps_epi8(__S, __A, __B);
+  return _mm512_mask_ipcvts_ps_epi8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvtps_epi8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtps_epi8(
+__m512i test_mm512_maskz_ipcvts_ps_epi8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs512
-  return _mm512_maskz_ipcvtps_epi8(__A, __B);
+  return _mm512_maskz_ipcvts_ps_epi8(__A, __B);
 }
 
-__m512i test_mm512_ipcvt_roundps_epi8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvt_roundps_epi8(
+__m512i test_mm512_ipcvts_roundps_epi8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_roundps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs512
-  return _mm512_ipcvt_roundps_epi8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_ipcvts_roundps_epi8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvt_roundps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvt_roundps_epi8
+__m512i test_mm512_mask_ipcvts_roundps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_roundps_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs512
-  return _mm512_mask_ipcvt_roundps_epi8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvts_roundps_epi8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_maskz_ipcvt_roundps_epi8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvt_roundps_epi8
+__m512i test_mm512_maskz_ipcvts_roundps_epi8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_roundps_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs512
-  return _mm512_maskz_ipcvt_roundps_epi8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvts_roundps_epi8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_ipcvtps_epu8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtps_epu8(
+__m512i test_mm512_ipcvts_ps_epu8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs512
-  return _mm512_ipcvtps_epu8(__A);
+  return _mm512_ipcvts_ps_epu8(__A);
 }
 
-__m512i test_mm512_mask_ipcvtps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtps_epu8(
+__m512i test_mm512_mask_ipcvts_ps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs512
-  return _mm512_mask_ipcvtps_epu8(__S, __A, __B);
+  return _mm512_mask_ipcvts_ps_epu8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvtps_epu8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtps_epu8(
+__m512i test_mm512_maskz_ipcvts_ps_epu8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs512
-  return _mm512_maskz_ipcvtps_epu8(__A, __B);
+  return _mm512_maskz_ipcvts_ps_epu8(__A, __B);
 }
 
-__m512i test_mm512_ipcvt_roundps_epu8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvt_roundps_epu8(
+__m512i test_mm512_ipcvts_roundps_epu8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvts_roundps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs512
-  return _mm512_ipcvt_roundps_epu8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_ipcvts_roundps_epu8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvt_roundps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvt_roundps_epu8
+__m512i test_mm512_mask_ipcvts_roundps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvts_roundps_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs512
-  return _mm512_mask_ipcvt_roundps_epu8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvts_roundps_epu8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_maskz_ipcvt_roundps_epu8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvt_roundps_epu8
+__m512i test_mm512_maskz_ipcvts_roundps_epu8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvts_roundps_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs512
-  return _mm512_maskz_ipcvt_roundps_epu8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvts_roundps_epu8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_ipcvttbf16_epi8(__m512bh __A) {
-  // CHECK-LABEL: @test_mm512_ipcvttbf16_epi8(
+__m512i test_mm512_ipcvtts_bf16_epi8(__m512bh __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs512(
-  return _mm512_ipcvttbf16_epi8(__A);
+  return _mm512_ipcvtts_bf16_epi8(__A);
 }
 
-__m512i test_mm512_mask_ipcvttbf16_epi8(__m512i __S, __mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvttbf16_epi8(
+__m512i test_mm512_mask_ipcvtts_bf16_epi8(__m512i __S, __mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs512(
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_mask_ipcvttbf16_epi8(__S, __A, __B);
+  return _mm512_mask_ipcvtts_bf16_epi8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvttbf16_epi8(__mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvttbf16_epi8
+__m512i test_mm512_maskz_ipcvtts_bf16_epi8(__mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_bf16_epi8
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs512(
   // CHECK: zeroinitializer
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_maskz_ipcvttbf16_epi8(__A, __B);
+  return _mm512_maskz_ipcvtts_bf16_epi8(__A, __B);
 }
 
-__m512i test_mm512_ipcvttbf16_epu8(__m512bh __A) {
-  // CHECK-LABEL: @test_mm512_ipcvttbf16_epu8(
+__m512i test_mm512_ipcvtts_bf16_epu8(__m512bh __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs512(
-  return _mm512_ipcvttbf16_epu8(__A);
+  return _mm512_ipcvtts_bf16_epu8(__A);
 }
 
-__m512i test_mm512_mask_ipcvttbf16_epu8(__m512i __S, __mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvttbf16_epu8(
+__m512i test_mm512_mask_ipcvtts_bf16_epu8(__m512i __S, __mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs512(
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_mask_ipcvttbf16_epu8(__S, __A, __B);
+  return _mm512_mask_ipcvtts_bf16_epu8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvttbf16_epu8(__mmask32 __A, __m512bh __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvttbf16_epu8
+__m512i test_mm512_maskz_ipcvtts_bf16_epu8(__mmask32 __A, __m512bh __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_bf16_epu8
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs512(
   // CHECK: zeroinitializer
   // CHECK: select <32 x i1> %{{.*}}, <32 x i16> %{{.*}}, <32 x i16> %{{.*}}
-  return _mm512_maskz_ipcvttbf16_epu8(__A, __B);
+  return _mm512_maskz_ipcvtts_bf16_epu8(__A, __B);
 }
 
-__m512i test_mm512_ipcvttph_epi8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvttph_epi8(
+__m512i test_mm512_ipcvtts_ph_epi8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs512
-  return _mm512_ipcvttph_epi8(__A);
+  return _mm512_ipcvtts_ph_epi8(__A);
 }
 
-__m512i test_mm512_mask_ipcvttph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvttph_epi8(
+__m512i test_mm512_mask_ipcvtts_ph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs512
-  return _mm512_mask_ipcvttph_epi8(__S, __A, __B);
+  return _mm512_mask_ipcvtts_ph_epi8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvttph_epi8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvttph_epi8
+__m512i test_mm512_maskz_ipcvtts_ph_epi8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_ph_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs512
-  return _mm512_maskz_ipcvttph_epi8(__A, __B);
+  return _mm512_maskz_ipcvtts_ph_epi8(__A, __B);
 }
 
-__m512i test_mm512_ipcvtt_roundph_epi8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtt_roundph_epi8
+__m512i test_mm512_ipcvtts_roundph_epi8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_roundph_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs512
-  return _mm512_ipcvtt_roundph_epi8(__A, _MM_FROUND_NO_EXC);
+  return _mm512_ipcvtts_roundph_epi8(__A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvtt_roundph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtt_roundph_epi8
+__m512i test_mm512_mask_ipcvtts_roundph_epi8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_roundph_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs512
-  return _mm512_mask_ipcvtt_roundph_epi8(__S, __A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvtts_roundph_epi8(__S, __A, __B, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_maskz_ipcvtt_roundph_epi8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtt_roundph_epi8
+__m512i test_mm512_maskz_ipcvtts_roundph_epi8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_roundph_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs512
-  return _mm512_maskz_ipcvtt_roundph_epi8(__A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvtts_roundph_epi8(__A, __B, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_ipcvttph_epu8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvttph_epu8(
+__m512i test_mm512_ipcvtts_ph_epu8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs512
-  return _mm512_ipcvttph_epu8(__A);
+  return _mm512_ipcvtts_ph_epu8(__A);
 }
 
-__m512i test_mm512_mask_ipcvttph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvttph_epu8(
+__m512i test_mm512_mask_ipcvtts_ph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs512
-  return _mm512_mask_ipcvttph_epu8(__S, __A, __B);
+  return _mm512_mask_ipcvtts_ph_epu8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvttph_epu8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvttph_epu8
+__m512i test_mm512_maskz_ipcvtts_ph_epu8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_ph_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs512
-  return _mm512_maskz_ipcvttph_epu8(__A, __B);
+  return _mm512_maskz_ipcvtts_ph_epu8(__A, __B);
 }
 
-__m512i test_mm512_ipcvtt_roundph_epu8(__m512h __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtt_roundph_epu8
+__m512i test_mm512_ipcvtts_roundph_epu8(__m512h __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_roundph_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs512
-  return _mm512_ipcvtt_roundph_epu8(__A, _MM_FROUND_NO_EXC);
+  return _mm512_ipcvtts_roundph_epu8(__A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvtt_roundph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtt_roundph_epu8
+__m512i test_mm512_mask_ipcvtts_roundph_epu8(__m512i __S, __mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_roundph_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs512
-  return _mm512_mask_ipcvtt_roundph_epu8(__S, __A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvtts_roundph_epu8(__S, __A, __B, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_maskz_ipcvtt_roundph_epu8(__mmask32 __A, __m512h __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtt_roundph_epu8
+__m512i test_mm512_maskz_ipcvtts_roundph_epu8(__mmask32 __A, __m512h __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_roundph_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs512
-  return _mm512_maskz_ipcvtt_roundph_epu8(__A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvtts_roundph_epu8(__A, __B, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_ipcvttps_epi8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvttps_epi8(
+__m512i test_mm512_ipcvtts_ps_epi8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs512
-  return _mm512_ipcvttps_epi8(__A);
+  return _mm512_ipcvtts_ps_epi8(__A);
 }
 
-__m512i test_mm512_mask_ipcvttps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvttps_epi8(
+__m512i test_mm512_mask_ipcvtts_ps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs512
-  return _mm512_mask_ipcvttps_epi8(__S, __A, __B);
+  return _mm512_mask_ipcvtts_ps_epi8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvttps_epi8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvttps_epi8
+__m512i test_mm512_maskz_ipcvtts_ps_epi8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_ps_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs512
-  return _mm512_maskz_ipcvttps_epi8(__A, __B);
+  return _mm512_maskz_ipcvtts_ps_epi8(__A, __B);
 }
 
-__m512i test_mm512_ipcvtt_roundps_epi8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtt_roundps_epi8
+__m512i test_mm512_ipcvtts_roundps_epi8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_roundps_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs512
-  return _mm512_ipcvtt_roundps_epi8(__A, _MM_FROUND_NO_EXC);
+  return _mm512_ipcvtts_roundps_epi8(__A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvtt_roundps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtt_roundps_epi8
+__m512i test_mm512_mask_ipcvtts_roundps_epi8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_roundps_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs512
-  return _mm512_mask_ipcvtt_roundps_epi8(__S, __A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvtts_roundps_epi8(__S, __A, __B, _MM_FROUND_NO_EXC);
 }
 
 
-__m512i test_mm512_maskz_ipcvtt_roundps_epi8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtt_roundps_epi8
+__m512i test_mm512_maskz_ipcvtts_roundps_epi8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_roundps_epi8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs512
-  return _mm512_maskz_ipcvtt_roundps_epi8(__A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvtts_roundps_epi8(__A, __B, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_ipcvttps_epu8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvttps_epu8(
+__m512i test_mm512_ipcvtts_ps_epu8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs512
-  return _mm512_ipcvttps_epu8(__A);
+  return _mm512_ipcvtts_ps_epu8(__A);
 }
 
-__m512i test_mm512_mask_ipcvttps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvttps_epu8(
+__m512i test_mm512_mask_ipcvtts_ps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs512
-  return _mm512_mask_ipcvttps_epu8(__S, __A, __B);
+  return _mm512_mask_ipcvtts_ps_epu8(__S, __A, __B);
 }
 
-__m512i test_mm512_maskz_ipcvttps_epu8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvttps_epu8
+__m512i test_mm512_maskz_ipcvtts_ps_epu8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_ps_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs512
-  return _mm512_maskz_ipcvttps_epu8(__A, __B);
+  return _mm512_maskz_ipcvtts_ps_epu8(__A, __B);
 }
 
-__m512i test_mm512_ipcvtt_roundps_epu8(__m512 __A) {
-  // CHECK-LABEL: @test_mm512_ipcvtt_roundps_epu8
+__m512i test_mm512_ipcvtts_roundps_epu8(__m512 __A) {
+  // CHECK-LABEL: @test_mm512_ipcvtts_roundps_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs512
-  return _mm512_ipcvtt_roundps_epu8(__A, _MM_FROUND_NO_EXC);
+  return _mm512_ipcvtts_roundps_epu8(__A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_mask_ipcvtt_roundps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_mask_ipcvtt_roundps_epu8
+__m512i test_mm512_mask_ipcvtts_roundps_epu8(__m512i __S, __mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_mask_ipcvtts_roundps_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs512
-  return _mm512_mask_ipcvtt_roundps_epu8(__S, __A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_mask_ipcvtts_roundps_epu8(__S, __A, __B, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_maskz_ipcvtt_roundps_epu8(__mmask16 __A, __m512 __B) {
-  // CHECK-LABEL: @test_mm512_maskz_ipcvtt_roundps_epu8
+__m512i test_mm512_maskz_ipcvtts_roundps_epu8(__mmask16 __A, __m512 __B) {
+  // CHECK-LABEL: @test_mm512_maskz_ipcvtts_roundps_epu8
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs512
-  return _mm512_maskz_ipcvtt_roundps_epu8(__A, __B, _MM_FROUND_NO_EXC);
+  return _mm512_maskz_ipcvtts_roundps_epu8(__A, __B, _MM_FROUND_NO_EXC);
 }
diff --git a/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins-x64.c b/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins-x64.c
index 8c8959a03d7b..1aaa6544d1f9 100644
--- a/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins-x64.c
+++ b/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins-x64.c
@@ -3,58 +3,58 @@
 #include <immintrin.h>
 #include <stddef.h>
 
-long long test_mm_cvttssd_si64(__m128d __A) {
-  // CHECK-LABEL: @test_mm_cvttssd_si64(
+long long test_mm_cvtts_sd_si64(__m128d __A) {
+  // CHECK-LABEL: @test_mm_cvtts_sd_si64(
   // CHECK: @llvm.x86.avx10.vcvttsd2sis64(<2 x double>
   return _mm_cvtts_roundsd_si64(__A, _MM_FROUND_NO_EXC);
 }
 
-long long test_mm_cvttssd_i64(__m128d __A) {
-  // CHECK-LABEL: @test_mm_cvttssd_i64(
+long long test_mm_cvtts_sd_i64(__m128d __A) {
+  // CHECK-LABEL: @test_mm_cvtts_sd_i64(
   // CHECK: @llvm.x86.avx10.vcvttsd2sis64(<2 x double>
   return _mm_cvtts_roundsd_i64(__A, _MM_FROUND_NO_EXC);
 }
 
-unsigned long long test_mm_cvttssd_u64(__m128d __A) {
-  // CHECK-LABEL: @test_mm_cvttssd_u64(
+unsigned long long test_mm_cvtts_sd_u64(__m128d __A) {
+  // CHECK-LABEL: @test_mm_cvtts_sd_u64(
   // CHECK: @llvm.x86.avx10.vcvttsd2usis64(<2 x double>
   return _mm_cvtts_roundsd_u64(__A, _MM_FROUND_NO_EXC);
 }
 
-float test_mm_cvttsss_i64(__m128 __A) {
-  // CHECK-LABEL: @test_mm_cvttsss_i64(
+float test_mm_cvtts_ss_i64(__m128 __A) {
+  // CHECK-LABEL: @test_mm_cvtts_ss_i64(
   // CHECK: @llvm.x86.avx10.vcvttss2sis64(<4 x float>
   return _mm_cvtts_roundss_i64(__A, _MM_FROUND_NO_EXC);
 }
 
-long long test_mm_cvttsss_si64(__m128 __A) {
-  // CHECK-LABEL: @test_mm_cvttsss_si64(
+long long test_mm_cvtts_ss_si64(__m128 __A) {
+  // CHECK-LABEL: @test_mm_cvtts_ss_si64(
   // CHECK: @llvm.x86.avx10.vcvttss2sis64(<4 x float>
   return _mm_cvtts_roundss_si64(__A, _MM_FROUND_NO_EXC);
 }
 
-unsigned long long test_mm_cvttsss_u64(__m128 __A) {
-  // CHECK-LABEL: @test_mm_cvttsss_u64(
+unsigned long long test_mm_cvtts_ss_u64(__m128 __A) {
+  // CHECK-LABEL: @test_mm_cvtts_ss_u64(
   // CHECK: @llvm.x86.avx10.vcvttss2usis64(<4 x float>
   return _mm_cvtts_roundss_u64(__A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_cvttspd_epi64(__m512d A) {
-  // CHECK-LABEL: test_mm512_cvttspd_epi64
+__m512i test_mm512_cvtts_pd_epi64(__m512d A) {
+  // CHECK-LABEL: test_mm512_cvtts_pd_epi64
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.512(<8 x double>
-  return _mm512_cvttspd_epi64(A);
+  return _mm512_cvtts_pd_epi64(A);
 }
 
-__m512i test_mm512_mask_cvttspd_epi64(__m512i W, __mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_mask_cvttspd_epi64
+__m512i test_mm512_mask_cvtts_pd_epi64(__m512i W, __mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_pd_epi64
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.512(<8 x double>
-  return _mm512_mask_cvttspd_epi64(W, U, A);
+  return _mm512_mask_cvtts_pd_epi64(W, U, A);
 }
 
-__m512i test_mm512_maskz_cvttspd_epi64(__mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttspd_epi64
+__m512i test_mm512_maskz_cvtts_pd_epi64(__mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_pd_epi64
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.512(<8 x double>
-  return _mm512_maskz_cvttspd_epi64(U, A);
+  return _mm512_maskz_cvtts_pd_epi64(U, A);
 }
 
 __m512i test_mm512_cvtts_roundpd_epi64(__m512d A) {
@@ -75,22 +75,22 @@ __m512i test_mm512_maskz_cvtts_roundpd_epi64(__mmask8 U, __m512d A) {
   return _mm512_maskz_cvtts_roundpd_epi64(U, A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_cvttspd_epu64(__m512d A) {
-  // CHECK-LABEL: test_mm512_cvttspd_epu64
+__m512i test_mm512_cvtts_pd_epu64(__m512d A) {
+  // CHECK-LABEL: test_mm512_cvtts_pd_epu64
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.512(<8 x double>
-  return _mm512_cvttspd_epu64(A);
+  return _mm512_cvtts_pd_epu64(A);
 }
 
-__m512i test_mm512_mask_cvttspd_epu64(__m512i W, __mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_mask_cvttspd_epu64
+__m512i test_mm512_mask_cvtts_pd_epu64(__m512i W, __mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_pd_epu64
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.512(<8 x double>
-  return _mm512_mask_cvttspd_epu64(W, U, A);
+  return _mm512_mask_cvtts_pd_epu64(W, U, A);
 }
 
-__m512i test_mm512_maskz_cvttspd_epu64(__mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttspd_epu64
+__m512i test_mm512_maskz_cvtts_pd_epu64(__mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_pd_epu64
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.512(<8 x double>
-  return _mm512_maskz_cvttspd_epu64(U, A);
+  return _mm512_maskz_cvtts_pd_epu64(U, A);
 }
 
 __m512i test_mm512_cvtts_roundpd_epu64(__m512d A) {
@@ -111,22 +111,22 @@ __m512i test_mm512_maskz_cvtts_roundpd_epu64(__mmask8 U, __m512d A) {
   return _mm512_maskz_cvtts_roundpd_epu64(U, A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_cvttsps_epi64(__m256 A) {
-  // CHECK-LABEL: test_mm512_cvttsps_epi64
+__m512i test_mm512_cvtts_ps_epi64(__m256 A) {
+  // CHECK-LABEL: test_mm512_cvtts_ps_epi64
   // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.512(<8 x float>
-  return _mm512_cvttsps_epi64(A);
+  return _mm512_cvtts_ps_epi64(A);
 }
 
-__m512i test_mm512_mask_cvttsps_epi64(__m512i W, __mmask8 U, __m256 A) {
-  // CHECK-LABEL: test_mm512_mask_cvttsps_epi64
+__m512i test_mm512_mask_cvtts_ps_epi64(__m512i W, __mmask8 U, __m256 A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_ps_epi64
   // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.512(<8 x float>
-  return _mm512_mask_cvttsps_epi64(W, U, A);
+  return _mm512_mask_cvtts_ps_epi64(W, U, A);
 }
 
-__m512i test_mm512_maskz_cvttsps_epi64(__mmask8 U, __m256 A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttsps_epi64
+__m512i test_mm512_maskz_cvtts_ps_epi64(__mmask8 U, __m256 A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_ps_epi64
   // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.512(<8 x float>
-  return _mm512_maskz_cvttsps_epi64(U, A);
+  return _mm512_maskz_cvtts_ps_epi64(U, A);
 }
 
 __m512i test_mm512_cvtts_roundps_epi64(__m256 A) {
@@ -147,22 +147,22 @@ __m512i test_mm512_maskz_cvtts_roundps_epi64(__mmask8 U, __m256 A) {
   return _mm512_maskz_cvtts_roundps_epi64(U, A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_cvttsps_epu64(__m256 A) {
-  // CHECK-LABEL: test_mm512_cvttsps_epu64
+__m512i test_mm512_cvtts_ps_epu64(__m256 A) {
+  // CHECK-LABEL: test_mm512_cvtts_ps_epu64
   // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.512(<8 x float>
-  return _mm512_cvttsps_epu64(A);
+  return _mm512_cvtts_ps_epu64(A);
 }
 
-__m512i test_mm512_mask_cvttsps_epu64(__m512i W, __mmask8 U, __m256 A) {
-  // CHECK-LABEL: test_mm512_mask_cvttsps_epu64
+__m512i test_mm512_mask_cvtts_ps_epu64(__m512i W, __mmask8 U, __m256 A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_ps_epu64
   // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.512(<8 x float>
-  return _mm512_mask_cvttsps_epu64(W, U, A);
+  return _mm512_mask_cvtts_ps_epu64(W, U, A);
 }
 
-__m512i test_mm512_maskz_cvttsps_epu64(__mmask8 U, __m256 A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttsps_epu64
+__m512i test_mm512_maskz_cvtts_ps_epu64(__mmask8 U, __m256 A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_ps_epu64
   // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.512(<8 x float>
-  return _mm512_maskz_cvttsps_epu64(U, A);
+  return _mm512_maskz_cvtts_ps_epu64(U, A);
 }
 
 __m512i test_mm512_cvtts_roundps_epu64(__m256 A) {
diff --git a/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins.c b/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins.c
index cccee04627d2..c1b6df3cb07f 100644
--- a/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2_512satcvtds-builtins.c
@@ -4,22 +4,22 @@
 #include <immintrin.h>
 #include <stddef.h>
 
-__m256i test_mm512_cvttspd_epi32(__m512d A) {
-  // CHECK-LABEL: test_mm512_cvttspd_epi32
+__m256i test_mm512_cvtts_pd_epi32(__m512d A) {
+  // CHECK-LABEL: test_mm512_cvtts_pd_epi32
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.512(<8 x double>
-  return _mm512_cvttspd_epi32(A);
+  return _mm512_cvtts_pd_epi32(A);
 }
 
-__m256i test_mm512_mask_cvttspd_epi32(__m256i W, __mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_mask_cvttspd_epi32
+__m256i test_mm512_mask_cvtts_pd_epi32(__m256i W, __mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_pd_epi32
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.512(<8 x double>
-  return _mm512_mask_cvttspd_epi32(W, U, A);
+  return _mm512_mask_cvtts_pd_epi32(W, U, A);
 }
 
-__m256i test_mm512_maskz_cvttspd_epi32(__mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttspd_epi32
+__m256i test_mm512_maskz_cvtts_pd_epi32(__mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_pd_epi32
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.512(<8 x double>
-  return _mm512_maskz_cvttspd_epi32(U, A);
+  return _mm512_maskz_cvtts_pd_epi32(U, A);
 }
 
 __m256i test_mm512_cvtts_roundpd_epi32(__m512d A) {
@@ -40,22 +40,22 @@ __m256i test_mm512_maskz_cvtts_roundpd_epi32(__mmask8 U, __m512d A) {
   return _mm512_maskz_cvtts_roundpd_epi32(U, A, _MM_FROUND_NO_EXC);
 }
 
-__m256i test_mm512_cvttspd_epu32(__m512d A) {
-  // CHECK-LABEL: test_mm512_cvttspd_epu32
+__m256i test_mm512_cvtts_pd_epu32(__m512d A) {
+  // CHECK-LABEL: test_mm512_cvtts_pd_epu32
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.512(<8 x double>
-  return _mm512_cvttspd_epu32(A);
+  return _mm512_cvtts_pd_epu32(A);
 }
 
-__m256i test_mm512_mask_cvttspd_epu32(__m256i W, __mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_mask_cvttspd_epu32
+__m256i test_mm512_mask_cvtts_pd_epu32(__m256i W, __mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_pd_epu32
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.512(<8 x double>
-  return _mm512_mask_cvttspd_epu32(W, U, A);
+  return _mm512_mask_cvtts_pd_epu32(W, U, A);
 }
 
-__m256i test_mm512_maskz_cvttspd_epu32(__mmask8 U, __m512d A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttspd_epu32
+__m256i test_mm512_maskz_cvtts_pd_epu32(__mmask8 U, __m512d A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_pd_epu32
   // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.512(<8 x double>
-  return _mm512_maskz_cvttspd_epu32(U, A);
+  return _mm512_maskz_cvtts_pd_epu32(U, A);
 }
 
 __m256i test_mm512_cvtts_roundpd_epu32(__m512d A) {
@@ -76,22 +76,22 @@ __m256i test_mm512_maskz_cvtts_roundpd_epu32(__mmask8 U, __m512d A) {
   return _mm512_maskz_cvtts_roundpd_epu32(U, A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_cvttsps_epi32(__m512 A) {
-  // CHECK-LABEL: test_mm512_cvttsps_epi32
+__m512i test_mm512_cvtts_ps_epi32(__m512 A) {
+  // CHECK-LABEL: test_mm512_cvtts_ps_epi32
   // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.512(<16 x float>
-  return _mm512_cvttsps_epi32(A);
+  return _mm512_cvtts_ps_epi32(A);
 }
 
-__m512i test_mm512_mask_cvttsps_epi32(__m512i W, __mmask8 U, __m512 A) {
-  // CHECK-LABEL: test_mm512_mask_cvttsps_epi32
+__m512i test_mm512_mask_cvtts_ps_epi32(__m512i W, __mmask8 U, __m512 A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_ps_epi32
   // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.512(<16 x float>
-  return _mm512_mask_cvttsps_epi32(W, U, A);
+  return _mm512_mask_cvtts_ps_epi32(W, U, A);
 }
 
-__m512i test_mm512_maskz_cvttsps_epi32(__mmask8 U, __m512 A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttsps_epi32
+__m512i test_mm512_maskz_cvtts_ps_epi32(__mmask8 U, __m512 A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_ps_epi32
   // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.512(<16 x float>
-  return _mm512_maskz_cvttsps_epi32(U, A);
+  return _mm512_maskz_cvtts_ps_epi32(U, A);
 }
 
 __m512i test_mm512_cvtts_roundps_epi32(__m512 A) {
@@ -112,22 +112,22 @@ __m512i test_mm512_maskz_cvtts_roundps_epi32(__mmask8 U, __m512 A) {
   return _mm512_maskz_cvtts_roundps_epi32(U, A, _MM_FROUND_NO_EXC);
 }
 
-__m512i test_mm512_cvttsps_epu32(__m512 A) {
-  // CHECK-LABEL: test_mm512_cvttsps_epu32
+__m512i test_mm512_cvtts_ps_epu32(__m512 A) {
+  // CHECK-LABEL: test_mm512_cvtts_ps_epu32
   // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.512(<16 x float>
-  return _mm512_cvttsps_epu32(A);
+  return _mm512_cvtts_ps_epu32(A);
 }
 
-__m512i test_mm512_mask_cvttsps_epu32(__m512i W, __mmask8 U, __m512 A) {
-  // CHECK-LABEL: test_mm512_mask_cvttsps_epu32
+__m512i test_mm512_mask_cvtts_ps_epu32(__m512i W, __mmask8 U, __m512 A) {
+  // CHECK-LABEL: test_mm512_mask_cvtts_ps_epu32
   // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.512(<16 x float>
-  return _mm512_mask_cvttsps_epu32(W, U, A);
+  return _mm512_mask_cvtts_ps_epu32(W, U, A);
 }
 
-__m512i test_mm512_maskz_cvttsps_epu32(__mmask8 U, __m512 A) {
-  // CHECK-LABEL: test_mm512_maskz_cvttsps_epu32
+__m512i test_mm512_maskz_cvtts_ps_epu32(__mmask8 U, __m512 A) {
+  // CHECK-LABEL: test_mm512_maskz_cvtts_ps_epu32
   // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.512(<16 x float>
-  return _mm512_maskz_cvttsps_epu32(U, A);
+  return _mm512_maskz_cvtts_ps_epu32(U, A);
 }
 
 __m512i test_mm512_cvtts_roundps_epu32(__m512 A) {
@@ -148,4 +148,4 @@ __m512i test_mm512_maskz_cvtts_roundps_epu32(__mmask8 U, __m512 A) {
 }
 
 // X64: {{.*}}
-// X86: {{.*}}
\ No newline at end of file
+// X86: {{.*}}
diff --git a/clang/test/CodeGen/X86/avx10_2convert-builtins.c b/clang/test/CodeGen/X86/avx10_2convert-builtins.c
index 87fc6ffd7bc1..31dd0ecc381e 100644
--- a/clang/test/CodeGen/X86/avx10_2convert-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2convert-builtins.c
@@ -41,24 +41,6 @@ __m256h test_mm256_maskz_cvtx2ps_ph(__mmask16 __U, __m256 __A, __m256 __B) {
   return _mm256_maskz_cvtx2ps_ph(__U, __A, __B);
 }
 
-__m256h test_mm256_cvtx_round2ps_ph(__m256 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_cvtx_round2ps_ph(
-  // CHECK: call <16 x half> @llvm.x86.avx10.mask.vcvt2ps2phx.256(
-  return _mm256_cvtx_round2ps_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_cvtx_round2ps_ph(__m256h __W, __mmask8 __U, __m256 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_cvtx_round2ps_ph(
-  // CHECK: call <16 x half> @llvm.x86.avx10.mask.vcvt2ps2phx.256(
-  return _mm256_mask_cvtx_round2ps_ph(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_cvtx_round2ps_ph(__mmask8 __U, __m256 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_cvtx_round2ps_ph(
-  // CHECK: call <16 x half> @llvm.x86.avx10.mask.vcvt2ps2phx.256(
-  return _mm256_maskz_cvtx_round2ps_ph(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
 __m128i test_mm_cvtbiasph_bf8(__m128i __A, __m128h __B) {
   // CHECK-LABEL: @test_mm_cvtbiasph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8128(
@@ -95,40 +77,40 @@ __m128i test_mm256_maskz_cvtbiasph_bf8(__mmask16 __U, __m256i __A, __m256h __B)
   return _mm256_maskz_cvtbiasph_bf8(__U, __A, __B);
 }
 
-__m128i test_mm_cvtbiassph_bf8(__m128i __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_cvtbiassph_bf8(
+__m128i test_mm_cvts_biasph_bf8(__m128i __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_cvts_biasph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s128(
-  return _mm_cvtbiassph_bf8(__A, __B);
+  return _mm_cvts_biasph_bf8(__A, __B);
 }
 
-__m128i test_mm_mask_cvtbiassph_bf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_cvtbiassph_bf8(
+__m128i test_mm_mask_cvts_biasph_bf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_cvts_biasph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s128(
-  return _mm_mask_cvtbiassph_bf8(__W, __U, __A, __B);
+  return _mm_mask_cvts_biasph_bf8(__W, __U, __A, __B);
 }
 
-__m128i test_mm_maskz_cvtbiassph_bf8(__mmask8 __U, __m128i __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_cvtbiassph_bf8(
+__m128i test_mm_maskz_cvts_biasph_bf8(__mmask8 __U, __m128i __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_cvts_biasph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s128(
-  return _mm_maskz_cvtbiassph_bf8(__U, __A, __B);
+  return _mm_maskz_cvts_biasph_bf8(__U, __A, __B);
 }
 
-__m128i test_mm256_cvtbiassph_bf8(__m256i __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_cvtbiassph_bf8(
+__m128i test_mm256_cvts_biasph_bf8(__m256i __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_cvts_biasph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s256(
-  return _mm256_cvtbiassph_bf8(__A, __B);
+  return _mm256_cvts_biasph_bf8(__A, __B);
 }
 
-__m128i test_mm256_mask_cvtbiassph_bf8(__m128i __W, __mmask16 __U, __m256i __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_cvtbiassph_bf8(
+__m128i test_mm256_mask_cvts_biasph_bf8(__m128i __W, __mmask16 __U, __m256i __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_cvts_biasph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s256(
-  return _mm256_mask_cvtbiassph_bf8(__W, __U, __A, __B);
+  return _mm256_mask_cvts_biasph_bf8(__W, __U, __A, __B);
 }
 
-__m128i test_mm256_maskz_cvtbiassph_bf8(__mmask16 __U, __m256i __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_cvtbiassph_bf8(
+__m128i test_mm256_maskz_cvts_biasph_bf8(__mmask16 __U, __m256i __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_cvts_biasph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2bf8s256(
-  return _mm256_maskz_cvtbiassph_bf8(__U, __A, __B);
+  return _mm256_maskz_cvts_biasph_bf8(__U, __A, __B);
 }
 
 __m128i test_mm_cvtbiasph_hf8(__m128i __A, __m128h __B) {
@@ -167,40 +149,40 @@ __m128i test_mm256_maskz_cvtbiasph_hf8(__mmask16 __U, __m256i __A, __m256h __B)
   return _mm256_maskz_cvtbiasph_hf8(__U, __A, __B);
 }
 
-__m128i test_mm_cvtbiassph_hf8(__m128i __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_cvtbiassph_hf8(
+__m128i test_mm_cvts_biasph_hf8(__m128i __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_cvts_biasph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s128(
-  return _mm_cvtbiassph_hf8(__A, __B);
+  return _mm_cvts_biasph_hf8(__A, __B);
 }
 
-__m128i test_mm_mask_cvtbiassph_hf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_cvtbiassph_hf8(
+__m128i test_mm_mask_cvts_biasph_hf8(__m128i __W, __mmask8 __U, __m128i __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_cvts_biasph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s128(
-  return _mm_mask_cvtbiassph_hf8(__W, __U, __A, __B);
+  return _mm_mask_cvts_biasph_hf8(__W, __U, __A, __B);
 }
 
-__m128i test_mm_maskz_cvtbiassph_hf8(__mmask8 __U, __m128i __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_cvtbiassph_hf8(
+__m128i test_mm_maskz_cvts_biasph_hf8(__mmask8 __U, __m128i __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_cvts_biasph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s128(
-  return _mm_maskz_cvtbiassph_hf8(__U, __A, __B);
+  return _mm_maskz_cvts_biasph_hf8(__U, __A, __B);
 }
 
-__m128i test_mm256_cvtbiassph_hf8(__m256i __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_cvtbiassph_hf8(
+__m128i test_mm256_cvts_biasph_hf8(__m256i __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_cvts_biasph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s256(
-  return _mm256_cvtbiassph_hf8(__A, __B);
+  return _mm256_cvts_biasph_hf8(__A, __B);
 }
 
-__m128i test_mm256_mask_cvtbiassph_hf8(__m128i __W, __mmask16 __U, __m256i __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_cvtbiassph_hf8(
+__m128i test_mm256_mask_cvts_biasph_hf8(__m128i __W, __mmask16 __U, __m256i __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_cvts_biasph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s256(
-  return _mm256_mask_cvtbiassph_hf8(__W, __U, __A, __B);
+  return _mm256_mask_cvts_biasph_hf8(__W, __U, __A, __B);
 }
 
-__m128i test_mm256_maskz_cvtbiassph_hf8(__mmask16 __U, __m256i __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_cvtbiassph_hf8(
+__m128i test_mm256_maskz_cvts_biasph_hf8(__mmask16 __U, __m256i __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_cvts_biasph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtbiasph2hf8s256(
-  return _mm256_maskz_cvtbiassph_hf8(__U, __A, __B);
+  return _mm256_maskz_cvts_biasph_hf8(__U, __A, __B);
 }
 
 __m128i test_mm_cvt2ph_bf8(__m128h __A, __m128h __B) {
@@ -247,48 +229,48 @@ __m256i test_mm256_maskz_cvt2ph_bf8(__mmask32 __U, __m256h __A, __m256h __B) {
   return _mm256_maskz_cvt2ph_bf8(__U, __A, __B);
 }
 
-__m128i test_mm_cvts2ph_bf8(__m128h __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_cvts2ph_bf8(
+__m128i test_mm_cvts_2ph_bf8(__m128h __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_cvts_2ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.vcvt2ph2bf8s128(
-  return _mm_cvts2ph_bf8(__A, __B);
+  return _mm_cvts_2ph_bf8(__A, __B);
 }
 
-__m128i test_mm_mask_cvts2ph_bf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_cvts2ph_bf8(
+__m128i test_mm_mask_cvts_2ph_bf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_cvts_2ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.vcvt2ph2bf8s128(
   // CHECK: select <16 x i1> %{{.*}}, <16 x i8> %{{.*}}, <16 x i8> %{{.*}}
   // CHECK: ret <2 x i64> %{{.*}}
-  return _mm_mask_cvts2ph_bf8(__W, __U, __A, __B);
+  return _mm_mask_cvts_2ph_bf8(__W, __U, __A, __B);
 }
 
-__m128i test_mm_maskz_cvts2ph_bf8(__mmask16 __U, __m128h __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_cvts2ph_bf8(
+__m128i test_mm_maskz_cvts_2ph_bf8(__mmask16 __U, __m128h __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_cvts_2ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.vcvt2ph2bf8s128(
   // CHECK: zeroinitializer
   // CHECK: select <16 x i1> %{{.*}}, <16 x i8> %{{.*}}, <16 x i8> %{{.*}}
-  return _mm_maskz_cvts2ph_bf8(__U, __A, __B);
+  return _mm_maskz_cvts_2ph_bf8(__U, __A, __B);
 }
 
-__m256i test_mm256_cvts2ph_bf8(__m256h __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_cvts2ph_bf8(
+__m256i test_mm256_cvts_2ph_bf8(__m256h __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_cvts_2ph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.vcvt2ph2bf8s256(
-  return _mm256_cvts2ph_bf8(__A, __B);
+  return _mm256_cvts_2ph_bf8(__A, __B);
 }
 
-__m256i test_mm256_mask_cvts2ph_bf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_cvts2ph_bf8(
+__m256i test_mm256_mask_cvts_2ph_bf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_cvts_2ph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.vcvt2ph2bf8s256(
   // CHECK: select <32 x i1> %{{.*}}, <32 x i8> %{{.*}}, <32 x i8> %{{.*}}
   // CHECK: ret <4 x i64> %{{.*}}
-  return _mm256_mask_cvts2ph_bf8(__W, __U, __A, __B);
+  return _mm256_mask_cvts_2ph_bf8(__W, __U, __A, __B);
 }
 
-__m256i test_mm256_maskz_cvts2ph_bf8(__mmask32 __U, __m256h __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_cvts2ph_bf8(
+__m256i test_mm256_maskz_cvts_2ph_bf8(__mmask32 __U, __m256h __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_cvts_2ph_bf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.vcvt2ph2bf8s256(
   // CHECK: zeroinitializer
   // CHECK: select <32 x i1> %{{.*}}, <32 x i8> %{{.*}}, <32 x i8> %{{.*}}
-  return _mm256_maskz_cvts2ph_bf8(__U, __A, __B);
+  return _mm256_maskz_cvts_2ph_bf8(__U, __A, __B);
 }
 
 __m128i test_mm_cvt2ph_hf8(__m128h __A, __m128h __B) {
@@ -335,48 +317,48 @@ __m256i test_mm256_maskz_cvt2ph_hf8(__mmask32 __U, __m256h __A, __m256h __B) {
   return _mm256_maskz_cvt2ph_hf8(__U, __A, __B);
 }
 
-__m128i test_mm_cvts2ph_hf8(__m128h __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_cvts2ph_hf8(
+__m128i test_mm_cvts_2ph_hf8(__m128h __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_cvts_2ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.vcvt2ph2hf8s128(
-  return _mm_cvts2ph_hf8(__A, __B);
+  return _mm_cvts_2ph_hf8(__A, __B);
 }
 
-__m128i test_mm_mask_cvts2ph_hf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_cvts2ph_hf8(
+__m128i test_mm_mask_cvts_2ph_hf8(__m128i __W, __mmask16 __U, __m128h __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_cvts_2ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.vcvt2ph2hf8s128(
   // CHECK: select <16 x i1> %{{.*}}, <16 x i8> %{{.*}}, <16 x i8> %{{.*}}
   // CHECK: ret <2 x i64> %{{.*}}
-  return _mm_mask_cvts2ph_hf8(__W, __U, __A, __B);
+  return _mm_mask_cvts_2ph_hf8(__W, __U, __A, __B);
 }
 
-__m128i test_mm_maskz_cvts2ph_hf8(__mmask16 __U, __m128h __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_cvts2ph_hf8(
+__m128i test_mm_maskz_cvts_2ph_hf8(__mmask16 __U, __m128h __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_cvts_2ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.vcvt2ph2hf8s128(
   // CHECK: zeroinitializer
   // CHECK: select <16 x i1> %{{.*}}, <16 x i8> %{{.*}}, <16 x i8> %{{.*}}
-  return _mm_maskz_cvts2ph_hf8(__U, __A, __B);
+  return _mm_maskz_cvts_2ph_hf8(__U, __A, __B);
 }
 
-__m256i test_mm256_cvts2ph_hf8(__m256h __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_cvts2ph_hf8(
+__m256i test_mm256_cvts_2ph_hf8(__m256h __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_cvts_2ph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.vcvt2ph2hf8s256(
-  return _mm256_cvts2ph_hf8(__A, __B);
+  return _mm256_cvts_2ph_hf8(__A, __B);
 }
 
-__m256i test_mm256_mask_cvts2ph_hf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_cvts2ph_hf8(
+__m256i test_mm256_mask_cvts_2ph_hf8(__m256i __W, __mmask32 __U, __m256h __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_cvts_2ph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.vcvt2ph2hf8s256(
   // CHECK: select <32 x i1> %{{.*}}, <32 x i8> %{{.*}}, <32 x i8> %{{.*}}
   // CHECK: ret <4 x i64> %{{.*}}
-  return _mm256_mask_cvts2ph_hf8(__W, __U, __A, __B);
+  return _mm256_mask_cvts_2ph_hf8(__W, __U, __A, __B);
 }
 
-__m256i test_mm256_maskz_cvts2ph_hf8(__mmask32 __U, __m256h __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_cvts2ph_hf8(
+__m256i test_mm256_maskz_cvts_2ph_hf8(__mmask32 __U, __m256h __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_cvts_2ph_hf8(
   // CHECK: call <32 x i8> @llvm.x86.avx10.vcvt2ph2hf8s256(
   // CHECK: zeroinitializer
   // CHECK: select <32 x i1> %{{.*}}, <32 x i8> %{{.*}}, <32 x i8> %{{.*}}
-  return _mm256_maskz_cvts2ph_hf8(__U, __A, __B);
+  return _mm256_maskz_cvts_2ph_hf8(__U, __A, __B);
 }
 
 __m128h test_mm_cvthf8_ph(__m128i __A) {
@@ -451,40 +433,40 @@ __m128i test_mm256_maskz_cvtph_bf8(__mmask16 __A, __m256h __B) {
   return _mm256_maskz_cvtph_bf8(__A, __B);
 }
 
-__m128i test_mm_cvtsph_bf8(__m128h __A) {
-  // CHECK-LABEL: @test_mm_cvtsph_bf8(
+__m128i test_mm_cvts_ph_bf8(__m128h __A) {
+  // CHECK-LABEL: @test_mm_cvts_ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s128(
-  return _mm_cvtsph_bf8(__A);
+  return _mm_cvts_ph_bf8(__A);
 }
 
-__m128i test_mm_mask_cvtsph_bf8(__m128i __A, __mmask8 __B, __m128h __C) {
-  // CHECK-LABEL: @test_mm_mask_cvtsph_bf8(
+__m128i test_mm_mask_cvts_ph_bf8(__m128i __A, __mmask8 __B, __m128h __C) {
+  // CHECK-LABEL: @test_mm_mask_cvts_ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s128(
-  return _mm_mask_cvtsph_bf8(__A, __B, __C);
+  return _mm_mask_cvts_ph_bf8(__A, __B, __C);
 }
 
-__m128i test_mm_maskz_cvtsph_bf8(__mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_cvtsph_bf8(
+__m128i test_mm_maskz_cvts_ph_bf8(__mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_cvts_ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s128(
-  return _mm_maskz_cvtsph_bf8(__A, __B);
+  return _mm_maskz_cvts_ph_bf8(__A, __B);
 }
 
-__m128i test_mm256_cvtsph_bf8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_cvtsph_bf8(
+__m128i test_mm256_cvts_ph_bf8(__m256h __A) {
+  // CHECK-LABEL: @test_mm256_cvts_ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s256(
-  return _mm256_cvtsph_bf8(__A);
+  return _mm256_cvts_ph_bf8(__A);
 }
 
-__m128i test_mm256_mask_cvtsph_bf8(__m128i __A, __mmask16 __B, __m256h __C) {
-  // CHECK-LABEL: @test_mm256_mask_cvtsph_bf8(
+__m128i test_mm256_mask_cvts_ph_bf8(__m128i __A, __mmask16 __B, __m256h __C) {
+  // CHECK-LABEL: @test_mm256_mask_cvts_ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s256(
-  return _mm256_mask_cvtsph_bf8(__A, __B, __C);
+  return _mm256_mask_cvts_ph_bf8(__A, __B, __C);
 }
 
-__m128i test_mm256_maskz_cvtsph_bf8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_cvtsph_bf8(
+__m128i test_mm256_maskz_cvts_ph_bf8(__mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_cvts_ph_bf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2bf8s256(
-  return _mm256_maskz_cvtsph_bf8(__A, __B);
+  return _mm256_maskz_cvts_ph_bf8(__A, __B);
 }
 
 __m128i test_mm_cvtph_hf8(__m128h __A) {
@@ -523,40 +505,40 @@ __m128i test_mm256_maskz_cvtph_hf8(__mmask16 __A, __m256h __B) {
   return _mm256_maskz_cvtph_hf8(__A, __B);
 }
 
-__m128i test_mm_cvtsph_hf8(__m128h __A) {
-  // CHECK-LABEL: @test_mm_cvtsph_hf8(
+__m128i test_mm_cvts_ph_hf8(__m128h __A) {
+  // CHECK-LABEL: @test_mm_cvts_ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s128(
-  return _mm_cvtsph_hf8(__A);
+  return _mm_cvts_ph_hf8(__A);
 }
 
-__m128i test_mm_mask_cvtsph_hf8(__m128i __A, __mmask8 __B, __m128h __C) {
-  // CHECK-LABEL: @test_mm_mask_cvtsph_hf8(
+__m128i test_mm_mask_cvts_ph_hf8(__m128i __A, __mmask8 __B, __m128h __C) {
+  // CHECK-LABEL: @test_mm_mask_cvts_ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s128(
-  return _mm_mask_cvtsph_hf8(__A, __B, __C);
+  return _mm_mask_cvts_ph_hf8(__A, __B, __C);
 }
 
-__m128i test_mm_maskz_cvtsph_hf8(__mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_cvtsph_hf8(
+__m128i test_mm_maskz_cvts_ph_hf8(__mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_cvts_ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s128(
-  return _mm_maskz_cvtsph_hf8(__A, __B);
+  return _mm_maskz_cvts_ph_hf8(__A, __B);
 }
 
-__m128i test_mm256_cvtsph_hf8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_cvtsph_hf8(
+__m128i test_mm256_cvts_ph_hf8(__m256h __A) {
+  // CHECK-LABEL: @test_mm256_cvts_ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s256(
-  return _mm256_cvtsph_hf8(__A);
+  return _mm256_cvts_ph_hf8(__A);
 }
 
-__m128i test_mm256_mask_cvtsph_hf8(__m128i __A, __mmask16 __B, __m256h __C) {
-  // CHECK-LABEL: @test_mm256_mask_cvtsph_hf8(
+__m128i test_mm256_mask_cvts_ph_hf8(__m128i __A, __mmask16 __B, __m256h __C) {
+  // CHECK-LABEL: @test_mm256_mask_cvts_ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s256(
-  return _mm256_mask_cvtsph_hf8(__A, __B, __C);
+  return _mm256_mask_cvts_ph_hf8(__A, __B, __C);
 }
 
-__m128i test_mm256_maskz_cvtsph_hf8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_cvtsph_hf8(
+__m128i test_mm256_maskz_cvts_ph_hf8(__mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_cvts_ph_hf8(
   // CHECK: call <16 x i8> @llvm.x86.avx10.mask.vcvtph2hf8s256(
-  return _mm256_maskz_cvtsph_hf8(__A, __B);
+  return _mm256_maskz_cvts_ph_hf8(__A, __B);
 }
 
 __m256h test_mm256_cvtbf8_ph(__m128i A) {
diff --git a/clang/test/CodeGen/X86/avx10_2minmax-builtins.c b/clang/test/CodeGen/X86/avx10_2minmax-builtins.c
index 7e21858c7183..f8238f40c15b 100644
--- a/clang/test/CodeGen/X86/avx10_2minmax-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2minmax-builtins.c
@@ -83,24 +83,6 @@ __m256d test_mm256_maskz_minmax_pd(__mmask8 __A, __m256d __B, __m256d __C) {
   return _mm256_maskz_minmax_pd(__A, __B, __C, 127);
 }
 
-__m256d test_mm256_minmax_round_pd(__m256d __A, __m256d __B) {
-  // CHECK-LABEL: @test_mm256_minmax_round_pd(
-  // CHECK: call <4 x double> @llvm.x86.avx10.mask.vminmaxpd256.round(
-  return _mm256_minmax_round_pd(__A, __B, 127, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_minmax_round_pd(__m256d __A, __mmask8 __B, __m256d __C, __m256d __D) {
-  // CHECK-LABEL: @test_mm256_mask_minmax_round_pd(
-  // CHECK: call <4 x double> @llvm.x86.avx10.mask.vminmaxpd256.round(
-  return _mm256_mask_minmax_round_pd(__A, __B, __C, __D, 127, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_minmax_round_pd(__mmask8 __A, __m256d __B, __m256d __C) {
-  // CHECK-LABEL: @test_mm256_maskz_minmax_round_pd(
-  // CHECK: call <4 x double> @llvm.x86.avx10.mask.vminmaxpd256.round(
-  return _mm256_maskz_minmax_round_pd(__A, __B, __C, 127, _MM_FROUND_NO_EXC);
-}
-
 __m128h test_mm_minmax_ph(__m128h __A, __m128h __B) {
   // CHECK-LABEL: @test_mm_minmax_ph(
   // CHECK: call <8 x half> @llvm.x86.avx10.mask.vminmaxph128(
@@ -137,24 +119,6 @@ __m256h test_mm256_maskz_minmax_ph(__mmask16 __A, __m256h __B, __m256h __C) {
   return _mm256_maskz_minmax_ph(__A, __B, __C, 127);
 }
 
-__m256h test_mm256_minmax_round_ph(__m256h __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_minmax_round_ph(
-  // CHECK: call <16 x half> @llvm.x86.avx10.mask.vminmaxph256.round(
-  return _mm256_minmax_round_ph(__A, __B, 127, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_minmax_round_ph(__m256h __A, __mmask16 __B, __m256h __C, __m256h __D) {
-  // CHECK-LABEL: @test_mm256_mask_minmax_round_ph(
-  // CHECK: call <16 x half> @llvm.x86.avx10.mask.vminmaxph256.round(
-  return _mm256_mask_minmax_round_ph(__A, __B, __C, __D, 127, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_minmax_round_ph(__mmask16 __A, __m256h __B, __m256h __C) {
-  // CHECK-LABEL: @test_mm256_maskz_minmax_round_ph(
-  // CHECK: call <16 x half> @llvm.x86.avx10.mask.vminmaxph256.round(
-  return _mm256_maskz_minmax_round_ph(__A, __B, __C, 127, _MM_FROUND_NO_EXC);
-}
-
 __m128 test_mm_minmax_ps(__m128 __A, __m128 __B) {
   // CHECK-LABEL: @test_mm_minmax_ps(
   // CHECK: call <4 x float> @llvm.x86.avx10.mask.vminmaxps128(
@@ -191,24 +155,6 @@ __m256 test_mm256_maskz_minmax_ps(__mmask8 __A, __m256 __B, __m256 __C) {
   return _mm256_maskz_minmax_ps(__A, __B, __C, 127);
 }
 
-__m256 test_mm256_minmax_round_ps(__m256 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_minmax_round_ps(
-  // CHECK: call <8 x float> @llvm.x86.avx10.mask.vminmaxps256.round(
-  return _mm256_minmax_round_ps(__A, __B, 127, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_minmax_round_ps(__m256 __A, __mmask8 __B, __m256 __C, __m256 __D) {
-  // CHECK-LABEL: @test_mm256_mask_minmax_round_ps(
-  // CHECK: call <8 x float> @llvm.x86.avx10.mask.vminmaxps256.round(
-  return _mm256_mask_minmax_round_ps(__A, __B, __C, __D, 127, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_minmax_round_ps(__mmask8 __A, __m256 __B, __m256 __C) {
-  // CHECK-LABEL: @test_mm256_maskz_minmax_round_ps(
-  // CHECK: call <8 x float> @llvm.x86.avx10.mask.vminmaxps256.round(
-  return _mm256_maskz_minmax_round_ps(__A, __B, __C, 127, _MM_FROUND_NO_EXC);
-}
-
 __m128d test_mm_minmax_sd(__m128d __A, __m128d __B) {
   // CHECK-LABEL: @test_mm_minmax_sd(
   // CHECK: call <2 x double> @llvm.x86.avx10.mask.vminmaxsd.round(
diff --git a/clang/test/CodeGen/X86/avx10_2ni-builtins.c b/clang/test/CodeGen/X86/avx10_2ni-builtins.c
index d06a008c09e7..936be27da61d 100644
--- a/clang/test/CodeGen/X86/avx10_2ni-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2ni-builtins.c
@@ -424,2408 +424,3 @@ __m256i test_mm256_maskz_dpwuuds_epi32(__m256i __A, __mmask8 __B, __m256i __C, _
 // CHECK: select <8 x i1> %{{.*}}, <8 x i32> %{{.*}}, <8 x i32> %{{.*}}
   return _mm256_maskz_dpwuuds_epi32(__A, __B, __C, __D);
 }
-
-// YMM Rounding
-__m256d test_mm256_add_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_add_round_pd
-// CHECK: @llvm.x86.avx10.vaddpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 11)
-  return _mm256_add_round_pd(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_add_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_add_round_pd
-// CHECK: @llvm.x86.avx10.vaddpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 10)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_add_round_pd(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_add_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_add_round_pd
-// CHECK: @llvm.x86.avx10.vaddpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 9)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_maskz_add_round_pd(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_add_round_ph(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_add_round_ph
-// CHECK: @llvm.x86.avx10.vaddph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 11)
-  return _mm256_add_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_add_round_ph(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_add_round_ph
-// CHECK: @llvm.x86.avx10.vaddph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 10)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_add_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_add_round_ph(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_add_round_ph
-// CHECK: @llvm.x86.avx10.vaddph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 9)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_maskz_add_round_ph(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_add_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_add_round_ps
-// CHECK: @llvm.x86.avx10.vaddps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 11)
-  return _mm256_add_round_ps(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_add_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_add_round_ps
-// CHECK: @llvm.x86.avx10.vaddps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 10)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_add_round_ps(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_add_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_add_round_ps
-// CHECK: @llvm.x86.avx10.vaddps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 9)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_add_round_ps(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__mmask8 test_mm256_cmp_round_pd_mask(__m256d a, __m256d b) {
-// CHECK-LABEL: @test_mm256_cmp_round_pd_mask
-// CHECK: fcmp oeq <4 x double> %{{.*}}, %{{.*}}
-  return _mm256_cmp_round_pd_mask(a, b, _CMP_EQ_OQ, _MM_FROUND_NO_EXC);
-}
-
-__mmask8 test_mm256_mask_cmp_round_pd_mask(__mmask8 m, __m256d a, __m256d b) {
-// CHECK-LABEL: @test_mm256_mask_cmp_round_pd_mask
-// CHECK: [[CMP:%.*]] = fcmp oeq <4 x double> %{{.*}}, %{{.*}}
-// CHECK: and <4 x i1> [[CMP]], {{.*}}
-  return _mm256_mask_cmp_round_pd_mask(m, a, b, _CMP_EQ_OQ, _MM_FROUND_NO_EXC);
-}
-
-__mmask16 test_mm256_cmp_round_ph_mask(__m256h a, __m256h b) {
-// CHECK-LABEL: @test_mm256_cmp_round_ph_mask
-// CHECK: fcmp oeq <16 x half> %{{.*}}, %{{.*}}
-  return _mm256_cmp_round_ph_mask(a, b, _CMP_EQ_OQ, _MM_FROUND_NO_EXC);
-}
-
-__mmask16 test_mm256_mask_cmp_round_ph_mask(__mmask16 m, __m256h a, __m256h b) {
-// CHECK-LABEL: @test_mm256_mask_cmp_round_ph_mask
-// CHECK: [[CMP:%.*]] = fcmp oeq <16 x half> %{{.*}}, %{{.*}}
-// CHECK: and <16 x i1> [[CMP]], {{.*}}
-  return _mm256_mask_cmp_round_ph_mask(m, a, b, _CMP_EQ_OQ, _MM_FROUND_NO_EXC);
-}
-
-__mmask8 test_mm256_cmp_round_ps_mask(__m256 a, __m256 b) {
-// CHECK-LABEL: @test_mm256_cmp_round_ps_mask
-// CHECK: fcmp oeq <8 x float> %{{.*}}, %{{.*}}
-  return _mm256_cmp_round_ps_mask(a, b, _CMP_EQ_OQ, _MM_FROUND_NO_EXC);
-}
-
-__mmask8 test_mm256_mask_cmp_round_ps_mask(__mmask8 m, __m256 a, __m256 b) {
-// CHECK-LABEL: @test_mm256_mask_cmp_round_ps_mask
-// CHECK: [[CMP:%.*]] = fcmp oeq <8 x float> %{{.*}}, %{{.*}}
-// CHECK: and <8 x i1> [[CMP]], {{.*}}
-  return _mm256_mask_cmp_round_ps_mask(m, a, b, _CMP_EQ_OQ, _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_cvt_roundepi32_ph(__m256i A) {
-// CHECK-LABEL: test_mm256_cvt_roundepi32_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f16.v8i32(<8 x i32> %{{.*}}, i32 11)
-  return _mm256_cvt_roundepi32_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_mask_cvt_roundepi32_ph(__m128h A, __mmask8 B, __m256i C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundepi32_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f16.v8i32(<8 x i32> %{{.*}}, i32 10)
-// CHECK: select <8 x i1> %{{.*}}, <8 x half> %{{.*}}, <8 x half> %{{.*}}
-  return _mm256_mask_cvt_roundepi32_ph(A, B, C, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_maskz_cvt_roundepi32_ph(__mmask8 A, __m256i B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundepi32_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f16.v8i32(<8 x i32> %{{.*}}, i32 9)
-// CHECK: select <8 x i1> %{{.*}}, <8 x half> %{{.*}}, <8 x half> %{{.*}}
-  return _mm256_maskz_cvt_roundepi32_ph(A, B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_cvt_roundepi32_ps(__m256i __A)
-{
-// CHECK-LABEL: @test_mm256_cvt_roundepi32_ps
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f32.v8i32
-  return _mm256_cvt_roundepi32_ps(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_cvt_roundepi32_ps(__m256 __W, __mmask8 __U, __m256i __A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvt_roundepi32_ps
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f32.v8i32
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_cvt_roundepi32_ps(__W, __U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_cvt_roundepi32_ps(__mmask8 __U, __m256i __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundepi32_ps
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f32.v8i32
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_cvt_roundepi32_ps(__U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_cvt_roundpd_epi32(__m256d A)
-{
-// CHECK-LABEL: @test_mm256_cvt_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2dq256
-  return _mm256_cvt_roundpd_epi32(A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_mask_cvt_roundpd_epi32(__m128i W,__mmask8 U,__m256d A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvt_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2dq256
-  return _mm256_mask_cvt_roundpd_epi32(W, U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_maskz_cvt_roundpd_epi32(__mmask8 U, __m256d A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2dq256
-  return _mm256_maskz_cvt_roundpd_epi32(U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_cvt_roundpd_ph(__m256d A) {
-// CHECK-LABEL: test_mm256_cvt_roundpd_ph
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2ph256
-  return _mm256_cvt_roundpd_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_mask_cvt_roundpd_ph(__m128h A, __mmask8 B, __m256d C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundpd_ph
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2ph256
-  return _mm256_mask_cvt_roundpd_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_maskz_cvt_roundpd_ph(__mmask8 A, __m256d B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundpd_ph
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2ph256
-  return _mm256_maskz_cvt_roundpd_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_cvt_roundpd_ps(__m256d A)
-{
-// CHECK-LABEL: @test_mm256_cvt_roundpd_ps
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2ps256
-  return _mm256_cvt_roundpd_ps(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_mask_cvt_roundpd_ps(__m128 W, __mmask8 U,__m256d A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvt_roundpd_ps
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2ps256
-  return _mm256_mask_cvt_roundpd_ps(W, U, A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_maskz_cvt_roundpd_ps(__mmask8 U, __m256d A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundpd_ps
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2ps256
-  return _mm256_maskz_cvt_roundpd_ps(U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundpd_epi64(__m256d __A) {
-// CHECK-LABEL: @test_mm256_cvt_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2qq256
-  return _mm256_cvt_roundpd_epi64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundpd_epi64(__m256i __W, __mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_mask_cvt_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2qq256
-  return _mm256_mask_cvt_roundpd_epi64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundpd_epi64(__mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2qq256
-  return _mm256_maskz_cvt_roundpd_epi64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_cvt_roundpd_epu32(__m256d A)
-{
-// CHECK-LABEL: @test_mm256_cvt_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2udq256
-  return _mm256_cvt_roundpd_epu32(A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_mask_cvt_roundpd_epu32(__m128i W,__mmask8 U,__m256d A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvt_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2udq256
-  return _mm256_mask_cvt_roundpd_epu32(W, U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_maskz_cvt_roundpd_epu32(__mmask8 U, __m256d A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2udq256
-  return _mm256_maskz_cvt_roundpd_epu32(U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundpd_epu64(__m256d __A) {
-// CHECK-LABEL: @test_mm256_cvt_roundpd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2uqq256
-  return _mm256_cvt_roundpd_epu64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundpd_epu64(__m256i __W, __mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_mask_cvt_roundpd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2uqq256
-  return _mm256_mask_cvt_roundpd_epu64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundpd_epu64(__mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundpd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtpd2uqq256
-  return _mm256_maskz_cvt_roundpd_epu64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundph_epi32(__m128h A) {
-// CHECK-LABEL: test_mm256_cvt_roundph_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtph2dq256
-  return _mm256_cvt_roundph_epi32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundph_epi32(__m256i A, __mmask16 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundph_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtph2dq256
-  return _mm256_mask_cvt_roundph_epi32(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundph_epi32(__mmask16 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundph_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtph2dq256
-  return _mm256_maskz_cvt_roundph_epi32(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_cvt_roundph_pd(__m128h A) {
-// CHECK-LABEL: test_mm256_cvt_roundph_pd
-// CHECK: @llvm.x86.avx10.mask.vcvtph2pd256
-  return _mm256_cvt_roundph_pd(A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_cvt_roundph_pd(__m256d A, __mmask8 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundph_pd
-// CHECK: @llvm.x86.avx10.mask.vcvtph2pd256
-  return _mm256_mask_cvt_roundph_pd(A, B, C, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_cvt_roundph_pd(__mmask8 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundph_pd
-// CHECK: @llvm.x86.avx10.mask.vcvtph2pd256
-  return _mm256_maskz_cvt_roundph_pd(A, B, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_cvtx_roundph_ps(__m128h A) {
-// CHECK-LABEL: test_mm256_cvtx_roundph_ps
-// CHECK: @llvm.x86.avx10.mask.vcvtph2psx256
-  return _mm256_cvtx_roundph_ps(A, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_cvtx_roundph_ps(__m256 A, __mmask16 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvtx_roundph_ps
-// CHECK: @llvm.x86.avx10.mask.vcvtph2psx256
-  return _mm256_mask_cvtx_roundph_ps(A, B, C, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_cvtx_roundph_ps(__mmask16 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvtx_roundph_ps
-// CHECK: @llvm.x86.avx10.mask.vcvtph2psx256
-  return _mm256_maskz_cvtx_roundph_ps(A, B, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundph_epi64(__m128h A) {
-// CHECK-LABEL: test_mm256_cvt_roundph_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtph2qq256
-  return _mm256_cvt_roundph_epi64(A, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundph_epi64(__m256i A, __mmask8 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundph_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtph2qq256
-  return _mm256_mask_cvt_roundph_epi64(A, B, C, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundph_epi64(__mmask8 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundph_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtph2qq256
-  return _mm256_maskz_cvt_roundph_epi64(A, B, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundph_epu32(__m128h A) {
-// CHECK-LABEL: test_mm256_cvt_roundph_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtph2udq256
-  return _mm256_cvt_roundph_epu32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundph_epu32(__m256i A, __mmask16 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundph_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtph2udq256
-  return _mm256_mask_cvt_roundph_epu32(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundph_epu32(__mmask16 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundph_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtph2udq256
-  return _mm256_maskz_cvt_roundph_epu32(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundph_epu64(__m128h A) {
-// CHECK-LABEL: test_mm256_cvt_roundph_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtph2uqq256
-  return _mm256_cvt_roundph_epu64(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundph_epu64(__m256i A, __mmask8 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundph_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtph2uqq256
-  return _mm256_mask_cvt_roundph_epu64(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundph_epu64(__mmask8 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundph_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtph2uqq256
-  return _mm256_maskz_cvt_roundph_epu64(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundph_epu16(__m256h A) {
-// CHECK-LABEL: test_mm256_cvt_roundph_epu16
-// CHECK: @llvm.x86.avx10.mask.vcvtph2uw256
-  return _mm256_cvt_roundph_epu16(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundph_epu16(__m256i A, __mmask32 B, __m256h C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundph_epu16
-// CHECK: @llvm.x86.avx10.mask.vcvtph2uw256
-  return _mm256_mask_cvt_roundph_epu16(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundph_epu16(__mmask32 A, __m256h B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundph_epu16
-// CHECK: @llvm.x86.avx10.mask.vcvtph2uw256
-  return _mm256_maskz_cvt_roundph_epu16(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundph_epi16(__m256h A) {
-// CHECK-LABEL: test_mm256_cvt_roundph_epi16
-// CHECK: @llvm.x86.avx10.mask.vcvtph2w256
-  return _mm256_cvt_roundph_epi16(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundph_epi16(__m256i A, __mmask32 B, __m256h C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundph_epi16
-// CHECK: @llvm.x86.avx10.mask.vcvtph2w256
-  return _mm256_mask_cvt_roundph_epi16(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundph_epi16(__mmask32 A, __m256h B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundph_epi16
-// CHECK: @llvm.x86.avx10.mask.vcvtph2w256
-  return _mm256_maskz_cvt_roundph_epi16(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundps_epi32(__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_cvt_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtps2dq256
-  return _mm256_cvt_roundps_epi32(__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundps_epi32(__m256i __W,__mmask16 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvt_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtps2dq256
-  return _mm256_mask_cvt_roundps_epi32(__W,__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundps_epi32(__mmask16 __U, __m256 __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvtps2dq256
-  return _mm256_maskz_cvt_roundps_epi32(__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_cvt_roundps_pd(__m128 __A) {
-// CHECK-LABEL: @test_mm256_cvt_roundps_pd
-// CHECK: @llvm.x86.avx10.mask.vcvtps2pd256
-  return _mm256_cvt_roundps_pd(__A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_cvt_roundps_pd(__m256d __W, __mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_mask_cvt_roundps_pd
-// CHECK: @llvm.x86.avx10.mask.vcvtps2pd256
-  return _mm256_mask_cvt_roundps_pd(__W, __U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_cvt_roundps_pd(__mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundps_pd
-// CHECK: @llvm.x86.avx10.mask.vcvtps2pd256
-  return _mm256_maskz_cvt_roundps_pd(__U, __A, _MM_FROUND_NO_EXC);
-}
-
-// FIXME: We may change to @llvm.x86.avx10.mask.vcvtps2ph256 in future.
-__m128i test_mm256_cvt_roundps_ph(__m256  __A)
-{
-  // CHECK-LABEL: @test_mm256_cvt_roundps_ph
-  // CHECK: @llvm.x86.avx512.mask.vcvtps2ph.256
-    return _mm256_cvt_roundps_ph(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_mask_cvt_roundps_ph(__m128i __W , __mmask16 __U, __m256  __A)
-{
-  // CHECK-LABEL: @test_mm256_mask_cvt_roundps_ph
-  // CHECK: @llvm.x86.avx512.mask.vcvtps2ph.256
-    return _mm256_mask_cvt_roundps_ph(__W, __U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_maskz_cvt_roundps_ph(__mmask16 __U, __m256  __A)
-{
-  // CHECK-LABEL: @test_mm256_maskz_cvt_roundps_ph
-  // CHECK: @llvm.x86.avx512.mask.vcvtps2ph.256
-    return _mm256_maskz_cvt_roundps_ph(__U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_cvtx_roundps_ph(__m256 A) {
-// CHECK-LABEL: test_mm256_cvtx_roundps_ph
-// CHECK: @llvm.x86.avx10.mask.vcvtps2phx256
-  return _mm256_cvtx_roundps_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_mask_cvtx_roundps_ph(__m128h A, __mmask16 B, __m256 C) {
-// CHECK-LABEL: test_mm256_mask_cvtx_roundps_ph
-// CHECK: @llvm.x86.avx10.mask.vcvtps2phx256
-  return _mm256_mask_cvtx_roundps_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_maskz_cvtx_roundps_ph(__mmask16 A, __m256 B) {
-// CHECK-LABEL: test_mm256_maskz_cvtx_roundps_ph
-// CHECK: @llvm.x86.avx10.mask.vcvtps2phx256
-  return _mm256_maskz_cvtx_roundps_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundps_epi64(__m128 __A) {
-// CHECK-LABEL: @test_mm256_cvt_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtps2qq256
-  return _mm256_cvt_roundps_epi64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundps_epi64(__m256i __W, __mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_mask_cvt_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtps2qq256
-  return _mm256_mask_cvt_roundps_epi64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundps_epi64(__mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvtps2qq256
-  return _mm256_maskz_cvt_roundps_epi64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundps_epu32(__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_cvt_roundps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtps2udq256
-  return _mm256_cvt_roundps_epu32(__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundps_epu32(__m256i __W,__mmask16 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvt_roundps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtps2udq256
-  return _mm256_mask_cvt_roundps_epu32(__W,__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundps_epu32(__mmask16 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvtps2udq256
-  return _mm256_maskz_cvt_roundps_epu32(__U,__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvt_roundps_epu64(__m128 __A) {
-// CHECK-LABEL: @test_mm256_cvt_roundps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtps2uqq256
-  return _mm256_cvt_roundps_epu64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvt_roundps_epu64(__m256i __W, __mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_mask_cvt_roundps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtps2uqq256
-  return _mm256_mask_cvt_roundps_epu64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvt_roundps_epu64(__mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvtps2uqq256
-  return _mm256_maskz_cvt_roundps_epu64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256d test__mm256_cvt_roundepi64_pd(__m256i __A) {
-// CHECK-LABEL: @test__mm256_cvt_roundepi64_pd
-// CHECK: @llvm.x86.avx512.sitofp.round.v4f64.v4i64
-  return _mm256_cvt_roundepi64_pd(__A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test__mm256_mask_cvt_roundepi64_pd(__m256d __W, __mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test__mm256_mask_cvt_roundepi64_pd
-// CHECK: @llvm.x86.avx512.sitofp.round.v4f64.v4i64
-  return _mm256_mask_cvt_roundepi64_pd(__W, __U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test__mm256_maskz_cvt_roundepi64_pd(__mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test__mm256_maskz_cvt_roundepi64_pd
-// CHECK: @llvm.x86.avx512.sitofp.round.v4f64.v4i64
-  return _mm256_maskz_cvt_roundepi64_pd(__U, __A, _MM_FROUND_NO_EXC);
-}
-
-// FIXME: We may change to @llvm.x86.avx10.mask.vcvtqq2ph256 in future.
-__m128h test_mm256_cvt_roundepi64_ph(__m256i A) {
-// CHECK-LABEL: test_mm256_cvt_roundepi64_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f16.v4i64
-  return _mm256_cvt_roundepi64_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_mask_cvt_roundepi64_ph(__m128h A, __mmask8 B, __m256i C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundepi64_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f16.v4i64
-  return _mm256_mask_cvt_roundepi64_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_maskz_cvt_roundepi64_ph(__mmask8 A, __m256i B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundepi64_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v8f16.v4i64
-  return _mm256_maskz_cvt_roundepi64_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_cvt_roundepi64_ps(__m256i __A) {
-// CHECK-LABEL: @test_mm256_cvt_roundepi64_ps
-// CHECK: @llvm.x86.avx512.sitofp.round.v4f32.v4i64
-  return _mm256_cvt_roundepi64_ps(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_mask_cvt_roundepi64_ps(__m128 __W, __mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test_mm256_mask_cvt_roundepi64_ps
-// CHECK: @llvm.x86.avx512.sitofp.round.v4f32.v4i64
-// CHECK: select <4 x i1> %{{.*}}, <4 x float> %{{.*}}, <4 x float> %{{.*}}
-  return _mm256_mask_cvt_roundepi64_ps(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_maskz_cvt_roundepi64_ps(__mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundepi64_ps
-// CHECK: @llvm.x86.avx512.sitofp.round.v4f32.v4i64
-// CHECK: select <4 x i1> %{{.*}}, <4 x float> %{{.*}}, <4 x float> %{{.*}}
-  return _mm256_maskz_cvt_roundepi64_ps(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_cvtt_roundpd_epi32(__m256d A)
-{
-// CHECK-LABEL: @test_mm256_cvtt_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2dq256
-  return _mm256_cvtt_roundpd_epi32(A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_mask_cvtt_roundpd_epi32(__m128i W,__mmask8 U,__m256d A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2dq256
-  return _mm256_mask_cvtt_roundpd_epi32(W, U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_maskz_cvtt_roundpd_epi32(__mmask8 U, __m256d A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2dq256
-  return _mm256_maskz_cvtt_roundpd_epi32(U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundpd_epi64(__m256d __A) {
-// CHECK-LABEL: @test_mm256_cvtt_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2qq256
-  return _mm256_cvtt_roundpd_epi64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundpd_epi64(__m256i __W, __mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2qq256
-  return _mm256_mask_cvtt_roundpd_epi64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundpd_epi64(__mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2qq256
-  return _mm256_maskz_cvtt_roundpd_epi64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_cvtt_roundpd_epu32(__m256d A)
-{
-// CHECK-LABEL: @test_mm256_cvtt_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2udq256
-  return _mm256_cvtt_roundpd_epu32(A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_mask_cvtt_roundpd_epu32(__m128i W,__mmask8 U,__m256d A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2udq256
-  return _mm256_mask_cvtt_roundpd_epu32(W, U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm256_maskz_cvtt_roundpd_epu32(__mmask8 U, __m256d A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2udq256
-  return _mm256_maskz_cvtt_roundpd_epu32(U, A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundpd_epu64(__m256d __A) {
-// CHECK-LABEL: @test_mm256_cvtt_roundpd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2uqq256
-  return _mm256_cvtt_roundpd_epu64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundpd_epu64(__m256i __W, __mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundpd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2uqq256
-  return _mm256_mask_cvtt_roundpd_epu64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundpd_epu64(__mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundpd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2uqq256
-  return _mm256_maskz_cvtt_roundpd_epu64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundph_epi32(__m128h A) {
-// CHECK-LABEL: test_mm256_cvtt_roundph_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttph2dq256
-  return _mm256_cvtt_roundph_epi32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundph_epi32(__m256i A, __mmask16 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvtt_roundph_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttph2dq256
-  return _mm256_mask_cvtt_roundph_epi32(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundph_epi32(__mmask16 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvtt_roundph_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttph2dq256
-  return _mm256_maskz_cvtt_roundph_epi32(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundph_epi64(__m128h A) {
-// CHECK-LABEL: test_mm256_cvtt_roundph_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttph2qq256
-  return _mm256_cvtt_roundph_epi64(A, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundph_epi64(__m256i A, __mmask8 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvtt_roundph_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttph2qq256
-  return _mm256_mask_cvtt_roundph_epi64(A, B, C, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundph_epi64(__mmask8 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvtt_roundph_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttph2qq256
-  return _mm256_maskz_cvtt_roundph_epi64(A, B, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundph_epu32(__m128h A) {
-// CHECK-LABEL: test_mm256_cvtt_roundph_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttph2udq256
-  return _mm256_cvtt_roundph_epu32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundph_epu32(__m256i A, __mmask16 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvtt_roundph_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttph2udq256
-  return _mm256_mask_cvtt_roundph_epu32(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundph_epu32(__mmask16 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvtt_roundph_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttph2udq256
-  return _mm256_maskz_cvtt_roundph_epu32(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundph_epu64(__m128h A) {
-// CHECK-LABEL: test_mm256_cvtt_roundph_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttph2uqq256
-  return _mm256_cvtt_roundph_epu64(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundph_epu64(__m256i A, __mmask8 B, __m128h C) {
-// CHECK-LABEL: test_mm256_mask_cvtt_roundph_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttph2uqq256
-  return _mm256_mask_cvtt_roundph_epu64(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundph_epu64(__mmask8 A, __m128h B) {
-// CHECK-LABEL: test_mm256_maskz_cvtt_roundph_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttph2uqq256
-  return _mm256_maskz_cvtt_roundph_epu64(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundph_epu16(__m256h A) {
-// CHECK-LABEL: test_mm256_cvtt_roundph_epu16
-// CHECK: @llvm.x86.avx10.mask.vcvttph2uw256
-  return _mm256_cvtt_roundph_epu16(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundph_epu16(__m256i A, __mmask32 B, __m256h C) {
-// CHECK-LABEL: test_mm256_mask_cvtt_roundph_epu16
-// CHECK: @llvm.x86.avx10.mask.vcvttph2uw256
-  return _mm256_mask_cvtt_roundph_epu16(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundph_epu16(__mmask32 A, __m256h B) {
-// CHECK-LABEL: test_mm256_maskz_cvtt_roundph_epu16
-// CHECK: @llvm.x86.avx10.mask.vcvttph2uw256
-  return _mm256_maskz_cvtt_roundph_epu16(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundph_epi16(__m256h A) {
-// CHECK-LABEL: test_mm256_cvtt_roundph_epi16
-// CHECK: @llvm.x86.avx10.mask.vcvttph2w256
-  return _mm256_cvtt_roundph_epi16(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundph_epi16(__m256i A, __mmask32 B, __m256h C) {
-// CHECK-LABEL: test_mm256_mask_cvtt_roundph_epi16
-// CHECK: @llvm.x86.avx10.mask.vcvttph2w256
-  return _mm256_mask_cvtt_roundph_epi16(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundph_epi16(__mmask32 A, __m256h B) {
-// CHECK-LABEL: test_mm256_maskz_cvtt_roundph_epi16
-// CHECK: @llvm.x86.avx10.mask.vcvttph2w256
-  return _mm256_maskz_cvtt_roundph_epi16(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundps_epi32(__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_cvtt_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2dq256
-  return _mm256_cvtt_roundps_epi32(__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundps_epi32(__m256i __W,__mmask16 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2dq256
-  return _mm256_mask_cvtt_roundps_epi32(__W,__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundps_epi32(__mmask16 __U, __m256 __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2dq256
-  return _mm256_maskz_cvtt_roundps_epi32(__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundps_epi64(__m128 __A) {
-// CHECK-LABEL: @test_mm256_cvtt_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2qq256
-  return _mm256_cvtt_roundps_epi64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundps_epi64(__m256i __W, __mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2qq256
-  return _mm256_mask_cvtt_roundps_epi64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundps_epi64(__mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2qq256
-  return _mm256_maskz_cvtt_roundps_epi64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundps_epu32(__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_cvtt_roundps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2udq256
-  return _mm256_cvtt_roundps_epu32(__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundps_epu32(__m256i __W,__mmask16 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2udq256
-  return _mm256_mask_cvtt_roundps_epu32(__W,__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundps_epu32(__mmask16 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2udq256
-  return _mm256_maskz_cvtt_roundps_epu32(__U,__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_cvtt_roundps_epu64(__m128 __A) {
-// CHECK-LABEL: @test_mm256_cvtt_roundps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2uqq256
-  return _mm256_cvtt_roundps_epu64(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_cvtt_roundps_epu64(__m256i __W, __mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_mask_cvtt_roundps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2uqq256
-  return _mm256_mask_cvtt_roundps_epu64(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_cvtt_roundps_epu64(__mmask8 __U, __m128 __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvtt_roundps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2uqq256
-  return _mm256_maskz_cvtt_roundps_epu64(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_cvt_roundepu32_ph(__m256i A) {
-// CHECK-LABEL: test_mm256_cvt_roundepu32_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f16.v8i32(<8 x i32> %{{.*}}, i32 11)
-  return _mm256_cvt_roundepu32_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_mask_cvt_roundepu32_ph(__m128h A, __mmask8 B, __m256i C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundepu32_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f16.v8i32(<8 x i32> %{{.*}}, i32 10)
-// CHECK: select <8 x i1> %{{.*}}, <8 x half> %{{.*}}, <8 x half> %{{.*}}
-  return _mm256_mask_cvt_roundepu32_ph(A, B, C, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_maskz_cvt_roundepu32_ph(__mmask8 A, __m256i B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundepu32_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f16.v8i32(<8 x i32> %{{.*}}, i32 9)
-// CHECK: select <8 x i1> %{{.*}}, <8 x half> %{{.*}}, <8 x half> %{{.*}}
-  return _mm256_maskz_cvt_roundepu32_ph(A, B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_cvt_roundepu32_ps(__m256i __A)
-{
-// CHECK-LABEL: @test_mm256_cvt_roundepu32_ps
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f32.v8i32
-  return _mm256_cvt_roundepu32_ps(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_cvt_roundepu32_ps(__m256 __W, __mmask8 __U, __m256i __A)
-{
-// CHECK-LABEL: @test_mm256_mask_cvt_roundepu32_ps
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f32.v8i32
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_cvt_roundepu32_ps(__W, __U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_cvt_roundepu32_ps(__mmask8 __U, __m256i __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundepu32_ps
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f32.v8i32
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_cvt_roundepu32_ps(__U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test__mm256_cvt_roundepu64_pd(__m256i __A) {
-// CHECK-LABEL: @test__mm256_cvt_roundepu64_pd
-// CHECK: @llvm.x86.avx512.uitofp.round.v4f64.v4i64
-  return _mm256_cvt_roundepu64_pd(__A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test__mm256_mask_cvt_roundepu64_pd(__m256d __W, __mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test__mm256_mask_cvt_roundepu64_pd
-// CHECK: @llvm.x86.avx512.uitofp.round.v4f64.v4i64
-  return _mm256_mask_cvt_roundepu64_pd(__W, __U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test__mm256_maskz_cvt_roundepu64_pd(__mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test__mm256_maskz_cvt_roundepu64_pd
-// CHECK: @llvm.x86.avx512.uitofp.round.v4f64.v4i64
-  return _mm256_maskz_cvt_roundepu64_pd(__U, __A, _MM_FROUND_NO_EXC);
-}
-
-// FIXME: We may change to @llvm.x86.avx10.mask.vcvtuqq2ph256 in future.
-__m128h test_mm256_cvt_roundepu64_ph(__m256i A) {
-// CHECK-LABEL: test_mm256_cvt_roundepu64_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f16.v4i64
-  return _mm256_cvt_roundepu64_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_mask_cvt_roundepu64_ph(__m128h A, __mmask8 B, __m256i C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundepu64_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f16.v4i64
-  return _mm256_mask_cvt_roundepu64_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128h test_mm256_maskz_cvt_roundepu64_ph(__mmask8 A, __m256i B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundepu64_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v8f16.v4i64
-  return _mm256_maskz_cvt_roundepu64_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_cvt_roundepu64_ps(__m256i __A) {
-// CHECK-LABEL: @test_mm256_cvt_roundepu64_ps
-// CHECK: @llvm.x86.avx512.uitofp.round.v4f32.v4i64
-  return _mm256_cvt_roundepu64_ps(__A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_mask_cvt_roundepu64_ps(__m128 __W, __mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test_mm256_mask_cvt_roundepu64_ps
-// CHECK: @llvm.x86.avx512.uitofp.round.v4f32.v4i64
-// CHECK: select <4 x i1> %{{.*}}, <4 x float> %{{.*}}, <4 x float> %{{.*}}
-  return _mm256_mask_cvt_roundepu64_ps(__W, __U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m128 test_mm256_maskz_cvt_roundepu64_ps(__mmask8 __U, __m256i __A) {
-// CHECK-LABEL: @test_mm256_maskz_cvt_roundepu64_ps
-// CHECK: @llvm.x86.avx512.uitofp.round.v4f32.v4i64
-// CHECK: select <4 x i1> %{{.*}}, <4 x float> %{{.*}}, <4 x float> %{{.*}}
-  return _mm256_maskz_cvt_roundepu64_ps(__U, __A, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_cvt_roundepi16_ph(__m256i A) {
-// CHECK-LABEL: test_mm256_cvt_roundepi16_ph
-// CHECK:   @llvm.x86.avx512.sitofp.round.v16f16.v16i16
-  return _mm256_cvt_roundepi16_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_cvt_roundepi16_ph(__m256h A, __mmask16 B, __m256i C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundepi16_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v16f16.v16i16
-  return _mm256_mask_cvt_roundepi16_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_cvt_roundepi16_ph(__mmask16 A, __m256i B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundepi16_ph
-// CHECK: @llvm.x86.avx512.sitofp.round.v16f16.v16i16
-  return _mm256_maskz_cvt_roundepi16_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_cvt_roundepu16_ph(__m256i A) {
-// CHECK-LABEL: test_mm256_cvt_roundepu16_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v16f16.v16i16
-  return _mm256_cvt_roundepu16_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_cvt_roundepu16_ph(__m256h A, __mmask16 B, __m256i C) {
-// CHECK-LABEL: test_mm256_mask_cvt_roundepu16_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v16f16.v16i16
-  return _mm256_mask_cvt_roundepu16_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_cvt_roundepu16_ph(__mmask16 A, __m256i B) {
-// CHECK-LABEL: test_mm256_maskz_cvt_roundepu16_ph
-// CHECK: @llvm.x86.avx512.uitofp.round.v16f16.v16i16
-  return _mm256_maskz_cvt_roundepu16_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_div_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_div_round_pd
-// CHECK: @llvm.x86.avx10.vdivpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 11)
-  return _mm256_div_round_pd(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_div_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_div_round_pd
-// CHECK: @llvm.x86.avx10.vdivpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 10)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_div_round_pd(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_div_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_div_round_pd
-// CHECK: @llvm.x86.avx10.vdivpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 9)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_maskz_div_round_pd(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_div_round_ph(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_div_round_ph
-// CHECK: @llvm.x86.avx10.vdivph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 11)
-  return _mm256_div_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_div_round_ph(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_div_round_ph
-// CHECK: @llvm.x86.avx10.vdivph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 10)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_div_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_div_round_ph(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_div_round_ph
-// CHECK: @llvm.x86.avx10.vdivph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 9)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_maskz_div_round_ph(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_div_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_div_round_ps
-// CHECK: @llvm.x86.avx10.vdivps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 11)
-  return _mm256_div_round_ps(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_div_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_div_round_ps
-// CHECK: @llvm.x86.avx10.vdivps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 10)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_div_round_ps(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_div_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_div_round_ps
-// CHECK: @llvm.x86.avx10.vdivps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 9)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_div_round_ps(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fcmadd_round_pch(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fcmadd_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfcmaddcph256
-  return _mm256_fcmadd_round_pch(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fcmadd_round_pch(__m256h __A, __mmask8 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fcmadd_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfcmaddcph256
-// CHECK:  %{{.*}} = select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fcmadd_round_pch(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fcmadd_round_pch(__m256h __A, __m256h __B, __m256h __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fcmadd_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfcmaddcph256
-// CHECK-NOT:  %{{.*}} = select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fcmadd_round_pch(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fcmadd_round_pch(__mmask8 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fcmadd_round_pch
-// CHECK: @llvm.x86.avx10.maskz.vfcmaddcph256
-  return _mm256_maskz_fcmadd_round_pch(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_cmul_round_pch(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_cmul_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfcmulcph256
-  return _mm256_cmul_round_pch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_cmul_round_pch(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_cmul_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfcmulcph256
-  return _mm256_mask_cmul_round_pch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_cmul_round_pch(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_cmul_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfcmulcph256
-  return _mm256_maskz_cmul_round_pch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_fixupimm_round_pd(__m256d __A, __m256d __B, __m256i __C) {
-// CHECK-LABEL: @test_mm256_fixupimm_round_pd
-// CHECK: @llvm.x86.avx10.mask.vfixupimmpd256
-  return _mm256_fixupimm_round_pd(__A, __B, __C, 5, 8);
-}
-
-__m256d test_mm256_mask_fixupimm_round_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256i __C) {
-// CHECK-LABEL: @test_mm256_mask_fixupimm_round_pd
-// CHECK: @llvm.x86.avx10.mask.vfixupimmpd256
-  return _mm256_mask_fixupimm_round_pd(__A, __U, __B, __C, 5, 8);
-}
-
-__m256d test_mm256_maskz_fixupimm_round_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256i __C) {
-// CHECK-LABEL: @test_mm256_maskz_fixupimm_round_pd
-// CHECK: @llvm.x86.avx10.maskz.vfixupimmpd256
-  return _mm256_maskz_fixupimm_round_pd(__U, __A, __B, __C, 5, 8);
-}
-
-__m256 test_mm256_fixupimm_round_ps(__m256 __A, __m256 __B, __m256i __C) {
-// CHECK-LABEL: @test_mm256_fixupimm_round_ps
-// CHECK: @llvm.x86.avx10.mask.vfixupimmps256
-  return _mm256_fixupimm_round_ps(__A, __B, __C, 5, 8);
-}
-
-__m256 test_mm256_mask_fixupimm_round_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256i __C) {
-// CHECK-LABEL: @test_mm256_mask_fixupimm_round_ps
-// CHECK: @llvm.x86.avx10.mask.vfixupimmps256
-  return _mm256_mask_fixupimm_round_ps(__A, __U, __B, __C, 5, 8);
-}
-
-__m256 test_mm256_maskz_fixupimm_round_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256i __C) {
-// CHECK-LABEL: @test_mm256_maskz_fixupimm_round_ps
-// CHECK: @llvm.x86.avx10.maskz.vfixupimmps256
-  return _mm256_maskz_fixupimm_round_ps(__U, __A, __B, __C, 5, 8);
-}
-
-__m256d test_mm256_fmadd_round_pd(__m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_fmadd_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-  return _mm256_fmadd_round_pd(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_fmadd_round_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_mask_fmadd_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_fmadd_round_pd(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask3_fmadd_round_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmadd_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask3_fmadd_round_pd(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_fmadd_round_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmadd_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> zeroinitializer
-  return _mm256_maskz_fmadd_round_pd(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_fmsub_round_pd(__m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_fmsub_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-  return _mm256_fmsub_round_pd(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_fmsub_round_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_mask_fmsub_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_fmsub_round_pd(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_fmsub_round_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmsub_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> zeroinitializer
-  return _mm256_maskz_fmsub_round_pd(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_fnmadd_round_pd(__m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_fnmadd_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-  return _mm256_fnmadd_round_pd(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask3_fnmadd_round_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fnmadd_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask3_fnmadd_round_pd(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_fnmadd_round_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_maskz_fnmadd_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> zeroinitializer
-  return _mm256_maskz_fnmadd_round_pd(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_fnmsub_round_pd(__m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_fnmsub_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-  return _mm256_fnmsub_round_pd(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_fnmsub_round_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_maskz_fnmsub_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> zeroinitializer
-  return _mm256_maskz_fnmsub_round_pd(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fmadd_round_ph(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fmadd_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddph256
-  return _mm256_fmadd_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fmadd_round_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fmadd_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_fmadd_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fmadd_round_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmadd_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask3_fmadd_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fmadd_round_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmadd_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> zeroinitializer
-  return _mm256_maskz_fmadd_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fmsub_round_ph(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fmsub_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-  return _mm256_fmsub_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fmsub_round_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fmsub_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_fmsub_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fmsub_round_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmsub_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> zeroinitializer
-  return _mm256_maskz_fmsub_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fnmadd_round_ph(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fnmadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-  return _mm256_fnmadd_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fnmadd_round_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fnmadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask3_fnmadd_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fnmadd_round_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fnmadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> zeroinitializer
-  return _mm256_maskz_fnmadd_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fnmsub_round_ph(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fnmsub_round_ph
-// CHECK: fneg
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-  return _mm256_fnmsub_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fnmsub_round_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fnmsub_round_ph
-// CHECK: fneg
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> zeroinitializer
-  return _mm256_maskz_fnmsub_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_fmadd_round_ps(__m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_fmadd_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddps256
-  return _mm256_fmadd_round_ps(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_fmadd_round_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_mask_fmadd_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fmadd_round_ps(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask3_fmadd_round_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmadd_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fmadd_round_ps(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_fmadd_round_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmadd_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> zeroinitializer
-  return _mm256_maskz_fmadd_round_ps(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_fmsub_round_ps(__m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_fmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-  return _mm256_fmsub_round_ps(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_fmsub_round_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_mask_fmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fmsub_round_ps(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_fmsub_round_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> zeroinitializer
-  return _mm256_maskz_fmsub_round_ps(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_fnmadd_round_ps(__m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_fnmadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-  return _mm256_fnmadd_round_ps(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask3_fnmadd_round_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fnmadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fnmadd_round_ps(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_fnmadd_round_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_maskz_fnmadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> zeroinitializer
-  return _mm256_maskz_fnmadd_round_ps(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_fnmsub_round_ps(__m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_fnmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-  return _mm256_fnmsub_round_ps(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_fnmsub_round_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_maskz_fnmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> zeroinitializer
-  return _mm256_maskz_fnmsub_round_ps(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fmadd_round_pch(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fmadd_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfmaddcph256
-  return _mm256_fmadd_round_pch(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fmadd_round_pch(__m256h __A, __mmask8 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fmadd_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfmaddcph256
-// CHECK:  %{{.*}} = select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fmadd_round_pch(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fmadd_round_pch(__m256h __A, __m256h __B, __m256h __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmadd_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfmaddcph256
-// CHECK-NOT:  %{{.*}} = select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fmadd_round_pch(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fmadd_round_pch(__mmask8 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmadd_round_pch
-// CHECK: @llvm.x86.avx10.maskz.vfmaddcph256
-  return _mm256_maskz_fmadd_round_pch(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_fmaddsub_round_pd(__m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_fmaddsub_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-  return _mm256_fmaddsub_round_pd(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_fmaddsub_round_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_mask_fmaddsub_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_fmaddsub_round_pd(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask3_fmaddsub_round_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmaddsub_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask3_fmaddsub_round_pd(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_fmaddsub_round_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmaddsub_round_pd
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> zeroinitializer
-  return _mm256_maskz_fmaddsub_round_pd(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_fmsubadd_round_pd(__m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_fmsubadd_round_pd
-// CHECK: fneg <4 x double> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-  return _mm256_fmsubadd_round_pd(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_fmsubadd_round_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_mask_fmsubadd_round_pd
-// CHECK: fneg <4 x double> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_fmsubadd_round_pd(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_fmsubadd_round_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmsubadd_round_pd
-// CHECK: fneg <4 x double> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> zeroinitializer
-  return _mm256_maskz_fmsubadd_round_pd(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fmaddsub_round_ph(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fmaddsub_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-  return _mm256_fmaddsub_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fmaddsub_round_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fmaddsub_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_fmaddsub_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fmaddsub_round_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmaddsub_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask3_fmaddsub_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fmaddsub_round_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmaddsub_round_ph
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> zeroinitializer
-  return _mm256_maskz_fmaddsub_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_fmsubadd_round_ph(__m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_fmsubadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-  return _mm256_fmsubadd_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fmsubadd_round_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fmsubadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_fmsubadd_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_fmsubadd_round_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmsubadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> zeroinitializer
-  return _mm256_maskz_fmsubadd_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_fmaddsub_round_ps(__m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_fmaddsub_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-  return _mm256_fmaddsub_round_ps(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_fmaddsub_round_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_mask_fmaddsub_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fmaddsub_round_ps(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask3_fmaddsub_round_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmaddsub_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fmaddsub_round_ps(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_fmaddsub_round_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmaddsub_round_ps
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> zeroinitializer
-  return _mm256_maskz_fmaddsub_round_ps(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_fmsubadd_round_ps(__m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_fmsubadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-  return _mm256_fmsubadd_round_ps(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_fmsubadd_round_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_mask_fmsubadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fmsubadd_round_ps(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_fmsubadd_round_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_maskz_fmsubadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> zeroinitializer
-  return _mm256_maskz_fmsubadd_round_ps(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask3_fmsub_round_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmsub_round_pd
-// CHECK: fneg <4 x double> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask3_fmsub_round_pd(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask3_fmsubadd_round_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmsubadd_round_pd
-// CHECK: fneg <4 x double> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask3_fmsubadd_round_pd(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_fnmadd_round_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_mask_fnmadd_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_fnmadd_round_pd(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_fnmsub_round_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C) {
-// CHECK-LABEL: @test_mm256_mask_fnmsub_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_fnmsub_round_pd(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask3_fnmsub_round_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fnmsub_round_pd
-// CHECK: fneg <4 x double>
-// CHECK: fneg <4 x double>
-// CHECK: @llvm.x86.avx10.vfmaddpd256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask3_fnmsub_round_pd(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fmsub_round_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmsub_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask3_fmsub_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fmsubadd_round_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmsubadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddsubph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask3_fmsubadd_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fnmadd_round_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fnmadd_round_ph
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_fnmadd_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_fnmsub_round_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_fnmsub_round_ph
-// CHECK: fneg
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_fnmsub_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask3_fnmsub_round_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fnmsub_round_ph
-// CHECK: fneg
-// CHECK: fneg
-// CHECK: @llvm.x86.avx10.vfmaddph256
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask3_fnmsub_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask3_fmsub_round_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fmsub_round_ps(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask3_fmsubadd_round_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fmsubadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddsubps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fmsubadd_round_ps(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_fnmadd_round_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_mask_fnmadd_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fnmadd_round_ps(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_fnmsub_round_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C) {
-// CHECK-LABEL: @test_mm256_mask_fnmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_fnmsub_round_ps(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask3_fnmsub_round_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_mask3_fnmsub_round_ps
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: fneg <8 x float> %{{.*}}
-// CHECK: @llvm.x86.avx10.vfmaddps256
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask3_fnmsub_round_ps(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mul_round_pch(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mul_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfmulcph256
-  return _mm256_mul_round_pch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_mul_round_pch(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_mul_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfmulcph256
-  return _mm256_mask_mul_round_pch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_mul_round_pch(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_mul_round_pch
-// CHECK: @llvm.x86.avx10.mask.vfmulcph256
-  return _mm256_maskz_mul_round_pch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_getexp_round_pd(__m256d __A) {
-// CHECK-LABEL: @test_mm256_getexp_round_pd
-// CHECK: @llvm.x86.avx10.mask.vgetexppd256
-  return _mm256_getexp_round_pd(__A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_getexp_round_pd(__m256d __W, __mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_mask_getexp_round_pd
-// CHECK: @llvm.x86.avx10.mask.vgetexppd256
-  return _mm256_mask_getexp_round_pd(__W, __U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_getexp_round_pd(__mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_maskz_getexp_round_pd
-// CHECK: @llvm.x86.avx10.mask.vgetexppd256
-  return _mm256_maskz_getexp_round_pd(__U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_getexp_round_ph(__m256h __A) {
-// CHECK-LABEL: @test_mm256_getexp_round_ph
-// CHECK: @llvm.x86.avx10.mask.vgetexpph256
-  return _mm256_getexp_round_ph(__A, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_getexp_round_ph(__m256h __W, __mmask16 __U, __m256h __A) {
-// CHECK-LABEL: @test_mm256_mask_getexp_round_ph
-// CHECK: @llvm.x86.avx10.mask.vgetexpph256
-  return _mm256_mask_getexp_round_ph(__W, __U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_getexp_round_ph(__mmask16 __U, __m256h __A) {
-// CHECK-LABEL: @test_mm256_maskz_getexp_round_ph
-// CHECK: @llvm.x86.avx10.mask.vgetexpph256
-  return _mm256_maskz_getexp_round_ph(__U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_getexp_round_ps(__m256 __A) {
-// CHECK-LABEL: @test_mm256_getexp_round_ps
-// CHECK: @llvm.x86.avx10.mask.vgetexpps256
-  return _mm256_getexp_round_ps(__A, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_getexp_round_ps(__m256 __W, __mmask8 __U, __m256 __A) {
-// CHECK-LABEL: @test_mm256_mask_getexp_round_ps
-// CHECK: @llvm.x86.avx10.mask.vgetexpps256
-  return _mm256_mask_getexp_round_ps(__W, __U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_getexp_round_ps(__mmask8 __U, __m256 __A) {
-// CHECK-LABEL: @test_mm256_maskz_getexp_round_ps
-// CHECK: @llvm.x86.avx10.mask.vgetexpps256
-  return _mm256_maskz_getexp_round_ps(__U, __A, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_getmant_round_pd(__m256d __A) {
-// CHECK-LABEL: @test_mm256_getmant_round_pd
-// CHECK: @llvm.x86.avx10.mask.vgetmantpd256
-  return _mm256_getmant_round_pd(__A,_MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_getmant_round_pd(__m256d __W, __mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_mask_getmant_round_pd
-// CHECK: @llvm.x86.avx10.mask.vgetmantpd256
-  return _mm256_mask_getmant_round_pd(__W, __U, __A,_MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_getmant_round_pd(__mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_maskz_getmant_round_pd
-// CHECK: @llvm.x86.avx10.mask.vgetmantpd256
-  return _mm256_maskz_getmant_round_pd(__U, __A,_MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_getmant_round_ph(__m256h __A) {
-// CHECK-LABEL: @test_mm256_getmant_round_ph
-// CHECK: @llvm.x86.avx10.mask.vgetmantph256
-  return _mm256_getmant_round_ph(__A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_getmant_round_ph(__m256h __W, __mmask16 __U, __m256h __A) {
-// CHECK-LABEL: @test_mm256_mask_getmant_round_ph
-// CHECK: @llvm.x86.avx10.mask.vgetmantph256
-  return _mm256_mask_getmant_round_ph(__W, __U, __A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_getmant_round_ph(__mmask16 __U, __m256h __A) {
-// CHECK-LABEL: @test_mm256_maskz_getmant_round_ph
-// CHECK: @llvm.x86.avx10.mask.vgetmantph256
-  return _mm256_maskz_getmant_round_ph(__U, __A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_getmant_round_ps(__m256 __A) {
-// CHECK-LABEL: @test_mm256_getmant_round_ps
-// CHECK: @llvm.x86.avx10.mask.vgetmantps256
-  return _mm256_getmant_round_ps(__A,_MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_getmant_round_ps(__m256 __W, __mmask8 __U, __m256 __A) {
-// CHECK-LABEL: @test_mm256_mask_getmant_round_ps
-// CHECK: @llvm.x86.avx10.mask.vgetmantps256
-  return _mm256_mask_getmant_round_ps(__W, __U, __A,_MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_getmant_round_ps(__mmask8 __U, __m256 __A) {
-// CHECK-LABEL: @test_mm256_maskz_getmant_round_ps
-// CHECK: @llvm.x86.avx10.mask.vgetmantps256
-  return _mm256_maskz_getmant_round_ps(__U, __A,_MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_max_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_max_round_pd
-// CHECK: @llvm.x86.avx10.vmaxpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 8)
-  return _mm256_max_round_pd(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_max_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_max_round_pd
-// CHECK: @llvm.x86.avx10.vmaxpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 8)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_max_round_pd(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_max_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_max_round_pd
-// CHECK: @llvm.x86.avx10.vmaxpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 8)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_maskz_max_round_pd(__U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_max_round_ph(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_max_round_ph
-// CHECK: @llvm.x86.avx10.vmaxph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 8)
-  return _mm256_max_round_ph(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_max_round_ph(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_max_round_ph
-// CHECK: @llvm.x86.avx10.vmaxph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 8)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_max_round_ph(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_max_round_ph(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_max_round_ph
-// CHECK: @llvm.x86.avx10.vmaxph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 8)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_maskz_max_round_ph(__U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_max_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_max_round_ps
-// CHECK: @llvm.x86.avx10.vmaxps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 8)
-  return _mm256_max_round_ps(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_max_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_max_round_ps
-// CHECK: @llvm.x86.avx10.vmaxps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 8)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_max_round_ps(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_max_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_max_round_ps
-// CHECK: @llvm.x86.avx10.vmaxps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 8)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_max_round_ps(__U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_min_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_min_round_pd
-// CHECK: @llvm.x86.avx10.vminpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 8)
-  return _mm256_min_round_pd(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_min_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_min_round_pd
-// CHECK: @llvm.x86.avx10.vminpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 8)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_min_round_pd(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_min_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_min_round_pd
-// CHECK: @llvm.x86.avx10.vminpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 8)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_maskz_min_round_pd(__U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_min_round_ph(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_min_round_ph
-// CHECK: @llvm.x86.avx10.vminph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 8)
-  return _mm256_min_round_ph(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_min_round_ph(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_min_round_ph
-// CHECK: @llvm.x86.avx10.vminph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 8)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_min_round_ph(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_min_round_ph(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_min_round_ph
-// CHECK: @llvm.x86.avx10.vminph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 8)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_maskz_min_round_ph(__U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_min_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_min_round_ps
-// CHECK: @llvm.x86.avx10.vminps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 8)
-  return _mm256_min_round_ps(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_min_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_min_round_ps
-// CHECK: @llvm.x86.avx10.vminps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 8)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_min_round_ps(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_min_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_min_round_ps
-// CHECK: @llvm.x86.avx10.vminps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 8)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_min_round_ps(__U, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mul_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mul_round_pd
-// CHECK: @llvm.x86.avx10.vmulpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 11)
-  return _mm256_mul_round_pd(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_mul_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_mul_round_pd
-// CHECK: @llvm.x86.avx10.vmulpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 10)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_mul_round_pd(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_mul_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_mul_round_pd
-// CHECK: @llvm.x86.avx10.vmulpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 9)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_maskz_mul_round_pd(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mul_round_ph(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mul_round_ph
-// CHECK: @llvm.x86.avx10.vmulph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 11)
-  return _mm256_mul_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_mul_round_ph(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_mul_round_ph
-// CHECK: @llvm.x86.avx10.vmulph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 10)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_mul_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_mul_round_ph(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_mul_round_ph
-// CHECK: @llvm.x86.avx10.vmulph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 9)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_maskz_mul_round_ph(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mul_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mul_round_ps
-// CHECK: @llvm.x86.avx10.vmulps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 11)
-  return _mm256_mul_round_ps(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_mul_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_mul_round_ps
-// CHECK: @llvm.x86.avx10.vmulps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 10)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_mul_round_ps(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_mul_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_mul_round_ps
-// CHECK: @llvm.x86.avx10.vmulps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 9)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_mul_round_ps(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_range_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_range_round_pd
-// CHECK: @llvm.x86.avx10.mask.vrangepd256
-  return _mm256_range_round_pd(__A, __B, 4, 8);
-}
-
-__m256d test_mm256_mask_range_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_range_round_pd
-// CHECK: @llvm.x86.avx10.mask.vrangepd256
-  return _mm256_mask_range_round_pd(__W, __U, __A, __B, 4, 8);
-}
-
-__m256d test_mm256_maskz_range_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_range_round_pd
-// CHECK: @llvm.x86.avx10.mask.vrangepd256
-  return _mm256_maskz_range_round_pd(__U, __A, __B, 4, 8);
-}
-
-__m256 test_mm256_range_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_range_round_ps
-// CHECK: @llvm.x86.avx10.mask.vrangeps256
-  return _mm256_range_round_ps(__A, __B, 4, 8);
-}
-
-__m256 test_mm256_mask_range_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_range_round_ps
-// CHECK: @llvm.x86.avx10.mask.vrangeps256
-  return _mm256_mask_range_round_ps(__W, __U, __A, __B, 4, 8);
-}
-
-__m256 test_mm256_maskz_range_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_range_round_ps
-// CHECK: @llvm.x86.avx10.mask.vrangeps256
-  return _mm256_maskz_range_round_ps(__U, __A, __B, 4, 8);
-}
-
-__m256d test_mm256_reduce_round_pd(__m256d __A) {
-// CHECK-LABEL: @test_mm256_reduce_round_pd
-// CHECK: @llvm.x86.avx10.mask.vreducepd256
-  return _mm256_reduce_round_pd(__A, 4, 8);
-}
-
-__m256d test_mm256_mask_reduce_round_pd(__m256d __W, __mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_mask_reduce_round_pd
-// CHECK: @llvm.x86.avx10.mask.vreducepd256
-  return _mm256_mask_reduce_round_pd(__W, __U, __A, 4, 8);
-}
-
-__m256d test_mm256_maskz_reduce_round_pd(__mmask8 __U, __m256d __A) {
-// CHECK-LABEL: @test_mm256_maskz_reduce_round_pd
-// CHECK: @llvm.x86.avx10.mask.vreducepd256
-  return _mm256_maskz_reduce_round_pd(__U, __A, 4, 8);
-}
-
-__m256h test_mm256_mask_reduce_round_ph(__m256h __A, __mmask8 __U, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_reduce_round_ph
-// CHECK: @llvm.x86.avx10.mask.vreduceph256
-  return _mm256_mask_reduce_round_ph(__A, __U, __C, 3, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_reduce_round_ph(__m256h __A, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_maskz_reduce_round_ph
-// CHECK: @llvm.x86.avx10.mask.vreduceph256
-  return _mm256_maskz_reduce_round_ph(__U, __A, 3, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_reduce_round_ph(__m256h __A) {
-// CHECK-LABEL: @test_mm256_reduce_round_ph
-// CHECK: @llvm.x86.avx10.mask.vreduceph256
-  return _mm256_reduce_round_ph(__A, 3, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_reduce_round_ps(__m256 __A) {
-// CHECK-LABEL: @test_mm256_reduce_round_ps
-// CHECK: @llvm.x86.avx10.mask.vreduceps256
-  return _mm256_reduce_round_ps(__A, 4, 8);
-}
-
-__m256 test_mm256_mask_reduce_round_ps(__m256 __W, __mmask8 __U, __m256 __A) {
-// CHECK-LABEL: @test_mm256_mask_reduce_round_ps
-// CHECK: @llvm.x86.avx10.mask.vreduceps256
-  return _mm256_mask_reduce_round_ps(__W, __U, __A, 4, 8);
-}
-
-__m256 test_mm256_maskz_reduce_round_ps(__mmask8 __U, __m256 __A) {
-// CHECK-LABEL: @test_mm256_maskz_reduce_round_ps
-// CHECK: @llvm.x86.avx10.mask.vreduceps256
-  return _mm256_maskz_reduce_round_ps(__U, __A, 4, 8);
-}
-
-__m256d test_mm256_roundscale_round_pd(__m256d __A)
-{
-// CHECK-LABEL: @test_mm256_roundscale_round_pd
-// CHECK: @llvm.x86.avx10.mask.vrndscalepd256
-  return _mm256_roundscale_round_pd(__A,_MM_FROUND_TO_ZERO,_MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_roundscale_round_pd(__m256d __A,__mmask8 __U,__m256d __C)
-{
-// CHECK-LABEL: @test_mm256_mask_roundscale_round_pd
-// CHECK: @llvm.x86.avx10.mask.vrndscalepd256
-  return _mm256_mask_roundscale_round_pd(__A,__U,__C,_MM_FROUND_TO_ZERO,_MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_roundscale_round_pd(__m256d __A,__mmask8 __U)
-{
-// CHECK-LABEL: @test_mm256_maskz_roundscale_round_pd
-// CHECK: @llvm.x86.avx10.mask.vrndscalepd256
-  return _mm256_maskz_roundscale_round_pd(__U,__A,_MM_FROUND_TO_ZERO,_MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_roundscale_round_ph(__m256h __A, __mmask8 __U, __m256h __C) {
-// CHECK-LABEL: @test_mm256_mask_roundscale_round_ph
-// CHECK: @llvm.x86.avx10.mask.vrndscaleph256
-  return _mm256_mask_roundscale_round_ph(__A, __U, __C, 3, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_roundscale_round_ph(__m256h __A, __mmask8 __U) {
-// CHECK-LABEL: @test_mm256_maskz_roundscale_round_ph
-// CHECK: @llvm.x86.avx10.mask.vrndscaleph256
-  return _mm256_maskz_roundscale_round_ph(__U, __A, 3, _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_roundscale_round_ph(__m256h __A) {
-// CHECK-LABEL: @test_mm256_roundscale_round_ph
-// CHECK: @llvm.x86.avx10.mask.vrndscaleph256
-  return _mm256_roundscale_round_ph(__A, 3, _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_roundscale_round_ps(__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_roundscale_round_ps
-// CHECK: @llvm.x86.avx10.mask.vrndscaleps256
-  return _mm256_roundscale_round_ps(__A,_MM_FROUND_TO_ZERO,_MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_roundscale_round_ps(__m256 __A,__mmask8 __U,__m256 __C)
-{
-// CHECK-LABEL: @test_mm256_mask_roundscale_round_ps
-// CHECK: @llvm.x86.avx10.mask.vrndscaleps256
-  return _mm256_mask_roundscale_round_ps(__A,__U,__C,_MM_FROUND_TO_ZERO,_MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_roundscale_round_ps(__m256 __A,__mmask8 __U)
-{
-// CHECK-LABEL: @test_mm256_maskz_roundscale_round_ps
-// CHECK: @llvm.x86.avx10.mask.vrndscaleps256
-  return _mm256_maskz_roundscale_round_ps(__U,__A,_MM_FROUND_TO_ZERO,_MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_scalef_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_scalef_round_pd
-// CHECK: @llvm.x86.avx10.mask.vscalefpd256
-  return _mm256_scalef_round_pd(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_scalef_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_scalef_round_pd
-// CHECK: @llvm.x86.avx10.mask.vscalefpd256
-  return _mm256_mask_scalef_round_pd(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_scalef_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_scalef_round_pd
-// CHECK: @llvm.x86.avx10.mask.vscalefpd256
-  return _mm256_maskz_scalef_round_pd(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_scalef_round_ph(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_scalef_round_ph
-// CHECK: @llvm.x86.avx10.mask.vscalefph256
-  return _mm256_scalef_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_scalef_round_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_scalef_round_ph
-// CHECK: @llvm.x86.avx10.mask.vscalefph256
-  return _mm256_mask_scalef_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_scalef_round_ph(__mmask16 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_scalef_round_ph
-// CHECK: @llvm.x86.avx10.mask.vscalefph256
-  return _mm256_maskz_scalef_round_ph(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_scalef_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_scalef_round_ps
-// CHECK: @llvm.x86.avx10.mask.vscalefps256
-  return _mm256_scalef_round_ps(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_scalef_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_scalef_round_ps
-// CHECK: @llvm.x86.avx10.mask.vscalefps256
-  return _mm256_mask_scalef_round_ps(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_scalef_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_scalef_round_ps
-// CHECK: @llvm.x86.avx10.mask.vscalefps256
-  return _mm256_maskz_scalef_round_ps(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_sqrt_round_pd(__m256d __A)
-{
-// CHECK-LABEL: @test_mm256_sqrt_round_pd
-// CHECK: call <4 x double> @llvm.x86.avx10.vsqrtpd256(<4 x double> %{{.*}}, i32 11)
-  return _mm256_sqrt_round_pd(__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_sqrt_round_pd(__m256d __W,__mmask8 __U,__m256d __A)
-{
-// CHECK-LABEL: @test_mm256_mask_sqrt_round_pd
-// CHECK: call <4 x double> @llvm.x86.avx10.vsqrtpd256(<4 x double> %{{.*}}, i32 11)
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_sqrt_round_pd(__W,__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_sqrt_round_pd(__mmask8 __U,__m256d __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_sqrt_round_pd
-// CHECK: call <4 x double> @llvm.x86.avx10.vsqrtpd256(<4 x double> %{{.*}}, i32 11)
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> {{.*}}
-  return _mm256_maskz_sqrt_round_pd(__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_sqrt_round_ph(__m256h __A) {
-// CHECK-LABEL: @test_mm256_sqrt_round_ph
-// CHECK: call <16 x half> @llvm.x86.avx10.vsqrtph256(<16 x half> %{{.*}}, i32 11)
-  return _mm256_sqrt_round_ph(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_sqrt_round_ph(__m256h __W, __mmask16 __U, __m256h __A) {
-// CHECK-LABEL: @test_mm256_mask_sqrt_round_ph
-// CHECK: call <16 x half> @llvm.x86.avx10.vsqrtph256(<16 x half> %{{.*}}, i32 11)
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_sqrt_round_ph(__W, __U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_sqrt_round_ph(__mmask16 __U, __m256h __A) {
-// CHECK-LABEL: @test_mm256_maskz_sqrt_round_ph
-// CHECK: call <16 x half> @llvm.x86.avx10.vsqrtph256(<16 x half> %{{.*}}, i32 11)
-// CHECK: bitcast i16 %{{.*}} to <16 x i1>
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> {{.*}}
-  return _mm256_maskz_sqrt_round_ph(__U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_sqrt_round_ps(__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_sqrt_round_ps
-// CHECK: call <8 x float> @llvm.x86.avx10.vsqrtps256(<8 x float> %{{.*}}, i32 11)
-  return _mm256_sqrt_round_ps(__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_sqrt_round_ps(__m256 __W,__mmask8 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_mask_sqrt_round_ps
-// CHECK: call <8 x float> @llvm.x86.avx10.vsqrtps256(<8 x float> %{{.*}}, i32 11)
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_sqrt_round_ps(__W,__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_sqrt_round_ps(__mmask8 __U,__m256 __A)
-{
-// CHECK-LABEL: @test_mm256_maskz_sqrt_round_ps
-// CHECK: call <8 x float> @llvm.x86.avx10.vsqrtps256(<8 x float> %{{.*}}, i32 11)
-// CHECK: bitcast i8 %{{.*}} to <8 x i1>
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> {{.*}}
-  return _mm256_maskz_sqrt_round_ps(__U,__A,_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_sub_round_pd(__m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_sub_round_pd
-// CHECK: @llvm.x86.avx10.vsubpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 11)
-  return _mm256_sub_round_pd(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_mask_sub_round_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_mask_sub_round_pd
-// CHECK: @llvm.x86.avx10.vsubpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 10)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_mask_sub_round_pd(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256d test_mm256_maskz_sub_round_pd(__mmask8 __U, __m256d __A, __m256d __B) {
-// CHECK-LABEL: @test_mm256_maskz_sub_round_pd
-// CHECK: @llvm.x86.avx10.vsubpd256(<4 x double> %{{.*}}, <4 x double> %{{.*}}, i32 9)
-// CHECK: select <4 x i1> %{{.*}}, <4 x double> %{{.*}}, <4 x double> %{{.*}}
-  return _mm256_maskz_sub_round_pd(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_sub_round_ph(__m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_sub_round_ph
-// CHECK: @llvm.x86.avx10.vsubph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 11)
-  return _mm256_sub_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_mask_sub_round_ph(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_mask_sub_round_ph
-// CHECK: @llvm.x86.avx10.vsubph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 10)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_mask_sub_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256h test_mm256_maskz_sub_round_ph(__mmask8 __U, __m256h __A, __m256h __B) {
-// CHECK-LABEL: @test_mm256_maskz_sub_round_ph
-// CHECK: @llvm.x86.avx10.vsubph256(<16 x half> %{{.*}}, <16 x half> %{{.*}}, i32 9)
-// CHECK: select <16 x i1> %{{.*}}, <16 x half> %{{.*}}, <16 x half> %{{.*}}
-  return _mm256_maskz_sub_round_ph(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_sub_round_ps(__m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_sub_round_ps
-// CHECK: @llvm.x86.avx10.vsubps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 11)
-  return _mm256_sub_round_ps(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_mask_sub_round_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_mask_sub_round_ps
-// CHECK: @llvm.x86.avx10.vsubps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 10)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_mask_sub_round_ps(__W, __U, __A, __B, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
-}
-
-__m256 test_mm256_maskz_sub_round_ps(__mmask8 __U, __m256 __A, __m256 __B) {
-// CHECK-LABEL: @test_mm256_maskz_sub_round_ps
-// CHECK: @llvm.x86.avx10.vsubps256(<8 x float> %{{.*}}, <8 x float> %{{.*}}, i32 9)
-// CHECK: select <8 x i1> %{{.*}}, <8 x float> %{{.*}}, <8 x float> %{{.*}}
-  return _mm256_maskz_sub_round_ps(__U, __A, __B, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
-}
diff --git a/clang/test/CodeGen/X86/avx10_2satcvt-builtins.c b/clang/test/CodeGen/X86/avx10_2satcvt-builtins.c
index 7c5fc087b9da..7f30befefffe 100644
--- a/clang/test/CodeGen/X86/avx10_2satcvt-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2satcvt-builtins.c
@@ -5,599 +5,457 @@
 
 #include <immintrin.h>
 
-__m128i test_mm_ipcvtbf16_epi8(__m128bh __A) {
-  // CHECK-LABEL: @test_mm_ipcvtbf16_epi8(
+__m128i test_mm_ipcvts_bf16_epi8(__m128bh __A) {
+  // CHECK-LABEL: @test_mm_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs128
-  return _mm_ipcvtbf16_epi8(__A);
+  return _mm_ipcvts_bf16_epi8(__A);
 }
 
-__m128i test_mm_mask_ipcvtbf16_epi8(__m128i __S, __mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvtbf16_epi8(
+__m128i test_mm_mask_ipcvts_bf16_epi8(__m128i __S, __mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs128
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_mask_ipcvtbf16_epi8(__S, __A, __B);
+  return _mm_mask_ipcvts_bf16_epi8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvtbf16_epi8(__mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvtbf16_epi8(
+__m128i test_mm_maskz_ipcvts_bf16_epi8(__mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs128
   // CHECK: zeroinitializer
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_maskz_ipcvtbf16_epi8(__A, __B);
+  return _mm_maskz_ipcvts_bf16_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvtbf16_epi8(__m256bh __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtbf16_epi8(
+__m256i test_mm256_ipcvts_bf16_epi8(__m256bh __A) {
+  // CHECK-LABEL: @test_mm256_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs256
-  return _mm256_ipcvtbf16_epi8(__A);
+  return _mm256_ipcvts_bf16_epi8(__A);
 }
 
-__m256i test_mm256_mask_ipcvtbf16_epi8(__m256i __S, __mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtbf16_epi8(
+__m256i test_mm256_mask_ipcvts_bf16_epi8(__m256i __S, __mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs256
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_mask_ipcvtbf16_epi8(__S, __A, __B);
+  return _mm256_mask_ipcvts_bf16_epi8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtbf16_epi8(__mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtbf16_epi8(
+__m256i test_mm256_maskz_ipcvts_bf16_epi8(__mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvtbf162ibs256
   // CHECK: zeroinitializer
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_maskz_ipcvtbf16_epi8(__A, __B);
+  return _mm256_maskz_ipcvts_bf16_epi8(__A, __B);
 }
 
-__m128i test_mm_ipcvtbf16_epu8(__m128bh __A) {
-  // CHECK-LABEL: @test_mm_ipcvtbf16_epu8(
+__m128i test_mm_ipcvts_bf16_epu8(__m128bh __A) {
+  // CHECK-LABEL: @test_mm_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs128
-  return _mm_ipcvtbf16_epu8(__A);
+  return _mm_ipcvts_bf16_epu8(__A);
 }
 
-__m128i test_mm_mask_ipcvtbf16_epu8(__m128i __S, __mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvtbf16_epu8(
+__m128i test_mm_mask_ipcvts_bf16_epu8(__m128i __S, __mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs128
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_mask_ipcvtbf16_epu8(__S, __A, __B);
+  return _mm_mask_ipcvts_bf16_epu8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvtbf16_epu8(__mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvtbf16_epu8(
+__m128i test_mm_maskz_ipcvts_bf16_epu8(__mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs128
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_maskz_ipcvtbf16_epu8(__A, __B);
+  return _mm_maskz_ipcvts_bf16_epu8(__A, __B);
 }
 
-__m256i test_mm256_ipcvtbf16_epu8(__m256bh __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtbf16_epu8(
+__m256i test_mm256_ipcvts_bf16_epu8(__m256bh __A) {
+  // CHECK-LABEL: @test_mm256_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs256
-  return _mm256_ipcvtbf16_epu8(__A);
+  return _mm256_ipcvts_bf16_epu8(__A);
 }
 
-__m256i test_mm256_mask_ipcvtbf16_epu8(__m256i __S, __mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtbf16_epu8(
+__m256i test_mm256_mask_ipcvts_bf16_epu8(__m256i __S, __mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs256
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_mask_ipcvtbf16_epu8(__S, __A, __B);
+  return _mm256_mask_ipcvts_bf16_epu8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtbf16_epu8(__mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtbf16_epu8(
+__m256i test_mm256_maskz_ipcvts_bf16_epu8(__mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvtbf162iubs256
   // CHECK: zeroinitializer
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_maskz_ipcvtbf16_epu8(__A, __B);
+  return _mm256_maskz_ipcvts_bf16_epu8(__A, __B);
 }
 
-__m128i test_mm_ipcvtph_epi8(__m128h __A) {
-  // CHECK-LABEL: @test_mm_ipcvtph_epi8(
+__m128i test_mm_ipcvts_ph_epi8(__m128h __A) {
+  // CHECK-LABEL: @test_mm_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs128
-  return _mm_ipcvtph_epi8(__A);
+  return _mm_ipcvts_ph_epi8(__A);
 }
 
-__m128i test_mm_mask_ipcvtph_epi8(__m128i __S, __mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvtph_epi8(
+__m128i test_mm_mask_ipcvts_ph_epi8(__m128i __S, __mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs128
-  return _mm_mask_ipcvtph_epi8(__S, __A, __B);
+  return _mm_mask_ipcvts_ph_epi8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvtph_epi8(__mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvtph_epi8(
+__m128i test_mm_maskz_ipcvts_ph_epi8(__mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs128
-  return _mm_maskz_ipcvtph_epi8(__A, __B);
+  return _mm_maskz_ipcvts_ph_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvtph_epi8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtph_epi8(
+__m256i test_mm256_ipcvts_ph_epi8(__m256h __A) {
+  // CHECK-LABEL: @test_mm256_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs256
-  return _mm256_ipcvtph_epi8(__A);
+  return _mm256_ipcvts_ph_epi8(__A);
 }
 
-__m256i test_mm256_mask_ipcvtph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtph_epi8(
+__m256i test_mm256_mask_ipcvts_ph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs256
-  return _mm256_mask_ipcvtph_epi8(__S, __A, __B);
+  return _mm256_mask_ipcvts_ph_epi8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtph_epi8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtph_epi8(
+__m256i test_mm256_maskz_ipcvts_ph_epi8(__mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs256
-  return _mm256_maskz_ipcvtph_epi8(__A, __B);
+  return _mm256_maskz_ipcvts_ph_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvt_roundph_epi8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvt_roundph_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs256
-  return _mm256_ipcvt_roundph_epi8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_ipcvt_roundph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvt_roundph_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs256
-  return _mm256_mask_ipcvt_roundph_epi8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-__m256i test_mm256_maskz_ipcvt_roundph_epi8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvt_roundph_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtph2ibs256
-  return _mm256_maskz_ipcvt_roundph_epi8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm_ipcvtph_epu8(__m128h __A) {
-  // CHECK-LABEL: @test_mm_ipcvtph_epu8(
+__m128i test_mm_ipcvts_ph_epu8(__m128h __A) {
+  // CHECK-LABEL: @test_mm_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs128
-  return _mm_ipcvtph_epu8(__A);
+  return _mm_ipcvts_ph_epu8(__A);
 }
 
-__m128i test_mm_mask_ipcvtph_epu8(__m128i __S, __mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvtph_epu8(
+__m128i test_mm_mask_ipcvts_ph_epu8(__m128i __S, __mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs128
-  return _mm_mask_ipcvtph_epu8(__S, __A, __B);
+  return _mm_mask_ipcvts_ph_epu8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvtph_epu8(__mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvtph_epu8(
+__m128i test_mm_maskz_ipcvts_ph_epu8(__mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs128
-  return _mm_maskz_ipcvtph_epu8(__A, __B);
-}
-
-__m256i test_mm256_ipcvtph_epu8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtph_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs256
-  return _mm256_ipcvtph_epu8(__A);
+  return _mm_maskz_ipcvts_ph_epu8(__A, __B);
 }
 
-__m256i test_mm256_mask_ipcvtph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtph_epu8(
+__m256i test_mm256_ipcvts_ph_epu8(__m256h __A) {
+  // CHECK-LABEL: @test_mm256_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs256
-  return _mm256_mask_ipcvtph_epu8(__S, __A, __B);
+  return _mm256_ipcvts_ph_epu8(__A);
 }
 
-__m256i test_mm256_maskz_ipcvtph_epu8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtph_epu8(
+__m256i test_mm256_mask_ipcvts_ph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs256
-  return _mm256_maskz_ipcvtph_epu8(__A, __B);
+  return _mm256_mask_ipcvts_ph_epu8(__S, __A, __B);
 }
 
-__m256i test_mm256_ipcvt_roundph_epu8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvt_roundph_epu8(
+__m256i test_mm256_maskz_ipcvts_ph_epu8(__mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs256
-  return _mm256_ipcvt_roundph_epu8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+  return _mm256_maskz_ipcvts_ph_epu8(__A, __B);
 }
 
-__m256i test_mm256_mask_ipcvt_roundph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvt_roundph_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs256
-  return _mm256_mask_ipcvt_roundph_epu8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-__m256i test_mm256_maskz_ipcvt_roundph_epu8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvt_roundph_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtph2iubs256
-  return _mm256_maskz_ipcvt_roundph_epu8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm_ipcvtps_epi8(__m128 __A) {
-  // CHECK-LABEL: @test_mm_ipcvtps_epi8(
+__m128i test_mm_ipcvts_ps_epi8(__m128 __A) {
+  // CHECK-LABEL: @test_mm_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs128
-  return _mm_ipcvtps_epi8(__A);
+  return _mm_ipcvts_ps_epi8(__A);
 }
 
-__m128i test_mm_mask_ipcvtps_epi8(__m128i __S, __mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvtps_epi8(
+__m128i test_mm_mask_ipcvts_ps_epi8(__m128i __S, __mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs128
-  return _mm_mask_ipcvtps_epi8(__S, __A, __B);
+  return _mm_mask_ipcvts_ps_epi8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvtps_epi8(__mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvtps_epi8(
+__m128i test_mm_maskz_ipcvts_ps_epi8(__mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs128
-  return _mm_maskz_ipcvtps_epi8(__A, __B);
+  return _mm_maskz_ipcvts_ps_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvtps_epi8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtps_epi8(
+__m256i test_mm256_ipcvts_ps_epi8(__m256 __A) {
+  // CHECK-LABEL: @test_mm256_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs256
-  return _mm256_ipcvtps_epi8(__A);
+  return _mm256_ipcvts_ps_epi8(__A);
 }
 
-__m256i test_mm256_mask_ipcvtps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtps_epi8(
+__m256i test_mm256_mask_ipcvts_ps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs256
-  return _mm256_mask_ipcvtps_epi8(__S, __A, __B);
+  return _mm256_mask_ipcvts_ps_epi8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtps_epi8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtps_epi8(
+__m256i test_mm256_maskz_ipcvts_ps_epi8(__mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs256
-  return _mm256_maskz_ipcvtps_epi8(__A, __B);
+  return _mm256_maskz_ipcvts_ps_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvt_roundps_epi8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvt_roundps_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs256
-  return _mm256_ipcvt_roundps_epi8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_ipcvt_roundps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvt_roundps_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs256
-  return _mm256_mask_ipcvt_roundps_epi8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_ipcvt_roundps_epi8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvt_roundps_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtps2ibs256
-  return _mm256_maskz_ipcvt_roundps_epi8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm_ipcvtps_epu8(__m128 __A) {
-  // CHECK-LABEL: @test_mm_ipcvtps_epu8(
+__m128i test_mm_ipcvts_ps_epu8(__m128 __A) {
+  // CHECK-LABEL: @test_mm_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs128
-  return _mm_ipcvtps_epu8(__A);
+  return _mm_ipcvts_ps_epu8(__A);
 }
 
-__m128i test_mm_mask_ipcvtps_epu8(__m128i __S, __mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvtps_epu8(
+__m128i test_mm_mask_ipcvts_ps_epu8(__m128i __S, __mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs128
-  return _mm_mask_ipcvtps_epu8(__S, __A, __B);
+  return _mm_mask_ipcvts_ps_epu8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvtps_epu8(__mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvtps_epu8(
+__m128i test_mm_maskz_ipcvts_ps_epu8(__mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs128
-  return _mm_maskz_ipcvtps_epu8(__A, __B);
+  return _mm_maskz_ipcvts_ps_epu8(__A, __B);
 }
 
-__m256i test_mm256_ipcvtps_epu8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtps_epu8(
+__m256i test_mm256_ipcvts_ps_epu8(__m256 __A) {
+  // CHECK-LABEL: @test_mm256_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs256
-  return _mm256_ipcvtps_epu8(__A);
+  return _mm256_ipcvts_ps_epu8(__A);
 }
 
-__m256i test_mm256_mask_ipcvtps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtps_epu8(
+__m256i test_mm256_mask_ipcvts_ps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs256
-  return _mm256_mask_ipcvtps_epu8(__S, __A, __B);
+  return _mm256_mask_ipcvts_ps_epu8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtps_epu8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtps_epu8(
+__m256i test_mm256_maskz_ipcvts_ps_epu8(__mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs256
-  return _mm256_maskz_ipcvtps_epu8(__A, __B);
+  return _mm256_maskz_ipcvts_ps_epu8(__A, __B);
 }
 
-__m256i test_mm256_ipcvt_roundps_epu8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvt_roundps_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs256
-  return _mm256_ipcvt_roundps_epu8(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_ipcvt_roundps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvt_roundps_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs256
-  return _mm256_mask_ipcvt_roundps_epu8(__S, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_ipcvt_roundps_epu8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvt_roundps_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvtps2iubs256
-  return _mm256_maskz_ipcvt_roundps_epu8(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm_ipcvttbf16_epi8(__m128bh __A) {
-  // CHECK-LABEL: @test_mm_ipcvttbf16_epi8(
+__m128i test_mm_ipcvtts_bf16_epi8(__m128bh __A) {
+  // CHECK-LABEL: @test_mm_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs128
-  return _mm_ipcvttbf16_epi8(__A);
+  return _mm_ipcvtts_bf16_epi8(__A);
 }
 
-__m128i test_mm_mask_ipcvttbf16_epi8(__m128i __S, __mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvttbf16_epi8(
+__m128i test_mm_mask_ipcvtts_bf16_epi8(__m128i __S, __mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs128
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_mask_ipcvttbf16_epi8(__S, __A, __B);
+  return _mm_mask_ipcvtts_bf16_epi8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvttbf16_epi8(__mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvttbf16_epi8(
+__m128i test_mm_maskz_ipcvtts_bf16_epi8(__mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs128
   // CHECK: zeroinitializer
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_maskz_ipcvttbf16_epi8(__A, __B);
+  return _mm_maskz_ipcvtts_bf16_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvttbf16_epi8(__m256bh __A) {
-  // CHECK-LABEL: @test_mm256_ipcvttbf16_epi8(
+__m256i test_mm256_ipcvtts_bf16_epi8(__m256bh __A) {
+  // CHECK-LABEL: @test_mm256_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs256
-  return _mm256_ipcvttbf16_epi8(__A);
+  return _mm256_ipcvtts_bf16_epi8(__A);
 }
 
-__m256i test_mm256_mask_ipcvttbf16_epi8(__m256i __S, __mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvttbf16_epi8(
+__m256i test_mm256_mask_ipcvtts_bf16_epi8(__m256i __S, __mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs256
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_mask_ipcvttbf16_epi8(__S, __A, __B);
+  return _mm256_mask_ipcvtts_bf16_epi8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvttbf16_epi8(__mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvttbf16_epi8(
+__m256i test_mm256_maskz_ipcvtts_bf16_epi8(__mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvtts_bf16_epi8(
   // CHECK: @llvm.x86.avx10.vcvttbf162ibs256
   // CHECK: zeroinitializer
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_maskz_ipcvttbf16_epi8(__A, __B);
+  return _mm256_maskz_ipcvtts_bf16_epi8(__A, __B);
 }
 
-__m128i test_mm_ipcvttbf16_epu8(__m128bh __A) {
-  // CHECK-LABEL: @test_mm_ipcvttbf16_epu8(
+__m128i test_mm_ipcvtts_bf16_epu8(__m128bh __A) {
+  // CHECK-LABEL: @test_mm_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs128
-  return _mm_ipcvttbf16_epu8(__A);
+  return _mm_ipcvtts_bf16_epu8(__A);
 }
 
-__m128i test_mm_mask_ipcvttbf16_epu8(__m128i __S, __mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvttbf16_epu8(
+__m128i test_mm_mask_ipcvtts_bf16_epu8(__m128i __S, __mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs128
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_mask_ipcvttbf16_epu8(__S, __A, __B);
+  return _mm_mask_ipcvtts_bf16_epu8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvttbf16_epu8(__mmask8 __A, __m128bh __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvttbf16_epu8(
+__m128i test_mm_maskz_ipcvtts_bf16_epu8(__mmask8 __A, __m128bh __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs128
   // CHECK: zeroinitializer
   // CHECK: select <8 x i1> %{{.*}}, <8 x i16> %{{.*}}, <8 x i16> %{{.*}}
-  return _mm_maskz_ipcvttbf16_epu8(__A, __B);
+  return _mm_maskz_ipcvtts_bf16_epu8(__A, __B);
 }
 
-__m256i test_mm256_ipcvttbf16_epu8(__m256bh __A) {
-  // CHECK-LABEL: @test_mm256_ipcvttbf16_epu8(
+__m256i test_mm256_ipcvtts_bf16_epu8(__m256bh __A) {
+  // CHECK-LABEL: @test_mm256_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs256
-  return _mm256_ipcvttbf16_epu8(__A);
+  return _mm256_ipcvtts_bf16_epu8(__A);
 }
 
-__m256i test_mm256_mask_ipcvttbf16_epu8(__m256i __S, __mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvttbf16_epu8(
+__m256i test_mm256_mask_ipcvtts_bf16_epu8(__m256i __S, __mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs256
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_mask_ipcvttbf16_epu8(__S, __A, __B);
+  return _mm256_mask_ipcvtts_bf16_epu8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvttbf16_epu8(__mmask16 __A, __m256bh __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvttbf16_epu8(
+__m256i test_mm256_maskz_ipcvtts_bf16_epu8(__mmask16 __A, __m256bh __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvtts_bf16_epu8(
   // CHECK: @llvm.x86.avx10.vcvttbf162iubs256
   // CHECK: zeroinitializer
   // CHECK: select <16 x i1> %{{.*}}, <16 x i16> %{{.*}}, <16 x i16> %{{.*}}
-  return _mm256_maskz_ipcvttbf16_epu8(__A, __B);
+  return _mm256_maskz_ipcvtts_bf16_epu8(__A, __B);
 }
 
-__m128i test_mm_ipcvttph_epi8(__m128h __A) {
-  // CHECK-LABEL: @test_mm_ipcvttph_epi8(
+__m128i test_mm_ipcvtts_ph_epi8(__m128h __A) {
+  // CHECK-LABEL: @test_mm_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs128
-  return _mm_ipcvttph_epi8(__A);
+  return _mm_ipcvtts_ph_epi8(__A);
 }
 
-__m128i test_mm_mask_ipcvttph_epi8(__m128i __S, __mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvttph_epi8(
+__m128i test_mm_mask_ipcvtts_ph_epi8(__m128i __S, __mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs128
-  return _mm_mask_ipcvttph_epi8(__S, __A, __B);
+  return _mm_mask_ipcvtts_ph_epi8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvttph_epi8(__mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvttph_epi8(
+__m128i test_mm_maskz_ipcvtts_ph_epi8(__mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs128
-  return _mm_maskz_ipcvttph_epi8(__A, __B);
-}
-
-__m256i test_mm256_ipcvttph_epi8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvttph_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs256
-  return _mm256_ipcvttph_epi8(__A);
-}
-
-__m256i test_mm256_mask_ipcvttph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvttph_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs256
-  return _mm256_mask_ipcvttph_epi8(__S, __A, __B);
+  return _mm_maskz_ipcvtts_ph_epi8(__A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvttph_epi8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvttph_epi8(
+__m256i test_mm256_ipcvtts_ph_epi8(__m256h __A) {
+  // CHECK-LABEL: @test_mm256_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs256
-  return _mm256_maskz_ipcvttph_epi8(__A, __B);
+  return _mm256_ipcvtts_ph_epi8(__A);
 }
 
-__m256i test_mm256_ipcvtt_roundph_epi8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtt_roundph_epi8(
+__m256i test_mm256_mask_ipcvtts_ph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs256
-  return _mm256_ipcvtt_roundph_epi8(__A, _MM_FROUND_NO_EXC);
+  return _mm256_mask_ipcvtts_ph_epi8(__S, __A, __B);
 }
 
-__m256i test_mm256_mask_ipcvtt_roundph_epi8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtt_roundph_epi8(
+__m256i test_mm256_maskz_ipcvtts_ph_epi8(__mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvtts_ph_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs256
-  return _mm256_mask_ipcvtt_roundph_epi8(__S, __A, __B, _MM_FROUND_NO_EXC);
+  return _mm256_maskz_ipcvtts_ph_epi8(__A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtt_roundph_epi8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtt_roundph_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttph2ibs256
-  return _mm256_maskz_ipcvtt_roundph_epi8(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm_ipcvttph_epu8(__m128h __A) {
-  // CHECK-LABEL: @test_mm_ipcvttph_epu8(
+__m128i test_mm_ipcvtts_ph_epu8(__m128h __A) {
+  // CHECK-LABEL: @test_mm_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs128
-  return _mm_ipcvttph_epu8(__A);
+  return _mm_ipcvtts_ph_epu8(__A);
 }
 
-__m128i test_mm_mask_ipcvttph_epu8(__m128i __S, __mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvttph_epu8(
+__m128i test_mm_mask_ipcvtts_ph_epu8(__m128i __S, __mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs128
-  return _mm_mask_ipcvttph_epu8(__S, __A, __B);
+  return _mm_mask_ipcvtts_ph_epu8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvttph_epu8(__mmask8 __A, __m128h __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvttph_epu8(
+__m128i test_mm_maskz_ipcvtts_ph_epu8(__mmask8 __A, __m128h __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs128
-  return _mm_maskz_ipcvttph_epu8(__A, __B);
-}
-
-__m256i test_mm256_ipcvttph_epu8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvttph_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs256
-  return _mm256_ipcvttph_epu8(__A);
-}
-
-__m256i test_mm256_mask_ipcvttph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvttph_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs256
-  return _mm256_mask_ipcvttph_epu8(__S, __A, __B);
+  return _mm_maskz_ipcvtts_ph_epu8(__A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvttph_epu8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvttph_epu8(
+__m256i test_mm256_ipcvtts_ph_epu8(__m256h __A) {
+  // CHECK-LABEL: @test_mm256_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs256
-  return _mm256_maskz_ipcvttph_epu8(__A, __B);
+  return _mm256_ipcvtts_ph_epu8(__A);
 }
 
-__m256i test_mm256_ipcvtt_roundph_epu8(__m256h __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtt_roundph_epu8(
+__m256i test_mm256_mask_ipcvtts_ph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs256
-  return _mm256_ipcvtt_roundph_epu8(__A, _MM_FROUND_NO_EXC);
+  return _mm256_mask_ipcvtts_ph_epu8(__S, __A, __B);
 }
 
-__m256i test_mm256_mask_ipcvtt_roundph_epu8(__m256i __S, __mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtt_roundph_epu8(
+__m256i test_mm256_maskz_ipcvtts_ph_epu8(__mmask16 __A, __m256h __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvtts_ph_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs256
-  return _mm256_mask_ipcvtt_roundph_epu8(__S, __A, __B, _MM_FROUND_NO_EXC);
+  return _mm256_maskz_ipcvtts_ph_epu8(__A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtt_roundph_epu8(__mmask16 __A, __m256h __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtt_roundph_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttph2iubs256
-  return _mm256_maskz_ipcvtt_roundph_epu8(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm_ipcvttps_epi8(__m128 __A) {
-  // CHECK-LABEL: @test_mm_ipcvttps_epi8(
+__m128i test_mm_ipcvtts_ps_epi8(__m128 __A) {
+  // CHECK-LABEL: @test_mm_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs128
-  return _mm_ipcvttps_epi8(__A);
+  return _mm_ipcvtts_ps_epi8(__A);
 }
 
-__m128i test_mm_mask_ipcvttps_epi8(__m128i __S, __mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvttps_epi8(
+__m128i test_mm_mask_ipcvtts_ps_epi8(__m128i __S, __mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs128
-  return _mm_mask_ipcvttps_epi8(__S, __A, __B);
+  return _mm_mask_ipcvtts_ps_epi8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvttps_epi8(__mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvttps_epi8(
+__m128i test_mm_maskz_ipcvtts_ps_epi8(__mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs128
-  return _mm_maskz_ipcvttps_epi8(__A, __B);
+  return _mm_maskz_ipcvtts_ps_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvttps_epi8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvttps_epi8(
+__m256i test_mm256_ipcvtts_ps_epi8(__m256 __A) {
+  // CHECK-LABEL: @test_mm256_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs256
-  return _mm256_ipcvttps_epi8(__A);
+  return _mm256_ipcvtts_ps_epi8(__A);
 }
 
-__m256i test_mm256_mask_ipcvttps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvttps_epi8(
+__m256i test_mm256_mask_ipcvtts_ps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs256
-  return _mm256_mask_ipcvttps_epi8(__S, __A, __B);
+  return _mm256_mask_ipcvtts_ps_epi8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvttps_epi8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvttps_epi8(
+__m256i test_mm256_maskz_ipcvtts_ps_epi8(__mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvtts_ps_epi8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs256
-  return _mm256_maskz_ipcvttps_epi8(__A, __B);
+  return _mm256_maskz_ipcvtts_ps_epi8(__A, __B);
 }
 
-__m256i test_mm256_ipcvtt_roundps_epi8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtt_roundps_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs256
-  return _mm256_ipcvtt_roundps_epi8(__A, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_mask_ipcvtt_roundps_epi8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtt_roundps_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs256
-  return _mm256_mask_ipcvtt_roundps_epi8(__S, __A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m256i test_mm256_maskz_ipcvtt_roundps_epi8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtt_roundps_epi8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttps2ibs256
-  return _mm256_maskz_ipcvtt_roundps_epi8(__A, __B, _MM_FROUND_NO_EXC);
-}
-
-__m128i test_mm_ipcvttps_epu8(__m128 __A) {
-  // CHECK-LABEL: @test_mm_ipcvttps_epu8(
+__m128i test_mm_ipcvtts_ps_epu8(__m128 __A) {
+  // CHECK-LABEL: @test_mm_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs128
-  return _mm_ipcvttps_epu8(__A);
+  return _mm_ipcvtts_ps_epu8(__A);
 }
 
-__m128i test_mm_mask_ipcvttps_epu8(__m128i __S, __mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_mask_ipcvttps_epu8(
+__m128i test_mm_mask_ipcvtts_ps_epu8(__m128i __S, __mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_mask_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs128
-  return _mm_mask_ipcvttps_epu8(__S, __A, __B);
+  return _mm_mask_ipcvtts_ps_epu8(__S, __A, __B);
 }
 
-__m128i test_mm_maskz_ipcvttps_epu8(__mmask8 __A, __m128 __B) {
-  // CHECK-LABEL: @test_mm_maskz_ipcvttps_epu8(
+__m128i test_mm_maskz_ipcvtts_ps_epu8(__mmask8 __A, __m128 __B) {
+  // CHECK-LABEL: @test_mm_maskz_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs128
-  return _mm_maskz_ipcvttps_epu8(__A, __B);
-}
-
-__m256i test_mm256_ipcvttps_epu8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvttps_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs256
-  return _mm256_ipcvttps_epu8(__A);
-}
-
-__m256i test_mm256_mask_ipcvttps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvttps_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs256
-  return _mm256_mask_ipcvttps_epu8(__S, __A, __B);
-}
-
-__m256i test_mm256_maskz_ipcvttps_epu8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvttps_epu8(
-  // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs256
-  return _mm256_maskz_ipcvttps_epu8(__A, __B);
+  return _mm_maskz_ipcvtts_ps_epu8(__A, __B);
 }
 
-__m256i test_mm256_ipcvtt_roundps_epu8(__m256 __A) {
-  // CHECK-LABEL: @test_mm256_ipcvtt_roundps_epu8(
+__m256i test_mm256_ipcvtts_ps_epu8(__m256 __A) {
+  // CHECK-LABEL: @test_mm256_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs256
-  return _mm256_ipcvtt_roundps_epu8(__A, _MM_FROUND_NO_EXC);
+  return _mm256_ipcvtts_ps_epu8(__A);
 }
 
-__m256i test_mm256_mask_ipcvtt_roundps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_mask_ipcvtt_roundps_epu8(
+__m256i test_mm256_mask_ipcvtts_ps_epu8(__m256i __S, __mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_mask_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs256
-  return _mm256_mask_ipcvtt_roundps_epu8(__S, __A, __B, _MM_FROUND_NO_EXC);
+  return _mm256_mask_ipcvtts_ps_epu8(__S, __A, __B);
 }
 
-__m256i test_mm256_maskz_ipcvtt_roundps_epu8(__mmask8 __A, __m256 __B) {
-  // CHECK-LABEL: @test_mm256_maskz_ipcvtt_roundps_epu8(
+__m256i test_mm256_maskz_ipcvtts_ps_epu8(__mmask8 __A, __m256 __B) {
+  // CHECK-LABEL: @test_mm256_maskz_ipcvtts_ps_epu8(
   // CHECK: @llvm.x86.avx10.mask.vcvttps2iubs256
-  return _mm256_maskz_ipcvtt_roundps_epu8(__A, __B, _MM_FROUND_NO_EXC);
+  return _mm256_maskz_ipcvtts_ps_epu8(__A, __B);
 }
diff --git a/clang/test/CodeGen/X86/avx10_2satcvtds-builtins-errors.c b/clang/test/CodeGen/X86/avx10_2satcvtds-builtins-errors.c
deleted file mode 100644
index f32dfba60132..000000000000
--- a/clang/test/CodeGen/X86/avx10_2satcvtds-builtins-errors.c
+++ /dev/null
@@ -1,57 +0,0 @@
-// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386-unknown-unknown -target-feature +avx10.2-256 -Wall -Werror -verify
-
-unsigned long long test_mm_cvttssd(unsigned long long __A) {
-  return _mm_cvttssd(__A); // expected-error {{call to undeclared function '_mm_cvttssd'}}
-}
-
-unsigned long long test_mm_cvttsss(unsigned long long __A) {
-  return _mm_cvttsss(__A); // expected-error {{call to undeclared function '_mm_cvttsss'}}
-}
-
-#include <immintrin.h>
-#include <stddef.h>
-
-__m128i test_mm256_cvtts_roundpd_epi32(__m256d A) {
-  return _mm256_cvtts_roundpd_epi32(A, 22); // expected-error {{invalid rounding argument}}
-}
-__m128i test_mm256_mask_cvtts_roundpd_epi32(__m128i W, __mmask8 U, __m256d A) {
-  return _mm256_mask_cvtts_roundpd_epi32(W, U, A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m128i test_mm256_maskz_cvtts_roundpd_epi32(__mmask8 U, __m256d A) {
-  return _mm256_maskz_cvtts_roundpd_epi32(U, A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m128i test_mm256_cvtts_roundpd_epu32(__m256d A) {
-  return _mm256_cvtts_roundpd_epu32(A, 22); // expected-error {{invalid rounding argument}}
-}
-__m128i test_mm256_mask_cvtts_roundpd_epu32(__m128i W, __mmask8 U, __m256d A) {
-  return _mm256_mask_cvtts_roundpd_epu32(W, U, A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m128i test_mm256_maskz_cvtts_roundpd_epu32(__mmask8 U, __m256d A) {
-  return _mm256_maskz_cvtts_roundpd_epu32(U, A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_cvtts_roundps_epi32(__m256 A) {
-  return _mm256_cvtts_roundps_epi32(A, 22); // expected-error {{invalid rounding argument}}
-}
-__m256i test_mm256_mask_cvtts_roundps_epi32(__m256i W, __mmask8 U, __m256 A) {
-  return _mm256_mask_cvtts_roundps_epi32(W, U, A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_cvtts_roundps_epi32(__mmask8 U, __m256 A) {
-  return _mm256_maskz_cvtts_roundps_epi32(U, A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_cvtts_roundps_epu32(__m256 A) {
-  return _mm256_cvtts_roundps_epu32(A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_mask_cvtts_roundps_epu32(__m256i W, __mmask8 U, __m256 A) {
-  return _mm256_mask_cvtts_roundps_epu32(W, U, A, 22); // expected-error {{invalid rounding argument}}
-}
-
-__m256i test_mm256_maskz_cvtts_roundps_epu32(__mmask8 U, __m256 A) {
-  return _mm256_maskz_cvtts_roundps_epu32(U, A, 22); // expected-error {{invalid rounding argument}}
-}
diff --git a/clang/test/CodeGen/X86/avx10_2satcvtds-builtins-x64.c b/clang/test/CodeGen/X86/avx10_2satcvtds-builtins-x64.c
index 00384731a51f..fe6755cc05ae 100644
--- a/clang/test/CodeGen/X86/avx10_2satcvtds-builtins-x64.c
+++ b/clang/test/CodeGen/X86/avx10_2satcvtds-builtins-x64.c
@@ -5,258 +5,186 @@
 
 // scalar
 
-int test_mm_cvttssd_i32(__m128d __A) {
-  // CHECK-LABEL: @test_mm_cvttssd_i32
+int test_mm_cvtts_sd_i32(__m128d __A) {
+  // CHECK-LABEL: @test_mm_cvtts_sd_i32
   // CHECK: @llvm.x86.avx10.vcvttsd2sis
   return _mm_cvtts_roundsd_i32(__A, _MM_FROUND_NO_EXC);
 }
 
-int test_mm_cvttssd_si32(__m128d __A) {
-  // CHECK-LABEL: @test_mm_cvttssd_si32(
+int test_mm_cvtts_sd_si32(__m128d __A) {
+  // CHECK-LABEL: @test_mm_cvtts_sd_si32(
   // CHECK: @llvm.x86.avx10.vcvttsd2sis(<2 x double>
   return _mm_cvtts_roundsd_si32(__A, _MM_FROUND_NO_EXC);
 }
 
-unsigned test_mm_cvttssd_u32(__m128d __A) {
-  // CHECK-LABEL: @test_mm_cvttssd_u32(
+unsigned test_mm_cvtts_sd_u32(__m128d __A) {
+  // CHECK-LABEL: @test_mm_cvtts_sd_u32(
   // CHECK: @llvm.x86.avx10.vcvttsd2usis(<2 x double>
   return _mm_cvtts_roundsd_u32(__A, _MM_FROUND_NO_EXC);
 }
 
-int test_mm_cvttsss_i32(__m128 __A) {
-  // CHECK-LABEL: @test_mm_cvttsss_i32(
+int test_mm_cvtts_ss_i32(__m128 __A) {
+  // CHECK-LABEL: @test_mm_cvtts_ss_i32(
   // CHECK: @llvm.x86.avx10.vcvttss2sis(<4 x float>
   return _mm_cvtts_roundss_i32(__A, _MM_FROUND_NO_EXC);
 }
 
-int test_mm_cvttsss_si32(__m128 __A) {
-  // CHECK-LABEL: @test_mm_cvttsss_si32(
+int test_mm_cvtts_ss_si32(__m128 __A) {
+  // CHECK-LABEL: @test_mm_cvtts_ss_si32(
   // CHECK: @llvm.x86.avx10.vcvttss2sis(<4 x float>
   return _mm_cvtts_roundss_si32(__A, _MM_FROUND_NO_EXC);
 }
 
-unsigned test_mm_cvttsss_u32(__m128 __A) {
-  // CHECK-LABEL: @test_mm_cvttsss_u32(
+unsigned test_mm_cvtts_ss_u32(__m128 __A) {
+  // CHECK-LABEL: @test_mm_cvtts_ss_u32(
   // CHECK: @llvm.x86.avx10.vcvttss2usis(<4 x float>
   return _mm_cvtts_roundss_u32(__A, _MM_FROUND_NO_EXC);
 }
 
 // vector
 // 128 bit
-__m128i test_mm_cvttspd_epi64(__m128d A){
-    // CHECK-LABEL: @test_mm_cvttspd_epi64
+__m128i test_mm_cvtts_pd_epi64(__m128d A){
+    // CHECK-LABEL: @test_mm_cvtts_pd_epi64
     // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.128(<2 x double>
-    return _mm_cvttspd_epi64(A);
+    return _mm_cvtts_pd_epi64(A);
 }
 
-__m128i test_mm_mask_cvttspd_epi64(__m128i W, __mmask8 U, __m128d A){
-    // CHECK-LABEL: @test_mm_mask_cvttspd_epi64
+__m128i test_mm_mask_cvtts_pd_epi64(__m128i W, __mmask8 U, __m128d A){
+    // CHECK-LABEL: @test_mm_mask_cvtts_pd_epi64
     // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.128(<2 x double>
-    return _mm_mask_cvttspd_epi64(W, U,  A);
+    return _mm_mask_cvtts_pd_epi64(W, U,  A);
 }
 
-__m128i test_mm_maskz_cvttspd_epi64(__mmask8 U,__m128d A){
-    // CHECK-LABEL: @test_mm_maskz_cvttspd_epi64
+__m128i test_mm_maskz_cvtts_pd_epi64(__mmask8 U,__m128d A){
+    // CHECK-LABEL: @test_mm_maskz_cvtts_pd_epi64
     // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.128(<2 x double>
-    return _mm_maskz_cvttspd_epi64(U, A);
+    return _mm_maskz_cvtts_pd_epi64(U, A);
 }
 
-__m128i test_mm_cvttspd_epu64(__m128d A){
-    // CHECK-LABEL: @test_mm_cvttspd_epu64
+__m128i test_mm_cvtts_pd_epu64(__m128d A){
+    // CHECK-LABEL: @test_mm_cvtts_pd_epu64
     // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.128(<2 x double>
-    return _mm_cvttspd_epu64(A);
+    return _mm_cvtts_pd_epu64(A);
 }
 
-__m128i test_mm_mask_cvttspd_epu64(__m128i W, __mmask8 U, __m128d A){
-    // CHECK-LABEL: @test_mm_mask_cvttspd_epu64
+__m128i test_mm_mask_cvtts_pd_epu64(__m128i W, __mmask8 U, __m128d A){
+    // CHECK-LABEL: @test_mm_mask_cvtts_pd_epu64
     // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.128(<2 x double>
-    return _mm_mask_cvttspd_epu64(W, U,  A);
+    return _mm_mask_cvtts_pd_epu64(W, U,  A);
 }
 
-__m128i test_mm_maskz_cvttspd_epu64(__mmask8 U,__m128d A){
-    // CHECK-LABEL: @test_mm_maskz_cvttspd_epu64
+__m128i test_mm_maskz_cvtts_pd_epu64(__mmask8 U,__m128d A){
+    // CHECK-LABEL: @test_mm_maskz_cvtts_pd_epu64
     // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.128(<2 x double>
-    return _mm_maskz_cvttspd_epu64(U, A);
+    return _mm_maskz_cvtts_pd_epu64(U, A);
 }
 
 // 256 bit
-__m256i test_mm256_cvttspd_epi64(__m256d A){
-// CHECK-LABEL: @test_mm256_cvttspd_epi64
+__m256i test_mm256_cvtts_pd_epi64(__m256d A){
+// CHECK-LABEL: @test_mm256_cvtts_pd_epi64
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.256(<4 x double>
-    return _mm256_cvttspd_epi64(A);
+    return _mm256_cvtts_pd_epi64(A);
 }
 
-__m256i test_mm256_mask_cvttspd_epi64(__m256i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvttspd_epi64
+__m256i test_mm256_mask_cvtts_pd_epi64(__m256i W,__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_pd_epi64
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.256(<4 x double>
-    return _mm256_mask_cvttspd_epi64(W,U, A);
+    return _mm256_mask_cvtts_pd_epi64(W,U, A);
 }
 
-__m256i test_mm256_maskz_cvttspd_epi64(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvttspd_epi64
+__m256i test_mm256_maskz_cvtts_pd_epi64(__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_pd_epi64
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.256(<4 x double>
-    return _mm256_maskz_cvttspd_epi64(U, A);
+    return _mm256_maskz_cvtts_pd_epi64(U, A);
 }
 
-__m256i test_mm256_cvtts_roundpd_epi64(__m256d A){
-// CHECK-LABEL: @test_mm256_cvtts_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.256(<4 x double>
-    return _mm256_cvtts_roundpd_epi64(A,_MM_FROUND_NEARBYINT );
-}
-
-__m256i test_mm256_mask_cvtts_roundpd_epi64(__m256i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.256(<4 x double>
-    return _mm256_mask_cvtts_roundpd_epi64(W,U,A,_MM_FROUND_NEARBYINT );
-}
-
-__m256i test_mm256_maskz_cvtts_roundpd_epi64(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundpd_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2qqs.round.256(<4 x double>
-    return _mm256_maskz_cvtts_roundpd_epi64(U,A,_MM_FROUND_NEARBYINT );
-}
-
-__m256i test_mm256_cvttspd_epu64(__m256d A){
-// CHECK-LABEL: @test_mm256_cvttspd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.256(<4 x double>
-    return _mm256_cvttspd_epu64(A);
-}
-
-__m256i test_mm256_mask_cvttspd_epu64(__m256i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvttspd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.256(<4 x double>
-    return _mm256_mask_cvttspd_epu64(W,U, A);
-}
-
-__m256i test_mm256_maskz_cvttspd_epu64(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvttspd_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.256(<4 x double>
-    return _mm256_maskz_cvttspd_epu64(U, A);
-}
-
-__m256i test_mm256_cvtts_roundpd_epu64(__m256d A){
-// CHECK-LABEL: @test_mm256_cvtts_roundpd_epu64
+__m256i test_mm256_cvtts_pd_epu64(__m256d A){
+// CHECK-LABEL: @test_mm256_cvtts_pd_epu64
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.256(<4 x double>
-    return _mm256_cvtts_roundpd_epu64(A,_MM_FROUND_NEARBYINT );
+    return _mm256_cvtts_pd_epu64(A);
 }
 
-__m256i test_mm256_mask_cvtts_roundpd_epu64(__m256i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundpd_epu64
+__m256i test_mm256_mask_cvtts_pd_epu64(__m256i W,__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_pd_epu64
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.256(<4 x double>
-    return _mm256_mask_cvtts_roundpd_epu64(W,U,A,_MM_FROUND_NEARBYINT );
+    return _mm256_mask_cvtts_pd_epu64(W,U, A);
 }
 
-__m256i test_mm256_maskz_cvtts_roundpd_epu64(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundpd_epu64
+__m256i test_mm256_maskz_cvtts_pd_epu64(__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_pd_epu64
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2uqqs.round.256(<4 x double>
-    return _mm256_maskz_cvtts_roundpd_epu64(U,A,_MM_FROUND_NEARBYINT );
+    return _mm256_maskz_cvtts_pd_epu64(U, A);
 }
 
 // 128 bit
-__m128i test_mm_cvttsps_epi64(__m128 A){
-    // CHECK-LABEL: @test_mm_cvttsps_epi64
+__m128i test_mm_cvtts_ps_epi64(__m128 A){
+    // CHECK-LABEL: @test_mm_cvtts_ps_epi64
     // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.128(<4 x float>
-    return _mm_cvttsps_epi64(A);
+    return _mm_cvtts_ps_epi64(A);
 }
 
-__m128i test_mm_mask_cvttsps_epi64(__m128i W, __mmask8 U, __m128 A){
-    // CHECK-LABEL: @test_mm_mask_cvttsps_epi64
+__m128i test_mm_mask_cvtts_ps_epi64(__m128i W, __mmask8 U, __m128 A){
+    // CHECK-LABEL: @test_mm_mask_cvtts_ps_epi64
     // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.128(<4 x float>
-    return _mm_mask_cvttsps_epi64(W, U,  A);
+    return _mm_mask_cvtts_ps_epi64(W, U,  A);
 }
 
-__m128i test_mm_maskz_cvttsps_epi64(__mmask8 U,__m128 A){
-    // CHECK-LABEL: @test_mm_maskz_cvttsps_epi64
+__m128i test_mm_maskz_cvtts_ps_epi64(__mmask8 U,__m128 A){
+    // CHECK-LABEL: @test_mm_maskz_cvtts_ps_epi64
     // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.128(<4 x float>
-    return _mm_maskz_cvttsps_epi64(U, A);
+    return _mm_maskz_cvtts_ps_epi64(U, A);
 }
 
-__m128i test_mm_cvttsps_epu64(__m128 A){
-    // CHECK-LABEL: @test_mm_cvttsps_epu64
+__m128i test_mm_cvtts_ps_epu64(__m128 A){
+    // CHECK-LABEL: @test_mm_cvtts_ps_epu64
     // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.128(<4 x float>
-    return _mm_cvttsps_epu64(A);
+    return _mm_cvtts_ps_epu64(A);
 }
 
-__m128i test_mm_mask_cvttsps_epu64(__m128i W, __mmask8 U, __m128 A){
-    // CHECK-LABEL: @test_mm_mask_cvttsps_epu64
+__m128i test_mm_mask_cvtts_ps_epu64(__m128i W, __mmask8 U, __m128 A){
+    // CHECK-LABEL: @test_mm_mask_cvtts_ps_epu64
     // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.128(<4 x float>
-    return _mm_mask_cvttsps_epu64(W, U,  A);
+    return _mm_mask_cvtts_ps_epu64(W, U,  A);
 }
 
-__m128i test_mm_maskz_cvttsps_epu64(__mmask8 U,__m128 A){
-    // CHECK-LABEL: @test_mm_maskz_cvttsps_epu64
+__m128i test_mm_maskz_cvtts_ps_epu64(__mmask8 U,__m128 A){
+    // CHECK-LABEL: @test_mm_maskz_cvtts_ps_epu64
     // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.128(<4 x float>
-    return _mm_maskz_cvttsps_epu64(U, A);
+    return _mm_maskz_cvtts_ps_epu64(U, A);
 }
 
-__m256i test_mm256_cvttsps_epi64(__m128 A){
-// CHECK-LABEL: @test_mm256_cvttsps_epi64
+__m256i test_mm256_cvtts_ps_epi64(__m128 A){
+// CHECK-LABEL: @test_mm256_cvtts_ps_epi64
 // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.256(<4 x float>
-  return _mm256_cvttsps_epi64(A);
+  return _mm256_cvtts_ps_epi64(A);
 }
 
-__m256i test_mm256_mask_cvttsps_epi64(__m256i W,__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_mask_cvttsps_epi64
+__m256i test_mm256_mask_cvtts_ps_epi64(__m256i W,__mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_ps_epi64
 // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.256(<4 x float>
-    return _mm256_mask_cvttsps_epi64(W,U, A);
+    return _mm256_mask_cvtts_ps_epi64(W,U, A);
 }
 
-__m256i test_mm256_maskz_cvttsps_epi64(__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_maskz_cvttsps_epi64
+__m256i test_mm256_maskz_cvtts_ps_epi64(__mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_ps_epi64
 // CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.256(<4 x float>
-    return _mm256_maskz_cvttsps_epi64(U, A);
-}
-
-__m256i test_mm256_cvtts_roundps_epi64(__m128 A){
-// CHECK-LABEL: @test_mm256_cvtts_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.256(<4 x float>
-    return _mm256_cvtts_roundps_epi64(A, _MM_FROUND_NEARBYINT );
-}
-
-__m256i test_mm256_mask_cvtts_roundps_epi64(__m256i W,__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.256(<4 x float>
-    return _mm256_mask_cvtts_roundps_epi64(W,U,A,_MM_FROUND_NEARBYINT );
-}
-
-__m256i test_mm256_maskz_cvtts_roundps_epi64(__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundps_epi64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2qqs.round.256(<4 x float>
-    return _mm256_maskz_cvtts_roundps_epi64(U,A,_MM_FROUND_NEARBYINT );
-}
-
-__m256i test_mm256_cvttsps_epu64(__m128 A){
-// CHECK-LABEL: @test_mm256_cvttsps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.256(<4 x float>
-  return _mm256_cvttsps_epu64(A);
-}
-
-__m256i test_mm256_mask_cvttsps_epu64(__m256i W,__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_mask_cvttsps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.256(<4 x float>
-    return _mm256_mask_cvttsps_epu64(W,U, A);
-}
-
-__m256i test_mm256_maskz_cvttsps_epu64(__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_maskz_cvttsps_epu64
-// CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.256(<4 x float>
-    return _mm256_maskz_cvttsps_epu64(U, A);
+    return _mm256_maskz_cvtts_ps_epi64(U, A);
 }
 
-__m256i test_mm256_cvtts_roundps_epu64(__m128 A){
-// CHECK-LABEL: @test_mm256_cvtts_roundps_epu64
+__m256i test_mm256_cvtts_ps_epu64(__m128 A){
+// CHECK-LABEL: @test_mm256_cvtts_ps_epu64
 // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.256(<4 x float>
-    return _mm256_cvtts_roundps_epu64(A, _MM_FROUND_NEARBYINT );
+  return _mm256_cvtts_ps_epu64(A);
 }
 
-__m256i test_mm256_mask_cvtts_roundps_epu64(__m256i W,__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundps_epu64
+__m256i test_mm256_mask_cvtts_ps_epu64(__m256i W,__mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_ps_epu64
 // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.256(<4 x float>
-    return _mm256_mask_cvtts_roundps_epu64(W,U,A,_MM_FROUND_NEARBYINT );
+    return _mm256_mask_cvtts_ps_epu64(W,U, A);
 }
 
-__m256i test_mm256_maskz_cvtts_roundps_epu64(__mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundps_epu64
+__m256i test_mm256_maskz_cvtts_ps_epu64(__mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_ps_epu64
 // CHECK: @llvm.x86.avx10.mask.vcvttps2uqqs.round.256(<4 x float>
-    return _mm256_maskz_cvtts_roundps_epu64(U,A,_MM_FROUND_NEARBYINT );
+    return _mm256_maskz_cvtts_ps_epu64(U, A);
 }
diff --git a/clang/test/CodeGen/X86/avx10_2satcvtds-builtins.c b/clang/test/CodeGen/X86/avx10_2satcvtds-builtins.c
index bb90f6a086fa..b91af7073a55 100644
--- a/clang/test/CodeGen/X86/avx10_2satcvtds-builtins.c
+++ b/clang/test/CodeGen/X86/avx10_2satcvtds-builtins.c
@@ -1,225 +1,150 @@
-// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386 -target-feature +avx10.2-256 -emit-llvm -o - | FileCheck %s --check-prefixes=CHECK,X86
-// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=x86_64 -target-feature +avx10.2-256 -emit-llvm -o - | FileCheck %s  --check-prefixes=CHECK,X64
+// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386 -target-feature +avx10.2-256 -emit-llvm -o - | FileCheck %s --check-prefixes=CHECK
+// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=x86_64 -target-feature +avx10.2-256 -emit-llvm -o - | FileCheck %s  --check-prefixes=CHECK
 
 #include <immintrin.h>
 #include <stddef.h>
 
-__m128i test_mm_cvttspd_epi32(__m128d A){
-// CHECK-LABEL: @test_mm_cvttspd_epi32
+__m128i test_mm_cvtts_pd_epi32(__m128d A){
+// CHECK-LABEL: @test_mm_cvtts_pd_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.128(<2 x double>
-  return _mm_cvttspd_epi32(A);
+  return _mm_cvtts_pd_epi32(A);
 }
 
-__m128i test_mm_mask_cvttspd_epi32(__m128i W, __mmask8 U, __m128d A){
-// CHECK-LABEL: @test_mm_mask_cvttspd_epi32
+__m128i test_mm_mask_cvtts_pd_epi32(__m128i W, __mmask8 U, __m128d A){
+// CHECK-LABEL: @test_mm_mask_cvtts_pd_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.128(<2 x double>
-    return _mm_mask_cvttspd_epi32(W,U,A);
+    return _mm_mask_cvtts_pd_epi32(W,U,A);
 }
 
-__m128i test_mm_maskz_cvttspd_epi32( __mmask8 U, __m128d A){
-// CHECK-LABEL: @test_mm_maskz_cvttspd_epi32(
+__m128i test_mm_maskz_cvtts_pd_epi32( __mmask8 U, __m128d A){
+// CHECK-LABEL: @test_mm_maskz_cvtts_pd_epi32(
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.128(<2 x double>
-    return _mm_maskz_cvttspd_epi32(U,A);
+    return _mm_maskz_cvtts_pd_epi32(U,A);
 }
 
-__m128i test_mm256_cvttspd_epi32(__m256d A){
-// CHECK-LABEL: @test_mm256_cvttspd_epi32
+__m128i test_mm256_cvtts_pd_epi32(__m256d A){
+// CHECK-LABEL: @test_mm256_cvtts_pd_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.256(<4 x double>
-  return _mm256_cvttspd_epi32(A);
+  return _mm256_cvtts_pd_epi32(A);
 }
 
-__m128i test_mm256_mask_cvttspd_epi32(__m128i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvttspd_epi32
+__m128i test_mm256_mask_cvtts_pd_epi32(__m128i W,__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_pd_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.256(<4 x double>
-    return _mm256_mask_cvttspd_epi32(W,U,A);
+    return _mm256_mask_cvtts_pd_epi32(W,U,A);
 }
 
-__m128i test_mm256_maskz_cvttspd_epi32(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvttspd_epi32
+__m128i test_mm256_maskz_cvtts_pd_epi32(__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_pd_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.256(<4 x double>
-    return _mm256_maskz_cvttspd_epi32(U,A);
+    return _mm256_maskz_cvtts_pd_epi32(U,A);
 }
 
-__m128i test_mm256_cvtts_roundpd_epi32(__m256d A){
-// CHECK-LABEL: @test_mm256_cvtts_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.256(<4 x double>
-    return _mm256_cvtts_roundpd_epi32(A, _MM_FROUND_NEARBYINT);
-}
-
-__m128i test_mm256_mask_cvtts_roundpd_epi32(__m128i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.256(<4 x double>
-    return _mm256_mask_cvtts_roundpd_epi32(W,U,A,_MM_FROUND_NEARBYINT);
-}
-
-__m128i test_mm256_maskz_cvtts_roundpd_epi32(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundpd_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2dqs.round.256(<4 x double>
-    return _mm256_maskz_cvtts_roundpd_epi32(U,A,_MM_FROUND_NEARBYINT);
-}
-
-__m128i test_mm_cvttspd_epu32(__m128d A){
-// CHECK-LABEL: @test_mm_cvttspd_epu32
+__m128i test_mm_cvtts_pd_epu32(__m128d A){
+// CHECK-LABEL: @test_mm_cvtts_pd_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.128(<2 x double>
-  return _mm_cvttspd_epu32(A);
+  return _mm_cvtts_pd_epu32(A);
 }
 
-__m128i test_mm_mask_cvttspd_epu32(__m128i W, __mmask8 U, __m128d A){
-// CHECK-LABEL: @test_mm_mask_cvttspd_epu32
+__m128i test_mm_mask_cvtts_pd_epu32(__m128i W, __mmask8 U, __m128d A){
+// CHECK-LABEL: @test_mm_mask_cvtts_pd_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.128(<2 x double>
-    return _mm_mask_cvttspd_epu32(W,U,A);
+    return _mm_mask_cvtts_pd_epu32(W,U,A);
 }
 
-__m128i test_mm_maskz_cvttspd_epu32( __mmask8 U, __m128d A){
-// CHECK-LABEL: @test_mm_maskz_cvttspd_epu32
+__m128i test_mm_maskz_cvtts_pd_epu32( __mmask8 U, __m128d A){
+// CHECK-LABEL: @test_mm_maskz_cvtts_pd_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.128(<2 x double>
-    return _mm_maskz_cvttspd_epu32(U,A);
+    return _mm_maskz_cvtts_pd_epu32(U,A);
 }
 
 
-__m128i test_mm256_cvttspd_epu32(__m256d A){
-// CHECK-LABEL: @test_mm256_cvttspd_epu32
+__m128i test_mm256_cvtts_pd_epu32(__m256d A){
+// CHECK-LABEL: @test_mm256_cvtts_pd_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.256(<4 x double>
-  return _mm256_cvttspd_epu32(A);
+  return _mm256_cvtts_pd_epu32(A);
 }
 
-__m128i test_mm256_mask_cvttspd_epu32(__m128i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvttspd_epu32
+__m128i test_mm256_mask_cvtts_pd_epu32(__m128i W,__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_pd_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.256(<4 x double>
-    return _mm256_mask_cvttspd_epu32(W,U,A);
+    return _mm256_mask_cvtts_pd_epu32(W,U,A);
 }
 
-__m128i test_mm256_maskz_cvttspd_epu32(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvttspd_epu32
+__m128i test_mm256_maskz_cvtts_pd_epu32(__mmask8 U, __m256d A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_pd_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.256(<4 x double>
-    return _mm256_maskz_cvttspd_epu32(U,A);
+    return _mm256_maskz_cvtts_pd_epu32(U,A);
 }
 
-__m128i test_mm256_cvtts_roundpd_epu32(__m256d A){
-// CHECK-LABEL: @test_mm256_cvtts_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.256(<4 x double>
-    return _mm256_cvtts_roundpd_epu32(A, _MM_FROUND_NEARBYINT);
-}
-
-__m128i test_mm256_mask_cvtts_roundpd_epu32(__m128i W,__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.256(<4 x double>
-    return _mm256_mask_cvtts_roundpd_epu32(W,U,A,_MM_FROUND_NEARBYINT);
-}
-
-__m128i test_mm256_maskz_cvtts_roundpd_epu32(__mmask8 U, __m256d A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundpd_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttpd2udqs.round.256(<4 x double>
-    return _mm256_maskz_cvtts_roundpd_epu32(U,A,_MM_FROUND_NEARBYINT);
-}
-
-__m128i test_mm_cvttsps_epi32(__m128 A){
-// CHECK-LABEL: @test_mm_cvttsps_epi32
+__m128i test_mm_cvtts_ps_epi32(__m128 A){
+// CHECK-LABEL: @test_mm_cvtts_ps_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.128(<4 x float>
-  return _mm_cvttsps_epi32(A);
+  return _mm_cvtts_ps_epi32(A);
 }
 
-__m128i test_mm_mask_cvttsps_epi32(__m128i W, __mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm_mask_cvttsps_epi32
+__m128i test_mm_mask_cvtts_ps_epi32(__m128i W, __mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm_mask_cvtts_ps_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.128(<4 x float>
-    return _mm_mask_cvttsps_epi32(W,U,A);
+    return _mm_mask_cvtts_ps_epi32(W,U,A);
 }
 
-__m128i test_mm_maskz_cvttsps_epi32( __mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm_maskz_cvttsps_epi32
+__m128i test_mm_maskz_cvtts_ps_epi32( __mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm_maskz_cvtts_ps_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.128(<4 x float>
-    return _mm_maskz_cvttsps_epi32(U,A);
+    return _mm_maskz_cvtts_ps_epi32(U,A);
 }
 
-__m256i test_mm256_cvttsps_epi32(__m256 A){
-// CHECK-LABEL: @test_mm256_cvttsps_epi32
+__m256i test_mm256_cvtts_ps_epi32(__m256 A){
+// CHECK-LABEL: @test_mm256_cvtts_ps_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.256(<8 x float>
-  return _mm256_cvttsps_epi32(A);
+  return _mm256_cvtts_ps_epi32(A);
 }
 
-__m256i test_mm256_mask_cvttsps_epi32(__m256i W,__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_mask_cvttsps_epi32
+__m256i test_mm256_mask_cvtts_ps_epi32(__m256i W,__mmask8 U, __m256 A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_ps_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.256(<8 x float>
-    return _mm256_mask_cvttsps_epi32(W,U,A);
+    return _mm256_mask_cvtts_ps_epi32(W,U,A);
 }
 
-__m256i test_mm256_maskz_cvttsps_epi32(__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_maskz_cvttsps_epi32
+__m256i test_mm256_maskz_cvtts_ps_epi32(__mmask8 U, __m256 A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_ps_epi32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.256(<8 x float>
-    return _mm256_maskz_cvttsps_epi32(U,A);
+    return _mm256_maskz_cvtts_ps_epi32(U,A);
 }
 
-__m256i test_mm256_cvtts_roundps_epi32(__m256 A){
-// CHECK-LABEL: @test_mm256_cvtts_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.256(<8 x float>
-    return _mm256_cvtts_roundps_epi32(A, _MM_FROUND_NEARBYINT);
-}
-
-__m256i test_mm256_mask_cvtts_roundps_epi32(__m256i W,__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.256(<8 x float>
-    return _mm256_mask_cvtts_roundps_epi32(W,U,A,_MM_FROUND_NEARBYINT);
-}
-
-__m256i test_mm256_maskz_cvtts_roundps_epi32(__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundps_epi32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2dqs.round.256(<8 x float>
-    return _mm256_maskz_cvtts_roundps_epi32(U,A,_MM_FROUND_NEARBYINT);
-}
-
-__m128i test_mm_cvttsps_epu32(__m128 A){
-// CHECK-LABEL: @test_mm_cvttsps_epu32
+__m128i test_mm_cvtts_ps_epu32(__m128 A){
+// CHECK-LABEL: @test_mm_cvtts_ps_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.128(<4 x float>
-  return _mm_cvttsps_epu32(A);
+  return _mm_cvtts_ps_epu32(A);
 }
 
-__m128i test_mm_mask_cvttsps_epu32(__m128i W, __mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm_mask_cvttsps_epu32
+__m128i test_mm_mask_cvtts_ps_epu32(__m128i W, __mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm_mask_cvtts_ps_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.128(<4 x float>
-    return _mm_mask_cvttsps_epu32(W,U,A);
+    return _mm_mask_cvtts_ps_epu32(W,U,A);
 }
 
-__m128i test_mm_maskz_cvttsps_epu32( __mmask8 U, __m128 A){
-// CHECK-LABEL: @test_mm_maskz_cvttsps_epu32
+__m128i test_mm_maskz_cvtts_ps_epu32( __mmask8 U, __m128 A){
+// CHECK-LABEL: @test_mm_maskz_cvtts_ps_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.128(<4 x float>
-    return _mm_maskz_cvttsps_epu32(U,A);
-}
-
-__m256i test_mm256_cvttsps_epu32(__m256 A){
-// CHECK-LABEL: @test_mm256_cvttsps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.256(<8 x float>
-  return _mm256_cvttsps_epu32(A);
-}
-
-__m256i test_mm256_mask_cvttsps_epu32(__m256i W,__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_mask_cvttsps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.256(<8 x float>
-    return _mm256_mask_cvttsps_epu32(W,U,A);
+    return _mm_maskz_cvtts_ps_epu32(U,A);
 }
 
-__m256i test_mm256_maskz_cvttsps_epu32(__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_maskz_cvttsps_epu32
+__m256i test_mm256_cvtts_ps_epu32(__m256 A){
+// CHECK-LABEL: @test_mm256_cvtts_ps_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.256(<8 x float>
-    return _mm256_maskz_cvttsps_epu32(U,A);
+  return _mm256_cvtts_ps_epu32(A);
 }
 
-__m256i test_mm256_cvtts_roundps_epu32(__m256 A){
-// CHECK-LABEL: @test_mm256_cvtts_roundps_epu32
+__m256i test_mm256_mask_cvtts_ps_epu32(__m256i W,__mmask8 U, __m256 A){
+// CHECK-LABEL: @test_mm256_mask_cvtts_ps_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.256(<8 x float>
-    return _mm256_cvtts_roundps_epu32(A, _MM_FROUND_NEARBYINT);
+    return _mm256_mask_cvtts_ps_epu32(W,U,A);
 }
 
-__m256i test_mm256_mask_cvtts_roundps_epu32(__m256i W,__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_mask_cvtts_roundps_epu32
+__m256i test_mm256_maskz_cvtts_ps_epu32(__mmask8 U, __m256 A){
+// CHECK-LABEL: @test_mm256_maskz_cvtts_ps_epu32
 // CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.256(<8 x float>
-    return _mm256_mask_cvtts_roundps_epu32(W,U,A,_MM_FROUND_NEARBYINT);
+    return _mm256_maskz_cvtts_ps_epu32(U,A);
 }
-
-__m256i test_mm256_maskz_cvtts_roundps_epu32(__mmask8 U, __m256 A){
-// CHECK-LABEL: @test_mm256_maskz_cvtts_roundps_epu32
-// CHECK: @llvm.x86.avx10.mask.vcvttps2udqs.round.256(<8 x float>
-    return _mm256_maskz_cvtts_roundps_epu32(U,A,_MM_FROUND_NEARBYINT);
-}
-
-// X64: {{.*}}
-// X86: {{.*}}
diff --git a/clang/test/CodeGen/attr-target-x86.c b/clang/test/CodeGen/attr-target-x86.c
index c92aad633082..e5067c1c3b07 100644
--- a/clang/test/CodeGen/attr-target-x86.c
+++ b/clang/test/CodeGen/attr-target-x86.c
@@ -56,7 +56,7 @@ void f_default2(void) {
 __attribute__((target("avx,      sse4.2,      arch=   ivybridge")))
 void f_avx_sse4_2_ivybridge_2(void) {}
 
-// CHECK: [[f_no_aes_ivybridge]] = {{.*}}"target-cpu"="ivybridge" "target-features"="+avx,+cmov,+crc32,+cx16,+cx8,+f16c,+fsgsbase,+fxsr,+mmx,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-aes,-amx-avx512,-avx10.1-256,-avx10.1-512,-avx10.2-256,-avx10.2-512,-vaes"
+// CHECK: [[f_no_aes_ivybridge]] = {{.*}}"target-cpu"="ivybridge" "target-features"="+avx,+cmov,+crc32,+cx16,+cx8,+f16c,+fsgsbase,+fxsr,+mmx,+pclmul,+popcnt,+rdrnd,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsaveopt,-aes,-vaes"
 __attribute__((target("no-aes, arch=ivybridge")))
 void f_no_aes_ivybridge(void) {}
 
@@ -98,11 +98,11 @@ void f_x86_64_v3(void) {}
 __attribute__((target("arch=x86-64-v4")))
 void f_x86_64_v4(void) {}
 
-// CHECK: [[f_avx10_1_256]] = {{.*}}"target-cpu"="i686" "target-features"="+aes,+avx,+avx10.1-256,+avx2,+avx512bf16,+avx512bitalg,+avx512bw,+avx512cd,+avx512dq,+avx512f,+avx512fp16,+avx512ifma,+avx512vbmi,+avx512vbmi2,+avx512vl,+avx512vnni,+avx512vpopcntdq,+cmov,+crc32,+cx8,+f16c,+fma,+mmx,+pclmul,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+vaes,+vpclmulqdq,+x87,+xsave,-amx-avx512,-avx10.1-512,-avx10.2-512,-evex512"
+// CHECK: [[f_avx10_1_256]] = {{.*}}"target-cpu"="i686" "target-features"="+avx,+avx10.1-256,+avx2,+avx512bf16,+avx512bitalg,+avx512bw,+avx512cd,+avx512dq,+avx512f,+avx512fp16,+avx512ifma,+avx512vbmi,+avx512vbmi2,+avx512vl,+avx512vnni,+avx512vpopcntdq,+cmov,+crc32,+cx8,+f16c,+fma,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,-amx-avx512,-avx10.1-512,-avx10.2-512,-evex512"
 __attribute__((target("avx10.1-256")))
 void f_avx10_1_256(void) {}
 
-// CHECK: [[f_avx10_1_512]] = {{.*}}"target-cpu"="i686" "target-features"="+aes,+avx,+avx10.1-256,+avx10.1-512,+avx2,+avx512bf16,+avx512bitalg,+avx512bw,+avx512cd,+avx512dq,+avx512f,+avx512fp16,+avx512ifma,+avx512vbmi,+avx512vbmi2,+avx512vl,+avx512vnni,+avx512vpopcntdq,+cmov,+crc32,+cx8,+evex512,+f16c,+fma,+mmx,+pclmul,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+vaes,+vpclmulqdq,+x87,+xsave"
+// CHECK: [[f_avx10_1_512]] = {{.*}}"target-cpu"="i686" "target-features"="+avx,+avx10.1-256,+avx10.1-512,+avx2,+avx512bf16,+avx512bitalg,+avx512bw,+avx512cd,+avx512dq,+avx512f,+avx512fp16,+avx512ifma,+avx512vbmi,+avx512vbmi2,+avx512vl,+avx512vnni,+avx512vpopcntdq,+cmov,+crc32,+cx8,+evex512,+f16c,+fma,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave"
 __attribute__((target("avx10.1-512")))
 void f_avx10_1_512(void) {}
 
@@ -112,4 +112,4 @@ void f_prefer_256_bit(void) {}
 
 // CHECK: [[f_no_prefer_256_bit]] = {{.*}}"target-features"="{{.*}}-prefer-256-bit
 __attribute__((target("no-prefer-256-bit")))
-void f_no_prefer_256_bit(void) {}
\ No newline at end of file
+void f_no_prefer_256_bit(void) {}
diff --git a/clang/test/CodeGenCXX/cxx23-p2280r4.cpp b/clang/test/CodeGenCXX/cxx23-p2280r4.cpp
new file mode 100644
index 000000000000..53b00695d9d6
--- /dev/null
+++ b/clang/test/CodeGenCXX/cxx23-p2280r4.cpp
@@ -0,0 +1,28 @@
+// RUN: %clang_cc1 -triple %itanium_abi_triple -std=c++23 %s -emit-llvm -o - | FileCheck %s
+// RUN: %clang_cc1 -triple %itanium_abi_triple -std=c++20 %s -emit-llvm -o - | FileCheck %s
+// RUN: %clang_cc1 -triple %itanium_abi_triple -std=c++17 %s -emit-llvm -o - | FileCheck %s
+
+extern int& s;
+
+// CHECK-LABEL: @_Z4testv()
+// CHECK-NEXT: entry:
+// CHECK-NEXT: [[I:%.*]] = alloca ptr, align {{.*}}
+// CHECK-NEXT: [[X:%.*]] = load ptr, ptr @s, align {{.*}}
+// CHECK-NEXT: store ptr [[X]], ptr [[I]], align {{.*}}
+int& test() {
+  auto &i = s;
+  return i;
+}
+
+// CHECK-LABEL: @_Z1fv(
+// CHECK: [[X1:%.*]] = load ptr, ptr @x, align {{.*}}
+// CHECK-NEXT: store ptr [[X1]]
+// CHECK: [[X2:%.*]] = load ptr, ptr @x, align {{.*}}
+// CHECK-NEXT: store ptr [[X2]]
+// CHECK: [[X3:%.*]] = load ptr, ptr @x, align {{.*}}
+// CHECK-NEXT: store ptr [[X3]]
+int &ff();
+int &x = ff();
+struct A { int& x; };
+struct B { A x[20]; };
+B f() { return {x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x}; }
diff --git a/clang/test/CodeGenCXX/mangle-ms-matrix.cpp b/clang/test/CodeGenCXX/mangle-ms-matrix.cpp
new file mode 100644
index 000000000000..b244aa6e33cf
--- /dev/null
+++ b/clang/test/CodeGenCXX/mangle-ms-matrix.cpp
@@ -0,0 +1,57 @@
+// RUN: %clang_cc1 -fenable-matrix -fms-extensions -fcxx-exceptions -ffreestanding -target-feature +avx -emit-llvm %s -o - -triple=i686-pc-win32 | FileCheck %s
+// RUN: %clang_cc1 -fenable-matrix -fms-extensions -fcxx-exceptions -ffreestanding -target-feature +avx -emit-llvm %s -o - -triple=i686-pc-win32 -fexperimental-new-constant-interpreter | FileCheck %s
+
+typedef float __attribute__((matrix_type(4, 4))) m4x4f;
+typedef float __attribute__((matrix_type(2, 2))) m2x2f;
+
+typedef int __attribute__((matrix_type(4, 4))) m4x4i;
+typedef int __attribute__((matrix_type(2, 2))) m2x2i;
+
+void thow(int i) {
+  switch (i) {
+    case 0: throw m4x4f();
+    // CHECK: ??_R0U?$__matrix@M$03$03@__clang@@@8
+    // CHECK: _CT??_R0U?$__matrix@M$03$03@__clang@@@864
+    // CHECK: _CTA1U?$__matrix@M$03$03@__clang@@
+    // CHECK: _TI1U?$__matrix@M$03$03@__clang@@
+    case 1: throw m2x2f();
+    // CHECK: ??_R0U?$__matrix@M$01$01@__clang@@@8
+    // CHECK: _CT??_R0U?$__matrix@M$01$01@__clang@@@816
+    // CHECK: _CTA1U?$__matrix@M$01$01@__clang@@
+    // CHECK: _TI1U?$__matrix@M$01$01@__clang@@
+    case 2: throw m4x4i();
+    // CHECK: ??_R0U?$__matrix@H$03$03@__clang@@@8
+    // CHECK: _CT??_R0U?$__matrix@H$03$03@__clang@@@864
+    // CHECK: _CTA1U?$__matrix@H$03$03@__clang@@
+    // CHECK: _TI1U?$__matrix@H$03$03@__clang@@
+    case 3: throw m2x2i();
+    // CHECK: ??_R0U?$__matrix@H$01$01@__clang@@@8
+    // CHECK: _CT??_R0U?$__matrix@H$01$01@__clang@@@816
+    // CHECK: _CTA1U?$__matrix@H$01$01@__clang@@
+    // CHECK: _TI1U?$__matrix@H$01$01@__clang@@
+  }
+}
+
+void foo44f(m4x4f) {}
+// CHECK: define dso_local void @"?foo44f@@YAXU?$__matrix@M$03$03@__clang@@@Z"
+
+m4x4f rfoo44f() { return m4x4f(); }
+// CHECK: define dso_local noundef <16 x float> @"?rfoo44f@@YAU?$__matrix@M$03$03@__clang@@XZ"
+
+void foo22f(m2x2f) {}
+// CHECK: define dso_local void @"?foo22f@@YAXU?$__matrix@M$01$01@__clang@@@Z"
+
+m2x2f rfoo22f() { return m2x2f(); }
+// CHECK: define dso_local noundef <4 x float> @"?rfoo22f@@YAU?$__matrix@M$01$01@__clang@@XZ"
+
+void foo44i(m4x4i) {}
+// CHECK: define dso_local void @"?foo44i@@YAXU?$__matrix@H$03$03@__clang@@@Z"
+
+m4x4i rfoo44i() { return m4x4i(); }
+// CHECK: define dso_local noundef <16 x i32> @"?rfoo44i@@YAU?$__matrix@H$03$03@__clang@@XZ"
+
+void foo22i(m2x2i) {}
+// CHECK: define dso_local void @"?foo22i@@YAXU?$__matrix@H$01$01@__clang@@@Z"
+
+m2x2i rfoo22i() { return m2x2i(); }
+// CHECK: define dso_local noundef <4 x i32> @"?rfoo22i@@YAU?$__matrix@H$01$01@__clang@@XZ"
\ No newline at end of file
diff --git a/clang/test/CodeGenCoroutines/pr134409.cpp b/clang/test/CodeGenCoroutines/pr134409.cpp
new file mode 100644
index 000000000000..142962d44ede
--- /dev/null
+++ b/clang/test/CodeGenCoroutines/pr134409.cpp
@@ -0,0 +1,43 @@
+// An end-to-end test to make sure coroutine passes are added for thinlto.
+// REQUIRES: x86-registered-target
+// RUN: %clang_cc1 -triple x86_64-unknown-linux-gnu -std=c++23 -ffat-lto-objects -flto=thin -emit-llvm %s -O3 -o - \
+// RUN:  | FileCheck %s
+
+#include "Inputs/coroutine.h"
+
+class BasicCoroutine {
+public:
+    struct Promise {
+        BasicCoroutine get_return_object() { return BasicCoroutine {}; }
+
+        void unhandled_exception() noexcept { }
+
+        void return_void() noexcept { }
+
+        std::suspend_never initial_suspend() noexcept { return {}; }
+        std::suspend_never final_suspend() noexcept { return {}; }
+    };
+    using promise_type = Promise;
+};
+
+// COM: match the embedded module, so we don't match something in it by accident.
+// CHECK: @llvm.embedded.object = {{.*}}
+// CHECK: @llvm.compiler.used = {{.*}}
+
+BasicCoroutine coro() {
+// CHECK: define {{.*}} void @_Z4corov() {{.*}} {
+// CHECK-NEXT: entry:
+// CHECK-NEXT: ret void
+// CHECK-NEXT: }
+    co_return;
+}
+
+int main() {
+// CHECK: define {{.*}} i32 @main() {{.*}} {
+// CHECK-NEXT: entry:
+// CHECK-NEXT: tail call void @_Z4corov()
+// CHECK-NEXT: ret i32 0
+// CHECK-NEXT: }
+    coro();
+}
+
diff --git a/clang/test/Driver/arm-mfpu.c b/clang/test/Driver/arm-mfpu.c
index 640e1b35c84b..a9bdcd598516 100644
--- a/clang/test/Driver/arm-mfpu.c
+++ b/clang/test/Driver/arm-mfpu.c
@@ -356,10 +356,8 @@
 // CHECK-HF-DAG: "-target-cpu" "arm1176jzf-s"
 
 // RUN: %clang -target armv7-apple-darwin -x assembler %s -### -c 2>&1 \
-// RUN:   | FileCheck --check-prefix=ASM-NEON %s
-// RUN: %clang -target armv7-windows -x assembler %s -### -c 2>&1 \
-// RUN:   | FileCheck --check-prefix=ASM-NEON %s
-// ASM-NEON: "-target-feature" "+neon"
+// RUN:   | FileCheck --check-prefix=ASM %s
+// ASM-NOT: -target-feature
 
 // RUN: %clang -target armv8-linux-gnueabi -mfloat-abi=soft -mfpu=none %s -### -c 2>&1 \
 // RUN:   | FileCheck --check-prefix=CHECK-SOFT-ABI-FP %s
diff --git a/clang/test/Driver/hexagon-cpu-default.c b/clang/test/Driver/hexagon-cpu-default.c
new file mode 100644
index 000000000000..31fb839f2165
--- /dev/null
+++ b/clang/test/Driver/hexagon-cpu-default.c
@@ -0,0 +1,4 @@
+// CHECK: "-target-cpu" "hexagonv68"
+
+// RUN: %clang -c %s -### --target=hexagon-unknown-elf \
+// RUN:  2>&1 | FileCheck  %s
diff --git a/clang/test/Driver/hexagon-toolchain-elf.c b/clang/test/Driver/hexagon-toolchain-elf.c
index be812dda40d5..de2ebfeeda26 100644
--- a/clang/test/Driver/hexagon-toolchain-elf.c
+++ b/clang/test/Driver/hexagon-toolchain-elf.c
@@ -555,6 +555,7 @@
 // RUN:   -ccc-install-dir %S/Inputs/hexagon_tree/Tools/bin \
 // RUN:   -mcpu=hexagonv60 \
 // RUN:   -fuse-ld=lld %s 2>&1 | FileCheck -check-prefix=CHECK382 %s
+// CHECK382:          "--eh-frame-hdr
 // CHECK382-NOT:      "-march=
 // CHECK382-NOT:      "-mcpu=
 // -----------------------------------------------------------------------------
diff --git a/clang/test/Driver/hexagon-toolchain-linux.c b/clang/test/Driver/hexagon-toolchain-linux.c
index 6f7f3b20f914..e791353cca07 100644
--- a/clang/test/Driver/hexagon-toolchain-linux.c
+++ b/clang/test/Driver/hexagon-toolchain-linux.c
@@ -127,6 +127,7 @@
 // RUN:    --target=hexagon-unknown-linux-musl %s -### 2>&1 \
 // RUN:    | FileCheck -check-prefix=CHECK011 %s
 // CHECK011:   InstalledDir: [[INSTALLED_DIR:.+]]
+// CHECK011:   "--eh-frame-hdr"
 // CHECK011:   crt1.o
 // CHECK011-NOT:  "-lunwind"
 // CHECK011-NOT:  "-lgcc_eh"
diff --git a/clang/test/Driver/print-enabled-extensions/aarch64-fujitsu-monaka.c b/clang/test/Driver/print-enabled-extensions/aarch64-fujitsu-monaka.c
index a80d0f5c79ec..29e9682d5870 100644
--- a/clang/test/Driver/print-enabled-extensions/aarch64-fujitsu-monaka.c
+++ b/clang/test/Driver/print-enabled-extensions/aarch64-fujitsu-monaka.c
@@ -28,6 +28,8 @@
 // CHECK-NEXT:     FEAT_FP16                                              Enable half-precision floating-point data processing
 // CHECK-NEXT:     FEAT_FP8                                               Enable FP8 instructions
 // CHECK-NEXT:     FEAT_FP8DOT2                                           Enable FP8 2-way dot instructions
+// CHECK-NEXT:     FEAT_FP8DOT4                                           Enable FP8 4-way dot instructions
+// CHECK-NEXT:     FEAT_FP8FMA                                            Enable Armv9.5-A FP8 multiply-add instructions
 // CHECK-NEXT:     FEAT_FPAC                                              Enable Armv8.3-A Pointer Authentication Faulting enhancement
 // CHECK-NEXT:     FEAT_FRINTTS                                           Enable FRInt[32|64][Z|X] instructions that round a floating-point number to an integer (in FP format) forcing it to fit into a 32- or 64-bit int
 // CHECK-NEXT:     FEAT_FlagM                                             Enable Armv8.4-A Flag Manipulation instructions
diff --git a/clang/test/Driver/systemz-march.c b/clang/test/Driver/systemz-march.c
index 93a11c6c9c01..8922db9f2d5d 100644
--- a/clang/test/Driver/systemz-march.c
+++ b/clang/test/Driver/systemz-march.c
@@ -15,6 +15,7 @@
 // RUN: %clang -target s390x -### -S -emit-llvm -march=arch13 %s 2>&1 | FileCheck --check-prefix=CHECK-ARCH13 %s
 // RUN: %clang -target s390x -### -S -emit-llvm -march=z16 %s 2>&1 | FileCheck --check-prefix=CHECK-Z16 %s
 // RUN: %clang -target s390x -### -S -emit-llvm -march=arch14 %s 2>&1 | FileCheck --check-prefix=CHECK-ARCH14 %s
+// RUN: %clang -target s390x -### -S -emit-llvm -march=z17 %s 2>&1 | FileCheck --check-prefix=CHECK-Z17 %s
 // RUN: %clang -target s390x -### -S -emit-llvm -march=arch15 %s 2>&1 | FileCheck --check-prefix=CHECK-ARCH15 %s
 
 // CHECK-Z9: error: unknown target CPU 'z9'
@@ -32,6 +33,7 @@
 // CHECK-ARCH13: "-target-cpu" "arch13"
 // CHECK-Z16: "-target-cpu" "z16"
 // CHECK-ARCH14: "-target-cpu" "arch14"
+// CHECK-Z17: "-target-cpu" "z17"
 // CHECK-ARCH15: "-target-cpu" "arch15"
 
 int x;
diff --git a/clang/test/Misc/target-invalid-cpu-note/systemz.c b/clang/test/Misc/target-invalid-cpu-note/systemz.c
index b70173f5feec..021c280d5319 100644
--- a/clang/test/Misc/target-invalid-cpu-note/systemz.c
+++ b/clang/test/Misc/target-invalid-cpu-note/systemz.c
@@ -20,4 +20,5 @@
 // CHECK-SAME: {{^}}, arch14
 // CHECK-SAME: {{^}}, z16
 // CHECK-SAME: {{^}}, arch15
+// CHECK-SAME: {{^}}, z17
 // CHECK-SAME: {{$}}
diff --git a/clang/test/Modules/MixedModulePrecompile.cpp b/clang/test/Modules/MixedModulePrecompile.cpp
new file mode 100644
index 000000000000..473817ef71de
--- /dev/null
+++ b/clang/test/Modules/MixedModulePrecompile.cpp
@@ -0,0 +1,63 @@
+// Tests mixed usage of precompiled headers and modules.
+//
+// RUN: rm -rf %t
+// RUN: mkdir -p %t
+// RUN: split-file %s %t
+//
+// RUN: %clang_cc1 -std=c++20 -x c++-header -emit-pch %t/a.hpp \
+// RUN: -o %t/a.pch
+
+// RUN: %clang_cc1 -std=c++20 -emit-module-interface %t/Part1.cppm \
+// RUN: -include-pch %t/a.pch -o %t/Part1.pcm
+// RUN: %clang_cc1 -std=c++20 -emit-module-interface %t/Part2.cppm \
+// RUN: -include-pch %t/a.pch -o %t/Part2.pcm
+// RUN: %clang_cc1 -std=c++20 -emit-module-interface %t/Part3.cppm \
+// RUN: -include-pch %t/a.pch -o %t/Part3.pcm
+// RUN: %clang_cc1 -std=c++20 -emit-module-interface %t/Part4.cppm \
+// RUN: -include-pch %t/a.pch -o %t/Part4.pcm
+
+// RUN: %clang_cc1 -std=c++20 -emit-module-interface \
+// RUN: -fmodule-file=mod:part1=%t/Part1.pcm \
+// RUN: -fmodule-file=mod:part2=%t/Part2.pcm \
+// RUN: -fmodule-file=mod:part3=%t/Part3.pcm \
+// RUN: -fmodule-file=mod:part4=%t/Part4.pcm \
+// RUN: %t/Mod.cppm \
+// RUN: -include-pch %t/a.pch -o %t/Mod.pcm
+
+// RUN: %clang_cc1 -std=c++20 -emit-obj \
+// RUN: -main-file-name Mod.cppm \
+// RUN: -fmodule-file=mod:part1=%t/Part1.pcm \
+// RUN: -fmodule-file=mod:part2=%t/Part2.pcm \
+// RUN: -fmodule-file=mod:part3=%t/Part3.pcm \
+// RUN: -fmodule-file=mod:part4=%t/Part4.pcm \
+// RUN: -x pcm %t/Mod.pcm \
+// RUN: -include-pch %t/a.pch -o %t/Mod.o
+
+
+//--- a.hpp
+#pragma once
+
+class a {
+  virtual ~a();
+  a() {}
+};
+
+//--- Part1.cppm
+export module mod:part1;
+
+//--- Part2.cppm
+export module mod:part2;
+
+//--- Part3.cppm
+export module mod:part3;
+
+//--- Part4.cppm
+export module mod:part4;
+
+//--- Mod.cppm
+export module mod;
+export import :part1;
+export import :part2;
+export import :part3;
+export import :part4;
+
diff --git a/clang/test/Parser/recovery.cpp b/clang/test/Parser/recovery.cpp
index 2fce67a52c6b..261f5dc99bad 100644
--- a/clang/test/Parser/recovery.cpp
+++ b/clang/test/Parser/recovery.cpp
@@ -222,3 +222,21 @@ void k() {
   func(1, ); // expected-error {{expected expression}}
 }
 }
+
+namespace GH136254 {
+
+void call() {
+  [a(42, )]() {} (); // expected-error {{expected expression}}
+
+  int *b = new int(42, ); // expected-error {{expected expression}}
+
+  struct S {
+    int c;
+
+    S() : c(42, ) {} // expected-error {{expected expression}}
+  };
+
+  int d(42, ); // expected-error {{expected expression}}
+}
+
+}
diff --git a/clang/test/Preprocessor/arm-target-features.c b/clang/test/Preprocessor/arm-target-features.c
index ecf9d7eb5c19..27eb9a322d7c 100644
--- a/clang/test/Preprocessor/arm-target-features.c
+++ b/clang/test/Preprocessor/arm-target-features.c
@@ -132,30 +132,6 @@
 // CHECK-V7VE-DEFAULT-ABI-SOFT: #define __ARM_ARCH_EXT_IDIV__ 1
 // CHECK-V7VE-DEFAULT-ABI-SOFT: #define __ARM_FP 0xc
 
-// RUN: %clang -target x86_64-apple-macosx10.10 -arch armv7 -x c -E -dM %s -o - | FileCheck -match-full-lines --check-prefix=CHECK-DARWIN-V7 %s
-// CHECK-DARWIN-V7: #define __ARMEL__ 1
-// CHECK-DARWIN-V7: #define __ARM_ARCH 7
-// CHECK-DARWIN-V7: #define __ARM_ARCH_7A__ 1
-// CHECK-DARWIN-V7-NOT: __ARM_FEATURE_CRC32
-// CHECK-DARWIN-V7-NOT: __ARM_FEATURE_NUMERIC_MAXMIN
-// CHECK-DARWIN-V7-NOT: __ARM_FEATURE_DIRECTED_ROUNDING
-// CHECK-DARWIN-V7: #define __ARM_FP 0xc
-// CHECK-DARWIN-V7: #define __ARM_NEON 1
-// CHECK-DARWIN-V7: #define __ARM_NEON_FP 0x4
-// CHECK-DARWIN-V7: #define __ARM_NEON__ 1
-
-// RUN: %clang -target armv7-windows -x c -E -dM %s -o - | FileCheck -match-full-lines --check-prefix=CHECK-WINDOWS-V7 %s
-// CHECK-WINDOWS-V7: #define __ARMEL__ 1
-// CHECK-WINDOWS-V7: #define __ARM_ARCH 7
-// CHECK-WINDOWS-V7: #define __ARM_ARCH_7A__ 1
-// CHECK-WINDOWS-V7-NOT: __ARM_FEATURE_CRC32
-// CHECK-WINDOWS-V7-NOT: __ARM_FEATURE_NUMERIC_MAXMIN
-// CHECK-WINDOWS-V7-NOT: __ARM_FEATURE_DIRECTED_ROUNDING
-// CHECK-WINDOWS-V7: #define __ARM_FP 0xe
-// CHECK-WINDOWS-V7: #define __ARM_NEON 1
-// CHECK-WINDOWS-V7: #define __ARM_NEON_FP 0x6
-// CHECK-WINDOWS-V7: #define __ARM_NEON__ 1
-
 // RUN: %clang -target x86_64-apple-macosx10.10 -arch armv7s -x c -E -dM %s -o - | FileCheck -match-full-lines --check-prefix=CHECK-V7S %s
 // CHECK-V7S: #define __ARMEL__ 1
 // CHECK-V7S: #define __ARM_ARCH 7
@@ -164,9 +140,6 @@
 // CHECK-V7S-NOT: __ARM_FEATURE_NUMERIC_MAXMIN
 // CHECK-V7S-NOT: __ARM_FEATURE_DIRECTED_ROUNDING
 // CHECK-V7S: #define __ARM_FP 0xe
-// CHECK-V7S: #define __ARM_NEON 1
-// CHECK-V7S: #define __ARM_NEON_FP 0x6
-// CHECK-V7S: #define __ARM_NEON__ 1
 
 // RUN: %clang -target arm-arm-none-eabi -march=armv7-m -mfloat-abi=soft -x c -E -dM %s | FileCheck -match-full-lines --check-prefix=CHECK-VFP-FP %s
 // RUN: %clang -target arm-arm-none-eabi -march=armv7-m -mfloat-abi=softfp -x c -E -dM %s | FileCheck -match-full-lines --check-prefix=CHECK-VFP-FP %s
diff --git a/clang/test/Preprocessor/embed_constexpr.c b/clang/test/Preprocessor/embed_constexpr.c
new file mode 100644
index 000000000000..e444dfec158b
--- /dev/null
+++ b/clang/test/Preprocessor/embed_constexpr.c
@@ -0,0 +1,21 @@
+// RUN: %clang_cc1 %s -fsyntax-only --embed-dir=%S/Inputs -verify -std=c23
+
+static constexpr unsigned char data[] = {
+#embed "big_char.txt"
+};
+
+static constexpr char data1[] = {
+#embed "big_char.txt" // expected-error {{constexpr initializer evaluates to 255 which is not exactly representable in type 'const char'}}
+};
+
+static constexpr int data2[] = {
+#embed "big_char.txt"
+};
+
+static constexpr unsigned data3[] = {
+#embed "big_char.txt" suffix(, -1) // expected-error {{constexpr initializer evaluates to -1 which is not exactly representable in type 'const unsigned int'}}
+};
+
+static constexpr int data4[] = {
+#embed "big_char.txt" suffix(, -1)
+};
diff --git a/clang/test/Preprocessor/predefined-arch-macros.c b/clang/test/Preprocessor/predefined-arch-macros.c
index f267f1759cdb..2d17891071aa 100644
--- a/clang/test/Preprocessor/predefined-arch-macros.c
+++ b/clang/test/Preprocessor/predefined-arch-macros.c
@@ -4394,6 +4394,9 @@
 // RUN: %clang -march=arch15 -E -dM %s -o - 2>&1 \
 // RUN:     -target s390x-unknown-linux \
 // RUN:   | FileCheck -match-full-lines %s -check-prefix=CHECK_SYSTEMZ_ARCH15
+// RUN: %clang -march=z17 -E -dM %s -o - 2>&1 \
+// RUN:     -target s390x-unknown-linux \
+// RUN:   | FileCheck -match-full-lines %s -check-prefix=CHECK_SYSTEMZ_ARCH15
 // CHECK_SYSTEMZ_ARCH15: #define __ARCH__ 15
 // CHECK_SYSTEMZ_ARCH15: #define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 1
 // CHECK_SYSTEMZ_ARCH15: #define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 1
diff --git a/clang/test/Sema/GH126231.cpp b/clang/test/Sema/GH126231.cpp
new file mode 100644
index 000000000000..d10fc79c3b62
--- /dev/null
+++ b/clang/test/Sema/GH126231.cpp
@@ -0,0 +1,18 @@
+// RUN: %clang_cc1 -std=c++20 -Wno-ignored-attributes -Wno-unused-value -verify %s
+// expected-no-diagnostics
+namespace std {
+template <class T>
+constexpr const T& as_const(T&) noexcept;
+
+// We need two declarations to see the error for some reason.
+template <class T> void as_const(const T&&) noexcept = delete;
+template <class T> void as_const(const T&&) noexcept;
+}
+
+namespace GH126231 {
+
+void test() {
+    int a = 1;
+    std::as_const(a);
+}
+}
diff --git a/clang/test/Sema/warn-cast-function-type-win.c b/clang/test/Sema/warn-cast-function-type-win.c
new file mode 100644
index 000000000000..4e7ba33b258d
--- /dev/null
+++ b/clang/test/Sema/warn-cast-function-type-win.c
@@ -0,0 +1,36 @@
+// RUN: %clang_cc1 %s -triple x86_64-windows -fsyntax-only -Wcast-function-type -Wno-cast-function-type-strict -verify=windows
+// RUN: %clang_cc1 %s -triple x86_64-windows -fsyntax-only -Wcast-function-type -Wno-cast-function-type-strict -x c++ -verify=windows
+// RUN: %clang_cc1 %s -triple x86_64-pc-linux -fsyntax-only -Wcast-function-type -Wno-cast-function-type-strict -verify=linux
+// RUN: %clang_cc1 %s -triple x86_64-pc-linux -fsyntax-only -Wcast-function-type -Wno-cast-function-type-strict -x c++ -verify=linux,linux-cpp
+// RUN: %clang_cc1 %s -triple x86_64-windows -fsyntax-only -Wcast-function-type -Wcast-function-type-strict -x c++ -verify=strict
+// windows-no-diagnostics
+
+// On Windows targets, this is expected to compile fine, and on non-Windows
+// targets, this should diagnose the mismatch. This is to allow for idiomatic
+// use of GetProcAddress, similar to what we do for dlsym. On non-Windows
+// targets, this should be diagnosed.
+typedef int (*FARPROC1)();
+typedef unsigned long long (*FARPROC2)();
+
+FARPROC1 GetProcAddress1(void);
+FARPROC2 GetProcAddress2(void);
+
+typedef int (*test1_type)(int);
+typedef float(*test2_type)();
+
+void test(void) {
+  // This does not diagnose on Linux in C mode because FARPROC1 has a matching
+  // return type to test1_type, but FARPROC1 has no prototype and so checking
+  // is disabled for further compatibility issues. In C++ mode, all functions
+  // have a prototype and so the check happens.
+  test1_type t1 = (test1_type)GetProcAddress1();
+  // linux-cpp-warning@-1 {{cast from 'FARPROC1' (aka 'int (*)()') to 'test1_type' (aka 'int (*)(int)') converts to incompatible function type}}
+  // strict-warning@-2 {{cast from 'FARPROC1' (aka 'int (*)()') to 'test1_type' (aka 'int (*)(int)') converts to incompatible function type}}
+  
+  // This case is diagnosed in both C and C++ modes on Linux because the return
+  // type of FARPROC2 does not match the return type of test2_type.
+  test2_type t2 = (test2_type)GetProcAddress2();
+  // linux-warning@-1 {{cast from 'FARPROC2' (aka 'unsigned long long (*)()') to 'test2_type' (aka 'float (*)()') converts to incompatible function type}}
+  // strict-warning@-2 {{cast from 'FARPROC2' (aka 'unsigned long long (*)()') to 'test2_type' (aka 'float (*)()') converts to incompatible function type}}
+}
+
diff --git a/clang/test/SemaCUDA/dtor.cu b/clang/test/SemaCUDA/dtor.cu
new file mode 100644
index 000000000000..cc37837e7079
--- /dev/null
+++ b/clang/test/SemaCUDA/dtor.cu
@@ -0,0 +1,104 @@
+// RUN: %clang_cc1 %s -std=c++20 -fsyntax-only -verify=host
+// RUN: %clang_cc1 %s -std=c++20 -fcuda-is-device -fsyntax-only -verify=dev
+
+// host-no-diagnostics
+
+#include "Inputs/cuda.h"
+
+// Virtual dtor ~B() of explicit instantiation B<float> must
+// be emitted, which causes host_fun() called.
+namespace ExplicitInstantiationExplicitDevDtor {
+void host_fun() // dev-note {{'host_fun' declared here}}
+{}
+
+template <unsigned>
+constexpr void hd_fun() {
+  host_fun(); // dev-error {{reference to __host__ function 'host_fun' in __host__ __device__ function}}
+}
+
+struct A {
+  constexpr ~A() { // dev-note {{called by '~B'}}
+     hd_fun<8>(); // dev-note {{called by '~A'}}
+  }
+};
+
+template <typename T>
+struct B {
+public:
+  virtual __device__ ~B() = default;
+  A _a;
+};
+
+template class B<float>;
+}
+
+// The implicit host/device attrs of virtual dtor ~B() should be
+// conservatively inferred, where constexpr member dtor's should
+// not be considered device since they may call host functions.
+// Therefore B<float>::~B() should not have implicit device attr.
+// However C<float>::~C() should have implicit device attr since
+// it is trivial.
+namespace ExplicitInstantiationDtorNoAttr {
+void host_fun()
+{}
+
+template <unsigned>
+constexpr void hd_fun() {
+  host_fun();
+}
+
+struct A {
+  constexpr ~A() {
+     hd_fun<8>();
+  }
+};
+
+template <typename T>
+struct B {
+public:
+  virtual ~B() = default;
+  A _a;
+};
+
+template <typename T>
+struct C {
+public:
+  virtual ~C() = default;
+};
+
+template class B<float>;
+template class C<float>;
+__device__ void foo() {
+  C<float> x;
+}
+}
+
+// Dtors of implicit template class instantiation are not
+// conservatively inferred because the invalid usage can
+// be diagnosed.
+namespace ImplicitInstantiation {
+void host_fun() // dev-note {{'host_fun' declared here}}
+{}
+
+template <unsigned>
+constexpr void hd_fun() {
+  host_fun(); // dev-error {{reference to __host__ function 'host_fun' in __host__ __device__ function}}
+}
+
+struct A {
+  constexpr ~A() { // dev-note {{called by '~B'}}
+     hd_fun<8>(); // dev-note {{called by '~A'}}
+  }
+};
+
+template <typename T>
+struct B {
+public:
+  ~B() = default; // dev-note {{called by 'foo'}}
+  A _a;
+};
+
+__device__ void foo() {
+  B<float> x;
+}
+}
diff --git a/clang/test/SemaCXX/builtin-object-size-cxx14.cpp b/clang/test/SemaCXX/builtin-object-size-cxx14.cpp
index b7c6f6be01f5..fdd3cb7af088 100644
--- a/clang/test/SemaCXX/builtin-object-size-cxx14.cpp
+++ b/clang/test/SemaCXX/builtin-object-size-cxx14.cpp
@@ -1,5 +1,7 @@
 // RUN: %clang_cc1 -fsyntax-only -verify=expected,cxx14 -std=c++14 %s
 // RUN: %clang_cc1 -fsyntax-only -verify -std=c++2a %s
+// RUN: %clang_cc1 -fsyntax-only -verify -std=c++2b %s
+
 
 typedef __SIZE_TYPE__ size_t;
 
@@ -119,3 +121,13 @@ constexpr int bos_new() { // cxx14-error {{constant expression}}
   void *p = new int; // cxx14-note {{until C++20}}
   return __builtin_object_size(p, 0);
 }
+
+
+namespace GH129397 {
+
+struct incomplete;
+void test(incomplete &ref) {
+  __builtin_object_size(&ref, 1);
+}
+
+}
diff --git a/clang/test/SemaCXX/concept-crash-on-diagnostic.cpp b/clang/test/SemaCXX/concept-crash-on-diagnostic.cpp
index 71e55c8290ee..c38f8888075d 100644
--- a/clang/test/SemaCXX/concept-crash-on-diagnostic.cpp
+++ b/clang/test/SemaCXX/concept-crash-on-diagnostic.cpp
@@ -36,3 +36,15 @@ void function() {
 // expected-note@#4 {{candidate template ignored: constraints not satisfied [with IteratorL = Object *, IteratorR = Object *]}}
 // We don't know exactly the substituted type for `lhs == rhs`, thus a placeholder 'expr-type' is emitted.
 // expected-note@#3 {{because 'convertible_to<expr-type, bool>' would be invalid}}
+
+namespace GH131530 {
+
+class foo {
+  struct bar {}; // expected-note {{implicitly declared private}}
+};
+
+template <typename T>
+concept is_foo_concept = __is_same(foo::bar, T);
+// expected-error@-1 {{'bar' is a private member of 'GH131530::foo'}}
+
+}
diff --git a/clang/test/SemaCXX/constant-expression-cxx11.cpp b/clang/test/SemaCXX/constant-expression-cxx11.cpp
index 76e2f8194705..c35f3a5632a0 100644
--- a/clang/test/SemaCXX/constant-expression-cxx11.cpp
+++ b/clang/test/SemaCXX/constant-expression-cxx11.cpp
@@ -1472,8 +1472,8 @@ namespace ConvertedConstantExpr {
   enum class E {
     em = m,
     en = n, // expected-error {{enumerator value is not a constant expression}} cxx11_20-note {{initializer of 'n' is unknown}}
-    eo = (m + // pre-cxx23-error {{not a constant expression}}
-          n // cxx11_20-note {{initializer of 'n' is unknown}} cxx23-error {{not a constant expression}}
+    eo = (m + // expected-error {{not a constant expression}}
+          n // cxx11_20-note {{initializer of 'n' is unknown}}
           ),
     eq = reinterpret_cast<long>((int*)0) // expected-error {{not a constant expression}} expected-note {{reinterpret_cast}}
   };
diff --git a/clang/test/SemaCXX/constant-expression-p2280r4.cpp b/clang/test/SemaCXX/constant-expression-p2280r4.cpp
index 8648350b397e..6c9a87267109 100644
--- a/clang/test/SemaCXX/constant-expression-p2280r4.cpp
+++ b/clang/test/SemaCXX/constant-expression-p2280r4.cpp
@@ -1,4 +1,7 @@
-// RUN: %clang_cc1 -std=c++23 -verify %s
+// RUN: %clang_cc1 -std=c++23 -verify=expected,nointerpreter %s
+// (Run line removed for backport to 20.x, so we don't need to backport
+// fexperimental-new-constant-interpreter changes)
+// UN: %clang_cc1 -std=c++23 -verify %s -fexperimental-new-constant-interpreter
 
 using size_t = decltype(sizeof(0));
 
@@ -38,8 +41,8 @@ void splash(Swim& swam) {
   static_assert(swam.phelps() == 28);     // ok
   static_assert((&swam)->phelps() == 28); // ok
   Swim* pswam = &swam;                    // expected-note {{declared here}}
-  static_assert(pswam->phelps() == 28);   // expected-error {{static assertion expression is not an integral constant expression}}
-                                          // expected-note@-1 {{read of non-constexpr variable 'pswam' is not allowed in a constant expression}}
+  static_assert(pswam->phelps() == 28);   // expected-error {{static assertion expression is not an integral constant expression}} \
+                                          // expected-note {{read of non-constexpr variable 'pswam' is not allowed in a constant expression}}
   static_assert(how_many(swam) == 28);    // ok
   static_assert(Swim().lochte() == 12);   // ok
   static_assert(swam.lochte() == 12);     // expected-error {{static assertion expression is not an integral constant expression}}
@@ -153,3 +156,26 @@ int g() {
     static_assert(f(arr) == 5);
 }
 }
+
+namespace GH128409 {
+  int &ff();
+  int &x = ff(); // nointerpreter-note {{declared here}}
+  constinit int &z = x; // expected-error {{variable does not have a constant initializer}} \
+                        // expected-note {{required by 'constinit' specifier here}} \
+                        // nointerpreter-note {{initializer of 'x' is not a constant expression}}
+}
+
+namespace GH129845 {
+  int &ff();
+  int &x = ff(); // nointerpreter-note {{declared here}}
+  struct A { int& x; };
+  constexpr A g = {x}; // expected-error {{constexpr variable 'g' must be initialized by a constant expression}} \
+                       // nointerpreter-note {{initializer of 'x' is not a constant expression}}
+  const A* gg = &g;
+}
+
+namespace extern_reference_used_as_unknown {
+  extern int &x;
+  int y;
+  constinit int& g = (x,y); // expected-warning {{left operand of comma operator has no effect}}
+}
diff --git a/clang/test/SemaCXX/ctad.cpp b/clang/test/SemaCXX/ctad.cpp
index 10806f107b4e..00a861d0f567 100644
--- a/clang/test/SemaCXX/ctad.cpp
+++ b/clang/test/SemaCXX/ctad.cpp
@@ -1,5 +1,4 @@
 // RUN: %clang_cc1 -fsyntax-only -verify -Wno-unused-value -std=c++20 %s
-// expected-no-diagnostics
 
 namespace GH64347 {
 
@@ -17,3 +16,134 @@ void k() {
 }
 
 } // namespace GH64347
+
+namespace GH123591 {
+
+
+template < typename... _Types >
+struct variant {
+  template <int N = sizeof...(_Types)>
+  variant(_Types...);
+};
+
+template <class T>
+using AstNode = variant<T, T, T>;
+
+AstNode tree(42, 43, 44);
+
+}
+
+namespace GH123591_2 {
+
+template <int>
+using enable_if_t = char;
+
+template < typename... Types >
+struct variant {
+  template < enable_if_t<sizeof...(Types)>>
+  variant();
+};
+
+template <int>
+using AstNode = variant<>;
+// expected-note@-1 {{couldn't infer template argument ''}} \
+// expected-note@-1 2{{implicit deduction guide declared as}} \
+// expected-note@-1 {{candidate function template not viable}}
+
+
+AstNode tree; // expected-error {{no viable constructor or deduction guide}}
+
+}
+
+namespace GH127539 {
+
+template <class...>
+struct A {
+    template <class... ArgTs>
+    A(ArgTs...) {}
+};
+
+template <class... ArgTs>
+A(ArgTs...) -> A<typename ArgTs::value_type...>;
+
+template <class... Ts>
+using AA = A<Ts..., Ts...>;
+
+AA a{};
+
+}
+
+namespace GH129077 {
+
+using size_t = decltype(sizeof(0));
+
+struct index_type
+{
+  size_t value = 0;
+  index_type() = default;
+  constexpr index_type(size_t i) noexcept : value(i) {}
+};
+
+template <index_type... Extents>
+struct extents
+{
+  constexpr extents(decltype(Extents)...) noexcept {}
+};
+
+template <class... Extents>
+extents(Extents...) -> extents<(requires { Extents::value; } ? Extents{} : ~0ull)...>;
+
+template <index_type... Index>
+using index = extents<Index...>;
+
+int main()
+{
+  extents i{0,0};
+  auto j = extents<64,{}>({}, 42);
+
+  index k{0,0};
+  auto l = index<64,{}>({}, 42);
+
+  return 0;
+}
+
+}
+
+namespace GH129620 {
+
+template <class... Ts>
+struct A {
+    constexpr A(Ts...) {}
+};
+
+template <class... Ts>
+using Foo = A<Ts...>;
+
+template <class T>
+using Bar = Foo<T, T>;
+
+Bar a{0, 0};
+
+}
+
+namespace GH129998 {
+
+struct converible_to_one {
+    constexpr operator int() const noexcept { return 1; }
+};
+
+template <int... Extents>
+struct class_template {
+    class_template() = default;
+    constexpr class_template(auto&&...) noexcept {}
+};
+
+template <class... Extents>
+class_template(Extents...) -> class_template<(true ? 0 : +Extents{})...>;
+
+template <int... Extents>
+using alias_template = class_template<Extents...>;
+
+alias_template var2{converible_to_one{}, 2};
+
+}
diff --git a/clang/test/SemaCXX/cxx2b-deducing-this.cpp b/clang/test/SemaCXX/cxx2b-deducing-this.cpp
index 6f17ce727545..7e392213710a 100644
--- a/clang/test/SemaCXX/cxx2b-deducing-this.cpp
+++ b/clang/test/SemaCXX/cxx2b-deducing-this.cpp
@@ -1134,3 +1134,10 @@ struct S {
 static_assert((S{} << 11) == a);
 // expected-error@-1 {{use of undeclared identifier 'a'}}
 }
+
+namespace GH135522 {
+struct S {
+  auto f(this auto) -> S;
+  bool g() { return f(); } // expected-error {{no viable conversion from returned value of type 'S' to function return type 'bool'}}
+};
+}
diff --git a/clang/test/SemaCXX/ms-property.cpp b/clang/test/SemaCXX/ms-property.cpp
index d5799a8a4d36..f1424b9cb12b 100644
--- a/clang/test/SemaCXX/ms-property.cpp
+++ b/clang/test/SemaCXX/ms-property.cpp
@@ -2,6 +2,7 @@
 // RUN: %clang_cc1 -triple=x86_64-pc-win32 -fms-compatibility -emit-pch -o %t -verify %s
 // RUN: %clang_cc1 -triple=x86_64-pc-win32 -fms-compatibility -include-pch %t %s -ast-print -o - | FileCheck %s
 // RUN: %clang_cc1 -fdeclspec -fsyntax-only -verify %s -std=c++23
+// expected-no-diagnostics
 
 #ifndef HEADER
 #define HEADER
@@ -103,7 +104,6 @@ struct X {
 void f() {
   (void) get_x().imp;
   (void) get_x().st;
-  // expected-warning@-1 {{ignoring return value of function declared with 'nodiscard' attribute}}
 #if __cplusplus >= 202302L
   (void) get_x().exp;
 #endif
diff --git a/clang/test/SemaTemplate/concepts-lambda.cpp b/clang/test/SemaTemplate/concepts-lambda.cpp
index dcb09c76d26b..1f67c2511e09 100644
--- a/clang/test/SemaTemplate/concepts-lambda.cpp
+++ b/clang/test/SemaTemplate/concepts-lambda.cpp
@@ -325,3 +325,18 @@ template <class> void f() {
 template void f<int>();
 
 }
+
+namespace GH133719 {
+
+template <class T>
+constexpr auto f{[] (auto arg) {
+  return [a{arg}] {
+      [] () requires true {}();
+  };
+}};
+
+void foo() {
+  f<int>(0);
+}
+
+}
diff --git a/clang/test/SemaTemplate/cwg2398.cpp b/clang/test/SemaTemplate/cwg2398.cpp
index 8592be469bb5..33b288acce82 100644
--- a/clang/test/SemaTemplate/cwg2398.cpp
+++ b/clang/test/SemaTemplate/cwg2398.cpp
@@ -650,6 +650,11 @@ namespace regression3 {
   template struct A<B, Node<None>>;
   // old-error@-1 {{different template}}
 } // namespace regression3
+namespace GH130362 {
+  template <template <template <class... T1> class TT1> class TT2> struct A {};
+  template <template <class U1> class UU1> struct B {};
+  template struct A<B>;
+} // namespace GH130362
 
 namespace nttp_auto {
   namespace t1 {
@@ -658,26 +663,19 @@ namespace nttp_auto {
     template struct A<B>;
   } // namespace t1
   namespace t2 {
-    // FIXME: Shouldn't accept parameters after a parameter pack.
     template<template<auto... Va1, auto Va2> class> struct A {};
-    // new-error@-1 {{deduced non-type template argument does not have the same type as the corresponding template parameter ('auto' vs 'int')}}
-    // expected-note@-2 {{previous template template parameter is here}}
+    // expected-error@-1 {{template parameter pack must be the last template parameter}}
+    // old-note@-2 {{previous template template parameter is here}}
     template<int... Vi> struct B;
-    // new-note@-1 {{template parameter is declared here}}
-    // old-note@-2 {{too few template parameters}}
+    // old-note@-1 {{too few template parameters}}
     template struct A<B>;
-    // new-note@-1 {{different template parameters}}
-    // old-error@-2 {{different template parameters}}
+    // old-error@-1 {{different template parameters}}
   } // namespace t2
   namespace t3 {
-    // FIXME: Shouldn't accept parameters after a parameter pack.
     template<template<auto... Va1, auto... Va2> class> struct A {};
-    // new-error@-1 {{deduced non-type template argument does not have the same type as the corresponding template parameter ('auto' vs 'int')}}
-    // new-note@-2 {{previous template template parameter is here}}
+    // expected-error@-1 {{template parameter pack must be the last template parameter}}
     template<int... Vi> struct B;
-    // new-note@-1 {{template parameter is declared here}}
     template struct A<B>;
-    // new-note@-1 {{different template parameters}}
   } // namespace t3
 } // namespace nttp_auto
 
diff --git a/clang/test/SemaTemplate/deduction-guide.cpp b/clang/test/SemaTemplate/deduction-guide.cpp
index a4c523595fca..6db132ca37c7 100644
--- a/clang/test/SemaTemplate/deduction-guide.cpp
+++ b/clang/test/SemaTemplate/deduction-guide.cpp
@@ -691,3 +691,83 @@ Test test(42);
 // CHECK-NEXT: | `-ParmVarDecl {{.*}} 'auto:1'
 
 } // namespace GH122134
+
+namespace GH128691 {
+
+template <typename = void>
+class NewDeleteAllocator;
+
+template <>
+struct NewDeleteAllocator<> {
+  template <typename T>
+  NewDeleteAllocator(T); // expected-note {{candidate template ignored}} \
+                         // expected-note {{implicit deduction guide declared as}}
+};
+
+template <typename>
+struct NewDeleteAllocator : NewDeleteAllocator<> { // expected-note {{candidate template ignored}} \
+                                                   // expected-note {{implicit deduction guide declared as}}
+  using NewDeleteAllocator<>::NewDeleteAllocator;
+};
+
+void test() { NewDeleteAllocator abc(42); } // expected-error {{no viable constructor or deduction guide}}
+
+// CHECK-LABEL: Dumping GH128691::<deduction guide for NewDeleteAllocator>:
+// CHECK-NEXT: FunctionTemplateDecl {{.+}} <deduction guide for NewDeleteAllocator>
+// CHECK-NEXT: |-TemplateTypeParmDecl {{.+}} typename depth 0 index 0
+// CHECK-NEXT: | `-TemplateArgument type 'void'
+// CHECK-NEXT: |   |-inherited from TemplateTypeParm {{.+}} depth 0 index 0
+// CHECK-NEXT: |   `-BuiltinType {{.+}} 'void'
+// CHECK-NEXT: |-TemplateTypeParmDecl {{.+}} typename depth 0 index 1 T
+// CHECK-NEXT: `-CXXDeductionGuideDecl {{.+}} <deduction guide for NewDeleteAllocator> 'auto (T) -> NewDeleteAllocator<type-parameter-0-0>'
+// CHECK-NEXT:  `-ParmVarDecl {{.+}} 'T'
+
+} // namespace GH128691
+
+namespace GH132616_DeductionGuide {
+
+template <class T> struct A {
+  template <class U>
+  A(U);
+};
+
+template <typename>
+struct B : A<int> {
+  using A::A;
+};
+
+template <class T>
+B(T) -> B<T>;
+
+B b(24);
+
+// CHECK-LABEL: Dumping GH132616_DeductionGuide::<deduction guide for B>:
+// CHECK-NEXT: FunctionTemplateDecl {{.+}} implicit <deduction guide for B>
+// CHECK-NEXT: |-TemplateTypeParmDecl {{.+}} typename depth 0 index 0
+// CHECK-NEXT: |-TemplateTypeParmDecl {{.+}} class depth 0 index 1 U
+// CHECK-NEXT: `-CXXDeductionGuideDecl {{.+}} implicit <deduction guide for B> 'auto (U) -> B<type-parameter-0-0>'
+// CHECK-NEXT:  `-ParmVarDecl {{.+}} 'U'
+
+struct C {
+  template <class U>
+  C(U);
+};
+
+template <typename>
+struct D : C {
+  using C::C;
+};
+
+template <class T>
+D(T) -> D<T>;
+
+D d(24);
+
+// CHECK-LABEL: Dumping GH132616_DeductionGuide::<deduction guide for D>:
+// CHECK-NEXT: FunctionTemplateDecl {{.+}} implicit <deduction guide for D>
+// CHECK-NEXT: |-TemplateTypeParmDecl {{.+}} typename depth 0 index 0
+// CHECK-NEXT: |-TemplateTypeParmDecl {{.+}} class depth 0 index 1 U
+// CHECK-NEXT: `-CXXDeductionGuideDecl {{.+}} implicit <deduction guide for D> 'auto (U) -> D<type-parameter-0-0>'
+// CHECK-NEXT:  `-ParmVarDecl {{.+}} 'U'
+
+} // namespace GH132616_DeductionGuide
diff --git a/clang/test/SemaTemplate/temp_arg_template_p0522.cpp b/clang/test/SemaTemplate/temp_arg_template_p0522.cpp
index 2e5a36ae6ed0..d8a81bb36311 100644
--- a/clang/test/SemaTemplate/temp_arg_template_p0522.cpp
+++ b/clang/test/SemaTemplate/temp_arg_template_p0522.cpp
@@ -7,7 +7,8 @@
 template<template<int> typename> struct Ti; // #Ti
 template<template<int...> typename> struct TPi; // #TPi
 template<template<int, int...> typename> struct TiPi;
-template<template<int..., int...> typename> struct TPiPi; // FIXME: Why is this not ill-formed?
+template<template<int..., int...> typename> struct TPiPi;
+// expected-error@-1 {{template parameter pack must be the last template parameter}}
 
 template<typename T, template<T> typename> struct tT0; // #tT0
 template<template<typename T, T> typename> struct Tt0; // #Tt0
diff --git a/clang/unittests/AST/DeclPrinterTest.cpp b/clang/unittests/AST/DeclPrinterTest.cpp
index 6945dff537ca..124b1a166cb1 100644
--- a/clang/unittests/AST/DeclPrinterTest.cpp
+++ b/clang/unittests/AST/DeclPrinterTest.cpp
@@ -1196,21 +1196,21 @@ TEST(DeclPrinter, TestUnnamedTemplateParameters) {
 }
 
 TEST(DeclPrinter, TestUnnamedTemplateParametersPacks) {
-  ASSERT_TRUE(PrintedDeclCXX17Matches(
-      "template <typename ..., int ...,"
-      " template <typename ..., bool ...> class ...> void A();",
-      functionTemplateDecl(hasName("A")).bind("id"),
-      "template <typename ..., int ...,"
-      " template <typename ..., bool ...> class ...> void A()"));
+  ASSERT_TRUE(
+      PrintedDeclCXX17Matches("template <typename ..., int ...,"
+                              " template <typename ...> class ...> void A();",
+                              functionTemplateDecl(hasName("A")).bind("id"),
+                              "template <typename ..., int ...,"
+                              " template <typename ...> class ...> void A()"));
 }
 
 TEST(DeclPrinter, TestNamedTemplateParametersPacks) {
   ASSERT_TRUE(PrintedDeclCXX17Matches(
       "template <typename ...T, int ...I,"
-      " template <typename ...X, bool ...B> class ...Z> void A();",
+      " template <typename ...X> class ...Z> void A();",
       functionTemplateDecl(hasName("A")).bind("id"),
       "template <typename ...T, int ...I,"
-      " template <typename ...X, bool ...B> class ...Z> void A()"));
+      " template <typename ...X> class ...Z> void A()"));
 }
 
 TEST(DeclPrinter, TestTemplateTemplateParameterWrittenWithTypename) {
diff --git a/clang/unittests/Format/ConfigParseTest.cpp b/clang/unittests/Format/ConfigParseTest.cpp
index 10788449a1a1..fcf07e660ddb 100644
--- a/clang/unittests/Format/ConfigParseTest.cpp
+++ b/clang/unittests/Format/ConfigParseTest.cpp
@@ -1214,6 +1214,26 @@ TEST(ConfigParseTest, ParsesConfigurationWithLanguages) {
               IndentWidth, 56u);
 }
 
+TEST(ConfigParseTest, AllowCppForC) {
+  FormatStyle Style = {};
+  Style.Language = FormatStyle::LK_C;
+  EXPECT_EQ(parseConfiguration("Language: Cpp", &Style), ParseError::Success);
+
+  CHECK_PARSE("---\n"
+              "IndentWidth: 4\n"
+              "---\n"
+              "Language: Cpp\n"
+              "IndentWidth: 8\n",
+              IndentWidth, 8u);
+
+  EXPECT_EQ(parseConfiguration("---\n"
+                               "Language: ObjC\n"
+                               "---\n"
+                               "Language: Cpp\n",
+                               &Style),
+            ParseError::Success);
+}
+
 TEST(ConfigParseTest, UsesLanguageForBasedOnStyle) {
   FormatStyle Style = {};
   Style.Language = FormatStyle::LK_JavaScript;
diff --git a/clang/unittests/Format/FormatTest.cpp b/clang/unittests/Format/FormatTest.cpp
index d1e96e0fa544..90a79230e9f4 100644
--- a/clang/unittests/Format/FormatTest.cpp
+++ b/clang/unittests/Format/FormatTest.cpp
@@ -9614,6 +9614,10 @@ TEST_F(FormatTest, AlignsAfterOpenBracket) {
                "        auto aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n"
                "    ) {};",
                Style);
+  verifyFormat("aaaaaaaaaaaaaaaaaaaaaaaa(\n"
+               "    &bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n"
+               ");",
+               Style);
 }
 
 TEST_F(FormatTest, ParenthesesAndOperandAlignment) {
@@ -13958,6 +13962,8 @@ TEST_F(FormatTest, IncorrectCodeUnbalancedBraces) {
   verifyNoCrash("struct Foo {\n"
                 "  operator foo(bar\n"
                 "};");
+  verifyNoCrash("decltype( {\n"
+                "  {");
 }
 
 TEST_F(FormatTest, IncorrectUnbalancedBracesInMacrosWithUnicode) {
@@ -25067,8 +25073,10 @@ TEST_F(FormatTest, AlternativeOperators) {
   verifyFormat("%:define ABC abc"); // #define ABC abc
   verifyFormat("%:%:");             // ##
 
+  verifyFormat("return not ::f();");
+  verifyFormat("return not *foo;");
+
   verifyFormat("a = v(not;);\n"
-               "b = v(not+);\n"
                "c = v(not x);\n"
                "d = v(not 1);\n"
                "e = v(not 123.f);");
@@ -25076,7 +25084,6 @@ TEST_F(FormatTest, AlternativeOperators) {
   verifyNoChange("#define ASSEMBLER_INSTRUCTION_LIST(V)  \\\n"
                  "  V(and)                               \\\n"
                  "  V(not)                               \\\n"
-                 "  V(not!)                              \\\n"
                  "  V(other)",
                  getLLVMStyleWithColumns(40));
 }
@@ -27881,11 +27888,17 @@ TEST_F(FormatTest, RemoveParentheses) {
   verifyFormat("foo((a, b));", "foo(((a), b));", Style);
   verifyFormat("foo((a, b));", "foo((a, (b)));", Style);
   verifyFormat("foo((a, b, c));", "foo((a, ((b)), c));", Style);
+  verifyFormat("(..., (hash_a = hash_combine(hash_a, hash_b)));",
+               "(..., ((hash_a = hash_combine(hash_a, hash_b))));", Style);
+  verifyFormat("((hash_a = hash_combine(hash_a, hash_b)), ...);",
+               "(((hash_a = hash_combine(hash_a, hash_b))), ...);", Style);
   verifyFormat("return (0);", "return (((0)));", Style);
   verifyFormat("return (({ 0; }));", "return ((({ 0; })));", Style);
   verifyFormat("return ((... && std::is_convertible_v<TArgsLocal, TArgs>));",
                "return (((... && std::is_convertible_v<TArgsLocal, TArgs>)));",
                Style);
+  verifyFormat("MOCK_METHOD(void, Function, (), override);",
+               "MOCK_METHOD(void, Function, (), (override));", Style);
 
   Style.RemoveParentheses = FormatStyle::RPS_ReturnStatement;
   verifyFormat("#define Return0 return (0);", Style);
diff --git a/clang/unittests/Format/TokenAnnotatorTest.cpp b/clang/unittests/Format/TokenAnnotatorTest.cpp
index f1a6999cfdfb..757db66c3e29 100644
--- a/clang/unittests/Format/TokenAnnotatorTest.cpp
+++ b/clang/unittests/Format/TokenAnnotatorTest.cpp
@@ -1073,6 +1073,11 @@ TEST_F(TokenAnnotatorTest, UnderstandsOverloadedOperators) {
   ASSERT_EQ(Tokens.size(), 11u) << Tokens;
   EXPECT_TOKEN(Tokens[3], tok::identifier, TT_FunctionDeclarationName);
   EXPECT_TOKEN(Tokens[7], tok::l_paren, TT_OverloadedOperatorLParen);
+
+  Tokens = annotate("using std::operator==;");
+  ASSERT_EQ(Tokens.size(), 7u) << Tokens;
+  // Not TT_FunctionDeclarationName.
+  EXPECT_TOKEN(Tokens[3], tok::kw_operator, TT_Unknown);
 }
 
 TEST_F(TokenAnnotatorTest, OverloadedOperatorInTemplate) {
@@ -3793,6 +3798,12 @@ TEST_F(TokenAnnotatorTest, AfterPPDirective) {
   EXPECT_TOKEN(Tokens[2], tok::minusminus, TT_AfterPPDirective);
 }
 
+TEST_F(TokenAnnotatorTest, UTF8StringLiteral) {
+  auto Tokens = annotate("return u8\"foo\";", getLLVMStyle(FormatStyle::LK_C));
+  ASSERT_EQ(Tokens.size(), 4u) << Tokens;
+  EXPECT_TOKEN(Tokens[1], tok::utf8_string_literal, TT_Unknown);
+}
+
 } // namespace
 } // namespace format
 } // namespace clang
diff --git a/clang/unittests/Tooling/QualTypeNamesTest.cpp b/clang/unittests/Tooling/QualTypeNamesTest.cpp
index 5ded64d4fcc8..49c40d633ad4 100644
--- a/clang/unittests/Tooling/QualTypeNamesTest.cpp
+++ b/clang/unittests/Tooling/QualTypeNamesTest.cpp
@@ -265,6 +265,102 @@ TEST(QualTypeNameTest, InlineNamespace) {
                           TypeNameVisitor::Lang_CXX11);
 }
 
+TEST(QualTypeNameTest, TemplatedClass) {
+  std::unique_ptr<ASTUnit> AST =
+      tooling::buildASTFromCode("template <unsigned U1> struct A {\n"
+                                "  template <unsigned U2> struct B {};\n"
+                                "};\n"
+                                "template struct A<1>;\n"
+                                "template struct A<2u>;\n"
+                                "template struct A<1>::B<3>;\n"
+                                "template struct A<2u>::B<4u>;\n");
+
+  auto &Context = AST->getASTContext();
+  auto &Policy = Context.getPrintingPolicy();
+  auto getFullyQualifiedName = [&](QualType QT) {
+    return TypeName::getFullyQualifiedName(QT, Context, Policy);
+  };
+
+  auto *A = Context.getTranslationUnitDecl()
+                ->lookup(&Context.Idents.get("A"))
+                .find_first<ClassTemplateDecl>();
+  ASSERT_NE(A, nullptr);
+
+  // A has two explicit instantiations: A<1> and A<2u>
+  auto ASpec = A->spec_begin();
+  ASSERT_NE(ASpec, A->spec_end());
+  auto *A1 = *ASpec;
+  ASpec++;
+  ASSERT_NE(ASpec, A->spec_end());
+  auto *A2 = *ASpec;
+
+  // Their type names follow the records.
+  QualType A1RecordTy = Context.getRecordType(A1);
+  EXPECT_EQ(getFullyQualifiedName(A1RecordTy), "A<1>");
+  QualType A2RecordTy = Context.getRecordType(A2);
+  EXPECT_EQ(getFullyQualifiedName(A2RecordTy), "A<2U>");
+
+  // getTemplateSpecializationType() gives types that print the integral
+  // argument directly.
+  TemplateArgument Args1[] = {
+      {Context, llvm::APSInt::getUnsigned(1u), Context.UnsignedIntTy}};
+  QualType A1TemplateSpecTy =
+      Context.getTemplateSpecializationType(TemplateName(A), Args1, A1RecordTy);
+  EXPECT_EQ(A1TemplateSpecTy.getAsString(), "A<1>");
+
+  TemplateArgument Args2[] = {
+      {Context, llvm::APSInt::getUnsigned(2u), Context.UnsignedIntTy}};
+  QualType A2TemplateSpecTy =
+      Context.getTemplateSpecializationType(TemplateName(A), Args2, A2RecordTy);
+  EXPECT_EQ(A2TemplateSpecTy.getAsString(), "A<2>");
+
+  // Find A<1>::B and its specialization B<3>.
+  auto *A1B =
+      A1->lookup(&Context.Idents.get("B")).find_first<ClassTemplateDecl>();
+  ASSERT_NE(A1B, nullptr);
+  auto A1BSpec = A1B->spec_begin();
+  ASSERT_NE(A1BSpec, A1B->spec_end());
+  auto *A1B3 = *A1BSpec;
+  QualType A1B3RecordTy = Context.getRecordType(A1B3);
+  EXPECT_EQ(getFullyQualifiedName(A1B3RecordTy), "A<1>::B<3>");
+
+  // Construct A<1>::B<3> and check name.
+  TemplateArgument Args3[] = {
+      {Context, llvm::APSInt::getUnsigned(3u), Context.UnsignedIntTy}};
+  QualType A1B3TemplateSpecTy = Context.getTemplateSpecializationType(
+      TemplateName(A1B), Args3, A1B3RecordTy);
+  EXPECT_EQ(A1B3TemplateSpecTy.getAsString(), "B<3>");
+
+  NestedNameSpecifier *A1Nested = NestedNameSpecifier::Create(
+      Context, nullptr, false, A1TemplateSpecTy.getTypePtr());
+  QualType A1B3ElaboratedTy = Context.getElaboratedType(
+      ElaboratedTypeKeyword::None, A1Nested, A1B3TemplateSpecTy);
+  EXPECT_EQ(A1B3ElaboratedTy.getAsString(), "A<1>::B<3>");
+
+  // Find A<2u>::B and its specialization B<4u>.
+  auto *A2B =
+      A2->lookup(&Context.Idents.get("B")).find_first<ClassTemplateDecl>();
+  ASSERT_NE(A2B, nullptr);
+  auto A2BSpec = A2B->spec_begin();
+  ASSERT_NE(A2BSpec, A2B->spec_end());
+  auto *A2B4 = *A2BSpec;
+  QualType A2B4RecordTy = Context.getRecordType(A2B4);
+  EXPECT_EQ(getFullyQualifiedName(A2B4RecordTy), "A<2U>::B<4U>");
+
+  // Construct A<2>::B<4> and check name.
+  TemplateArgument Args4[] = {
+      {Context, llvm::APSInt::getUnsigned(4u), Context.UnsignedIntTy}};
+  QualType A2B4TemplateSpecTy = Context.getTemplateSpecializationType(
+      TemplateName(A2B), Args4, A2B4RecordTy);
+  EXPECT_EQ(A2B4TemplateSpecTy.getAsString(), "B<4>");
+
+  NestedNameSpecifier *A2Nested = NestedNameSpecifier::Create(
+      Context, nullptr, false, A2TemplateSpecTy.getTypePtr());
+  QualType A2B4ElaboratedTy = Context.getElaboratedType(
+      ElaboratedTypeKeyword::None, A2Nested, A2B4TemplateSpecTy);
+  EXPECT_EQ(A2B4ElaboratedTy.getAsString(), "A<2>::B<4>");
+}
+
 TEST(QualTypeNameTest, AnonStrucs) {
   TypeNameVisitor AnonStrucs;
   AnonStrucs.ExpectedQualTypeNames["a"] = "short";
diff --git a/cmake/Modules/LLVMVersion.cmake b/cmake/Modules/LLVMVersion.cmake
index be4bb4329715..c8cc0b8968b0 100644
--- a/cmake/Modules/LLVMVersion.cmake
+++ b/cmake/Modules/LLVMVersion.cmake
@@ -7,7 +7,7 @@ if(NOT DEFINED LLVM_VERSION_MINOR)
   set(LLVM_VERSION_MINOR 1)
 endif()
 if(NOT DEFINED LLVM_VERSION_PATCH)
-  set(LLVM_VERSION_PATCH 0)
+  set(LLVM_VERSION_PATCH 5)
 endif()
 if(NOT DEFINED LLVM_VERSION_SUFFIX)
   set(LLVM_VERSION_SUFFIX)
diff --git a/compiler-rt/lib/rtsan/rtsan_interceptors_posix.cpp b/compiler-rt/lib/rtsan/rtsan_interceptors_posix.cpp
index 681611906526..040f501ee52e 100644
--- a/compiler-rt/lib/rtsan/rtsan_interceptors_posix.cpp
+++ b/compiler-rt/lib/rtsan/rtsan_interceptors_posix.cpp
@@ -21,18 +21,6 @@
 #include "rtsan/rtsan.h"
 
 #if SANITIZER_APPLE
-
-#if TARGET_OS_MAC
-// On MacOS OSSpinLockLock is deprecated and no longer present in the headers,
-// but the symbol still exists on the system. Forward declare here so we
-// don't get compilation errors.
-#include <stdint.h>
-extern "C" {
-typedef int32_t OSSpinLock;
-void OSSpinLockLock(volatile OSSpinLock *__lock);
-}
-#endif // TARGET_OS_MAC
-
 #include <libkern/OSAtomic.h>
 #include <os/lock.h>
 #endif // SANITIZER_APPLE
@@ -627,21 +615,35 @@ INTERCEPTOR(mode_t, umask, mode_t cmask) {
 #pragma clang diagnostic push
 // OSSpinLockLock is deprecated, but still in use in libc++
 #pragma clang diagnostic ignored "-Wdeprecated-declarations"
+#undef OSSpinLockLock
+
 INTERCEPTOR(void, OSSpinLockLock, volatile OSSpinLock *lock) {
   __rtsan_notify_intercepted_call("OSSpinLockLock");
   return REAL(OSSpinLockLock)(lock);
 }
-#pragma clang diagnostic pop
+
 #define RTSAN_MAYBE_INTERCEPT_OSSPINLOCKLOCK INTERCEPT_FUNCTION(OSSpinLockLock)
 #else
 #define RTSAN_MAYBE_INTERCEPT_OSSPINLOCKLOCK
 #endif // SANITIZER_APPLE
 
+#if SANITIZER_APPLE
+// _os_nospin_lock_lock may replace OSSpinLockLock due to deprecation macro.
+typedef volatile OSSpinLock *_os_nospin_lock_t;
+
+INTERCEPTOR(void, _os_nospin_lock_lock, _os_nospin_lock_t lock) {
+  __rtsan_notify_intercepted_call("_os_nospin_lock_lock");
+  return REAL(_os_nospin_lock_lock)(lock);
+}
+#pragma clang diagnostic pop // "-Wdeprecated-declarations"
+#endif                       // SANITIZER_APPLE
+
 #if SANITIZER_APPLE
 INTERCEPTOR(void, os_unfair_lock_lock, os_unfair_lock_t lock) {
   __rtsan_notify_intercepted_call("os_unfair_lock_lock");
   return REAL(os_unfair_lock_lock)(lock);
 }
+
 #define RTSAN_MAYBE_INTERCEPT_OS_UNFAIR_LOCK_LOCK                              \
   INTERCEPT_FUNCTION(os_unfair_lock_lock)
 #else
diff --git a/compiler-rt/lib/rtsan/tests/rtsan_test_interceptors_posix.cpp b/compiler-rt/lib/rtsan/tests/rtsan_test_interceptors_posix.cpp
index 59663776366b..7eda884951c8 100644
--- a/compiler-rt/lib/rtsan/tests/rtsan_test_interceptors_posix.cpp
+++ b/compiler-rt/lib/rtsan/tests/rtsan_test_interceptors_posix.cpp
@@ -1036,10 +1036,18 @@ TEST(TestRtsanInterceptors, PthreadJoinDiesWhenRealtime) {
 }
 
 #if SANITIZER_APPLE
-
 #pragma clang diagnostic push
 // OSSpinLockLock is deprecated, but still in use in libc++
 #pragma clang diagnostic ignored "-Wdeprecated-declarations"
+#undef OSSpinLockLock
+extern "C" {
+typedef int32_t OSSpinLock;
+void OSSpinLockLock(volatile OSSpinLock *__lock);
+// _os_nospin_lock_lock may replace OSSpinLockLock due to deprecation macro.
+typedef volatile OSSpinLock *_os_nospin_lock_t;
+void _os_nospin_lock_lock(_os_nospin_lock_t lock);
+}
+
 TEST(TestRtsanInterceptors, OsSpinLockLockDiesWhenRealtime) {
   auto Func = []() {
     OSSpinLock spin_lock{};
@@ -1048,7 +1056,14 @@ TEST(TestRtsanInterceptors, OsSpinLockLockDiesWhenRealtime) {
   ExpectRealtimeDeath(Func, "OSSpinLockLock");
   ExpectNonRealtimeSurvival(Func);
 }
-#pragma clang diagnostic pop
+
+TEST(TestRtsanInterceptors, OsNoSpinLockLockDiesWhenRealtime) {
+  OSSpinLock lock{};
+  auto Func = [&]() { _os_nospin_lock_lock(&lock); };
+  ExpectRealtimeDeath(Func, "_os_nospin_lock_lock");
+  ExpectNonRealtimeSurvival(Func);
+}
+#pragma clang diagnostic pop //"-Wdeprecated-declarations"
 
 TEST(TestRtsanInterceptors, OsUnfairLockLockDiesWhenRealtime) {
   auto Func = []() {
@@ -1058,7 +1073,7 @@ TEST(TestRtsanInterceptors, OsUnfairLockLockDiesWhenRealtime) {
   ExpectRealtimeDeath(Func, "os_unfair_lock_lock");
   ExpectNonRealtimeSurvival(Func);
 }
-#endif
+#endif // SANITIZER_APPLE
 
 #if SANITIZER_LINUX
 TEST(TestRtsanInterceptors, SpinLockLockDiesWhenRealtime) {
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_linux.cpp b/compiler-rt/lib/sanitizer_common/sanitizer_linux.cpp
index 7aa48d29d2d5..a4d526b4466c 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_linux.cpp
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_linux.cpp
@@ -86,6 +86,10 @@
 #    include <sys/sysmacros.h>
 #  endif
 
+#  if SANITIZER_LINUX && defined(__powerpc64__)
+#    include <asm/ptrace.h>
+#  endif
+
 #  if SANITIZER_FREEBSD
 #    include <machine/atomic.h>
 #    include <sys/exec.h>
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_linux_libcdep.cpp b/compiler-rt/lib/sanitizer_common/sanitizer_linux_libcdep.cpp
index e11eff13cd32..331e1c7d8d15 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_linux_libcdep.cpp
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_linux_libcdep.cpp
@@ -619,21 +619,22 @@ static void GetTls(uptr *addr, uptr *size) {
   *addr = tp - RoundUpTo(*size, align);
   *size = tp - *addr + ThreadDescriptorSize();
 #      else
-  if (SANITIZER_GLIBC)
-    *size += 1664;
-  else if (SANITIZER_FREEBSD)
-    *size += 128;  // RTLD_STATIC_TLS_EXTRA
-#        if defined(__mips__) || defined(__powerpc64__) || SANITIZER_RISCV64
+#        if SANITIZER_GLIBC
+  *size += 1664;
+#        elif SANITIZER_FREEBSD
+  *size += 128;  // RTLD_STATIC_TLS_EXTRA
+#          if defined(__mips__) || defined(__powerpc64__) || SANITIZER_RISCV64
   const uptr pre_tcb_size = TlsPreTcbSize();
   *addr -= pre_tcb_size;
   *size += pre_tcb_size;
-#        else
+#          else
   // arm and aarch64 reserve two words at TP, so this underestimates the range.
   // However, this is sufficient for the purpose of finding the pointers to
   // thread-specific data keys.
   const uptr tcb_size = ThreadDescriptorSize();
   *addr -= tcb_size;
   *size += tcb_size;
+#          endif
 #        endif
 #      endif
 #    elif SANITIZER_NETBSD
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cpp b/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cpp
index a5311d266b0c..ec5f2edab6a6 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cpp
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cpp
@@ -96,7 +96,7 @@
 # include <sys/ptrace.h>
 #    if defined(__mips64) || defined(__aarch64__) || defined(__arm__) ||       \
         defined(__hexagon__) || defined(__loongarch__) || SANITIZER_RISCV64 || \
-        defined(__sparc__)
+        defined(__sparc__) || defined(__powerpc64__)
 #      include <asm/ptrace.h>
 #      ifdef __arm__
 typedef struct user_fpregs elf_fpregset_t;
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cpp b/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cpp
index 945da99d41f4..58d17d90c343 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cpp
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cpp
@@ -31,7 +31,8 @@
 #include <sys/types.h> // for pid_t
 #include <sys/uio.h> // for iovec
 #include <elf.h> // for NT_PRSTATUS
-#if (defined(__aarch64__) || SANITIZER_RISCV64 || SANITIZER_LOONGARCH64) && \
+#if (defined(__aarch64__) || defined(__powerpc64__) || \
+     SANITIZER_RISCV64 || SANITIZER_LOONGARCH64) &&    \
      !SANITIZER_ANDROID
 // GLIBC 2.20+ sys/user does not include asm/ptrace.h
 # include <asm/ptrace.h>
diff --git a/compiler-rt/test/profile/instrprof-darwin-exports.c b/compiler-rt/test/profile/instrprof-darwin-exports.c
index 079d5d28ed24..1a2ac8c81327 100644
--- a/compiler-rt/test/profile/instrprof-darwin-exports.c
+++ b/compiler-rt/test/profile/instrprof-darwin-exports.c
@@ -7,13 +7,13 @@
 // just "_main" produces no warnings or errors.
 //
 // RUN: echo "_main" > %t.exports
-// RUN: %clang_pgogen -Werror -Wl,-exported_symbols_list,%t.exports -o %t %s 2>&1 | tee %t.log
-// RUN: %clang_profgen -Werror -fcoverage-mapping -Wl,-exported_symbols_list,%t.exports -o %t %s 2>&1 | tee -a %t.log
+// RUN: %clang_pgogen -Werror -Wl,-exported_symbols_list,%t.exports -Wl,-w -o %t %s 2>&1 | tee %t.log
+// RUN: %clang_profgen -Werror -fcoverage-mapping -Wl,-exported_symbols_list,%t.exports -Wl,-w -o %t %s 2>&1 | tee -a %t.log
 // RUN: cat %t.log | count 0
 
 // 2) Ditto (1), but for GCOV.
 //
-// RUN: %clang -Werror -Wl,-exported_symbols_list,%t.exports --coverage -o %t.gcov %s | tee -a %t.gcov.log
+// RUN: %clang -Werror -Wl,-exported_symbols_list,%t.exports -Wl,-w --coverage -o %t.gcov %s | tee -a %t.gcov.log
 // RUN: cat %t.gcov.log | count 0
 
 // 3) The default set of weak external symbols should match the set of symbols
diff --git a/compiler-rt/test/sanitizer_common/TestCases/Darwin/malloc_zone.cpp b/compiler-rt/test/sanitizer_common/TestCases/Darwin/malloc_zone.cpp
index fd6ef0362943..5aa087fb4ca1 100644
--- a/compiler-rt/test/sanitizer_common/TestCases/Darwin/malloc_zone.cpp
+++ b/compiler-rt/test/sanitizer_common/TestCases/Darwin/malloc_zone.cpp
@@ -17,6 +17,8 @@
 // UBSan does not install a malloc zone.
 // XFAIL: ubsan
 //
+// Currently fails on darwin/lsan
+// XFAIL: darwin && lsan
 
 #include <malloc/malloc.h>
 #include <stdlib.h>
diff --git a/flang/lib/Semantics/check-declarations.cpp b/flang/lib/Semantics/check-declarations.cpp
index 5c26469b9fa2..8da9252133bd 100644
--- a/flang/lib/Semantics/check-declarations.cpp
+++ b/flang/lib/Semantics/check-declarations.cpp
@@ -359,7 +359,10 @@ void CheckHelper::Check(const Symbol &symbol) {
       // are not pertinent to the characteristics of the procedure.
       // Restrictions on entities in pure procedure interfaces don't need
       // enforcement.
-    } else if (!FindCommonBlockContaining(symbol) && IsSaved(symbol)) {
+    } else if (symbol.has<AssocEntityDetails>() ||
+        FindCommonBlockContaining(symbol)) {
+      // can look like they have SAVE but are fine in PURE
+    } else if (IsSaved(symbol)) {
       if (IsInitialized(symbol)) {
         messages_.Say(
             "A pure subprogram may not initialize a variable"_err_en_US);
diff --git a/flang/test/Semantics/call10.f90 b/flang/test/Semantics/call10.f90
index 2d2f57934cd8..1e186f7b4048 100644
--- a/flang/test/Semantics/call10.f90
+++ b/flang/test/Semantics/call10.f90
@@ -36,6 +36,8 @@ module m
     end subroutine
   end interface
 
+  real :: moduleVar = 1.
+
  contains
 
   subroutine impure(x)
@@ -117,6 +119,8 @@ module m
     !ERROR: A pure subprogram may not initialize a variable
       real :: v6 = 0.
     end block
+    associate (x => moduleVar) ! ok
+    end associate
   end subroutine
   pure subroutine s06 ! C1589
     !ERROR: A pure subprogram may not have a variable with the VOLATILE attribute
diff --git a/libcxx/docs/ReleaseNotes/20.rst b/libcxx/docs/ReleaseNotes/20.rst
index 57ab0c167544..f81a573845e6 100644
--- a/libcxx/docs/ReleaseNotes/20.rst
+++ b/libcxx/docs/ReleaseNotes/20.rst
@@ -153,15 +153,18 @@ Deprecations and Removals
   headers as an extension and only deprecates them. The ``_LIBCPP_DISABLE_DEPRECATION_WARNINGS`` macro can be defined to
   suppress deprecation for these headers.
 
-- The ``_LIBCPP_DISABLE_AVAILABILITY`` macro that was used to force-disable availability markup has now been removed.
-  Whether availability markup is used by the library is now solely controlled at configuration-time.
-
 - The pointer safety functions ``declare_reachable``, ``declare_no_pointers``, ``undeclare_no_pointers`` and
   ``__undeclare_reachable`` have been removed from the library. These functions were never implemented in a non-trivial
   way, making it very unlikely that any binary depends on them.
 
 - Non-conforming extension ``packaged_task::result_type`` is deprecated. It will be removed in LLVM 21.
 
+- The changes for ``ranges::zip_view`` from `P2165R4 <https://wg21.link/P2165R4>`_ have been implemented. This can
+  lead to code assuming that ``zip_view`` produces ``std::pair`` to stop compiling now that it produces ``std::tuple``.
+  The cases are rare since ``tuple`` and ``pair`` are compatible for the most part, but this can lead to code that
+  was previously accepted now being rejected. This is necessary for libc++ to be conforming, so we don't provide any
+  way to opt-out of that behavior.
+
 Upcoming Deprecations and Removals
 ----------------------------------
 
@@ -205,3 +208,8 @@ ABI Affecting Changes
 
 - The localization support base API has been reimplemented, leading to different functions being exported from the
   libc++ built library on Windows and Windows-like platforms.
+
+- The changes for ``ranges::zip_view`` from `P2165R4 <https://wg21.link/P2165R4>`_ have been implemented. This changes
+  the element type of ``zip_view`` from a ``std::pair`` to a ``std::tuple`` in some cases. This is technically an ABI
+  break, however since ``zip_view`` is generally not an ABI sensitive type, we don't expect users to encounter any
+  issues and we don't provide a way to change this behavior, which would make libc++ non-conforming.
diff --git a/libcxx/include/__configuration/availability.h b/libcxx/include/__configuration/availability.h
index 261cf9c1ae9d..aa2e75b6f6fe 100644
--- a/libcxx/include/__configuration/availability.h
+++ b/libcxx/include/__configuration/availability.h
@@ -69,7 +69,13 @@
 
 // Availability markup is disabled when building the library, or when a non-Clang
 // compiler is used because only Clang supports the necessary attributes.
-#if defined(_LIBCPP_BUILDING_LIBRARY) || defined(_LIBCXXABI_BUILDING_LIBRARY) || !defined(_LIBCPP_COMPILER_CLANG_BASED)
+//
+// We also allow users to force-disable availability markup via the `_LIBCPP_DISABLE_AVAILABILITY`
+// macro because that is the only way to work around a Clang bug related to availability
+// attributes: https://github.com/llvm/llvm-project/issues/134151.
+// Once that bug has been fixed, we should remove the macro.
+#if defined(_LIBCPP_BUILDING_LIBRARY) || defined(_LIBCXXABI_BUILDING_LIBRARY) ||                                       \
+    !defined(_LIBCPP_COMPILER_CLANG_BASED) || defined(_LIBCPP_DISABLE_AVAILABILITY)
 #  undef _LIBCPP_HAS_VENDOR_AVAILABILITY_ANNOTATIONS
 #  define _LIBCPP_HAS_VENDOR_AVAILABILITY_ANNOTATIONS 0
 #endif
@@ -163,10 +169,10 @@
     __attribute__((availability(driverkit, strict, introduced = 23.0)))
 
 // LLVM 15
-#  if (defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 130400) ||   \
-      (defined(__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__ < 160500) || \
-      (defined(__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__ < 160500) ||         \
-      (defined(__ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__ < 90500) ||    \
+#  if (defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 130300) ||   \
+      (defined(__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__ < 160300) || \
+      (defined(__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__ < 160300) ||         \
+      (defined(__ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__ < 90300) ||    \
       (defined(__ENVIRONMENT_BRIDGE_OS_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_BRIDGE_OS_VERSION_MIN_REQUIRED__ < 70500) ||  \
       (defined(__ENVIRONMENT_DRIVERKIT_VERSION_MIN_REQUIRED__) && __ENVIRONMENT_DRIVERKIT_VERSION_MIN_REQUIRED__ < 220400)
 #    define _LIBCPP_INTRODUCED_IN_LLVM_15 0
@@ -174,10 +180,10 @@
 #    define _LIBCPP_INTRODUCED_IN_LLVM_15 1
 #  endif
 #  define _LIBCPP_INTRODUCED_IN_LLVM_15_ATTRIBUTE                                                                 \
-    __attribute__((availability(macos, strict, introduced = 13.4)))                                               \
-    __attribute__((availability(ios, strict, introduced = 16.5)))                                                 \
-    __attribute__((availability(tvos, strict, introduced = 16.5)))                                                \
-    __attribute__((availability(watchos, strict, introduced = 9.5)))                                              \
+    __attribute__((availability(macos, strict, introduced = 13.3)))                                               \
+    __attribute__((availability(ios, strict, introduced = 16.3)))                                                 \
+    __attribute__((availability(tvos, strict, introduced = 16.3)))                                                \
+    __attribute__((availability(watchos, strict, introduced = 9.3)))                                              \
     __attribute__((availability(bridgeos, strict, introduced = 7.5)))                                             \
     __attribute__((availability(driverkit, strict, introduced = 22.4)))
 
diff --git a/libcxx/include/__configuration/platform.h b/libcxx/include/__configuration/platform.h
index 8d0f8f63f521..f3c199dee172 100644
--- a/libcxx/include/__configuration/platform.h
+++ b/libcxx/include/__configuration/platform.h
@@ -42,6 +42,13 @@
 #  endif
 #endif
 
+// This is required in order for _NEWLIB_VERSION to be defined in places where we use it.
+// TODO: We shouldn't be including arbitrarily-named headers from libc++ since this can break valid
+//       user code. Move code paths that need _NEWLIB_VERSION to another customization mechanism.
+#if __has_include(<picolibc.h>)
+#  include <picolibc.h>
+#endif
+
 #ifndef __BYTE_ORDER__
 #  error                                                                                                               \
       "Your compiler doesn't seem to define __BYTE_ORDER__, which is required by libc++ to know the endianness of your target platform"
diff --git a/libcxx/include/__locale b/libcxx/include/__locale
index dfe79d5e506f..93187dc1d0d9 100644
--- a/libcxx/include/__locale
+++ b/libcxx/include/__locale
@@ -11,6 +11,9 @@
 #define _LIBCPP___LOCALE
 
 #include <__config>
+
+#if _LIBCPP_HAS_LOCALIZATION
+
 #include <__locale_dir/locale_base_api.h>
 #include <__memory/shared_count.h>
 #include <__mutex/once_flag.h>
@@ -24,18 +27,18 @@
 #include <string>
 
 // Some platforms require more includes than others. Keep the includes on all plaforms for now.
-#include <cstddef>
-#include <cstring>
+#  include <cstddef>
+#  include <cstring>
 
-#if _LIBCPP_HAS_WIDE_CHARACTERS
-#  include <cwchar>
-#else
-#  include <__std_mbstate_t.h>
-#endif
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    include <cwchar>
+#  else
+#    include <__std_mbstate_t.h>
+#  endif
 
-#if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
-#  pragma GCC system_header
-#endif
+#  if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
+#    pragma GCC system_header
+#  endif
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
@@ -85,9 +88,9 @@ public:
   // locale operations:
   string name() const;
   bool operator==(const locale&) const;
-#if _LIBCPP_STD_VER <= 17
+#  if _LIBCPP_STD_VER <= 17
   _LIBCPP_HIDE_FROM_ABI bool operator!=(const locale& __y) const { return !(*this == __y); }
-#endif
+#  endif
   template <class _CharT, class _Traits, class _Allocator>
   _LIBCPP_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS bool
   operator()(const basic_string<_CharT, _Traits, _Allocator>&, const basic_string<_CharT, _Traits, _Allocator>&) const;
@@ -237,9 +240,9 @@ long collate<_CharT>::do_hash(const char_type* __lo, const char_type* __hi) cons
 }
 
 extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS collate<char>;
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS collate<wchar_t>;
-#endif
+#  endif
 
 // template <class CharT> class collate_byname;
 
@@ -264,7 +267,7 @@ protected:
   string_type do_transform(const char_type* __lo, const char_type* __hi) const override;
 };
 
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI collate_byname<wchar_t> : public collate<wchar_t> {
   __locale::__locale_t __l_;
@@ -283,7 +286,7 @@ protected:
       const char_type* __lo1, const char_type* __hi1, const char_type* __lo2, const char_type* __hi2) const override;
   string_type do_transform(const char_type* __lo, const char_type* __hi) const override;
 };
-#endif
+#  endif
 
 template <class _CharT, class _Traits, class _Allocator>
 bool locale::operator()(const basic_string<_CharT, _Traits, _Allocator>& __x,
@@ -296,7 +299,7 @@ bool locale::operator()(const basic_string<_CharT, _Traits, _Allocator>& __x,
 
 class _LIBCPP_EXPORTED_FROM_ABI ctype_base {
 public:
-#if defined(_LIBCPP_PROVIDES_DEFAULT_RUNE_TABLE)
+#  if defined(_LIBCPP_PROVIDES_DEFAULT_RUNE_TABLE)
   typedef unsigned long mask;
   static const mask space  = 1 << 0;
   static const mask print  = 1 << 1;
@@ -308,14 +311,14 @@ public:
   static const mask punct  = 1 << 7;
   static const mask xdigit = 1 << 8;
   static const mask blank  = 1 << 9;
-#  if defined(__BIONIC__)
+#    if defined(__BIONIC__)
   // Historically this was a part of regex_traits rather than ctype_base. The
   // historical value of the constant is preserved for ABI compatibility.
   static const mask __regex_word = 0x8000;
-#  else
+#    else
   static const mask __regex_word = 1 << 10;
-#  endif // defined(__BIONIC__)
-#elif defined(__GLIBC__)
+#    endif // defined(__BIONIC__)
+#  elif defined(__GLIBC__)
   typedef unsigned short mask;
   static const mask space  = _ISspace;
   static const mask print  = _ISprint;
@@ -327,12 +330,12 @@ public:
   static const mask punct  = _ISpunct;
   static const mask xdigit = _ISxdigit;
   static const mask blank  = _ISblank;
-#  if defined(__mips__) || (BYTE_ORDER == BIG_ENDIAN)
+#    if defined(__mips__) || (BYTE_ORDER == BIG_ENDIAN)
   static const mask __regex_word = static_cast<mask>(_ISbit(15));
-#  else
+#    else
   static const mask __regex_word = 0x80;
-#  endif
-#elif defined(_LIBCPP_MSVCRT_LIKE)
+#    endif
+#  elif defined(_LIBCPP_MSVCRT_LIKE)
   typedef unsigned short mask;
   static const mask space        = _SPACE;
   static const mask print        = _BLANK | _PUNCT | _ALPHA | _DIGIT;
@@ -345,16 +348,16 @@ public:
   static const mask xdigit       = _HEX;
   static const mask blank        = _BLANK;
   static const mask __regex_word = 0x4000; // 0x8000 and 0x0100 and 0x00ff are used
-#  define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_PRINT
-#  define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_ALPHA
-#elif defined(__APPLE__) || defined(__FreeBSD__) || defined(__NetBSD__)
-#  ifdef __APPLE__
+#    define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_PRINT
+#    define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_ALPHA
+#  elif defined(__APPLE__) || defined(__FreeBSD__) || defined(__NetBSD__)
+#    ifdef __APPLE__
   typedef uint32_t mask;
-#  elif defined(__FreeBSD__)
+#    elif defined(__FreeBSD__)
   typedef unsigned long mask;
-#  elif defined(__NetBSD__)
+#    elif defined(__NetBSD__)
   typedef unsigned short mask;
-#  endif
+#    endif
   static const mask space  = _CTYPE_S;
   static const mask print  = _CTYPE_R;
   static const mask cntrl  = _CTYPE_C;
@@ -365,16 +368,16 @@ public:
   static const mask punct  = _CTYPE_P;
   static const mask xdigit = _CTYPE_X;
 
-#  if defined(__NetBSD__)
+#    if defined(__NetBSD__)
   static const mask blank = _CTYPE_BL;
   // NetBSD defines classes up to 0x2000
   // see sys/ctype_bits.h, _CTYPE_Q
   static const mask __regex_word = 0x8000;
-#  else
+#    else
   static const mask blank        = _CTYPE_B;
   static const mask __regex_word = 0x80;
-#  endif
-#elif defined(_AIX)
+#    endif
+#  elif defined(_AIX)
   typedef unsigned int mask;
   static const mask space        = _ISSPACE;
   static const mask print        = _ISPRINT;
@@ -387,7 +390,7 @@ public:
   static const mask xdigit       = _ISXDIGIT;
   static const mask blank        = _ISBLANK;
   static const mask __regex_word = 0x8000;
-#elif defined(_NEWLIB_VERSION)
+#  elif defined(_NEWLIB_VERSION)
   // Same type as Newlib's _ctype_ array in newlib/libc/include/ctype.h.
   typedef char mask;
   // In case char is signed, static_cast is needed to avoid warning on
@@ -404,11 +407,11 @@ public:
   static const mask blank  = static_cast<mask>(_B);
   // mask is already fully saturated, use a different type in regex_type_traits.
   static const unsigned short __regex_word = 0x100;
-#  define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_PRINT
-#  define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_ALPHA
-#  define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_XDIGIT
-#elif defined(__MVS__)
-#  if defined(__NATIVE_ASCII_F)
+#    define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_PRINT
+#    define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_ALPHA
+#    define _LIBCPP_CTYPE_MASK_IS_COMPOSITE_XDIGIT
+#  elif defined(__MVS__)
+#    if defined(__NATIVE_ASCII_F)
   typedef unsigned int mask;
   static const mask space  = _ISSPACE_A;
   static const mask print  = _ISPRINT_A;
@@ -420,7 +423,7 @@ public:
   static const mask punct  = _ISPUNCT_A;
   static const mask xdigit = _ISXDIGIT_A;
   static const mask blank  = _ISBLANK_A;
-#  else
+#    else
   typedef unsigned short mask;
   static const mask space  = __ISSPACE;
   static const mask print  = __ISPRINT;
@@ -432,11 +435,11 @@ public:
   static const mask punct  = __ISPUNCT;
   static const mask xdigit = __ISXDIGIT;
   static const mask blank  = __ISBLANK;
-#  endif
+#    endif
   static const mask __regex_word = 0x8000;
-#else
-#  error unknown rune table for this platform -- do you mean to define _LIBCPP_PROVIDES_DEFAULT_RUNE_TABLE?
-#endif
+#  else
+#    error unknown rune table for this platform -- do you mean to define _LIBCPP_PROVIDES_DEFAULT_RUNE_TABLE?
+#  endif
   static const mask alnum = alpha | digit;
   static const mask graph = alnum | punct;
 
@@ -450,7 +453,7 @@ public:
 template <class _CharT>
 class _LIBCPP_TEMPLATE_VIS ctype;
 
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI ctype<wchar_t> : public locale::facet, public ctype_base {
 public:
@@ -515,7 +518,7 @@ protected:
   virtual const char_type*
   do_narrow(const char_type* __low, const char_type* __high, char __dfault, char* __dest) const;
 };
-#endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#  endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 inline _LIBCPP_HIDE_FROM_ABI bool __libcpp_isascii(int __c) { return (__c & ~0x7F) == 0; }
 
@@ -580,25 +583,25 @@ public:
 
   static locale::id id;
 
-#ifdef _CACHED_RUNES
+#  ifdef _CACHED_RUNES
   static const size_t table_size = _CACHED_RUNES;
-#else
+#  else
   static const size_t table_size = 256; // FIXME: Don't hardcode this.
-#endif
+#  endif
   _LIBCPP_HIDE_FROM_ABI const mask* table() const _NOEXCEPT { return __tab_; }
   static const mask* classic_table() _NOEXCEPT;
-#if defined(__GLIBC__) || defined(__EMSCRIPTEN__)
+#  if defined(__GLIBC__) || defined(__EMSCRIPTEN__)
   static const int* __classic_upper_table() _NOEXCEPT;
   static const int* __classic_lower_table() _NOEXCEPT;
-#endif
-#if defined(__NetBSD__)
+#  endif
+#  if defined(__NetBSD__)
   static const short* __classic_upper_table() _NOEXCEPT;
   static const short* __classic_lower_table() _NOEXCEPT;
-#endif
-#if defined(__MVS__)
+#  endif
+#  if defined(__MVS__)
   static const unsigned short* __classic_upper_table() _NOEXCEPT;
   static const unsigned short* __classic_lower_table() _NOEXCEPT;
-#endif
+#  endif
 
 protected:
   ~ctype() override;
@@ -633,7 +636,7 @@ protected:
   const char_type* do_tolower(char_type* __low, const char_type* __high) const override;
 };
 
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI ctype_byname<wchar_t> : public ctype<wchar_t> {
   __locale::__locale_t __l_;
@@ -658,7 +661,7 @@ protected:
   const char_type*
   do_narrow(const char_type* __low, const char_type* __high, char __dfault, char* __dest) const override;
 };
-#endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#  endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 template <class _CharT>
 inline _LIBCPP_HIDE_FROM_ABI bool isspace(_CharT __c, const locale& __loc) {
@@ -824,7 +827,7 @@ protected:
 
 // template <> class codecvt<wchar_t, char, mbstate_t>
 
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI codecvt<wchar_t, char, mbstate_t> : public locale::facet, public codecvt_base {
   __locale::__locale_t __l_;
@@ -903,7 +906,7 @@ protected:
   virtual int do_length(state_type&, const extern_type* __frm, const extern_type* __end, size_t __mx) const;
   virtual int do_max_length() const _NOEXCEPT;
 };
-#endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#  endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 // template <> class codecvt<char16_t, char, mbstate_t> // deprecated in C++20
 
@@ -985,7 +988,7 @@ protected:
   virtual int do_max_length() const _NOEXCEPT;
 };
 
-#if _LIBCPP_HAS_CHAR8_T
+#  if _LIBCPP_HAS_CHAR8_T
 
 // template <> class codecvt<char16_t, char8_t, mbstate_t> // C++20
 
@@ -1066,7 +1069,7 @@ protected:
   virtual int do_max_length() const _NOEXCEPT;
 };
 
-#endif
+#  endif
 
 // template <> class codecvt<char32_t, char, mbstate_t> // deprecated in C++20
 
@@ -1148,7 +1151,7 @@ protected:
   virtual int do_max_length() const _NOEXCEPT;
 };
 
-#if _LIBCPP_HAS_CHAR8_T
+#  if _LIBCPP_HAS_CHAR8_T
 
 // template <> class codecvt<char32_t, char8_t, mbstate_t> // C++20
 
@@ -1229,7 +1232,7 @@ protected:
   virtual int do_max_length() const _NOEXCEPT;
 };
 
-#endif
+#  endif
 
 // template <class _InternT, class _ExternT, class _StateT> class codecvt_byname
 
@@ -1251,17 +1254,17 @@ codecvt_byname<_InternT, _ExternT, _StateT>::~codecvt_byname() {}
 _LIBCPP_SUPPRESS_DEPRECATED_POP
 
 extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS codecvt_byname<char, char, mbstate_t>;
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS codecvt_byname<wchar_t, char, mbstate_t>;
-#endif
+#  endif
 extern template class _LIBCPP_DEPRECATED_IN_CXX20
 _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS codecvt_byname<char16_t, char, mbstate_t>; // deprecated in C++20
 extern template class _LIBCPP_DEPRECATED_IN_CXX20
 _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS codecvt_byname<char32_t, char, mbstate_t>; // deprecated in C++20
-#if _LIBCPP_HAS_CHAR8_T
+#  if _LIBCPP_HAS_CHAR8_T
 extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS codecvt_byname<char16_t, char8_t, mbstate_t>; // C++20
 extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS codecvt_byname<char32_t, char8_t, mbstate_t>; // C++20
-#endif
+#  endif
 
 template <size_t _Np>
 struct __narrow_to_utf8 {
@@ -1441,7 +1444,7 @@ protected:
   string __grouping_;
 };
 
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI numpunct<wchar_t> : public locale::facet {
 public:
@@ -1470,7 +1473,7 @@ protected:
   char_type __thousands_sep_;
   string __grouping_;
 };
-#endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#  endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 // template <class charT> class numpunct_byname
 
@@ -1493,7 +1496,7 @@ private:
   void __init(const char*);
 };
 
-#if _LIBCPP_HAS_WIDE_CHARACTERS
+#  if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI numpunct_byname<wchar_t> : public numpunct<wchar_t> {
 public:
@@ -1509,8 +1512,10 @@ protected:
 private:
   void __init(const char*);
 };
-#endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#  endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 _LIBCPP_END_NAMESPACE_STD
 
+#endif // _LIBCPP_HAS_LOCALIZATION
+
 #endif // _LIBCPP___LOCALE
diff --git a/libcxx/include/__locale_dir/locale_base_api.h b/libcxx/include/__locale_dir/locale_base_api.h
index c1e73caeecce..9d92a5ea1f0a 100644
--- a/libcxx/include/__locale_dir/locale_base_api.h
+++ b/libcxx/include/__locale_dir/locale_base_api.h
@@ -111,57 +111,59 @@
 //  int     __sscanf(const char*, __locale_t, const char*, ...);     // required by the headers
 // }
 
-#if defined(__APPLE__)
-#  include <__locale_dir/support/apple.h>
-#elif defined(__FreeBSD__)
-#  include <__locale_dir/support/freebsd.h>
-#elif defined(_LIBCPP_MSVCRT_LIKE)
-#  include <__locale_dir/support/windows.h>
-#elif defined(__Fuchsia__)
-#  include <__locale_dir/support/fuchsia.h>
-#else
+#if _LIBCPP_HAS_LOCALIZATION
+
+#  if defined(__APPLE__)
+#    include <__locale_dir/support/apple.h>
+#  elif defined(__FreeBSD__)
+#    include <__locale_dir/support/freebsd.h>
+#  elif defined(_LIBCPP_MSVCRT_LIKE)
+#    include <__locale_dir/support/windows.h>
+#  elif defined(__Fuchsia__)
+#    include <__locale_dir/support/fuchsia.h>
+#  else
 
 // TODO: This is a temporary definition to bridge between the old way we defined the locale base API
 //       (by providing global non-reserved names) and the new API. As we move individual platforms
 //       towards the new way of defining the locale base API, this should disappear since each platform
 //       will define those directly.
-#  if defined(_AIX) || defined(__MVS__)
-#    include <__locale_dir/locale_base_api/ibm.h>
-#  elif defined(__ANDROID__)
-#    include <__locale_dir/locale_base_api/android.h>
-#  elif defined(__OpenBSD__)
-#    include <__locale_dir/locale_base_api/openbsd.h>
-#  elif defined(__wasi__) || _LIBCPP_HAS_MUSL_LIBC
-#    include <__locale_dir/locale_base_api/musl.h>
-#  endif
-
-#  include <__locale_dir/locale_base_api/bsd_locale_fallbacks.h>
-
-#  include <__cstddef/size_t.h>
-#  include <__utility/forward.h>
-#  include <ctype.h>
-#  include <string.h>
-#  include <time.h>
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
-#    include <wctype.h>
-#  endif
+#    if defined(_AIX) || defined(__MVS__)
+#      include <__locale_dir/locale_base_api/ibm.h>
+#    elif defined(__ANDROID__)
+#      include <__locale_dir/locale_base_api/android.h>
+#    elif defined(__OpenBSD__)
+#      include <__locale_dir/locale_base_api/openbsd.h>
+#    elif defined(__wasi__) || _LIBCPP_HAS_MUSL_LIBC
+#      include <__locale_dir/locale_base_api/musl.h>
+#    endif
+
+#    include <__locale_dir/locale_base_api/bsd_locale_fallbacks.h>
+
+#    include <__cstddef/size_t.h>
+#    include <__utility/forward.h>
+#    include <ctype.h>
+#    include <string.h>
+#    include <time.h>
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
+#      include <wctype.h>
+#    endif
 _LIBCPP_BEGIN_NAMESPACE_STD
 namespace __locale {
 //
 // Locale management
 //
-#  define _LIBCPP_COLLATE_MASK LC_COLLATE_MASK
-#  define _LIBCPP_CTYPE_MASK LC_CTYPE_MASK
-#  define _LIBCPP_MONETARY_MASK LC_MONETARY_MASK
-#  define _LIBCPP_NUMERIC_MASK LC_NUMERIC_MASK
-#  define _LIBCPP_TIME_MASK LC_TIME_MASK
-#  define _LIBCPP_MESSAGES_MASK LC_MESSAGES_MASK
-#  define _LIBCPP_ALL_MASK LC_ALL_MASK
-#  define _LIBCPP_LC_ALL LC_ALL
+#    define _LIBCPP_COLLATE_MASK LC_COLLATE_MASK
+#    define _LIBCPP_CTYPE_MASK LC_CTYPE_MASK
+#    define _LIBCPP_MONETARY_MASK LC_MONETARY_MASK
+#    define _LIBCPP_NUMERIC_MASK LC_NUMERIC_MASK
+#    define _LIBCPP_TIME_MASK LC_TIME_MASK
+#    define _LIBCPP_MESSAGES_MASK LC_MESSAGES_MASK
+#    define _LIBCPP_ALL_MASK LC_ALL_MASK
+#    define _LIBCPP_LC_ALL LC_ALL
 
 using __locale_t _LIBCPP_NODEBUG = locale_t;
 
-#  if defined(_LIBCPP_BUILDING_LIBRARY)
+#    if defined(_LIBCPP_BUILDING_LIBRARY)
 using __lconv_t _LIBCPP_NODEBUG = lconv;
 
 inline _LIBCPP_HIDE_FROM_ABI __locale_t __newlocale(int __category_mask, const char* __name, __locale_t __loc) {
@@ -175,7 +177,7 @@ inline _LIBCPP_HIDE_FROM_ABI char* __setlocale(int __category, char const* __loc
 inline _LIBCPP_HIDE_FROM_ABI void __freelocale(__locale_t __loc) { freelocale(__loc); }
 
 inline _LIBCPP_HIDE_FROM_ABI __lconv_t* __localeconv(__locale_t& __loc) { return __libcpp_localeconv_l(__loc); }
-#  endif // _LIBCPP_BUILDING_LIBRARY
+#    endif // _LIBCPP_BUILDING_LIBRARY
 
 //
 // Strtonum functions
@@ -204,15 +206,15 @@ __strtoull(const char* __nptr, char** __endptr, int __base, __locale_t __loc) {
 //
 // Character manipulation functions
 //
-#  if defined(_LIBCPP_BUILDING_LIBRARY)
+#    if defined(_LIBCPP_BUILDING_LIBRARY)
 inline _LIBCPP_HIDE_FROM_ABI int __islower(int __ch, __locale_t __loc) { return islower_l(__ch, __loc); }
 inline _LIBCPP_HIDE_FROM_ABI int __isupper(int __ch, __locale_t __loc) { return isupper_l(__ch, __loc); }
-#  endif
+#    endif
 
 inline _LIBCPP_HIDE_FROM_ABI int __isdigit(int __ch, __locale_t __loc) { return isdigit_l(__ch, __loc); }
 inline _LIBCPP_HIDE_FROM_ABI int __isxdigit(int __ch, __locale_t __loc) { return isxdigit_l(__ch, __loc); }
 
-#  if defined(_LIBCPP_BUILDING_LIBRARY)
+#    if defined(_LIBCPP_BUILDING_LIBRARY)
 inline _LIBCPP_HIDE_FROM_ABI int __strcoll(const char* __s1, const char* __s2, __locale_t __loc) {
   return strcoll_l(__s1, __s2, __loc);
 }
@@ -222,7 +224,7 @@ inline _LIBCPP_HIDE_FROM_ABI size_t __strxfrm(char* __dest, const char* __src, s
 inline _LIBCPP_HIDE_FROM_ABI int __toupper(int __ch, __locale_t __loc) { return toupper_l(__ch, __loc); }
 inline _LIBCPP_HIDE_FROM_ABI int __tolower(int __ch, __locale_t __loc) { return tolower_l(__ch, __loc); }
 
-#    if _LIBCPP_HAS_WIDE_CHARACTERS
+#      if _LIBCPP_HAS_WIDE_CHARACTERS
 inline _LIBCPP_HIDE_FROM_ABI int __wcscoll(const wchar_t* __s1, const wchar_t* __s2, __locale_t __loc) {
   return wcscoll_l(__s1, __s2, __loc);
 }
@@ -244,7 +246,7 @@ inline _LIBCPP_HIDE_FROM_ABI int __iswpunct(wint_t __ch, __locale_t __loc) { ret
 inline _LIBCPP_HIDE_FROM_ABI int __iswxdigit(wint_t __ch, __locale_t __loc) { return iswxdigit_l(__ch, __loc); }
 inline _LIBCPP_HIDE_FROM_ABI wint_t __towupper(wint_t __ch, __locale_t __loc) { return towupper_l(__ch, __loc); }
 inline _LIBCPP_HIDE_FROM_ABI wint_t __towlower(wint_t __ch, __locale_t __loc) { return towlower_l(__ch, __loc); }
-#    endif
+#      endif
 
 inline _LIBCPP_HIDE_FROM_ABI size_t
 __strftime(char* __s, size_t __max, const char* __format, const tm* __tm, __locale_t __loc) {
@@ -257,7 +259,7 @@ __strftime(char* __s, size_t __max, const char* __format, const tm* __tm, __loca
 inline _LIBCPP_HIDE_FROM_ABI decltype(__libcpp_mb_cur_max_l(__locale_t())) __mb_len_max(__locale_t __loc) {
   return __libcpp_mb_cur_max_l(__loc);
 }
-#    if _LIBCPP_HAS_WIDE_CHARACTERS
+#      if _LIBCPP_HAS_WIDE_CHARACTERS
 inline _LIBCPP_HIDE_FROM_ABI wint_t __btowc(int __ch, __locale_t __loc) { return __libcpp_btowc_l(__ch, __loc); }
 inline _LIBCPP_HIDE_FROM_ABI int __wctob(wint_t __ch, __locale_t __loc) { return __libcpp_wctob_l(__ch, __loc); }
 inline _LIBCPP_HIDE_FROM_ABI size_t
@@ -285,17 +287,17 @@ inline _LIBCPP_HIDE_FROM_ABI size_t
 __mbsrtowcs(wchar_t* __dest, const char** __src, size_t __len, mbstate_t* __ps, __locale_t __loc) {
   return __libcpp_mbsrtowcs_l(__dest, __src, __len, __ps, __loc);
 }
-#    endif // _LIBCPP_HAS_WIDE_CHARACTERS
-#  endif   // _LIBCPP_BUILDING_LIBRARY
+#      endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#    endif   // _LIBCPP_BUILDING_LIBRARY
 
 _LIBCPP_DIAGNOSTIC_PUSH
 _LIBCPP_CLANG_DIAGNOSTIC_IGNORED("-Wgcc-compat")
 _LIBCPP_GCC_DIAGNOSTIC_IGNORED("-Wformat-nonliteral") // GCC doesn't support [[gnu::format]] on variadic templates
-#  ifdef _LIBCPP_COMPILER_CLANG_BASED
-#    define _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT(...) _LIBCPP_ATTRIBUTE_FORMAT(__VA_ARGS__)
-#  else
-#    define _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT(...) /* nothing */
-#  endif
+#    ifdef _LIBCPP_COMPILER_CLANG_BASED
+#      define _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT(...) _LIBCPP_ATTRIBUTE_FORMAT(__VA_ARGS__)
+#    else
+#      define _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT(...) /* nothing */
+#    endif
 
 template <class... _Args>
 _LIBCPP_HIDE_FROM_ABI _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT(__printf__, 4, 5) int __snprintf(
@@ -313,11 +315,13 @@ _LIBCPP_HIDE_FROM_ABI _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT(__scanf__, 3, 4) int __s
   return std::__libcpp_sscanf_l(__s, __loc, __format, std::forward<_Args>(__args)...);
 }
 _LIBCPP_DIAGNOSTIC_POP
-#  undef _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT
+#    undef _LIBCPP_VARIADIC_ATTRIBUTE_FORMAT
 
 } // namespace __locale
 _LIBCPP_END_NAMESPACE_STD
 
-#endif // Compatibility definition of locale base APIs
+#  endif // Compatibility definition of locale base APIs
+
+#endif // _LIBCPP_HAS_LOCALIZATION
 
 #endif // _LIBCPP___LOCALE_DIR_LOCALE_BASE_API_H
diff --git a/libcxx/include/__vector/vector_bool.h b/libcxx/include/__vector/vector_bool.h
index 4f1c442ce0be..feff646a35dc 100644
--- a/libcxx/include/__vector/vector_bool.h
+++ b/libcxx/include/__vector/vector_bool.h
@@ -17,6 +17,7 @@
 #include <__bit_reference>
 #include <__config>
 #include <__functional/unary_function.h>
+#include <__fwd/bit_reference.h>
 #include <__fwd/functional.h>
 #include <__fwd/vector.h>
 #include <__iterator/distance.h>
diff --git a/libcxx/include/codecvt b/libcxx/include/codecvt
index f7ae804c6789..0526b8512175 100644
--- a/libcxx/include/codecvt
+++ b/libcxx/include/codecvt
@@ -58,14 +58,17 @@ class codecvt_utf8_utf16
 #  include <__cxx03/codecvt>
 #else
 #  include <__config>
-#  include <__locale>
-#  include <version>
 
-#  if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
-#    pragma GCC system_header
-#  endif
+#  if _LIBCPP_HAS_LOCALIZATION
+
+#    include <__locale>
+#    include <version>
 
-#  if _LIBCPP_STD_VER < 26 || defined(_LIBCPP_BUILDING_LIBRARY) || defined(_LIBCPP_ENABLE_CXX26_REMOVED_CODECVT)
+#    if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
+#      pragma GCC system_header
+#    endif
+
+#    if _LIBCPP_STD_VER < 26 || defined(_LIBCPP_BUILDING_LIBRARY) || defined(_LIBCPP_ENABLE_CXX26_REMOVED_CODECVT)
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
@@ -76,7 +79,7 @@ enum _LIBCPP_DEPRECATED_IN_CXX17 codecvt_mode { consume_header = 4, generate_hea
 template <class _Elem>
 class __codecvt_utf8;
 
-#    if _LIBCPP_HAS_WIDE_CHARACTERS
+#      if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI __codecvt_utf8<wchar_t> : public codecvt<wchar_t, char, mbstate_t> {
   unsigned long __maxcode_;
@@ -115,7 +118,7 @@ protected:
   int do_length(state_type&, const extern_type* __frm, const extern_type* __end, size_t __mx) const override;
   int do_max_length() const _NOEXCEPT override;
 };
-#    endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#      endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 _LIBCPP_SUPPRESS_DEPRECATED_PUSH
 template <>
@@ -206,7 +209,7 @@ _LIBCPP_SUPPRESS_DEPRECATED_POP
 template <class _Elem, bool _LittleEndian>
 class __codecvt_utf16;
 
-#    if _LIBCPP_HAS_WIDE_CHARACTERS
+#      if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI __codecvt_utf16<wchar_t, false> : public codecvt<wchar_t, char, mbstate_t> {
   unsigned long __maxcode_;
@@ -284,7 +287,7 @@ protected:
   int do_length(state_type&, const extern_type* __frm, const extern_type* __end, size_t __mx) const override;
   int do_max_length() const _NOEXCEPT override;
 };
-#    endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#      endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 _LIBCPP_SUPPRESS_DEPRECATED_PUSH
 template <>
@@ -451,7 +454,7 @@ _LIBCPP_SUPPRESS_DEPRECATED_POP
 template <class _Elem>
 class __codecvt_utf8_utf16;
 
-#    if _LIBCPP_HAS_WIDE_CHARACTERS
+#      if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 class _LIBCPP_EXPORTED_FROM_ABI __codecvt_utf8_utf16<wchar_t> : public codecvt<wchar_t, char, mbstate_t> {
   unsigned long __maxcode_;
@@ -490,7 +493,7 @@ protected:
   int do_length(state_type&, const extern_type* __frm, const extern_type* __end, size_t __mx) const override;
   int do_max_length() const _NOEXCEPT override;
 };
-#    endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#      endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 _LIBCPP_SUPPRESS_DEPRECATED_PUSH
 template <>
@@ -579,7 +582,9 @@ _LIBCPP_SUPPRESS_DEPRECATED_POP
 
 _LIBCPP_END_NAMESPACE_STD
 
-#  endif // _LIBCPP_STD_VER < 26 || defined(_LIBCPP_BUILDING_LIBRARY) || defined(_LIBCPP_ENABLE_CXX26_REMOVED_CODECVT)
+#    endif // _LIBCPP_STD_VER < 26 || defined(_LIBCPP_BUILDING_LIBRARY) || defined(_LIBCPP_ENABLE_CXX26_REMOVED_CODECVT)
+
+#  endif // _LIBCPP_HAS_LOCALIZATION
 
 #  if !defined(_LIBCPP_REMOVE_TRANSITIVE_INCLUDES) && _LIBCPP_STD_VER <= 20
 #    include <atomic>
diff --git a/libcxx/include/fstream b/libcxx/include/fstream
index de5c07035dba..9a35f7ab9701 100644
--- a/libcxx/include/fstream
+++ b/libcxx/include/fstream
@@ -189,35 +189,36 @@ typedef basic_fstream<wchar_t> wfstream;
 #if __cplusplus < 201103L && defined(_LIBCPP_USE_FROZEN_CXX03_HEADERS)
 #  include <__cxx03/fstream>
 #else
-#  include <__algorithm/max.h>
-#  include <__assert>
 #  include <__config>
-#  include <__filesystem/path.h>
-#  include <__fwd/fstream.h>
-#  include <__locale>
-#  include <__memory/addressof.h>
-#  include <__memory/unique_ptr.h>
-#  include <__ostream/basic_ostream.h>
-#  include <__type_traits/enable_if.h>
-#  include <__type_traits/is_same.h>
-#  include <__utility/move.h>
-#  include <__utility/swap.h>
-#  include <__utility/unreachable.h>
-#  include <cstdio>
-#  include <istream>
-#  include <streambuf>
-#  include <typeinfo>
-#  include <version>
-
-#  if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
-#    pragma GCC system_header
-#  endif
-
-_LIBCPP_PUSH_MACROS
-#  include <__undef_macros>
 
 #  if _LIBCPP_HAS_FILESYSTEM && _LIBCPP_HAS_LOCALIZATION
 
+#    include <__algorithm/max.h>
+#    include <__assert>
+#    include <__filesystem/path.h>
+#    include <__fwd/fstream.h>
+#    include <__locale>
+#    include <__memory/addressof.h>
+#    include <__memory/unique_ptr.h>
+#    include <__ostream/basic_ostream.h>
+#    include <__type_traits/enable_if.h>
+#    include <__type_traits/is_same.h>
+#    include <__utility/move.h>
+#    include <__utility/swap.h>
+#    include <__utility/unreachable.h>
+#    include <cstdio>
+#    include <istream>
+#    include <streambuf>
+#    include <typeinfo>
+#    include <version>
+
+#    if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
+#      pragma GCC system_header
+#    endif
+
+_LIBCPP_PUSH_MACROS
+#    include <__undef_macros>
+
 _LIBCPP_BEGIN_NAMESPACE_STD
 
 #    if _LIBCPP_STD_VER >= 26 && defined(_LIBCPP_WIN32API)
@@ -1570,10 +1571,10 @@ extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS basic_filebuf<char>;
 
 _LIBCPP_END_NAMESPACE_STD
 
-#  endif // _LIBCPP_HAS_FILESYSTEM && _LIBCPP_HAS_LOCALIZATION
-
 _LIBCPP_POP_MACROS
 
+#  endif // _LIBCPP_HAS_FILESYSTEM && _LIBCPP_HAS_LOCALIZATION
+
 #  if !defined(_LIBCPP_REMOVE_TRANSITIVE_INCLUDES) && _LIBCPP_STD_VER <= 20
 #    include <atomic>
 #    include <concepts>
diff --git a/libcxx/include/istream b/libcxx/include/istream
index 4b177c41cc32..e28d99fd84e5 100644
--- a/libcxx/include/istream
+++ b/libcxx/include/istream
@@ -1373,6 +1373,8 @@ extern template class _LIBCPP_EXTERN_TEMPLATE_TYPE_VIS basic_iostream<char>;
 
 _LIBCPP_END_NAMESPACE_STD
 
+_LIBCPP_POP_MACROS
+
 #  endif // _LIBCPP_HAS_LOCALIZATION
 
 #  if !defined(_LIBCPP_REMOVE_TRANSITIVE_INCLUDES) && _LIBCPP_STD_VER <= 20
@@ -1382,8 +1384,6 @@ _LIBCPP_END_NAMESPACE_STD
 #    include <type_traits>
 #  endif
 
-_LIBCPP_POP_MACROS
-
 #endif // __cplusplus < 201103L && defined(_LIBCPP_USE_FROZEN_CXX03_HEADERS)
 
 #endif // _LIBCPP_ISTREAM
diff --git a/libcxx/include/regex b/libcxx/include/regex
index dcee77cfacc3..cfcbc2c6b21f 100644
--- a/libcxx/include/regex
+++ b/libcxx/include/regex
@@ -792,48 +792,50 @@ typedef regex_token_iterator<wstring::const_iterator> wsregex_token_iterator;
 #if __cplusplus < 201103L && defined(_LIBCPP_USE_FROZEN_CXX03_HEADERS)
 #  include <__cxx03/regex>
 #else
-#  include <__algorithm/find.h>
-#  include <__algorithm/search.h>
-#  include <__assert>
 #  include <__config>
-#  include <__iterator/back_insert_iterator.h>
-#  include <__iterator/default_sentinel.h>
-#  include <__iterator/wrap_iter.h>
-#  include <__locale>
-#  include <__memory/shared_ptr.h>
-#  include <__memory_resource/polymorphic_allocator.h>
-#  include <__type_traits/is_swappable.h>
-#  include <__utility/move.h>
-#  include <__utility/pair.h>
-#  include <__utility/swap.h>
-#  include <__verbose_abort>
-#  include <deque>
-#  include <stdexcept>
-#  include <string>
-#  include <vector>
-#  include <version>
+
+#  if _LIBCPP_HAS_LOCALIZATION
+
+#    include <__algorithm/find.h>
+#    include <__algorithm/search.h>
+#    include <__iterator/back_insert_iterator.h>
+#    include <__iterator/default_sentinel.h>
+#    include <__iterator/wrap_iter.h>
+#    include <__locale>
+#    include <__memory/shared_ptr.h>
+#    include <__memory_resource/polymorphic_allocator.h>
+#    include <__type_traits/is_swappable.h>
+#    include <__utility/move.h>
+#    include <__utility/pair.h>
+#    include <__utility/swap.h>
+#    include <__verbose_abort>
+#    include <deque>
+#    include <stdexcept>
+#    include <string>
+#    include <vector>
+#    include <version>
 
 // standard-mandated includes
 
 // [iterator.range]
-#  include <__iterator/access.h>
-#  include <__iterator/data.h>
-#  include <__iterator/empty.h>
-#  include <__iterator/reverse_access.h>
-#  include <__iterator/size.h>
+#    include <__iterator/access.h>
+#    include <__iterator/data.h>
+#    include <__iterator/empty.h>
+#    include <__iterator/reverse_access.h>
+#    include <__iterator/size.h>
 
 // [re.syn]
-#  include <compare>
-#  include <initializer_list>
+#    include <compare>
+#    include <initializer_list>
 
-#  if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
-#    pragma GCC system_header
-#  endif
+#    if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
+#      pragma GCC system_header
+#    endif
 
 _LIBCPP_PUSH_MACROS
-#  include <__undef_macros>
+#    include <__undef_macros>
 
-#  define _LIBCPP_REGEX_COMPLEXITY_FACTOR 4096
+#    define _LIBCPP_REGEX_COMPLEXITY_FACTOR 4096
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
@@ -846,11 +848,11 @@ enum syntax_option_type {
   nosubs   = 1 << 1,
   optimize = 1 << 2,
   collate  = 1 << 3,
-#  ifdef _LIBCPP_ABI_REGEX_CONSTANTS_NONZERO
+#    ifdef _LIBCPP_ABI_REGEX_CONSTANTS_NONZERO
   ECMAScript = 1 << 9,
-#  else
+#    else
   ECMAScript = 0,
-#  endif
+#    endif
   basic    = 1 << 4,
   extended = 1 << 5,
   awk      = 1 << 6,
@@ -861,11 +863,11 @@ enum syntax_option_type {
 };
 
 _LIBCPP_HIDE_FROM_ABI inline _LIBCPP_CONSTEXPR syntax_option_type __get_grammar(syntax_option_type __g) {
-#  ifdef _LIBCPP_ABI_REGEX_CONSTANTS_NONZERO
+#    ifdef _LIBCPP_ABI_REGEX_CONSTANTS_NONZERO
   return static_cast<syntax_option_type>(__g & 0x3F0);
-#  else
+#    else
   return static_cast<syntax_option_type>(__g & 0x1F0);
-#  endif
+#    endif
 }
 
 inline _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR syntax_option_type operator~(syntax_option_type __x) {
@@ -987,11 +989,11 @@ public:
 
 template <regex_constants::error_type _Ev>
 [[__noreturn__]] inline _LIBCPP_HIDE_FROM_ABI void __throw_regex_error() {
-#  if _LIBCPP_HAS_EXCEPTIONS
+#    if _LIBCPP_HAS_EXCEPTIONS
   throw regex_error(_Ev);
-#  else
+#    else
   _LIBCPP_VERBOSE_ABORT("regex_error was thrown in -fno-exceptions mode");
-#  endif
+#    endif
 }
 
 template <class _CharT>
@@ -1000,7 +1002,7 @@ public:
   typedef _CharT char_type;
   typedef basic_string<char_type> string_type;
   typedef locale locale_type;
-#  if defined(__BIONIC__) || defined(_NEWLIB_VERSION)
+#    if defined(__BIONIC__) || defined(_NEWLIB_VERSION)
   // Originally bionic's ctype_base used its own ctype masks because the
   // builtin ctype implementation wasn't in libc++ yet. Bionic's ctype mask
   // was only 8 bits wide and already saturated, so it used a wider type here
@@ -1015,9 +1017,9 @@ public:
   // often used for space constrained environments, so it makes sense not to
   // duplicate the ctype table.
   typedef uint16_t char_class_type;
-#  else
+#    else
   typedef ctype_base::mask char_class_type;
-#  endif
+#    endif
 
   static const char_class_type __regex_word = ctype_base::__regex_word;
 
@@ -1057,30 +1059,30 @@ private:
 
   template <class _ForwardIterator>
   string_type __transform_primary(_ForwardIterator __f, _ForwardIterator __l, char) const;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
   template <class _ForwardIterator>
   string_type __transform_primary(_ForwardIterator __f, _ForwardIterator __l, wchar_t) const;
-#  endif
+#    endif
   template <class _ForwardIterator>
   string_type __lookup_collatename(_ForwardIterator __f, _ForwardIterator __l, char) const;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
   template <class _ForwardIterator>
   string_type __lookup_collatename(_ForwardIterator __f, _ForwardIterator __l, wchar_t) const;
-#  endif
+#    endif
   template <class _ForwardIterator>
   char_class_type __lookup_classname(_ForwardIterator __f, _ForwardIterator __l, bool __icase, char) const;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
   template <class _ForwardIterator>
   char_class_type __lookup_classname(_ForwardIterator __f, _ForwardIterator __l, bool __icase, wchar_t) const;
-#  endif
+#    endif
 
   static int __regex_traits_value(unsigned char __ch, int __radix);
   _LIBCPP_HIDE_FROM_ABI int __regex_traits_value(char __ch, int __radix) const {
     return __regex_traits_value(static_cast<unsigned char>(__ch), __radix);
   }
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
   _LIBCPP_HIDE_FROM_ABI int __regex_traits_value(wchar_t __ch, int __radix) const;
-#  endif
+#    endif
 };
 
 template <class _CharT>
@@ -1139,7 +1141,7 @@ regex_traits<_CharT>::__transform_primary(_ForwardIterator __f, _ForwardIterator
   return __d;
 }
 
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 template <class _CharT>
 template <class _ForwardIterator>
 typename regex_traits<_CharT>::string_type
@@ -1158,7 +1160,7 @@ regex_traits<_CharT>::__transform_primary(_ForwardIterator __f, _ForwardIterator
   }
   return __d;
 }
-#  endif
+#    endif
 
 // lookup_collatename is very FreeBSD-specific
 
@@ -1183,7 +1185,7 @@ regex_traits<_CharT>::__lookup_collatename(_ForwardIterator __f, _ForwardIterato
   return __r;
 }
 
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 template <class _CharT>
 template <class _ForwardIterator>
 typename regex_traits<_CharT>::string_type
@@ -1211,7 +1213,7 @@ regex_traits<_CharT>::__lookup_collatename(_ForwardIterator __f, _ForwardIterato
   }
   return __r;
 }
-#  endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#    endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 // lookup_classname
 
@@ -1226,7 +1228,7 @@ regex_traits<_CharT>::__lookup_classname(_ForwardIterator __f, _ForwardIterator
   return std::__get_classname(__s.c_str(), __icase);
 }
 
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 template <class _CharT>
 template <class _ForwardIterator>
 typename regex_traits<_CharT>::char_class_type
@@ -1242,7 +1244,7 @@ regex_traits<_CharT>::__lookup_classname(_ForwardIterator __f, _ForwardIterator
   }
   return __get_classname(__n.c_str(), __icase);
 }
-#  endif // _LIBCPP_HAS_WIDE_CHARACTERS
+#    endif // _LIBCPP_HAS_WIDE_CHARACTERS
 
 template <class _CharT>
 bool regex_traits<_CharT>::isctype(char_type __c, char_class_type __m) const {
@@ -1253,28 +1255,28 @@ bool regex_traits<_CharT>::isctype(char_type __c, char_class_type __m) const {
 
 inline _LIBCPP_HIDE_FROM_ABI bool __is_07(unsigned char __c) {
   return (__c & 0xF8u) ==
-#  if defined(__MVS__) && !defined(__NATIVE_ASCII_F)
+#    if defined(__MVS__) && !defined(__NATIVE_ASCII_F)
          0xF0;
-#  else
+#    else
          0x30;
-#  endif
+#    endif
 }
 
 inline _LIBCPP_HIDE_FROM_ABI bool __is_89(unsigned char __c) {
   return (__c & 0xFEu) ==
-#  if defined(__MVS__) && !defined(__NATIVE_ASCII_F)
+#    if defined(__MVS__) && !defined(__NATIVE_ASCII_F)
          0xF8;
-#  else
+#    else
          0x38;
-#  endif
+#    endif
 }
 
 inline _LIBCPP_HIDE_FROM_ABI unsigned char __to_lower(unsigned char __c) {
-#  if defined(__MVS__) && !defined(__NATIVE_ASCII_F)
+#    if defined(__MVS__) && !defined(__NATIVE_ASCII_F)
   return __c & 0xBF;
-#  else
+#    else
   return __c | 0x20;
-#  endif
+#    endif
 }
 
 template <class _CharT>
@@ -1293,12 +1295,12 @@ int regex_traits<_CharT>::__regex_traits_value(unsigned char __ch, int __radix)
   return -1;
 }
 
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 template <class _CharT>
 inline int regex_traits<_CharT>::__regex_traits_value(wchar_t __ch, int __radix) const {
   return __regex_traits_value(static_cast<unsigned char>(__ct_->narrow(__ch, char_type())), __radix);
 }
-#  endif
+#    endif
 
 template <class _CharT>
 class __node;
@@ -1941,10 +1943,10 @@ public:
 
 template <>
 _LIBCPP_EXPORTED_FROM_ABI void __match_any_but_newline<char>::__exec(__state&) const;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 template <>
 _LIBCPP_EXPORTED_FROM_ABI void __match_any_but_newline<wchar_t>::__exec(__state&) const;
-#  endif
+#    endif
 
 // __match_char
 
@@ -2265,13 +2267,13 @@ template <class _CharT, class _Traits = regex_traits<_CharT> >
 class _LIBCPP_TEMPLATE_VIS basic_regex;
 
 typedef basic_regex<char> regex;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 typedef basic_regex<wchar_t> wregex;
-#  endif
+#    endif
 
 template <class _CharT, class _Traits>
-class _LIBCPP_TEMPLATE_VIS _LIBCPP_PREFERRED_NAME(regex)
-    _LIBCPP_IF_WIDE_CHARACTERS(_LIBCPP_PREFERRED_NAME(wregex)) basic_regex {
+class _LIBCPP_TEMPLATE_VIS _LIBCPP_PREFERRED_NAME(regex) _LIBCPP_IF_WIDE_CHARACTERS(_LIBCPP_PREFERRED_NAME(wregex))
+    basic_regex {
 public:
   // types:
   typedef _CharT value_type;
@@ -2338,21 +2340,21 @@ public:
       : __flags_(__f), __marked_count_(0), __loop_count_(0), __open_count_(0), __end_(nullptr) {
     __init(__first, __last);
   }
-#  ifndef _LIBCPP_CXX03_LANG
+#    ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI basic_regex(initializer_list<value_type> __il, flag_type __f = regex_constants::ECMAScript)
       : __flags_(__f), __marked_count_(0), __loop_count_(0), __open_count_(0), __end_(nullptr) {
     __init(__il.begin(), __il.end());
   }
-#  endif // _LIBCPP_CXX03_LANG
+#    endif // _LIBCPP_CXX03_LANG
 
   //    ~basic_regex() = default;
 
   //     basic_regex& operator=(const basic_regex&) = default;
   //     basic_regex& operator=(basic_regex&&) = default;
   _LIBCPP_HIDE_FROM_ABI basic_regex& operator=(const value_type* __p) { return assign(__p); }
-#  ifndef _LIBCPP_CXX03_LANG
+#    ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI basic_regex& operator=(initializer_list<value_type> __il) { return assign(__il); }
-#  endif // _LIBCPP_CXX03_LANG
+#    endif // _LIBCPP_CXX03_LANG
   template <class _ST, class _SA>
   _LIBCPP_HIDE_FROM_ABI basic_regex& operator=(const basic_string<value_type, _ST, _SA>& __p) {
     return assign(__p);
@@ -2360,9 +2362,9 @@ public:
 
   // assign:
   _LIBCPP_HIDE_FROM_ABI basic_regex& assign(const basic_regex& __that) { return *this = __that; }
-#  ifndef _LIBCPP_CXX03_LANG
+#    ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI basic_regex& assign(basic_regex&& __that) _NOEXCEPT { return *this = std::move(__that); }
-#  endif
+#    endif
   _LIBCPP_HIDE_FROM_ABI basic_regex& assign(const value_type* __p, flag_type __f = regex_constants::ECMAScript) {
     return assign(__p, __p + __traits_.length(__p), __f);
   }
@@ -2399,14 +2401,14 @@ public:
     return assign(basic_regex(__first, __last, __f));
   }
 
-#  ifndef _LIBCPP_CXX03_LANG
+#    ifndef _LIBCPP_CXX03_LANG
 
   _LIBCPP_HIDE_FROM_ABI basic_regex&
   assign(initializer_list<value_type> __il, flag_type __f = regex_constants::ECMAScript) {
     return assign(__il.begin(), __il.end(), __f);
   }
 
-#  endif // _LIBCPP_CXX03_LANG
+#    endif // _LIBCPP_CXX03_LANG
 
   // const operations:
   _LIBCPP_HIDE_FROM_ABI unsigned mark_count() const { return __marked_count_; }
@@ -2647,11 +2649,11 @@ private:
   friend class __lookahead;
 };
 
-#  if _LIBCPP_STD_VER >= 17
+#    if _LIBCPP_STD_VER >= 17
 template <class _ForwardIterator, __enable_if_t<__has_forward_iterator_category<_ForwardIterator>::value, int> = 0>
 basic_regex(_ForwardIterator, _ForwardIterator, regex_constants::syntax_option_type = regex_constants::ECMAScript)
     -> basic_regex<typename iterator_traits<_ForwardIterator>::value_type>;
-#  endif
+#    endif
 
 template <class _CharT, class _Traits>
 const regex_constants::syntax_option_type basic_regex<_CharT, _Traits>::icase;
@@ -4184,10 +4186,10 @@ void basic_regex<_CharT, _Traits>::__push_lookahead(const basic_regex& __exp, bo
 
 typedef sub_match<const char*> csub_match;
 typedef sub_match<string::const_iterator> ssub_match;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 typedef sub_match<const wchar_t*> wcsub_match;
 typedef sub_match<wstring::const_iterator> wssub_match;
-#  endif
+#    endif
 
 template <class _BidirectionalIterator>
 class _LIBCPP_TEMPLATE_VIS _LIBCPP_PREFERRED_NAME(csub_match)
@@ -4227,7 +4229,7 @@ inline _LIBCPP_HIDE_FROM_ABI bool operator==(const sub_match<_BiIter>& __x, cons
   return __x.compare(__y) == 0;
 }
 
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
 template <class _BiIter>
 using __sub_match_cat _LIBCPP_NODEBUG =
     compare_three_way_result_t<basic_string<typename iterator_traits<_BiIter>::value_type>>;
@@ -4236,7 +4238,7 @@ template <class _BiIter>
 _LIBCPP_HIDE_FROM_ABI auto operator<=>(const sub_match<_BiIter>& __x, const sub_match<_BiIter>& __y) {
   return static_cast<__sub_match_cat<_BiIter>>(__x.compare(__y) <=> 0);
 }
-#  else  // _LIBCPP_STD_VER >= 20
+#    else  // _LIBCPP_STD_VER >= 20
 template <class _BiIter>
 inline _LIBCPP_HIDE_FROM_ABI bool operator!=(const sub_match<_BiIter>& __x, const sub_match<_BiIter>& __y) {
   return !(__x == __y);
@@ -4303,7 +4305,7 @@ operator<=(const basic_string<typename iterator_traits<_BiIter>::value_type, _ST
            const sub_match<_BiIter>& __y) {
   return !(__y < __x);
 }
-#  endif // _LIBCPP_STD_VER >= 20
+#    endif // _LIBCPP_STD_VER >= 20
 
 template <class _BiIter, class _ST, class _SA>
 inline _LIBCPP_HIDE_FROM_ABI bool
@@ -4312,7 +4314,7 @@ operator==(const sub_match<_BiIter>& __x,
   return __x.compare(typename sub_match<_BiIter>::string_type(__y.data(), __y.size())) == 0;
 }
 
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
 template <class _BiIter, class _ST, class _SA>
 _LIBCPP_HIDE_FROM_ABI auto
 operator<=>(const sub_match<_BiIter>& __x,
@@ -4320,7 +4322,7 @@ operator<=>(const sub_match<_BiIter>& __x,
   return static_cast<__sub_match_cat<_BiIter>>(
       __x.compare(typename sub_match<_BiIter>::string_type(__y.data(), __y.size())) <=> 0);
 }
-#  else  // _LIBCPP_STD_VER >= 20
+#    else  // _LIBCPP_STD_VER >= 20
 template <class _BiIter, class _ST, class _SA>
 inline _LIBCPP_HIDE_FROM_ABI bool
 operator!=(const sub_match<_BiIter>& __x,
@@ -4391,7 +4393,7 @@ inline _LIBCPP_HIDE_FROM_ABI bool
 operator<=(typename iterator_traits<_BiIter>::value_type const* __x, const sub_match<_BiIter>& __y) {
   return !(__y < __x);
 }
-#  endif // _LIBCPP_STD_VER >= 20
+#    endif // _LIBCPP_STD_VER >= 20
 
 template <class _BiIter>
 inline _LIBCPP_HIDE_FROM_ABI bool
@@ -4399,13 +4401,13 @@ operator==(const sub_match<_BiIter>& __x, typename iterator_traits<_BiIter>::val
   return __x.compare(__y) == 0;
 }
 
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
 template <class _BiIter>
 _LIBCPP_HIDE_FROM_ABI auto
 operator<=>(const sub_match<_BiIter>& __x, typename iterator_traits<_BiIter>::value_type const* __y) {
   return static_cast<__sub_match_cat<_BiIter>>(__x.compare(__y) <=> 0);
 }
-#  else  // _LIBCPP_STD_VER >= 20
+#    else  // _LIBCPP_STD_VER >= 20
 template <class _BiIter>
 inline _LIBCPP_HIDE_FROM_ABI bool
 operator!=(const sub_match<_BiIter>& __x, typename iterator_traits<_BiIter>::value_type const* __y) {
@@ -4473,7 +4475,7 @@ inline _LIBCPP_HIDE_FROM_ABI bool
 operator<=(typename iterator_traits<_BiIter>::value_type const& __x, const sub_match<_BiIter>& __y) {
   return !(__y < __x);
 }
-#  endif // _LIBCPP_STD_VER >= 20
+#    endif // _LIBCPP_STD_VER >= 20
 
 template <class _BiIter>
 inline _LIBCPP_HIDE_FROM_ABI bool
@@ -4482,14 +4484,14 @@ operator==(const sub_match<_BiIter>& __x, typename iterator_traits<_BiIter>::val
   return __x.compare(string_type(1, __y)) == 0;
 }
 
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
 template <class _BiIter>
 _LIBCPP_HIDE_FROM_ABI auto
 operator<=>(const sub_match<_BiIter>& __x, typename iterator_traits<_BiIter>::value_type const& __y) {
   using string_type = basic_string<typename iterator_traits<_BiIter>::value_type>;
   return static_cast<__sub_match_cat<_BiIter>>(__x.compare(string_type(1, __y)) <=> 0);
 }
-#  else  // _LIBCPP_STD_VER >= 20
+#    else  // _LIBCPP_STD_VER >= 20
 template <class _BiIter>
 inline _LIBCPP_HIDE_FROM_ABI bool
 operator!=(const sub_match<_BiIter>& __x, typename iterator_traits<_BiIter>::value_type const& __y) {
@@ -4520,7 +4522,7 @@ inline _LIBCPP_HIDE_FROM_ABI bool
 operator<=(const sub_match<_BiIter>& __x, typename iterator_traits<_BiIter>::value_type const& __y) {
   return !(__y < __x);
 }
-#  endif // _LIBCPP_STD_VER >= 20
+#    endif // _LIBCPP_STD_VER >= 20
 
 template <class _CharT, class _ST, class _BiIter>
 inline _LIBCPP_HIDE_FROM_ABI basic_ostream<_CharT, _ST>&
@@ -4530,10 +4532,10 @@ operator<<(basic_ostream<_CharT, _ST>& __os, const sub_match<_BiIter>& __m) {
 
 typedef match_results<const char*> cmatch;
 typedef match_results<string::const_iterator> smatch;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 typedef match_results<const wchar_t*> wcmatch;
 typedef match_results<wstring::const_iterator> wsmatch;
-#  endif
+#    endif
 
 template <class _BidirectionalIterator, class _Allocator>
 class _LIBCPP_TEMPLATE_VIS _LIBCPP_PREFERRED_NAME(cmatch) _LIBCPP_IF_WIDE_CHARACTERS(_LIBCPP_PREFERRED_NAME(wcmatch))
@@ -4563,12 +4565,12 @@ public:
   typedef basic_string<char_type> string_type;
 
   // construct/copy/destroy:
-#  ifndef _LIBCPP_CXX03_LANG
+#    ifndef _LIBCPP_CXX03_LANG
   match_results() : match_results(allocator_type()) {}
   explicit match_results(const allocator_type& __a);
-#  else
+#    else
   explicit match_results(const allocator_type& __a = allocator_type());
-#  endif
+#    endif
 
   //    match_results(const match_results&) = default;
   //    match_results& operator=(const match_results&) = default;
@@ -4818,13 +4820,13 @@ _LIBCPP_HIDE_FROM_ABI bool operator==(const match_results<_BidirectionalIterator
   return __x.__matches_ == __y.__matches_ && __x.__prefix_ == __y.__prefix_ && __x.__suffix_ == __y.__suffix_;
 }
 
-#  if _LIBCPP_STD_VER < 20
+#    if _LIBCPP_STD_VER < 20
 template <class _BidirectionalIterator, class _Allocator>
 inline _LIBCPP_HIDE_FROM_ABI bool operator!=(const match_results<_BidirectionalIterator, _Allocator>& __x,
                                              const match_results<_BidirectionalIterator, _Allocator>& __y) {
   return !(__x == __y);
 }
-#  endif
+#    endif
 
 template <class _BidirectionalIterator, class _Allocator>
 inline _LIBCPP_HIDE_FROM_ABI void
@@ -5236,13 +5238,13 @@ regex_search(const basic_string<_CharT, _ST, _SA>& __s,
   return __r;
 }
 
-#  if _LIBCPP_STD_VER >= 14
+#    if _LIBCPP_STD_VER >= 14
 template <class _ST, class _SA, class _Ap, class _Cp, class _Tp>
 bool regex_search(const basic_string<_Cp, _ST, _SA>&& __s,
                   match_results<typename basic_string<_Cp, _ST, _SA>::const_iterator, _Ap>&,
                   const basic_regex<_Cp, _Tp>& __e,
                   regex_constants::match_flag_type __flags = regex_constants::match_default) = delete;
-#  endif
+#    endif
 
 // regex_match
 
@@ -5291,14 +5293,14 @@ regex_match(const basic_string<_CharT, _ST, _SA>& __s,
   return std::regex_match(__s.begin(), __s.end(), __m, __e, __flags);
 }
 
-#  if _LIBCPP_STD_VER >= 14
+#    if _LIBCPP_STD_VER >= 14
 template <class _ST, class _SA, class _Allocator, class _CharT, class _Traits>
 inline _LIBCPP_HIDE_FROM_ABI bool
 regex_match(const basic_string<_CharT, _ST, _SA>&& __s,
             match_results<typename basic_string<_CharT, _ST, _SA>::const_iterator, _Allocator>& __m,
             const basic_regex<_CharT, _Traits>& __e,
             regex_constants::match_flag_type __flags = regex_constants::match_default) = delete;
-#  endif
+#    endif
 
 template <class _CharT, class _Traits>
 inline _LIBCPP_HIDE_FROM_ABI bool
@@ -5325,10 +5327,10 @@ class _LIBCPP_TEMPLATE_VIS regex_iterator;
 
 typedef regex_iterator<const char*> cregex_iterator;
 typedef regex_iterator<string::const_iterator> sregex_iterator;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 typedef regex_iterator<const wchar_t*> wcregex_iterator;
 typedef regex_iterator<wstring::const_iterator> wsregex_iterator;
-#  endif
+#    endif
 
 template <class _BidirectionalIterator, class _CharT, class _Traits>
 class _LIBCPP_TEMPLATE_VIS _LIBCPP_PREFERRED_NAME(cregex_iterator)
@@ -5341,9 +5343,9 @@ public:
   typedef const value_type* pointer;
   typedef const value_type& reference;
   typedef forward_iterator_tag iterator_category;
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
   typedef input_iterator_tag iterator_concept;
-#  endif
+#    endif
 
 private:
   _BidirectionalIterator __begin_;
@@ -5358,20 +5360,20 @@ public:
                  _BidirectionalIterator __b,
                  const regex_type& __re,
                  regex_constants::match_flag_type __m = regex_constants::match_default);
-#  if _LIBCPP_STD_VER >= 14
+#    if _LIBCPP_STD_VER >= 14
   regex_iterator(_BidirectionalIterator __a,
                  _BidirectionalIterator __b,
                  const regex_type&& __re,
                  regex_constants::match_flag_type __m = regex_constants::match_default) = delete;
-#  endif
+#    endif
 
   _LIBCPP_HIDE_FROM_ABI bool operator==(const regex_iterator& __x) const;
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
   _LIBCPP_HIDE_FROM_ABI bool operator==(default_sentinel_t) const { return *this == regex_iterator(); }
-#  endif
-#  if _LIBCPP_STD_VER < 20
+#    endif
+#    if _LIBCPP_STD_VER < 20
   _LIBCPP_HIDE_FROM_ABI bool operator!=(const regex_iterator& __x) const { return !(*this == __x); }
-#  endif
+#    endif
 
   _LIBCPP_HIDE_FROM_ABI reference operator*() const { return __match_; }
   _LIBCPP_HIDE_FROM_ABI pointer operator->() const { return std::addressof(__match_); }
@@ -5455,10 +5457,10 @@ class _LIBCPP_TEMPLATE_VIS regex_token_iterator;
 
 typedef regex_token_iterator<const char*> cregex_token_iterator;
 typedef regex_token_iterator<string::const_iterator> sregex_token_iterator;
-#  if _LIBCPP_HAS_WIDE_CHARACTERS
+#    if _LIBCPP_HAS_WIDE_CHARACTERS
 typedef regex_token_iterator<const wchar_t*> wcregex_token_iterator;
 typedef regex_token_iterator<wstring::const_iterator> wsregex_token_iterator;
-#  endif
+#    endif
 
 template <class _BidirectionalIterator, class _CharT, class _Traits>
 class _LIBCPP_TEMPLATE_VIS _LIBCPP_PREFERRED_NAME(cregex_token_iterator)
@@ -5472,9 +5474,9 @@ public:
   typedef const value_type* pointer;
   typedef const value_type& reference;
   typedef forward_iterator_tag iterator_category;
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
   typedef input_iterator_tag iterator_concept;
-#  endif
+#    endif
 
 private:
   typedef regex_iterator<_BidirectionalIterator, _CharT, _Traits> _Position;
@@ -5492,67 +5494,67 @@ public:
                        const regex_type& __re,
                        int __submatch                       = 0,
                        regex_constants::match_flag_type __m = regex_constants::match_default);
-#  if _LIBCPP_STD_VER >= 14
+#    if _LIBCPP_STD_VER >= 14
   regex_token_iterator(_BidirectionalIterator __a,
                        _BidirectionalIterator __b,
                        const regex_type&& __re,
                        int __submatch                       = 0,
                        regex_constants::match_flag_type __m = regex_constants::match_default) = delete;
-#  endif
+#    endif
 
   regex_token_iterator(_BidirectionalIterator __a,
                        _BidirectionalIterator __b,
                        const regex_type& __re,
                        const vector<int>& __submatches,
                        regex_constants::match_flag_type __m = regex_constants::match_default);
-#  if _LIBCPP_STD_VER >= 14
+#    if _LIBCPP_STD_VER >= 14
   regex_token_iterator(_BidirectionalIterator __a,
                        _BidirectionalIterator __b,
                        const regex_type&& __re,
                        const vector<int>& __submatches,
                        regex_constants::match_flag_type __m = regex_constants::match_default) = delete;
-#  endif
+#    endif
 
-#  ifndef _LIBCPP_CXX03_LANG
+#    ifndef _LIBCPP_CXX03_LANG
   regex_token_iterator(_BidirectionalIterator __a,
                        _BidirectionalIterator __b,
                        const regex_type& __re,
                        initializer_list<int> __submatches,
                        regex_constants::match_flag_type __m = regex_constants::match_default);
 
-#    if _LIBCPP_STD_VER >= 14
+#      if _LIBCPP_STD_VER >= 14
   regex_token_iterator(_BidirectionalIterator __a,
                        _BidirectionalIterator __b,
                        const regex_type&& __re,
                        initializer_list<int> __submatches,
                        regex_constants::match_flag_type __m = regex_constants::match_default) = delete;
-#    endif
-#  endif // _LIBCPP_CXX03_LANG
+#      endif
+#    endif // _LIBCPP_CXX03_LANG
   template <size_t _Np>
   regex_token_iterator(_BidirectionalIterator __a,
                        _BidirectionalIterator __b,
                        const regex_type& __re,
                        const int (&__submatches)[_Np],
                        regex_constants::match_flag_type __m = regex_constants::match_default);
-#  if _LIBCPP_STD_VER >= 14
+#    if _LIBCPP_STD_VER >= 14
   template <size_t _Np>
   regex_token_iterator(_BidirectionalIterator __a,
                        _BidirectionalIterator __b,
                        const regex_type&& __re,
                        const int (&__submatches)[_Np],
                        regex_constants::match_flag_type __m = regex_constants::match_default) = delete;
-#  endif
+#    endif
 
   regex_token_iterator(const regex_token_iterator&);
   regex_token_iterator& operator=(const regex_token_iterator&);
 
   _LIBCPP_HIDE_FROM_ABI bool operator==(const regex_token_iterator& __x) const;
-#  if _LIBCPP_STD_VER >= 20
+#    if _LIBCPP_STD_VER >= 20
   _LIBCPP_HIDE_FROM_ABI bool operator==(default_sentinel_t) const { return *this == regex_token_iterator(); }
-#  endif
-#  if _LIBCPP_STD_VER < 20
+#    endif
+#    if _LIBCPP_STD_VER < 20
   _LIBCPP_HIDE_FROM_ABI bool operator!=(const regex_token_iterator& __x) const { return !(*this == __x); }
-#  endif
+#    endif
 
   _LIBCPP_HIDE_FROM_ABI const value_type& operator*() const { return *__result_; }
   _LIBCPP_HIDE_FROM_ABI const value_type* operator->() const { return __result_; }
@@ -5614,7 +5616,7 @@ regex_token_iterator<_BidirectionalIterator, _CharT, _Traits>::regex_token_itera
   __init(__a, __b);
 }
 
-#  ifndef _LIBCPP_CXX03_LANG
+#    ifndef _LIBCPP_CXX03_LANG
 
 template <class _BidirectionalIterator, class _CharT, class _Traits>
 regex_token_iterator<_BidirectionalIterator, _CharT, _Traits>::regex_token_iterator(
@@ -5627,7 +5629,7 @@ regex_token_iterator<_BidirectionalIterator, _CharT, _Traits>::regex_token_itera
   __init(__a, __b);
 }
 
-#  endif // _LIBCPP_CXX03_LANG
+#    endif // _LIBCPP_CXX03_LANG
 
 template <class _BidirectionalIterator, class _CharT, class _Traits>
 template <size_t _Np>
@@ -5802,7 +5804,7 @@ regex_replace(const _CharT* __s,
 
 _LIBCPP_END_NAMESPACE_STD
 
-#  if _LIBCPP_STD_VER >= 17
+#    if _LIBCPP_STD_VER >= 17
 _LIBCPP_BEGIN_NAMESPACE_STD
 namespace pmr {
 template <class _BidirT>
@@ -5812,16 +5814,18 @@ using match_results _LIBCPP_AVAILABILITY_PMR =
 using cmatch _LIBCPP_AVAILABILITY_PMR = match_results<const char*>;
 using smatch _LIBCPP_AVAILABILITY_PMR = match_results<std::pmr::string::const_iterator>;
 
-#    if _LIBCPP_HAS_WIDE_CHARACTERS
+#      if _LIBCPP_HAS_WIDE_CHARACTERS
 using wcmatch _LIBCPP_AVAILABILITY_PMR = match_results<const wchar_t*>;
 using wsmatch _LIBCPP_AVAILABILITY_PMR = match_results<std::pmr::wstring::const_iterator>;
-#    endif
+#      endif
 } // namespace pmr
 _LIBCPP_END_NAMESPACE_STD
-#  endif
+#    endif
 
 _LIBCPP_POP_MACROS
 
+#  endif // _LIBCPP_HAS_LOCALIZATION
+
 #  if !defined(_LIBCPP_REMOVE_TRANSITIVE_INCLUDES) && _LIBCPP_STD_VER <= 20
 #    include <atomic>
 #    include <concepts>
diff --git a/libcxx/include/strstream b/libcxx/include/strstream
index 90d56694e7a6..1a17f8389c07 100644
--- a/libcxx/include/strstream
+++ b/libcxx/include/strstream
@@ -133,30 +133,33 @@ private:
 #  include <__cxx03/strstream>
 #else
 #  include <__config>
-#  include <__ostream/basic_ostream.h>
-#  include <istream>
-#  include <streambuf>
-#  include <version>
 
-#  if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
-#    pragma GCC system_header
-#  endif
+#  if _LIBCPP_HAS_LOCALIZATION
 
-#  if _LIBCPP_STD_VER < 26 || defined(_LIBCPP_ENABLE_CXX26_REMOVED_STRSTREAM) || defined(_LIBCPP_BUILDING_LIBRARY)
+#    include <__ostream/basic_ostream.h>
+#    include <istream>
+#    include <streambuf>
+#    include <version>
+
+#    if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
+#      pragma GCC system_header
+#    endif
+
+#    if _LIBCPP_STD_VER < 26 || defined(_LIBCPP_ENABLE_CXX26_REMOVED_STRSTREAM) || defined(_LIBCPP_BUILDING_LIBRARY)
 
 _LIBCPP_PUSH_MACROS
-#    include <__undef_macros>
+#      include <__undef_macros>
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
 class _LIBCPP_DEPRECATED _LIBCPP_EXPORTED_FROM_ABI strstreambuf : public streambuf {
 public:
-#    ifndef _LIBCPP_CXX03_LANG
+#      ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI strstreambuf() : strstreambuf(0) {}
   explicit strstreambuf(streamsize __alsize);
-#    else
+#      else
   explicit strstreambuf(streamsize __alsize = 0);
-#    endif
+#      endif
   strstreambuf(void* (*__palloc)(size_t), void (*__pfree)(void*));
   strstreambuf(char* __gnext, streamsize __n, char* __pbeg = nullptr);
   strstreambuf(const char* __gnext, streamsize __n);
@@ -166,10 +169,10 @@ public:
   strstreambuf(unsigned char* __gnext, streamsize __n, unsigned char* __pbeg = nullptr);
   strstreambuf(const unsigned char* __gnext, streamsize __n);
 
-#    ifndef _LIBCPP_CXX03_LANG
+#      ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI strstreambuf(strstreambuf&& __rhs);
   _LIBCPP_HIDE_FROM_ABI strstreambuf& operator=(strstreambuf&& __rhs);
-#    endif // _LIBCPP_CXX03_LANG
+#      endif // _LIBCPP_CXX03_LANG
 
   ~strstreambuf() override;
 
@@ -203,7 +206,7 @@ private:
   void __init(char* __gnext, streamsize __n, char* __pbeg);
 };
 
-#    ifndef _LIBCPP_CXX03_LANG
+#      ifndef _LIBCPP_CXX03_LANG
 
 inline _LIBCPP_HIDE_FROM_ABI strstreambuf::strstreambuf(strstreambuf&& __rhs)
     : streambuf(__rhs),
@@ -232,7 +235,7 @@ inline _LIBCPP_HIDE_FROM_ABI strstreambuf& strstreambuf::operator=(strstreambuf&
   return *this;
 }
 
-#    endif // _LIBCPP_CXX03_LANG
+#      endif // _LIBCPP_CXX03_LANG
 
 class _LIBCPP_DEPRECATED _LIBCPP_EXPORTED_FROM_ABI istrstream : public istream {
 public:
@@ -241,7 +244,7 @@ public:
   _LIBCPP_HIDE_FROM_ABI istrstream(const char* __s, streamsize __n) : istream(&__sb_), __sb_(__s, __n) {}
   _LIBCPP_HIDE_FROM_ABI istrstream(char* __s, streamsize __n) : istream(&__sb_), __sb_(__s, __n) {}
 
-#    ifndef _LIBCPP_CXX03_LANG
+#      ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI istrstream(istrstream&& __rhs) // extension
       : istream(std::move(static_cast<istream&>(__rhs))), __sb_(std::move(__rhs.__sb_)) {
     istream::set_rdbuf(&__sb_);
@@ -252,7 +255,7 @@ public:
     istream::operator=(std::move(__rhs));
     return *this;
   }
-#    endif // _LIBCPP_CXX03_LANG
+#      endif // _LIBCPP_CXX03_LANG
 
   ~istrstream() override;
 
@@ -274,7 +277,7 @@ public:
   _LIBCPP_HIDE_FROM_ABI ostrstream(char* __s, int __n, ios_base::openmode __mode = ios_base::out)
       : ostream(&__sb_), __sb_(__s, __n, __s + (__mode & ios::app ? std::strlen(__s) : 0)) {}
 
-#    ifndef _LIBCPP_CXX03_LANG
+#      ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI ostrstream(ostrstream&& __rhs) // extension
       : ostream(std::move(static_cast<ostream&>(__rhs))), __sb_(std::move(__rhs.__sb_)) {
     ostream::set_rdbuf(&__sb_);
@@ -285,7 +288,7 @@ public:
     ostream::operator=(std::move(__rhs));
     return *this;
   }
-#    endif // _LIBCPP_CXX03_LANG
+#      endif // _LIBCPP_CXX03_LANG
 
   ~ostrstream() override;
 
@@ -316,7 +319,7 @@ public:
   _LIBCPP_HIDE_FROM_ABI strstream(char* __s, int __n, ios_base::openmode __mode = ios_base::in | ios_base::out)
       : iostream(&__sb_), __sb_(__s, __n, __s + (__mode & ios::app ? std::strlen(__s) : 0)) {}
 
-#    ifndef _LIBCPP_CXX03_LANG
+#      ifndef _LIBCPP_CXX03_LANG
   _LIBCPP_HIDE_FROM_ABI strstream(strstream&& __rhs) // extension
       : iostream(std::move(static_cast<iostream&>(__rhs))), __sb_(std::move(__rhs.__sb_)) {
     iostream::set_rdbuf(&__sb_);
@@ -327,7 +330,7 @@ public:
     iostream::operator=(std::move(__rhs));
     return *this;
   }
-#    endif // _LIBCPP_CXX03_LANG
+#      endif // _LIBCPP_CXX03_LANG
 
   ~strstream() override;
 
@@ -350,7 +353,11 @@ _LIBCPP_END_NAMESPACE_STD
 
 _LIBCPP_POP_MACROS
 
-#  endif // _LIBCPP_STD_VER < 26 || defined(_LIBCPP_ENABLE_CXX26_REMOVED_STRSTREAM) || defined(_LIBCPP_BUILDING_LIBRARY)
-#endif   // __cplusplus < 201103L && defined(_LIBCPP_USE_FROZEN_CXX03_HEADERS)
+#    endif // _LIBCPP_STD_VER < 26 || defined(_LIBCPP_ENABLE_CXX26_REMOVED_STRSTREAM) ||
+           // defined(_LIBCPP_BUILDING_LIBRARY)
+
+#  endif // _LIBCPP_HAS_LOCALIZATION
+
+#endif // __cplusplus < 201103L && defined(_LIBCPP_USE_FROZEN_CXX03_HEADERS)
 
 #endif // _LIBCPP_STRSTREAM
diff --git a/libcxx/test/configs/armv7m-picolibc-libc++.cfg.in b/libcxx/test/configs/armv7m-picolibc-libc++.cfg.in
index 7aedfde89916..9bff5021494e 100644
--- a/libcxx/test/configs/armv7m-picolibc-libc++.cfg.in
+++ b/libcxx/test/configs/armv7m-picolibc-libc++.cfg.in
@@ -11,10 +11,6 @@ config.substitutions.append(('%{compile_flags}',
     # "large atomic operation may incur significant performance penalty; the
     # access size (4 bytes) exceeds the max lock-free size (0 bytes)"
     ' -Wno-atomic-alignment'
-
-    # Various libc++ headers check for the definition of _NEWLIB_VERSION
-    # which for picolibc is defined in picolibc.h.
-    ' -include picolibc.h'
 ))
 config.substitutions.append(('%{link_flags}',
     '-nostdlib -nostdlib++ -L %{lib-dir} -lc++ -lc++abi'
diff --git a/libcxx/test/libcxx/system_reserved_names.gen.py b/libcxx/test/libcxx/system_reserved_names.gen.py
index f01126249c88..244af4bd1f38 100644
--- a/libcxx/test/libcxx/system_reserved_names.gen.py
+++ b/libcxx/test/libcxx/system_reserved_names.gen.py
@@ -11,6 +11,7 @@
 # provided macros (in other words, ensure that we push/pop correctly everywhere).
 
 # RUN: %{python} %s %{libcxx-dir}/utils
+# END.
 
 import sys
 
@@ -28,6 +29,11 @@ for header in public_headers:
 {lit_header_restrictions.get(header, '')}
 {lit_header_undeprecations.get(header, '')}
 
+// UNSUPPORTED: FROZEN-CXX03-HEADERS-FIXME
+
+// This is required to detect the platform we're building for below.
+#include <__config>
+
 #define SYSTEM_RESERVED_NAME This name should not be used in libc++
 
 // libc++ does not use single-letter names as a matter of principle.
diff --git a/libcxx/test/libcxx/vendor/apple/disable-availability.sh.cpp b/libcxx/test/libcxx/vendor/apple/disable-availability.sh.cpp
new file mode 100644
index 000000000000..474b3f83c604
--- /dev/null
+++ b/libcxx/test/libcxx/vendor/apple/disable-availability.sh.cpp
@@ -0,0 +1,49 @@
+//===----------------------------------------------------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+// REQUIRES: stdlib=apple-libc++
+
+// This test is dependent on the code generated by the compiler, and it doesn't
+// work properly with older AppleClangs.
+// UNSUPPORTED: apple-clang-15
+
+// This test ensures that we retain a way to disable availability markup on Apple platforms
+// in order to work around Clang bug https://github.com/llvm/llvm-project/issues/134151.
+//
+// Once that bug has been fixed or once we've made changes to libc++'s use of availability
+// that render that workaround unnecessary, the macro and this test can be removed.
+//
+// The test works by creating a final linked image that refers to a function marked with
+// both an availability attribute and with _LIBCPP_HIDE_FROM_ABI. We then check that this
+// generates a weak reference to the function -- without the bug, we'd expect a strong
+// reference or no reference at all instead.
+
+// First, test the test. Make sure that we do (incorrectly) produce a weak definition when we
+// don't define _LIBCPP_DISABLE_AVAILABILITY. Otherwise, something may have changed in libc++
+// and this test might not work anymore.
+// RUN: %{cxx} %s %{flags} %{compile_flags} %{link_flags} -fvisibility=hidden -fvisibility-inlines-hidden -shared -o %t.1.dylib
+// RUN: nm -m %t.1.dylib | c++filt | grep value > %t.1.symbols
+// RUN: grep weak %t.1.symbols
+
+// Now, make sure that 'weak' goes away when we define _LIBCPP_DISABLE_AVAILABILITY.
+// In fact, all references to the function might go away, so we just check that we don't emit
+// any weak reference.
+// RUN: %{cxx} %s %{flags} %{compile_flags} %{link_flags} -fvisibility=hidden -fvisibility-inlines-hidden -D_LIBCPP_DISABLE_AVAILABILITY -shared -o %t.2.dylib
+// RUN: nm -m %t.2.dylib | c++filt | grep value > %t.2.symbols
+// RUN: not grep weak %t.2.symbols
+
+#include <version>
+
+template <class T>
+struct optional {
+  T val_;
+  _LIBCPP_HIDE_FROM_ABI _LIBCPP_INTRODUCED_IN_LLVM_11_ATTRIBUTE T value() const { return val_; }
+};
+
+using PMF = int (optional<int>::*)() const;
+PMF f() { return &optional<int>::value; }
diff --git a/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.array/sized_delete_array.pass.cpp b/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.array/sized_delete_array.pass.cpp
index 1d763d6caba6..dd0b1d60d28d 100644
--- a/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.array/sized_delete_array.pass.cpp
+++ b/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.array/sized_delete_array.pass.cpp
@@ -14,8 +14,11 @@
 // ADDITIONAL_COMPILE_FLAGS(clang-18): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(apple-clang-15): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(apple-clang-16): -fsized-deallocation
+// ADDITIONAL_COMPILE_FLAGS(apple-clang-17): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(target=x86_64-w64-windows-gnu): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(target=i686-w64-windows-gnu): -fsized-deallocation
+// ADDITIONAL_COMPILE_FLAGS(target=aarch64-w64-windows-gnu): -fsized-deallocation
+// ADDITIONAL_COMPILE_FLAGS(target=armv7-w64-windows-gnu): -fsized-deallocation
 
 // Android clang-r536225 identifies as clang-19.0 but it predates the real
 // LLVM 19.0.0, so it also leaves sized deallocation off by default.
diff --git a/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.single/sized_delete.pass.cpp b/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.single/sized_delete.pass.cpp
index 462037e53374..9469af1c78f3 100644
--- a/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.single/sized_delete.pass.cpp
+++ b/libcxx/test/std/language.support/support.dynamic/new.delete/new.delete.single/sized_delete.pass.cpp
@@ -14,8 +14,11 @@
 // ADDITIONAL_COMPILE_FLAGS(clang-18): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(apple-clang-15): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(apple-clang-16): -fsized-deallocation
+// ADDITIONAL_COMPILE_FLAGS(apple-clang-17): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(target=x86_64-w64-windows-gnu): -fsized-deallocation
 // ADDITIONAL_COMPILE_FLAGS(target=i686-w64-windows-gnu): -fsized-deallocation
+// ADDITIONAL_COMPILE_FLAGS(target=aarch64-w64-windows-gnu): -fsized-deallocation
+// ADDITIONAL_COMPILE_FLAGS(target=armv7-w64-windows-gnu): -fsized-deallocation
 
 // Android clang-r536225 identifies as clang-19.0 but it predates the real
 // LLVM 19.0.0, so it also leaves sized deallocation off by default.
diff --git a/libcxx/test/std/numerics/c.math/signbit.pass.cpp b/libcxx/test/std/numerics/c.math/signbit.pass.cpp
index 143baf1fec94..b5e63dedf136 100644
--- a/libcxx/test/std/numerics/c.math/signbit.pass.cpp
+++ b/libcxx/test/std/numerics/c.math/signbit.pass.cpp
@@ -12,7 +12,7 @@
 // UNSUPPORTED: windows
 
 // These compilers don't support constexpr `__builtin_signbit` yet.
-// UNSUPPORTED: clang-18, clang-19, apple-clang-15, apple-clang-16
+// UNSUPPORTED: clang-18, clang-19, apple-clang-15, apple-clang-16, apple-clang-17
 
 // XFAIL: FROZEN-CXX03-HEADERS-FIXME
 
diff --git a/libcxx/test/std/utilities/meta/meta.rel/is_virtual_base_of.pass.cpp b/libcxx/test/std/utilities/meta/meta.rel/is_virtual_base_of.pass.cpp
index bcffa5812d04..f443d2030961 100644
--- a/libcxx/test/std/utilities/meta/meta.rel/is_virtual_base_of.pass.cpp
+++ b/libcxx/test/std/utilities/meta/meta.rel/is_virtual_base_of.pass.cpp
@@ -9,7 +9,7 @@
 // UNSUPPORTED: c++03, c++11, c++14, c++17, c++20, c++23
 
 // These compilers don't support __builtin_is_virtual_base_of yet.
-// UNSUPPORTED: clang-18, clang-19, gcc-14, apple-clang-16
+// UNSUPPORTED: clang-18, clang-19, gcc-14, apple-clang-16, apple-clang-17
 
 // <type_traits>
 
diff --git a/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.pass.cpp b/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.pass.cpp
index 24adec37431e..681ad13a07df 100644
--- a/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.pass.cpp
+++ b/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.pass.cpp
@@ -9,7 +9,7 @@
 // UNSUPPORTED: c++03, c++11, c++14, c++17, c++20
 
 // These compilers don't support __builtin_is_implicit_lifetime yet.
-// UNSUPPORTED: clang-18, clang-19, gcc-14, apple-clang-15, apple-clang-16
+// UNSUPPORTED: clang-18, clang-19, gcc-14, apple-clang-15, apple-clang-16, apple-clang-17
 
 // <type_traits>
 
diff --git a/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.verify.cpp b/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.verify.cpp
index 4bcb10d0b757..34462f9bf0ec 100644
--- a/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.verify.cpp
+++ b/libcxx/test/std/utilities/meta/meta.unary/meta.unary.prop/is_implicit_lifetime.verify.cpp
@@ -9,7 +9,7 @@
 // UNSUPPORTED: c++03, c++11, c++14, c++17, c++20
 
 // These compilers don't support __builtin_is_implicit_lifetime yet.
-// UNSUPPORTED: clang-18, clang-19, gcc-14, apple-clang-15, apple-clang-16
+// UNSUPPORTED: clang-18, clang-19, gcc-14, apple-clang-15, apple-clang-16, apple-clang-17
 
 // <type_traits>
 
diff --git a/libcxx/test/tools/clang_tidy_checks/CMakeLists.txt b/libcxx/test/tools/clang_tidy_checks/CMakeLists.txt
index 0f8f0e8864d0..da045fac92ce 100644
--- a/libcxx/test/tools/clang_tidy_checks/CMakeLists.txt
+++ b/libcxx/test/tools/clang_tidy_checks/CMakeLists.txt
@@ -8,6 +8,10 @@ set(Clang_DIR_SAVE ${Clang_DIR})
 # versions must match. Otherwise there likely will be ODR-violations. This had
 # led to crashes and incorrect output of the clang-tidy based checks.
 find_package(Clang ${CMAKE_CXX_COMPILER_VERSION})
+
+set(LLVM_DIR "${LLVM_DIR_SAVE}" CACHE PATH "The directory containing a CMake configuration file for LLVM." FORCE)
+set(Clang_DIR "${Clang_DIR_SAVE}" CACHE PATH "The directory containing a CMake configuration file for Clang." FORCE)
+
 if(NOT Clang_FOUND)
   message(STATUS "Clang-tidy tests are disabled since the "
                  "Clang development package is unavailable.")
@@ -19,9 +23,6 @@ if(NOT TARGET clangTidy)
   return()
 endif()
 
-set(LLVM_DIR "${LLVM_DIR_SAVE}" CACHE PATH "The directory containing a CMake configuration file for LLVM." FORCE)
-set(Clang_DIR "${Clang_DIR_SAVE}" CACHE PATH "The directory containing a CMake configuration file for Clang." FORCE)
-
 message(STATUS "Found system-installed LLVM ${LLVM_PACKAGE_VERSION} with headers in ${LLVM_INCLUDE_DIRS}")
 
 set(CMAKE_CXX_STANDARD 20)
diff --git a/lld/COFF/Driver.cpp b/lld/COFF/Driver.cpp
index ac3ac57bd17f..f50ca529df4d 100644
--- a/lld/COFF/Driver.cpp
+++ b/lld/COFF/Driver.cpp
@@ -2639,10 +2639,8 @@ void LinkerDriver::linkerMain(ArrayRef<const char *> argsArr) {
     createECExportThunks();
 
   // Resolve remaining undefined symbols and warn about imported locals.
-  ctx.forEachSymtab([&](SymbolTable &symtab) {
-    while (symtab.resolveRemainingUndefines())
-      run();
-  });
+  ctx.forEachSymtab(
+      [&](SymbolTable &symtab) { symtab.resolveRemainingUndefines(); });
 
   if (errorCount())
     return;
diff --git a/lld/COFF/MinGW.cpp b/lld/COFF/MinGW.cpp
index 76f5a0a7500b..097cf228f7d6 100644
--- a/lld/COFF/MinGW.cpp
+++ b/lld/COFF/MinGW.cpp
@@ -54,7 +54,12 @@ AutoExporter::AutoExporter(
       "libFortranDecimal",
       "libunwind",
       "libmsvcrt",
+      "libmsvcrt-os",
       "libucrtbase",
+      "libucrt",
+      "libucrtapp",
+      "libpthread",
+      "libwinpthread",
   };
 
   excludeObjects = {
diff --git a/lld/COFF/SymbolTable.cpp b/lld/COFF/SymbolTable.cpp
index 307bd4a0c941..a146e5211736 100644
--- a/lld/COFF/SymbolTable.cpp
+++ b/lld/COFF/SymbolTable.cpp
@@ -214,7 +214,8 @@ struct UndefinedDiag {
   std::vector<File> files;
 };
 
-static void reportUndefinedSymbol(COFFLinkerContext &ctx,
+static void reportUndefinedSymbol(SymbolTable *symTab,
+                                  COFFLinkerContext &ctx,
                                   const UndefinedDiag &undefDiag) {
   auto diag = errorOrWarn(ctx);
   diag << "undefined symbol: " << undefDiag.sym;
@@ -232,6 +233,17 @@ static void reportUndefinedSymbol(COFFLinkerContext &ctx,
   }
   if (numDisplayedRefs < numRefs)
     diag << "\n>>> referenced " << numRefs - numDisplayedRefs << " more times";
+
+  // Hints
+  StringRef name = undefDiag.sym->getName();
+  if (name.consume_front("__imp_")) {
+    Symbol *imp = symTab->find(name);
+    if (imp && imp->isLazy()) {
+      diag << "\nNOTE: a relevant symbol '" << imp->getName()
+           << "' is available in " << toString(imp->getFile())
+           << " but cannot be used because it is not an import library.";
+    }
+  }
 }
 
 void SymbolTable::loadMinGWSymbols() {
@@ -402,7 +414,7 @@ void SymbolTable::reportProblemSymbols(
       processFile(file, file->getSymbols());
 
   for (const UndefinedDiag &undefDiag : undefDiags)
-    reportUndefinedSymbol(ctx, undefDiag);
+    reportUndefinedSymbol(this, ctx, undefDiag);
 }
 
 void SymbolTable::reportUnresolvable() {
@@ -432,11 +444,10 @@ void SymbolTable::reportUnresolvable() {
   reportProblemSymbols(undefs, /*localImports=*/nullptr, true);
 }
 
-bool SymbolTable::resolveRemainingUndefines() {
+void SymbolTable::resolveRemainingUndefines() {
   llvm::TimeTraceScope timeScope("Resolve remaining undefined symbols");
   SmallPtrSet<Symbol *, 8> undefs;
   DenseMap<Symbol *, Symbol *> localImports;
-  bool foundLazy = false;
 
   for (auto &i : symMap) {
     Symbol *sym = i.second;
@@ -481,11 +492,6 @@ bool SymbolTable::resolveRemainingUndefines() {
             imp = findLocalSym(*mangledName);
         }
       }
-      if (imp && imp->isLazy()) {
-        forceLazy(imp);
-        foundLazy = true;
-        continue;
-      }
       if (imp && isa<Defined>(imp)) {
         auto *d = cast<Defined>(imp);
         replaceSymbol<DefinedLocalImport>(sym, ctx, name, d);
@@ -513,7 +519,6 @@ bool SymbolTable::resolveRemainingUndefines() {
   reportProblemSymbols(
       undefs, ctx.config.warnLocallyDefinedImported ? &localImports : nullptr,
       false);
-  return foundLazy;
 }
 
 std::pair<Symbol *, bool> SymbolTable::insert(StringRef name) {
diff --git a/lld/COFF/SymbolTable.h b/lld/COFF/SymbolTable.h
index ff6e8487f073..2916c23d95c8 100644
--- a/lld/COFF/SymbolTable.h
+++ b/lld/COFF/SymbolTable.h
@@ -58,10 +58,7 @@ public:
   // Try to resolve any undefined symbols and update the symbol table
   // accordingly, then print an error message for any remaining undefined
   // symbols and warn about imported local symbols.
-  // Returns whether more files might need to be linked in to resolve lazy
-  // symbols, in which case the caller is expected to call the function again
-  // after linking those files.
-  bool resolveRemainingUndefines();
+  void resolveRemainingUndefines();
 
   // Load lazy objects that are needed for MinGW automatic import and for
   // doing stdcall fixups.
diff --git a/lld/ELF/Arch/Hexagon.cpp b/lld/ELF/Arch/Hexagon.cpp
index 23b60672f631..4ba61db2733c 100644
--- a/lld/ELF/Arch/Hexagon.cpp
+++ b/lld/ELF/Arch/Hexagon.cpp
@@ -68,7 +68,7 @@ uint32_t Hexagon::calcEFlags() const {
     if (!ret || eflags > *ret)
       ret = eflags;
   }
-  return ret.value_or(/* Default Arch Rev: */ 0x60);
+  return ret.value_or(/* Default Arch Rev: */ EF_HEXAGON_MACH_V68);
 }
 
 static uint32_t applyMask(uint32_t mask, uint32_t data) {
diff --git a/lld/docs/ReleaseNotes.rst b/lld/docs/ReleaseNotes.rst
index e13b0cf0678c..b8604611e286 100644
--- a/lld/docs/ReleaseNotes.rst
+++ b/lld/docs/ReleaseNotes.rst
@@ -76,6 +76,11 @@ ELF Improvements
 * Supported relocation types for LoongArch target: ``R_LARCH_TLS_{LD,GD,DESC}_PCREL20_S2``.
   (`#100105 <https://github.com/llvm/llvm-project/pull/100105>`_)
 
+* The default Hexagon architecture version in ELF object files produced by
+  lld is changed to v68. This change is only effective when the version is
+  not provided in the command line by the user and cannot be inferred from
+  inputs.
+
 Breaking changes
 ----------------
 
diff --git a/lld/test/COFF/imports-static-lib-indirect.test b/lld/test/COFF/imports-static-lib-indirect.test
new file mode 100644
index 000000000000..beda0d7a31af
--- /dev/null
+++ b/lld/test/COFF/imports-static-lib-indirect.test
@@ -0,0 +1,26 @@
+# REQUIRES: x86
+
+# Pulling in on both a dllimport symbol and a static symbol should only warn.
+# RUN: split-file %s %t.dir
+# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/other.s -o %t.other.obj
+# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/main.s -o %t.main.obj
+# RUN: llvm-lib %t.other.obj -out:%t.other.lib
+# RUN: lld-link %t.other.lib %t.main.obj -out:%t.dll -dll 2>&1 | FileCheck %s
+
+CHECK: warning: {{.*}} locally defined symbol imported: foo {{.*}} [LNK4217]
+
+#--- other.s
+.text
+.globl other
+.globl foo
+other:
+  ret
+foo:
+  ret
+#--- main.s
+.text
+.global _DllMainCRTStartup
+_DllMainCRTStartup:
+  call *other(%rip)
+  call *__imp_foo(%rip)
+  ret
diff --git a/lld/test/COFF/imports-static-lib.test b/lld/test/COFF/imports-static-lib.test
new file mode 100644
index 000000000000..8e9525dab528
--- /dev/null
+++ b/lld/test/COFF/imports-static-lib.test
@@ -0,0 +1,33 @@
+# REQUIRES: x86
+
+# Ensure that we don't import dllimport symbols from static (non-import) libraries
+# RUN: split-file %s %t.dir
+# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/foo.s -o %t.foo.obj
+# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/main.s -o %t.main.obj
+# RUN: llvm-lib %t.foo.obj -out:%t.foo.lib
+# RUN: not lld-link %t.foo.lib %t.main.obj -out:%t.dll -dll 2>&1 | FileCheck %s
+
+CHECK: error: undefined symbol: __declspec(dllimport) foo
+CHECK: NOTE: a relevant symbol 'foo' is available in {{.*}}.foo.lib but cannot be used because it is not an import library.
+
+# Now do the same thing, but import the symbol from a import library.
+# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/foo_dll_main.s -o %t.foo_dll_main.obj
+# RUN: lld-link /out:%t.dll /dll %t.foo.obj %t.foo_dll_main.obj /export:foo /implib:%t.foo.imp.lib
+# RUN: lld-link %t.main.obj %t.foo.imp.lib -out:%t.exe -dll
+
+#--- foo.s
+.text
+.globl foo
+foo:
+  ret
+#--- foo_dll_main.s
+.text
+.global _DllMainCRTStartup
+_DllMainCRTStartup:
+  ret
+#--- main.s
+.text
+.global _DllMainCRTStartup
+_DllMainCRTStartup:
+  call *__imp_foo(%rip)
+  ret
diff --git a/lld/test/COFF/undefined_lazy.test b/lld/test/COFF/undefined_lazy.test
deleted file mode 100644
index ed5cd358b5cd..000000000000
--- a/lld/test/COFF/undefined_lazy.test
+++ /dev/null
@@ -1,26 +0,0 @@
-# REQUIRES: x86
-
-# RUN: split-file %s %t.dir
-# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/foo.s -o %t.foo.obj
-# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/bar.s -o %t.bar.obj
-# RUN: llvm-mc --filetype=obj -triple=x86_64-windows-msvc %t.dir/qux.s -o %t.qux.obj
-# RUN: llvm-lib %t.foo.obj -out:%t.foo.lib
-# RUN: llvm-lib %t.bar.obj -out:%t.bar.lib
-# RUN: lld-link %t.foo.lib %t.bar.lib %t.qux.obj -out:%t.dll -dll
-#
-#--- foo.s
-.text
-.globl foo
-foo:
-  call bar
-#--- bar.s
-.text
-.globl bar
-bar:
-  ret
-#--- qux.s
-.text
-.global _DllMainCRTStartup
-_DllMainCRTStartup:
-  call *__imp_foo(%rip)
-  ret
diff --git a/lld/test/ELF/emulation-hexagon.s b/lld/test/ELF/emulation-hexagon.s
index a8a02d4c428b..5bdd88941c26 100644
--- a/lld/test/ELF/emulation-hexagon.s
+++ b/lld/test/ELF/emulation-hexagon.s
@@ -1,5 +1,5 @@
 # REQUIRES: hexagon
-# RUN: llvm-mc -filetype=obj -triple=hexagon %s -o %t.o
+# RUN: llvm-mc -filetype=obj -triple=hexagon --mcpu=hexagonv73 %s -o %t.o
 # RUN: ld.lld %t.o -o %t
 # RUN: llvm-readelf --file-headers %t | FileCheck --check-prefix=CHECK %s
 # RUN: ld.lld -m hexagonelf %t.o -o %t
@@ -26,7 +26,7 @@
 # CHECK-NEXT:    Entry point address:               0x200B4
 # CHECK-NEXT:    Start of program headers:          52 (bytes into file)
 # CHECK-NEXT:    Start of section headers:
-# CHECK-NEXT:    Flags:                             0x60
+# CHECK-NEXT:    Flags:                             0x73
 # CHECK-NEXT:    Size of this header:               52 (bytes)
 # CHECK-NEXT:    Size of program headers:           32 (bytes)
 
diff --git a/lld/test/ELF/hexagon-eflag.s b/lld/test/ELF/hexagon-eflag.s
index dbe8604f69fd..ac9123832ac8 100644
--- a/lld/test/ELF/hexagon-eflag.s
+++ b/lld/test/ELF/hexagon-eflag.s
@@ -3,10 +3,11 @@
 # RUN: llvm-mc -filetype=obj -mv60 -triple=hexagon-unknown-elf %S/Inputs/hexagon.s -o %t2
 # RUN: ld.lld %t2 %t  -o %t3
 # RUN: llvm-readelf -h  %t3 | FileCheck %s
-# Verify that the largest arch in the input list is selected.
+## Verify that the largest arch in the input list is selected.
 # CHECK: Flags: 0x62
 
+## Verify the arch version when it cannot be inferred from inputs.
 # RUN: llvm-ar rcsD %t4
 # RUN: ld.lld -m hexagonelf %t4 -o %t5
 # RUN: llvm-readelf -h  %t5 | FileCheck --check-prefix=CHECK-EMPTYARCHIVE %s
-# CHECK-EMPTYARCHIVE: Flags: 0x60
+# CHECK-EMPTYARCHIVE: Flags: 0x68
diff --git a/lld/wasm/Config.h b/lld/wasm/Config.h
index 1fa6c42d9cd8..527edc11c48e 100644
--- a/lld/wasm/Config.h
+++ b/lld/wasm/Config.h
@@ -32,6 +32,11 @@ class InputTable;
 class InputGlobal;
 class InputFunction;
 class Symbol;
+class DefinedData;
+class GlobalSymbol;
+class DefinedFunction;
+class UndefinedGlobal;
+class TableSymbol;
 
 // For --unresolved-symbols.
 enum class UnresolvedPolicy { ReportError, Warn, Ignore, ImportDynamic };
@@ -139,6 +144,107 @@ struct Ctx {
   llvm::SmallVector<InputGlobal *, 0> syntheticGlobals;
   llvm::SmallVector<InputTable *, 0> syntheticTables;
 
+  // linker-generated symbols
+  struct WasmSym {
+    // __global_base
+    // Symbol marking the start of the global section.
+    DefinedData *globalBase;
+
+    // __stack_pointer/__stack_low/__stack_high
+    // Global that holds current value of stack pointer and data symbols marking
+    // the start and end of the stack region.  stackPointer is initialized to
+    // stackHigh and grows downwards towards stackLow
+    GlobalSymbol *stackPointer;
+    DefinedData *stackLow;
+    DefinedData *stackHigh;
+
+    // __tls_base
+    // Global that holds the address of the base of the current thread's
+    // TLS block.
+    GlobalSymbol *tlsBase;
+
+    // __tls_size
+    // Symbol whose value is the size of the TLS block.
+    GlobalSymbol *tlsSize;
+
+    // __tls_size
+    // Symbol whose value is the alignment of the TLS block.
+    GlobalSymbol *tlsAlign;
+
+    // __data_end
+    // Symbol marking the end of the data and bss.
+    DefinedData *dataEnd;
+
+    // __heap_base/__heap_end
+    // Symbols marking the beginning and end of the "heap". It starts at the end
+    // of the data, bss and explicit stack, and extends to the end of the linear
+    // memory allocated by wasm-ld. This region of memory is not used by the
+    // linked code, so it may be used as a backing store for `sbrk` or `malloc`
+    // implementations.
+    DefinedData *heapBase;
+    DefinedData *heapEnd;
+
+    // __wasm_init_memory_flag
+    // Symbol whose contents are nonzero iff memory has already been
+    // initialized.
+    DefinedData *initMemoryFlag;
+
+    // __wasm_init_memory
+    // Function that initializes passive data segments during instantiation.
+    DefinedFunction *initMemory;
+
+    // __wasm_call_ctors
+    // Function that directly calls all ctors in priority order.
+    DefinedFunction *callCtors;
+
+    // __wasm_call_dtors
+    // Function that calls the libc/etc. cleanup function.
+    DefinedFunction *callDtors;
+
+    // __wasm_apply_global_relocs
+    // Function that applies relocations to wasm globals post-instantiation.
+    // Unlike __wasm_apply_data_relocs this needs to run on every thread.
+    DefinedFunction *applyGlobalRelocs;
+
+    // __wasm_apply_tls_relocs
+    // Like __wasm_apply_data_relocs but for TLS section.  These must be
+    // delayed until __wasm_init_tls.
+    DefinedFunction *applyTLSRelocs;
+
+    // __wasm_apply_global_tls_relocs
+    // Like applyGlobalRelocs but for globals that hold TLS addresses.  These
+    // must be delayed until __wasm_init_tls.
+    DefinedFunction *applyGlobalTLSRelocs;
+
+    // __wasm_init_tls
+    // Function that allocates thread-local storage and initializes it.
+    DefinedFunction *initTLS;
+
+    // Pointer to the function that is to be used in the start section.
+    // (normally an alias of initMemory, or applyGlobalRelocs).
+    DefinedFunction *startFunction;
+
+    // __dso_handle
+    // Symbol used in calls to __cxa_atexit to determine current DLL
+    DefinedData *dsoHandle;
+
+    // __table_base
+    // Used in PIC code for offset of indirect function table
+    UndefinedGlobal *tableBase;
+    DefinedData *definedTableBase;
+
+    // __memory_base
+    // Used in PIC code for offset of global data
+    UndefinedGlobal *memoryBase;
+    DefinedData *definedMemoryBase;
+
+    // __indirect_function_table
+    // Used as an address space for function pointers, with each function that
+    // is used as a function pointer being allocated a slot.
+    TableSymbol *indirectFunctionTable;
+  };
+  WasmSym sym;
+
   // True if we are creating position-independent code.
   bool isPic = false;
 
diff --git a/lld/wasm/Driver.cpp b/lld/wasm/Driver.cpp
index c3a74dde6480..467c49e9981b 100644
--- a/lld/wasm/Driver.cpp
+++ b/lld/wasm/Driver.cpp
@@ -70,6 +70,7 @@ void Ctx::reset() {
   isPic = false;
   legacyFunctionTable = false;
   emitBssSegments = false;
+  sym = WasmSym{};
 }
 
 namespace {
@@ -941,14 +942,14 @@ static void createSyntheticSymbols() {
                                                             true};
   static llvm::wasm::WasmGlobalType mutableGlobalTypeI64 = {WASM_TYPE_I64,
                                                             true};
-  WasmSym::callCtors = symtab->addSyntheticFunction(
+  ctx.sym.callCtors = symtab->addSyntheticFunction(
       "__wasm_call_ctors", WASM_SYMBOL_VISIBILITY_HIDDEN,
       make<SyntheticFunction>(nullSignature, "__wasm_call_ctors"));
 
   bool is64 = ctx.arg.is64.value_or(false);
 
   if (ctx.isPic) {
-    WasmSym::stackPointer =
+    ctx.sym.stackPointer =
         createUndefinedGlobal("__stack_pointer", ctx.arg.is64.value_or(false)
                                                      ? &mutableGlobalTypeI64
                                                      : &mutableGlobalTypeI32);
@@ -958,25 +959,24 @@ static void createSyntheticSymbols() {
     // See:
     // https://github.com/WebAssembly/tool-conventions/blob/main/DynamicLinking.md
     auto *globalType = is64 ? &globalTypeI64 : &globalTypeI32;
-    WasmSym::memoryBase = createUndefinedGlobal("__memory_base", globalType);
-    WasmSym::tableBase = createUndefinedGlobal("__table_base", globalType);
-    WasmSym::memoryBase->markLive();
-    WasmSym::tableBase->markLive();
+    ctx.sym.memoryBase = createUndefinedGlobal("__memory_base", globalType);
+    ctx.sym.tableBase = createUndefinedGlobal("__table_base", globalType);
+    ctx.sym.memoryBase->markLive();
+    ctx.sym.tableBase->markLive();
   } else {
     // For non-PIC code
-    WasmSym::stackPointer = createGlobalVariable("__stack_pointer", true);
-    WasmSym::stackPointer->markLive();
+    ctx.sym.stackPointer = createGlobalVariable("__stack_pointer", true);
+    ctx.sym.stackPointer->markLive();
   }
 
   if (ctx.arg.sharedMemory) {
-    WasmSym::tlsBase = createGlobalVariable("__tls_base", true);
-    WasmSym::tlsSize = createGlobalVariable("__tls_size", false);
-    WasmSym::tlsAlign = createGlobalVariable("__tls_align", false);
-    WasmSym::initTLS = symtab->addSyntheticFunction(
+    ctx.sym.tlsBase = createGlobalVariable("__tls_base", true);
+    ctx.sym.tlsSize = createGlobalVariable("__tls_size", false);
+    ctx.sym.tlsAlign = createGlobalVariable("__tls_align", false);
+    ctx.sym.initTLS = symtab->addSyntheticFunction(
         "__wasm_init_tls", WASM_SYMBOL_VISIBILITY_HIDDEN,
-        make<SyntheticFunction>(
-            is64 ? i64ArgSignature : i32ArgSignature,
-            "__wasm_init_tls"));
+        make<SyntheticFunction>(is64 ? i64ArgSignature : i32ArgSignature,
+                                "__wasm_init_tls"));
   }
 }
 
@@ -984,19 +984,19 @@ static void createOptionalSymbols() {
   if (ctx.arg.relocatable)
     return;
 
-  WasmSym::dsoHandle = symtab->addOptionalDataSymbol("__dso_handle");
+  ctx.sym.dsoHandle = symtab->addOptionalDataSymbol("__dso_handle");
 
   if (!ctx.arg.shared)
-    WasmSym::dataEnd = symtab->addOptionalDataSymbol("__data_end");
+    ctx.sym.dataEnd = symtab->addOptionalDataSymbol("__data_end");
 
   if (!ctx.isPic) {
-    WasmSym::stackLow = symtab->addOptionalDataSymbol("__stack_low");
-    WasmSym::stackHigh = symtab->addOptionalDataSymbol("__stack_high");
-    WasmSym::globalBase = symtab->addOptionalDataSymbol("__global_base");
-    WasmSym::heapBase = symtab->addOptionalDataSymbol("__heap_base");
-    WasmSym::heapEnd = symtab->addOptionalDataSymbol("__heap_end");
-    WasmSym::definedMemoryBase = symtab->addOptionalDataSymbol("__memory_base");
-    WasmSym::definedTableBase = symtab->addOptionalDataSymbol("__table_base");
+    ctx.sym.stackLow = symtab->addOptionalDataSymbol("__stack_low");
+    ctx.sym.stackHigh = symtab->addOptionalDataSymbol("__stack_high");
+    ctx.sym.globalBase = symtab->addOptionalDataSymbol("__global_base");
+    ctx.sym.heapBase = symtab->addOptionalDataSymbol("__heap_base");
+    ctx.sym.heapEnd = symtab->addOptionalDataSymbol("__heap_end");
+    ctx.sym.definedMemoryBase = symtab->addOptionalDataSymbol("__memory_base");
+    ctx.sym.definedTableBase = symtab->addOptionalDataSymbol("__table_base");
   }
 
   // For non-shared memory programs we still need to define __tls_base since we
@@ -1009,7 +1009,7 @@ static void createOptionalSymbols() {
   // __tls_size and __tls_align are not needed in this case since they are only
   // needed for __wasm_init_tls (which we do not create in this case).
   if (!ctx.arg.sharedMemory)
-    WasmSym::tlsBase = createOptionalGlobal("__tls_base", false);
+    ctx.sym.tlsBase = createOptionalGlobal("__tls_base", false);
 }
 
 static void processStubLibrariesPreLTO() {
@@ -1384,9 +1384,9 @@ void LinkerDriver::linkerMain(ArrayRef<const char *> argsArr) {
   // by libc/etc., because destructors are registered dynamically with
   // `__cxa_atexit` and friends.
   if (!ctx.arg.relocatable && !ctx.arg.shared &&
-      !WasmSym::callCtors->isUsedInRegularObj &&
-      WasmSym::callCtors->getName() != ctx.arg.entry &&
-      !ctx.arg.exportedSymbols.count(WasmSym::callCtors->getName())) {
+      !ctx.sym.callCtors->isUsedInRegularObj &&
+      ctx.sym.callCtors->getName() != ctx.arg.entry &&
+      !ctx.arg.exportedSymbols.count(ctx.sym.callCtors->getName())) {
     if (Symbol *callDtors =
             handleUndefined("__wasm_call_dtors", "<internal>")) {
       if (auto *callDtorsFunc = dyn_cast<DefinedFunction>(callDtors)) {
@@ -1395,7 +1395,7 @@ void LinkerDriver::linkerMain(ArrayRef<const char *> argsArr) {
              !callDtorsFunc->signature->Returns.empty())) {
           error("__wasm_call_dtors must have no argument or return values");
         }
-        WasmSym::callDtors = callDtorsFunc;
+        ctx.sym.callDtors = callDtorsFunc;
       } else {
         error("__wasm_call_dtors must be a function");
       }
@@ -1488,7 +1488,7 @@ void LinkerDriver::linkerMain(ArrayRef<const char *> argsArr) {
   markLive();
 
   // Provide the indirect function table if needed.
-  WasmSym::indirectFunctionTable =
+  ctx.sym.indirectFunctionTable =
       symtab->resolveIndirectFunctionTable(/*required =*/false);
 
   if (errorCount())
diff --git a/lld/wasm/InputChunks.cpp b/lld/wasm/InputChunks.cpp
index ccdc92f5c8d7..0e6c4e691be1 100644
--- a/lld/wasm/InputChunks.cpp
+++ b/lld/wasm/InputChunks.cpp
@@ -397,9 +397,9 @@ bool InputChunk::generateRelocationCode(raw_ostream &os) const {
     if (ctx.isPic) {
       writeU8(os, WASM_OPCODE_GLOBAL_GET, "GLOBAL_GET");
       if (isTLS())
-        writeUleb128(os, WasmSym::tlsBase->getGlobalIndex(), "tls_base");
+        writeUleb128(os, ctx.sym.tlsBase->getGlobalIndex(), "tls_base");
       else
-        writeUleb128(os, WasmSym::memoryBase->getGlobalIndex(), "memory_base");
+        writeUleb128(os, ctx.sym.memoryBase->getGlobalIndex(), "memory_base");
       writeU8(os, opcode_ptr_add, "ADD");
     }
 
@@ -422,12 +422,12 @@ bool InputChunk::generateRelocationCode(raw_ostream &os) const {
       }
     } else {
       assert(ctx.isPic);
-      const GlobalSymbol* baseSymbol = WasmSym::memoryBase;
+      const GlobalSymbol *baseSymbol = ctx.sym.memoryBase;
       if (rel.Type == R_WASM_TABLE_INDEX_I32 ||
           rel.Type == R_WASM_TABLE_INDEX_I64)
-        baseSymbol = WasmSym::tableBase;
+        baseSymbol = ctx.sym.tableBase;
       else if (sym->isTLS())
-        baseSymbol = WasmSym::tlsBase;
+        baseSymbol = ctx.sym.tlsBase;
       writeU8(os, WASM_OPCODE_GLOBAL_GET, "GLOBAL_GET");
       writeUleb128(os, baseSymbol->getGlobalIndex(), "base");
       writeU8(os, opcode_reloc_const, "CONST");
diff --git a/lld/wasm/MarkLive.cpp b/lld/wasm/MarkLive.cpp
index 13c7a3d894fe..2b2cf19f14b3 100644
--- a/lld/wasm/MarkLive.cpp
+++ b/lld/wasm/MarkLive.cpp
@@ -114,8 +114,8 @@ void MarkLive::run() {
     if (sym->isNoStrip() || sym->isExported())
       enqueue(sym);
 
-  if (WasmSym::callDtors)
-    enqueue(WasmSym::callDtors);
+  if (ctx.sym.callDtors)
+    enqueue(ctx.sym.callDtors);
 
   for (const ObjFile *obj : ctx.objectFiles)
     if (obj->isLive()) {
@@ -131,7 +131,7 @@ void MarkLive::run() {
   // If we have any non-discarded init functions, mark `__wasm_call_ctors` as
   // live so that we assign it an index and call it.
   if (isCallCtorsLive())
-    WasmSym::callCtors->markLive();
+    ctx.sym.callCtors->markLive();
 }
 
 void MarkLive::mark() {
diff --git a/lld/wasm/OutputSections.cpp b/lld/wasm/OutputSections.cpp
index 95f7ecc29de6..4142a913c8cb 100644
--- a/lld/wasm/OutputSections.cpp
+++ b/lld/wasm/OutputSections.cpp
@@ -123,7 +123,7 @@ void DataSection::finalizeContents() {
     if ((segment->initFlags & WASM_DATA_SEGMENT_IS_PASSIVE) == 0) {
       if (ctx.isPic && ctx.arg.extendedConst) {
         writeU8(os, WASM_OPCODE_GLOBAL_GET, "global get");
-        writeUleb128(os, WasmSym::memoryBase->getGlobalIndex(),
+        writeUleb128(os, ctx.sym.memoryBase->getGlobalIndex(),
                      "literal (global index)");
         if (segment->startVA) {
           writePtrConst(os, segment->startVA, is64, "offset");
@@ -136,7 +136,7 @@ void DataSection::finalizeContents() {
         if (ctx.isPic) {
           assert(segment->startVA == 0);
           initExpr.Inst.Opcode = WASM_OPCODE_GLOBAL_GET;
-          initExpr.Inst.Value.Global = WasmSym::memoryBase->getGlobalIndex();
+          initExpr.Inst.Value.Global = ctx.sym.memoryBase->getGlobalIndex();
         } else {
           initExpr = intConst(segment->startVA, is64);
         }
diff --git a/lld/wasm/Symbols.cpp b/lld/wasm/Symbols.cpp
index a687fd6d6c4e..92a933ecbb02 100644
--- a/lld/wasm/Symbols.cpp
+++ b/lld/wasm/Symbols.cpp
@@ -77,31 +77,6 @@ std::string toString(wasm::Symbol::Kind kind) {
 }
 
 namespace wasm {
-DefinedFunction *WasmSym::callCtors;
-DefinedFunction *WasmSym::callDtors;
-DefinedFunction *WasmSym::initMemory;
-DefinedFunction *WasmSym::applyGlobalRelocs;
-DefinedFunction *WasmSym::applyTLSRelocs;
-DefinedFunction *WasmSym::applyGlobalTLSRelocs;
-DefinedFunction *WasmSym::initTLS;
-DefinedFunction *WasmSym::startFunction;
-DefinedData *WasmSym::dsoHandle;
-DefinedData *WasmSym::dataEnd;
-DefinedData *WasmSym::globalBase;
-DefinedData *WasmSym::heapBase;
-DefinedData *WasmSym::heapEnd;
-DefinedData *WasmSym::initMemoryFlag;
-GlobalSymbol *WasmSym::stackPointer;
-DefinedData *WasmSym::stackLow;
-DefinedData *WasmSym::stackHigh;
-GlobalSymbol *WasmSym::tlsBase;
-GlobalSymbol *WasmSym::tlsSize;
-GlobalSymbol *WasmSym::tlsAlign;
-UndefinedGlobal *WasmSym::tableBase;
-DefinedData *WasmSym::definedTableBase;
-UndefinedGlobal *WasmSym::memoryBase;
-DefinedData *WasmSym::definedMemoryBase;
-TableSymbol *WasmSym::indirectFunctionTable;
 
 WasmSymbolType Symbol::getWasmType() const {
   if (isa<FunctionSymbol>(this))
diff --git a/lld/wasm/Symbols.h b/lld/wasm/Symbols.h
index b409fffc50a6..55ee21939ce0 100644
--- a/lld/wasm/Symbols.h
+++ b/lld/wasm/Symbols.h
@@ -537,105 +537,6 @@ public:
   const WasmSignature *signature = nullptr;
 };
 
-// linker-generated symbols
-struct WasmSym {
-  // __global_base
-  // Symbol marking the start of the global section.
-  static DefinedData *globalBase;
-
-  // __stack_pointer/__stack_low/__stack_high
-  // Global that holds current value of stack pointer and data symbols marking
-  // the start and end of the stack region.  stackPointer is initialized to
-  // stackHigh and grows downwards towards stackLow
-  static GlobalSymbol *stackPointer;
-  static DefinedData *stackLow;
-  static DefinedData *stackHigh;
-
-  // __tls_base
-  // Global that holds the address of the base of the current thread's
-  // TLS block.
-  static GlobalSymbol *tlsBase;
-
-  // __tls_size
-  // Symbol whose value is the size of the TLS block.
-  static GlobalSymbol *tlsSize;
-
-  // __tls_size
-  // Symbol whose value is the alignment of the TLS block.
-  static GlobalSymbol *tlsAlign;
-
-  // __data_end
-  // Symbol marking the end of the data and bss.
-  static DefinedData *dataEnd;
-
-  // __heap_base/__heap_end
-  // Symbols marking the beginning and end of the "heap". It starts at the end
-  // of the data, bss and explicit stack, and extends to the end of the linear
-  // memory allocated by wasm-ld. This region of memory is not used by the
-  // linked code, so it may be used as a backing store for `sbrk` or `malloc`
-  // implementations.
-  static DefinedData *heapBase;
-  static DefinedData *heapEnd;
-
-  // __wasm_init_memory_flag
-  // Symbol whose contents are nonzero iff memory has already been initialized.
-  static DefinedData *initMemoryFlag;
-
-  // __wasm_init_memory
-  // Function that initializes passive data segments during instantiation.
-  static DefinedFunction *initMemory;
-
-  // __wasm_call_ctors
-  // Function that directly calls all ctors in priority order.
-  static DefinedFunction *callCtors;
-
-  // __wasm_call_dtors
-  // Function that calls the libc/etc. cleanup function.
-  static DefinedFunction *callDtors;
-
-  // __wasm_apply_global_relocs
-  // Function that applies relocations to wasm globals post-instantiation.
-  // Unlike __wasm_apply_data_relocs this needs to run on every thread.
-  static DefinedFunction *applyGlobalRelocs;
-
-  // __wasm_apply_tls_relocs
-  // Like __wasm_apply_data_relocs but for TLS section.  These must be
-  // delayed until __wasm_init_tls.
-  static DefinedFunction *applyTLSRelocs;
-
-  // __wasm_apply_global_tls_relocs
-  // Like applyGlobalRelocs but for globals that hold TLS addresses.  These
-  // must be delayed until __wasm_init_tls.
-  static DefinedFunction *applyGlobalTLSRelocs;
-
-  // __wasm_init_tls
-  // Function that allocates thread-local storage and initializes it.
-  static DefinedFunction *initTLS;
-
-  // Pointer to the function that is to be used in the start section.
-  // (normally an alias of initMemory, or applyGlobalRelocs).
-  static DefinedFunction *startFunction;
-
-  // __dso_handle
-  // Symbol used in calls to __cxa_atexit to determine current DLL
-  static DefinedData *dsoHandle;
-
-  // __table_base
-  // Used in PIC code for offset of indirect function table
-  static UndefinedGlobal *tableBase;
-  static DefinedData *definedTableBase;
-
-  // __memory_base
-  // Used in PIC code for offset of global data
-  static UndefinedGlobal *memoryBase;
-  static DefinedData *definedMemoryBase;
-
-  // __indirect_function_table
-  // Used as an address space for function pointers, with each function that is
-  // used as a function pointer being allocated a slot.
-  static TableSymbol *indirectFunctionTable;
-};
-
 // A buffer class that is large enough to hold any Symbol-derived
 // object. We allocate memory using this class and instantiate a symbol
 // using the placement new.
diff --git a/lld/wasm/SyntheticSections.cpp b/lld/wasm/SyntheticSections.cpp
index 7fb44b9f0c00..0e2aa57e9048 100644
--- a/lld/wasm/SyntheticSections.cpp
+++ b/lld/wasm/SyntheticSections.cpp
@@ -319,8 +319,8 @@ void TableSection::addTable(InputTable *table) {
   // Some inputs require that the indirect function table be assigned to table
   // number 0.
   if (ctx.legacyFunctionTable &&
-      isa<DefinedTable>(WasmSym::indirectFunctionTable) &&
-      cast<DefinedTable>(WasmSym::indirectFunctionTable)->table == table) {
+      isa<DefinedTable>(ctx.sym.indirectFunctionTable) &&
+      cast<DefinedTable>(ctx.sym.indirectFunctionTable)->table == table) {
     if (out.importSec->getNumImportedTables()) {
       // Alack!  Some other input imported a table, meaning that we are unable
       // to assign table number 0 to the indirect function table.
@@ -395,8 +395,8 @@ void GlobalSection::assignIndexes() {
 }
 
 static void ensureIndirectFunctionTable() {
-  if (!WasmSym::indirectFunctionTable)
-    WasmSym::indirectFunctionTable =
+  if (!ctx.sym.indirectFunctionTable)
+    ctx.sym.indirectFunctionTable =
         symtab->resolveIndirectFunctionTable(/*required =*/true);
 }
 
@@ -430,10 +430,9 @@ void GlobalSection::generateRelocationCode(raw_ostream &os, bool TLS) const {
       // Get __memory_base
       writeU8(os, WASM_OPCODE_GLOBAL_GET, "GLOBAL_GET");
       if (sym->isTLS())
-        writeUleb128(os, WasmSym::tlsBase->getGlobalIndex(), "__tls_base");
+        writeUleb128(os, ctx.sym.tlsBase->getGlobalIndex(), "__tls_base");
       else
-        writeUleb128(os, WasmSym::memoryBase->getGlobalIndex(),
-                     "__memory_base");
+        writeUleb128(os, ctx.sym.memoryBase->getGlobalIndex(), "__memory_base");
 
       // Add the virtual address of the data symbol
       writeU8(os, opcode_ptr_const, "CONST");
@@ -443,7 +442,7 @@ void GlobalSection::generateRelocationCode(raw_ostream &os, bool TLS) const {
         continue;
       // Get __table_base
       writeU8(os, WASM_OPCODE_GLOBAL_GET, "GLOBAL_GET");
-      writeUleb128(os, WasmSym::tableBase->getGlobalIndex(), "__table_base");
+      writeUleb128(os, ctx.sym.tableBase->getGlobalIndex(), "__table_base");
 
       // Add the table index to __table_base
       writeU8(os, opcode_ptr_const, "CONST");
@@ -490,13 +489,13 @@ void GlobalSection::writeBody() {
     if (ctx.arg.extendedConst && ctx.isPic) {
       if (auto *d = dyn_cast<DefinedData>(sym)) {
         if (!sym->isTLS()) {
-          globalIdx = WasmSym::memoryBase->getGlobalIndex();
+          globalIdx = ctx.sym.memoryBase->getGlobalIndex();
           offset = d->getVA();
           useExtendedConst = true;
         }
       } else if (auto *f = dyn_cast<FunctionSymbol>(sym)) {
         if (!sym->isStub) {
-          globalIdx = WasmSym::tableBase->getGlobalIndex();
+          globalIdx = ctx.sym.tableBase->getGlobalIndex();
           offset = f->getTableIndex();
           useExtendedConst = true;
         }
@@ -550,14 +549,11 @@ void ExportSection::writeBody() {
     writeExport(os, export_);
 }
 
-bool StartSection::isNeeded() const {
-  return WasmSym::startFunction != nullptr;
-}
+bool StartSection::isNeeded() const { return ctx.sym.startFunction != nullptr; }
 
 void StartSection::writeBody() {
   raw_ostream &os = bodyOutputStream;
-  writeUleb128(os, WasmSym::startFunction->getFunctionIndex(),
-               "function index");
+  writeUleb128(os, ctx.sym.startFunction->getFunctionIndex(), "function index");
 }
 
 void ElemSection::addEntry(FunctionSymbol *sym) {
@@ -573,9 +569,9 @@ void ElemSection::addEntry(FunctionSymbol *sym) {
 void ElemSection::writeBody() {
   raw_ostream &os = bodyOutputStream;
 
-  assert(WasmSym::indirectFunctionTable);
+  assert(ctx.sym.indirectFunctionTable);
   writeUleb128(os, 1, "segment count");
-  uint32_t tableNumber = WasmSym::indirectFunctionTable->getTableNumber();
+  uint32_t tableNumber = ctx.sym.indirectFunctionTable->getTableNumber();
   uint32_t flags = 0;
   if (tableNumber)
     flags |= WASM_ELEM_SEGMENT_HAS_TABLE_NUMBER;
@@ -587,7 +583,7 @@ void ElemSection::writeBody() {
   initExpr.Extended = false;
   if (ctx.isPic) {
     initExpr.Inst.Opcode = WASM_OPCODE_GLOBAL_GET;
-    initExpr.Inst.Value.Global = WasmSym::tableBase->getGlobalIndex();
+    initExpr.Inst.Value.Global = ctx.sym.tableBase->getGlobalIndex();
   } else {
     bool is64 = ctx.arg.is64.value_or(false);
     initExpr = intConst(ctx.arg.tableBase, is64);
diff --git a/lld/wasm/Writer.cpp b/lld/wasm/Writer.cpp
index 76e38f548157..2bf4b370a7db 100644
--- a/lld/wasm/Writer.cpp
+++ b/lld/wasm/Writer.cpp
@@ -340,16 +340,16 @@ void Writer::layoutMemory() {
     if (ctx.arg.relocatable || ctx.isPic)
       return;
     memoryPtr = alignTo(memoryPtr, stackAlignment);
-    if (WasmSym::stackLow)
-      WasmSym::stackLow->setVA(memoryPtr);
+    if (ctx.sym.stackLow)
+      ctx.sym.stackLow->setVA(memoryPtr);
     if (ctx.arg.zStackSize != alignTo(ctx.arg.zStackSize, stackAlignment))
       error("stack size must be " + Twine(stackAlignment) + "-byte aligned");
     log("mem: stack size  = " + Twine(ctx.arg.zStackSize));
     log("mem: stack base  = " + Twine(memoryPtr));
     memoryPtr += ctx.arg.zStackSize;
-    setGlobalPtr(cast<DefinedGlobal>(WasmSym::stackPointer), memoryPtr);
-    if (WasmSym::stackHigh)
-      WasmSym::stackHigh->setVA(memoryPtr);
+    setGlobalPtr(cast<DefinedGlobal>(ctx.sym.stackPointer), memoryPtr);
+    if (ctx.sym.stackHigh)
+      ctx.sym.stackHigh->setVA(memoryPtr);
     log("mem: stack top   = " + Twine(memoryPtr));
   };
 
@@ -367,15 +367,15 @@ void Writer::layoutMemory() {
   }
 
   log("mem: global base = " + Twine(memoryPtr));
-  if (WasmSym::globalBase)
-    WasmSym::globalBase->setVA(memoryPtr);
+  if (ctx.sym.globalBase)
+    ctx.sym.globalBase->setVA(memoryPtr);
 
   uint64_t dataStart = memoryPtr;
 
   // Arbitrarily set __dso_handle handle to point to the start of the data
   // segments.
-  if (WasmSym::dsoHandle)
-    WasmSym::dsoHandle->setVA(dataStart);
+  if (ctx.sym.dsoHandle)
+    ctx.sym.dsoHandle->setVA(dataStart);
 
   out.dylinkSec->memAlign = 0;
   for (OutputSegment *seg : segments) {
@@ -386,16 +386,16 @@ void Writer::layoutMemory() {
                 memoryPtr, seg->size, seg->alignment));
 
     if (!ctx.arg.relocatable && seg->isTLS()) {
-      if (WasmSym::tlsSize) {
-        auto *tlsSize = cast<DefinedGlobal>(WasmSym::tlsSize);
+      if (ctx.sym.tlsSize) {
+        auto *tlsSize = cast<DefinedGlobal>(ctx.sym.tlsSize);
         setGlobalPtr(tlsSize, seg->size);
       }
-      if (WasmSym::tlsAlign) {
-        auto *tlsAlign = cast<DefinedGlobal>(WasmSym::tlsAlign);
+      if (ctx.sym.tlsAlign) {
+        auto *tlsAlign = cast<DefinedGlobal>(ctx.sym.tlsAlign);
         setGlobalPtr(tlsAlign, int64_t{1} << seg->alignment);
       }
-      if (!ctx.arg.sharedMemory && WasmSym::tlsBase) {
-        auto *tlsBase = cast<DefinedGlobal>(WasmSym::tlsBase);
+      if (!ctx.arg.sharedMemory && ctx.sym.tlsBase) {
+        auto *tlsBase = cast<DefinedGlobal>(ctx.sym.tlsBase);
         setGlobalPtr(tlsBase, memoryPtr);
       }
     }
@@ -406,17 +406,17 @@ void Writer::layoutMemory() {
   // Make space for the memory initialization flag
   if (ctx.arg.sharedMemory && hasPassiveInitializedSegments()) {
     memoryPtr = alignTo(memoryPtr, 4);
-    WasmSym::initMemoryFlag = symtab->addSyntheticDataSymbol(
+    ctx.sym.initMemoryFlag = symtab->addSyntheticDataSymbol(
         "__wasm_init_memory_flag", WASM_SYMBOL_VISIBILITY_HIDDEN);
-    WasmSym::initMemoryFlag->markLive();
-    WasmSym::initMemoryFlag->setVA(memoryPtr);
+    ctx.sym.initMemoryFlag->markLive();
+    ctx.sym.initMemoryFlag->setVA(memoryPtr);
     log(formatv("mem: {0,-15} offset={1,-8} size={2,-8} align={3}",
                 "__wasm_init_memory_flag", memoryPtr, 4, 4));
     memoryPtr += 4;
   }
 
-  if (WasmSym::dataEnd)
-    WasmSym::dataEnd->setVA(memoryPtr);
+  if (ctx.sym.dataEnd)
+    ctx.sym.dataEnd->setVA(memoryPtr);
 
   uint64_t staticDataSize = memoryPtr - dataStart;
   log("mem: static data = " + Twine(staticDataSize));
@@ -426,7 +426,7 @@ void Writer::layoutMemory() {
   if (!ctx.arg.stackFirst)
     placeStack();
 
-  if (WasmSym::heapBase) {
+  if (ctx.sym.heapBase) {
     // Set `__heap_base` to follow the end of the stack or global data. The
     // fact that this comes last means that a malloc/brk implementation can
     // grow the heap at runtime.
@@ -434,7 +434,7 @@ void Writer::layoutMemory() {
     // __heap_base to be aligned already.
     memoryPtr = alignTo(memoryPtr, heapAlignment);
     log("mem: heap base   = " + Twine(memoryPtr));
-    WasmSym::heapBase->setVA(memoryPtr);
+    ctx.sym.heapBase->setVA(memoryPtr);
   }
 
   uint64_t maxMemorySetting = 1ULL << 32;
@@ -470,12 +470,12 @@ void Writer::layoutMemory() {
   out.memorySec->numMemoryPages = memoryPtr / WasmPageSize;
   log("mem: total pages = " + Twine(out.memorySec->numMemoryPages));
 
-  if (WasmSym::heapEnd) {
+  if (ctx.sym.heapEnd) {
     // Set `__heap_end` to follow the end of the statically allocated linear
     // memory. The fact that this comes last means that a malloc/brk
     // implementation can grow the heap at runtime.
     log("mem: heap end    = " + Twine(memoryPtr));
-    WasmSym::heapEnd->setVA(memoryPtr);
+    ctx.sym.heapEnd->setVA(memoryPtr);
   }
 
   uint64_t maxMemory = 0;
@@ -758,14 +758,14 @@ void Writer::calculateImports() {
   // Some inputs require that the indirect function table be assigned to table
   // number 0, so if it is present and is an import, allocate it before any
   // other tables.
-  if (WasmSym::indirectFunctionTable &&
-      shouldImport(WasmSym::indirectFunctionTable))
-    out.importSec->addImport(WasmSym::indirectFunctionTable);
+  if (ctx.sym.indirectFunctionTable &&
+      shouldImport(ctx.sym.indirectFunctionTable))
+    out.importSec->addImport(ctx.sym.indirectFunctionTable);
 
   for (Symbol *sym : symtab->symbols()) {
     if (!shouldImport(sym))
       continue;
-    if (sym == WasmSym::indirectFunctionTable)
+    if (sym == ctx.sym.indirectFunctionTable)
       continue;
     LLVM_DEBUG(dbgs() << "import: " << sym->getName() << "\n");
     out.importSec->addImport(sym);
@@ -879,7 +879,7 @@ void Writer::createCommandExportWrappers() {
 
   // If there are no ctors and there's no libc `__wasm_call_dtors` to
   // call, don't wrap the exports.
-  if (initFunctions.empty() && WasmSym::callDtors == nullptr)
+  if (initFunctions.empty() && ctx.sym.callDtors == nullptr)
     return;
 
   std::vector<DefinedFunction *> toWrap;
@@ -919,27 +919,27 @@ void Writer::createCommandExportWrappers() {
 }
 
 static void finalizeIndirectFunctionTable() {
-  if (!WasmSym::indirectFunctionTable)
+  if (!ctx.sym.indirectFunctionTable)
     return;
 
-  if (shouldImport(WasmSym::indirectFunctionTable) &&
-      !WasmSym::indirectFunctionTable->hasTableNumber()) {
+  if (shouldImport(ctx.sym.indirectFunctionTable) &&
+      !ctx.sym.indirectFunctionTable->hasTableNumber()) {
     // Processing -Bsymbolic relocations resulted in a late requirement that the
     // indirect function table be present, and we are running in --import-table
     // mode.  Add the table now to the imports section.  Otherwise it will be
     // added to the tables section later in assignIndexes.
-    out.importSec->addImport(WasmSym::indirectFunctionTable);
+    out.importSec->addImport(ctx.sym.indirectFunctionTable);
   }
 
   uint32_t tableSize = ctx.arg.tableBase + out.elemSec->numEntries();
   WasmLimits limits = {0, tableSize, 0};
-  if (WasmSym::indirectFunctionTable->isDefined() && !ctx.arg.growableTable) {
+  if (ctx.sym.indirectFunctionTable->isDefined() && !ctx.arg.growableTable) {
     limits.Flags |= WASM_LIMITS_FLAG_HAS_MAX;
     limits.Maximum = limits.Minimum;
   }
   if (ctx.arg.is64.value_or(false))
     limits.Flags |= WASM_LIMITS_FLAG_IS_64;
-  WasmSym::indirectFunctionTable->setLimits(limits);
+  ctx.sym.indirectFunctionTable->setLimits(limits);
 }
 
 static void scanRelocations() {
@@ -1142,26 +1142,26 @@ void Writer::createSyntheticInitFunctions() {
   // We also initialize bss segments (using memory.fill) as part of this
   // function.
   if (hasPassiveInitializedSegments()) {
-    WasmSym::initMemory = symtab->addSyntheticFunction(
+    ctx.sym.initMemory = symtab->addSyntheticFunction(
         "__wasm_init_memory", WASM_SYMBOL_VISIBILITY_HIDDEN,
         make<SyntheticFunction>(nullSignature, "__wasm_init_memory"));
-    WasmSym::initMemory->markLive();
+    ctx.sym.initMemory->markLive();
     if (ctx.arg.sharedMemory) {
       // This global is assigned during  __wasm_init_memory in the shared memory
       // case.
-      WasmSym::tlsBase->markLive();
+      ctx.sym.tlsBase->markLive();
     }
   }
 
   if (ctx.arg.sharedMemory) {
     if (out.globalSec->needsTLSRelocations()) {
-      WasmSym::applyGlobalTLSRelocs = symtab->addSyntheticFunction(
+      ctx.sym.applyGlobalTLSRelocs = symtab->addSyntheticFunction(
           "__wasm_apply_global_tls_relocs", WASM_SYMBOL_VISIBILITY_HIDDEN,
           make<SyntheticFunction>(nullSignature,
                                   "__wasm_apply_global_tls_relocs"));
-      WasmSym::applyGlobalTLSRelocs->markLive();
+      ctx.sym.applyGlobalTLSRelocs->markLive();
       // TLS relocations depend on  the __tls_base symbols
-      WasmSym::tlsBase->markLive();
+      ctx.sym.tlsBase->markLive();
     }
 
     auto hasTLSRelocs = [](const OutputSegment *segment) {
@@ -1172,40 +1172,39 @@ void Writer::createSyntheticInitFunctions() {
       return false;
     };
     if (llvm::any_of(segments, hasTLSRelocs)) {
-      WasmSym::applyTLSRelocs = symtab->addSyntheticFunction(
+      ctx.sym.applyTLSRelocs = symtab->addSyntheticFunction(
           "__wasm_apply_tls_relocs", WASM_SYMBOL_VISIBILITY_HIDDEN,
-          make<SyntheticFunction>(nullSignature,
-                                  "__wasm_apply_tls_relocs"));
-      WasmSym::applyTLSRelocs->markLive();
+          make<SyntheticFunction>(nullSignature, "__wasm_apply_tls_relocs"));
+      ctx.sym.applyTLSRelocs->markLive();
     }
   }
 
   if (ctx.isPic && out.globalSec->needsRelocations()) {
-    WasmSym::applyGlobalRelocs = symtab->addSyntheticFunction(
+    ctx.sym.applyGlobalRelocs = symtab->addSyntheticFunction(
         "__wasm_apply_global_relocs", WASM_SYMBOL_VISIBILITY_HIDDEN,
         make<SyntheticFunction>(nullSignature, "__wasm_apply_global_relocs"));
-    WasmSym::applyGlobalRelocs->markLive();
+    ctx.sym.applyGlobalRelocs->markLive();
   }
 
   // If there is only one start function we can just use that function
   // itself as the Wasm start function, otherwise we need to synthesize
   // a new function to call them in sequence.
-  if (WasmSym::applyGlobalRelocs && WasmSym::initMemory) {
-    WasmSym::startFunction = symtab->addSyntheticFunction(
+  if (ctx.sym.applyGlobalRelocs && ctx.sym.initMemory) {
+    ctx.sym.startFunction = symtab->addSyntheticFunction(
         "__wasm_start", WASM_SYMBOL_VISIBILITY_HIDDEN,
         make<SyntheticFunction>(nullSignature, "__wasm_start"));
-    WasmSym::startFunction->markLive();
+    ctx.sym.startFunction->markLive();
   }
 }
 
 void Writer::createInitMemoryFunction() {
   LLVM_DEBUG(dbgs() << "createInitMemoryFunction\n");
-  assert(WasmSym::initMemory);
+  assert(ctx.sym.initMemory);
   assert(hasPassiveInitializedSegments());
   uint64_t flagAddress;
   if (ctx.arg.sharedMemory) {
-    assert(WasmSym::initMemoryFlag);
-    flagAddress = WasmSym::initMemoryFlag->getVA();
+    assert(ctx.sym.initMemoryFlag);
+    flagAddress = ctx.sym.initMemoryFlag->getVA();
   }
   bool is64 = ctx.arg.is64.value_or(false);
   std::string bodyContent;
@@ -1278,7 +1277,7 @@ void Writer::createInitMemoryFunction() {
         writeUleb128(os, 2, "local count");
         writeU8(os, is64 ? WASM_TYPE_I64 : WASM_TYPE_I32, "address type");
         writeU8(os, WASM_OPCODE_GLOBAL_GET, "GLOBAL_GET");
-        writeUleb128(os, WasmSym::memoryBase->getGlobalIndex(), "memory_base");
+        writeUleb128(os, ctx.sym.memoryBase->getGlobalIndex(), "memory_base");
         writePtrConst(os, flagAddress, is64, "flag address");
         writeU8(os, is64 ? WASM_OPCODE_I64_ADD : WASM_OPCODE_I32_ADD, "add");
         writeU8(os, WASM_OPCODE_LOCAL_SET, "local.set");
@@ -1325,7 +1324,7 @@ void Writer::createInitMemoryFunction() {
         writePtrConst(os, s->startVA, is64, "destination address");
         if (ctx.isPic) {
           writeU8(os, WASM_OPCODE_GLOBAL_GET, "GLOBAL_GET");
-          writeUleb128(os, WasmSym::memoryBase->getGlobalIndex(),
+          writeUleb128(os, ctx.sym.memoryBase->getGlobalIndex(),
                        "__memory_base");
           writeU8(os, is64 ? WASM_OPCODE_I64_ADD : WASM_OPCODE_I32_ADD,
                   "i32.add");
@@ -1343,8 +1342,7 @@ void Writer::createInitMemoryFunction() {
             writePtrConst(os, s->startVA, is64, "destination address");
           }
           writeU8(os, WASM_OPCODE_GLOBAL_SET, "GLOBAL_SET");
-          writeUleb128(os, WasmSym::tlsBase->getGlobalIndex(),
-                       "__tls_base");
+          writeUleb128(os, ctx.sym.tlsBase->getGlobalIndex(), "__tls_base");
           if (ctx.isPic) {
             writeU8(os, WASM_OPCODE_LOCAL_GET, "local.tee");
             writeUleb128(os, 1, "local 1");
@@ -1420,30 +1418,30 @@ void Writer::createInitMemoryFunction() {
     writeU8(os, WASM_OPCODE_END, "END");
   }
 
-  createFunction(WasmSym::initMemory, bodyContent);
+  createFunction(ctx.sym.initMemory, bodyContent);
 }
 
 void Writer::createStartFunction() {
   // If the start function exists when we have more than one function to call.
-  if (WasmSym::initMemory && WasmSym::applyGlobalRelocs) {
-    assert(WasmSym::startFunction);
+  if (ctx.sym.initMemory && ctx.sym.applyGlobalRelocs) {
+    assert(ctx.sym.startFunction);
     std::string bodyContent;
     {
       raw_string_ostream os(bodyContent);
       writeUleb128(os, 0, "num locals");
       writeU8(os, WASM_OPCODE_CALL, "CALL");
-      writeUleb128(os, WasmSym::applyGlobalRelocs->getFunctionIndex(),
+      writeUleb128(os, ctx.sym.applyGlobalRelocs->getFunctionIndex(),
                    "function index");
       writeU8(os, WASM_OPCODE_CALL, "CALL");
-      writeUleb128(os, WasmSym::initMemory->getFunctionIndex(),
+      writeUleb128(os, ctx.sym.initMemory->getFunctionIndex(),
                    "function index");
       writeU8(os, WASM_OPCODE_END, "END");
     }
-    createFunction(WasmSym::startFunction, bodyContent);
-  } else if (WasmSym::initMemory) {
-    WasmSym::startFunction = WasmSym::initMemory;
-  } else if (WasmSym::applyGlobalRelocs) {
-    WasmSym::startFunction = WasmSym::applyGlobalRelocs;
+    createFunction(ctx.sym.startFunction, bodyContent);
+  } else if (ctx.sym.initMemory) {
+    ctx.sym.startFunction = ctx.sym.initMemory;
+  } else if (ctx.sym.applyGlobalRelocs) {
+    ctx.sym.startFunction = ctx.sym.applyGlobalRelocs;
   }
 }
 
@@ -1497,7 +1495,7 @@ void Writer::createApplyTLSRelocationsFunction() {
     writeU8(os, WASM_OPCODE_END, "END");
   }
 
-  createFunction(WasmSym::applyTLSRelocs, bodyContent);
+  createFunction(ctx.sym.applyTLSRelocs, bodyContent);
 }
 
 // Similar to createApplyDataRelocationsFunction but generates relocation code
@@ -1513,7 +1511,7 @@ void Writer::createApplyGlobalRelocationsFunction() {
     writeU8(os, WASM_OPCODE_END, "END");
   }
 
-  createFunction(WasmSym::applyGlobalRelocs, bodyContent);
+  createFunction(ctx.sym.applyGlobalRelocs, bodyContent);
 }
 
 // Similar to createApplyGlobalRelocationsFunction but for
@@ -1529,7 +1527,7 @@ void Writer::createApplyGlobalTLSRelocationsFunction() {
     writeU8(os, WASM_OPCODE_END, "END");
   }
 
-  createFunction(WasmSym::applyGlobalTLSRelocs, bodyContent);
+  createFunction(ctx.sym.applyGlobalTLSRelocs, bodyContent);
 }
 
 // Create synthetic "__wasm_call_ctors" function based on ctor functions
@@ -1537,7 +1535,7 @@ void Writer::createApplyGlobalTLSRelocationsFunction() {
 void Writer::createCallCtorsFunction() {
   // If __wasm_call_ctors isn't referenced, there aren't any ctors, don't
   // define the `__wasm_call_ctors` function.
-  if (!WasmSym::callCtors->isLive() && initFunctions.empty())
+  if (!ctx.sym.callCtors->isLive() && initFunctions.empty())
     return;
 
   // First write the body's contents to a string.
@@ -1558,7 +1556,7 @@ void Writer::createCallCtorsFunction() {
     writeU8(os, WASM_OPCODE_END, "END");
   }
 
-  createFunction(WasmSym::callCtors, bodyContent);
+  createFunction(ctx.sym.callCtors, bodyContent);
 }
 
 // Create a wrapper around a function export which calls the
@@ -1573,10 +1571,9 @@ void Writer::createCommandExportWrapper(uint32_t functionIndex,
 
     // Call `__wasm_call_ctors` which call static constructors (and
     // applies any runtime relocations in Emscripten-style PIC mode)
-    if (WasmSym::callCtors->isLive()) {
+    if (ctx.sym.callCtors->isLive()) {
       writeU8(os, WASM_OPCODE_CALL, "CALL");
-      writeUleb128(os, WasmSym::callCtors->getFunctionIndex(),
-                   "function index");
+      writeUleb128(os, ctx.sym.callCtors->getFunctionIndex(), "function index");
     }
 
     // Call the user's code, leaving any return values on the operand stack.
@@ -1588,7 +1585,7 @@ void Writer::createCommandExportWrapper(uint32_t functionIndex,
     writeUleb128(os, functionIndex, "function index");
 
     // Call the function that calls the destructors.
-    if (DefinedFunction *callDtors = WasmSym::callDtors) {
+    if (DefinedFunction *callDtors = ctx.sym.callDtors) {
       writeU8(os, WASM_OPCODE_CALL, "CALL");
       writeUleb128(os, callDtors->getFunctionIndex(), "function index");
     }
@@ -1619,7 +1616,7 @@ void Writer::createInitTLSFunction() {
       writeUleb128(os, 0, "local index");
 
       writeU8(os, WASM_OPCODE_GLOBAL_SET, "global.set");
-      writeUleb128(os, WasmSym::tlsBase->getGlobalIndex(), "global index");
+      writeUleb128(os, ctx.sym.tlsBase->getGlobalIndex(), "global index");
 
       // FIXME(wvo): this local needs to be I64 in wasm64, or we need an extend op.
       writeU8(os, WASM_OPCODE_LOCAL_GET, "local.get");
@@ -1635,28 +1632,28 @@ void Writer::createInitTLSFunction() {
       writeU8(os, 0, "memory index immediate");
     }
 
-    if (WasmSym::applyTLSRelocs) {
+    if (ctx.sym.applyTLSRelocs) {
       writeU8(os, WASM_OPCODE_CALL, "CALL");
-      writeUleb128(os, WasmSym::applyTLSRelocs->getFunctionIndex(),
+      writeUleb128(os, ctx.sym.applyTLSRelocs->getFunctionIndex(),
                    "function index");
     }
 
-    if (WasmSym::applyGlobalTLSRelocs) {
+    if (ctx.sym.applyGlobalTLSRelocs) {
       writeU8(os, WASM_OPCODE_CALL, "CALL");
-      writeUleb128(os, WasmSym::applyGlobalTLSRelocs->getFunctionIndex(),
+      writeUleb128(os, ctx.sym.applyGlobalTLSRelocs->getFunctionIndex(),
                    "function index");
     }
     writeU8(os, WASM_OPCODE_END, "end function");
   }
 
-  createFunction(WasmSym::initTLS, bodyContent);
+  createFunction(ctx.sym.initTLS, bodyContent);
 }
 
 // Populate InitFunctions vector with init functions from all input objects.
 // This is then used either when creating the output linking section or to
 // synthesize the "__wasm_call_ctors" function.
 void Writer::calculateInitFunctions() {
-  if (!ctx.arg.relocatable && !WasmSym::callCtors->isLive())
+  if (!ctx.arg.relocatable && !ctx.sym.callCtors->isLive())
     return;
 
   for (ObjFile *file : ctx.objectFiles) {
@@ -1707,8 +1704,8 @@ void Writer::createSyntheticSectionsPostLayout() {
 void Writer::run() {
   // For PIC code the table base is assigned dynamically by the loader.
   // For non-PIC, we start at 1 so that accessing table index 0 always traps.
-  if (!ctx.isPic && WasmSym::definedTableBase)
-    WasmSym::definedTableBase->setVA(ctx.arg.tableBase);
+  if (!ctx.isPic && ctx.sym.definedTableBase)
+    ctx.sym.definedTableBase->setVA(ctx.arg.tableBase);
 
   log("-- createOutputSegments");
   createOutputSegments();
@@ -1776,14 +1773,18 @@ void Writer::run() {
 
   if (!ctx.arg.relocatable) {
     // Create linker synthesized functions
-    if (WasmSym::applyGlobalRelocs)
+    if (ctx.sym.applyGlobalRelocs) {
       createApplyGlobalRelocationsFunction();
-    if (WasmSym::applyTLSRelocs)
+    }
+    if (ctx.sym.applyTLSRelocs) {
       createApplyTLSRelocationsFunction();
-    if (WasmSym::applyGlobalTLSRelocs)
+    }
+    if (ctx.sym.applyGlobalTLSRelocs) {
       createApplyGlobalTLSRelocationsFunction();
-    if (WasmSym::initMemory)
+    }
+    if (ctx.sym.initMemory) {
       createInitMemoryFunction();
+    }
     createStartFunction();
 
     createCallCtorsFunction();
@@ -1794,14 +1795,14 @@ void Writer::run() {
     // the input objects or an explicit export from the command-line, we
     // assume ctors and dtors are taken care of already.
     if (!ctx.arg.relocatable && !ctx.isPic &&
-        !WasmSym::callCtors->isUsedInRegularObj &&
-        !WasmSym::callCtors->isExported()) {
+        !ctx.sym.callCtors->isUsedInRegularObj &&
+        !ctx.sym.callCtors->isExported()) {
       log("-- createCommandExportWrappers");
       createCommandExportWrappers();
     }
   }
 
-  if (WasmSym::initTLS && WasmSym::initTLS->isLive()) {
+  if (ctx.sym.initTLS && ctx.sym.initTLS->isLive()) {
     log("-- createInitTLSFunction");
     createInitTLSFunction();
   }
diff --git a/lldb/cmake/modules/FindCursesAndPanel.cmake b/lldb/cmake/modules/FindCursesAndPanel.cmake
index aaadf214bf54..8628059f91ba 100644
--- a/lldb/cmake/modules/FindCursesAndPanel.cmake
+++ b/lldb/cmake/modules/FindCursesAndPanel.cmake
@@ -2,23 +2,67 @@
 # FindCursesAndPanel
 # -----------
 #
-# Find the curses and panel library as a whole.
+# Find the curses, terminfo, and panel library as a whole.
+
+include(CMakePushCheckState)
+
+function(lldb_check_curses_tinfo CURSES_INCLUDE_DIRS CURSES_LIBRARIES CURSES_HAS_TINFO)
+  cmake_reset_check_state()
+  set(CMAKE_REQUIRED_INCLUDES "${CURSES_INCLUDE_DIRS}")
+  set(CMAKE_REQUIRED_LIBRARIES "${CURSES_LIBRARIES}")
+  # acs_map is one of many symbols that are part of tinfo but could
+  # be bundled in curses.
+  check_symbol_exists(acs_map "curses.h" CURSES_HAS_TINFO)
+endfunction()
 
 if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND PANEL_LIBRARIES)
+  if(NOT HAS_TERMINFO_SYMBOLS)
+    lldb_check_curses_tinfo("${CURSES_INCLUDE_DIRS}"
+                            "${CURSES_LIBRARIES}"
+                            CURSES_HAS_TINFO)
+    if(NOT CURSES_HAS_TINFO)
+      message(WARNING "CURSES_LIBRARIES was provided manually but is missing terminfo symbols")
+    endif()
+    mark_as_advanced(CURSES_HAS_TINFO)
+  endif()
   set(CURSESANDPANEL_FOUND TRUE)
 else()
   find_package(Curses QUIET)
   find_library(PANEL_LIBRARIES NAMES panel DOC "The curses panel library" QUIET)
   include(FindPackageHandleStandardArgs)
+
+  if(CURSES_FOUND AND PANEL_LIBRARIES)
+    # Sometimes the curses libraries define their own terminfo symbols,
+    # other times they're extern and are defined by a separate terminfo library.
+    # Auto-detect which.
+    lldb_check_curses_tinfo("${CURSES_INCLUDE_DIRS}"
+                            "${CURSES_LIBRARIES}"
+                            CURSES_HAS_TINFO)
+    if(NOT CURSES_HAS_TINFO)
+      message(STATUS "curses library missing terminfo symbols, looking for tinfo separately")
+      find_library(TINFO_LIBRARIES NAMES tinfo DOC "The curses tinfo library" QUIET)
+      list(APPEND CURSES_LIBRARIES "${TINFO_LIBRARIES}")
+    endif()
+    set(HAS_TERMINFO_SYMBOLS "$<OR:$<BOOL:${TERMINFO_LIBRARIES}>,$<BOOL:${CURSES_HAS_TINFO}>>")
+  endif()
+
   find_package_handle_standard_args(CursesAndPanel
                                     FOUND_VAR
                                       CURSESANDPANEL_FOUND
                                     REQUIRED_VARS
                                       CURSES_INCLUDE_DIRS
                                       CURSES_LIBRARIES
-                                      PANEL_LIBRARIES)
-  if(CURSES_FOUND AND PANEL_LIBRARIES)
-    mark_as_advanced(CURSES_INCLUDE_DIRS CURSES_LIBRARIES PANEL_LIBRARIES)
+                                      PANEL_LIBRARIES
+                                      HAS_TERMINFO_SYMBOLS)
+
+  if(CURSES_FOUND AND PANEL_LIBRARIES AND HAS_TERMINFO_SYMBOLS)
+    mark_as_advanced(CURSES_INCLUDE_DIRS
+                      PANEL_LIBRARIES
+                      HAS_TERMINFO_SYMBOLS
+                      CURSES_HAS_TINFO)
+  endif()
+  if(TINFO_LIBRARIES)
+    mark_as_advanced(TINFO_LIBRARIES)
   endif()
 endif()
 
diff --git a/lldb/source/API/SBTarget.cpp b/lldb/source/API/SBTarget.cpp
index 2a33161bc21e..665b903aff79 100644
--- a/lldb/source/API/SBTarget.cpp
+++ b/lldb/source/API/SBTarget.cpp
@@ -2020,9 +2020,9 @@ lldb::SBInstructionList SBTarget::ReadInstructions(lldb::SBAddress base_addr,
                                 error, force_live_memory, &load_addr);
       const bool data_from_file = load_addr == LLDB_INVALID_ADDRESS;
       sb_instructions.SetDisassembler(Disassembler::DisassembleBytes(
-          target_sp->GetArchitecture(), nullptr, target_sp->GetDisassemblyCPU(),
-          target_sp->GetDisassemblyFeatures(), flavor_string, *addr_ptr,
-          data.GetBytes(), bytes_read, count, data_from_file));
+          target_sp->GetArchitecture(), nullptr, flavor_string,
+          target_sp->GetDisassemblyCPU(), target_sp->GetDisassemblyFeatures(),
+          *addr_ptr, data.GetBytes(), bytes_read, count, data_from_file));
     }
   }
 
diff --git a/lldb/source/Host/posix/ProcessLauncherPosixFork.cpp b/lldb/source/Host/posix/ProcessLauncherPosixFork.cpp
index 7d856954684c..903b18b10976 100644
--- a/lldb/source/Host/posix/ProcessLauncherPosixFork.cpp
+++ b/lldb/source/Host/posix/ProcessLauncherPosixFork.cpp
@@ -94,6 +94,7 @@ struct ForkLaunchInfo {
   bool debug;
   bool disable_aslr;
   std::string wd;
+  std::string executable;
   const char **argv;
   Environment::Envp envp;
   std::vector<ForkFileAction> actions;
@@ -194,7 +195,8 @@ struct ForkLaunchInfo {
   }
 
   // Execute.  We should never return...
-  execve(info.argv[0], const_cast<char *const *>(info.argv), info.envp);
+  execve(info.executable.c_str(), const_cast<char *const *>(info.argv),
+         info.envp);
 
 #if defined(__linux__)
   if (errno == ETXTBSY) {
@@ -207,7 +209,8 @@ struct ForkLaunchInfo {
     // Since this state should clear up quickly, wait a while and then give it
     // one more go.
     usleep(50000);
-    execve(info.argv[0], const_cast<char *const *>(info.argv), info.envp);
+    execve(info.executable.c_str(), const_cast<char *const *>(info.argv),
+           info.envp);
   }
 #endif
 
@@ -246,6 +249,7 @@ ForkLaunchInfo::ForkLaunchInfo(const ProcessLaunchInfo &info)
       debug(info.GetFlags().Test(eLaunchFlagDebug)),
       disable_aslr(info.GetFlags().Test(eLaunchFlagDisableASLR)),
       wd(info.GetWorkingDirectory().GetPath()),
+      executable(info.GetExecutableFile().GetPath()),
       argv(info.GetArguments().GetConstArgumentVector()),
       envp(FixupEnvironment(info.GetEnvironment())),
       actions(MakeForkActions(info)) {}
diff --git a/lldb/source/Plugins/Process/Linux/NativeRegisterContextLinux_loongarch64.cpp b/lldb/source/Plugins/Process/Linux/NativeRegisterContextLinux_loongarch64.cpp
index b04018ee243f..601dde250094 100644
--- a/lldb/source/Plugins/Process/Linux/NativeRegisterContextLinux_loongarch64.cpp
+++ b/lldb/source/Plugins/Process/Linux/NativeRegisterContextLinux_loongarch64.cpp
@@ -27,13 +27,24 @@
 // struct iovec definition
 #include <sys/uio.h>
 
+// LoongArch SIMD eXtension registers
 #ifndef NT_LOONGARCH_LSX
-#define NT_LOONGARCH_LSX 0xa02 /* LoongArch SIMD eXtension registers */
+#define NT_LOONGARCH_LSX 0xa02
 #endif
 
+// LoongArch Advanced SIMD eXtension registers
 #ifndef NT_LOONGARCH_LASX
-#define NT_LOONGARCH_LASX                                                      \
-  0xa03 /* LoongArch Advanced SIMD eXtension registers */
+#define NT_LOONGARCH_LASX 0xa03
+#endif
+
+// LoongArch hardware breakpoint registers
+#ifndef NT_LOONGARCH_HW_BREAK
+#define NT_LOONGARCH_HW_BREAK 0xa05
+#endif
+
+// LoongArch hardware watchpoint registers
+#ifndef NT_LOONGARCH_HW_WATCH
+#define NT_LOONGARCH_HW_WATCH 0xa06
 #endif
 
 #define REG_CONTEXT_SIZE                                                       \
diff --git a/lldb/test/API/python_api/target/read-instructions-flavor/Makefile b/lldb/test/API/python_api/target/read-instructions-flavor/Makefile
new file mode 100644
index 000000000000..10495940055b
--- /dev/null
+++ b/lldb/test/API/python_api/target/read-instructions-flavor/Makefile
@@ -0,0 +1,3 @@
+C_SOURCES := main.c
+
+include Makefile.rules
diff --git a/lldb/test/API/python_api/target/read-instructions-flavor/TestTargetReadInstructionsFlavor.py b/lldb/test/API/python_api/target/read-instructions-flavor/TestTargetReadInstructionsFlavor.py
new file mode 100644
index 000000000000..12805985798d
--- /dev/null
+++ b/lldb/test/API/python_api/target/read-instructions-flavor/TestTargetReadInstructionsFlavor.py
@@ -0,0 +1,40 @@
+"""
+Test SBTarget Read Instruction.
+"""
+
+from lldbsuite.test.decorators import *
+from lldbsuite.test.lldbtest import *
+
+
+class TargetReadInstructionsFlavor(TestBase):
+    @skipIfWindows
+    @skipIf(archs=no_match(["x86_64", "x86", "i386"]))
+    def test_read_instructions_with_flavor(self):
+        self.build()
+        executable = self.getBuildArtifact("a.out")
+
+        # create a target
+        target = self.dbg.CreateTarget(executable)
+        self.assertTrue(target.IsValid(), "target is not valid")
+
+        functions = target.FindFunctions("test_add")
+        self.assertEqual(len(functions), 1)
+        test_add = functions[0]
+
+        test_add_symbols = test_add.GetSymbol()
+        self.assertTrue(
+            test_add_symbols.IsValid(), "test_add function symbols is not valid"
+        )
+
+        expected_instructions = (("mov", "eax, edi"), ("add", "eax, esi"), ("ret", ""))
+        test_add_insts = test_add_symbols.GetInstructions(target, "intel")
+        # clang adds an extra nop instruction but gcc does not. It makes more sense
+        # to check if it is at least 3
+        self.assertLessEqual(len(expected_instructions), len(test_add_insts))
+
+        # compares only the expected instructions
+        for expected_instr, instr in zip(expected_instructions, test_add_insts):
+            self.assertTrue(instr.IsValid(), "instruction is not valid")
+            expected_mnemonic, expected_op_str = expected_instr
+            self.assertEqual(instr.GetMnemonic(target), expected_mnemonic)
+            self.assertEqual(instr.GetOperands(target), expected_op_str)
diff --git a/lldb/test/API/python_api/target/read-instructions-flavor/main.c b/lldb/test/API/python_api/target/read-instructions-flavor/main.c
new file mode 100644
index 000000000000..6022d63fb6ed
--- /dev/null
+++ b/lldb/test/API/python_api/target/read-instructions-flavor/main.c
@@ -0,0 +1,21 @@
+
+// This simple program is to test the lldb Python API SBTarget ReadInstruction
+// function.
+//
+// When the target is create we get all the instructions using the intel
+// flavor and see if it is correct.
+
+int test_add(int a, int b);
+
+__asm__("test_add:\n"
+        "    movl    %edi, %eax\n"
+        "    addl    %esi, %eax\n"
+        "    ret     \n");
+
+int main(int argc, char **argv) {
+  int a = 10;
+  int b = 20;
+  int result = test_add(a, b);
+
+  return 0;
+}
\ No newline at end of file
diff --git a/lldb/tools/lldb-server/lldb-platform.cpp b/lldb/tools/lldb-server/lldb-platform.cpp
index 880b45b989b9..51174a0f443c 100644
--- a/lldb/tools/lldb-server/lldb-platform.cpp
+++ b/lldb/tools/lldb-server/lldb-platform.cpp
@@ -31,6 +31,7 @@
 #include "Plugins/Process/gdb-remote/ProcessGDBRemoteLog.h"
 #include "lldb/Host/ConnectionFileDescriptor.h"
 #include "lldb/Host/HostGetOpt.h"
+#include "lldb/Host/HostInfo.h"
 #include "lldb/Host/MainLoop.h"
 #include "lldb/Host/OptionParser.h"
 #include "lldb/Host/Socket.h"
@@ -256,8 +257,9 @@ static void client_handle(GDBRemoteCommunicationServerPlatform &platform,
   printf("Disconnected.\n");
 }
 
-static Status spawn_process(const char *progname, const Socket *conn_socket,
-                            uint16_t gdb_port, const lldb_private::Args &args,
+static Status spawn_process(const char *progname, const FileSpec &prog,
+                            const Socket *conn_socket, uint16_t gdb_port,
+                            const lldb_private::Args &args,
                             const std::string &log_file,
                             const StringRef log_channels, MainLoop &main_loop) {
   Status error;
@@ -267,9 +269,10 @@ static Status spawn_process(const char *progname, const Socket *conn_socket,
 
   ProcessLaunchInfo launch_info;
 
-  FileSpec self_spec(progname, FileSpec::Style::native);
-  launch_info.SetExecutableFile(self_spec, true);
+  launch_info.SetExecutableFile(prog, false);
+  launch_info.SetArg0(progname);
   Args &self_args = launch_info.GetArguments();
+  self_args.AppendArgument(progname);
   self_args.AppendArgument(llvm::StringRef("platform"));
   self_args.AppendArgument(llvm::StringRef("--child-platform-fd"));
   self_args.AppendArgument(llvm::to_string(shared_socket.GetSendableFD()));
@@ -551,9 +554,10 @@ int main_platform(int argc, char *argv[]) {
                         log_channels, &main_loop,
                         &platform_handles](std::unique_ptr<Socket> sock_up) {
               printf("Connection established.\n");
-              Status error = spawn_process(progname, sock_up.get(),
-                                           gdbserver_port, inferior_arguments,
-                                           log_file, log_channels, main_loop);
+              Status error = spawn_process(
+                  progname, HostInfo::GetProgramFileSpec(), sock_up.get(),
+                  gdbserver_port, inferior_arguments, log_file, log_channels,
+                  main_loop);
               if (error.Fail()) {
                 Log *log = GetLog(LLDBLog::Platform);
                 LLDB_LOGF(log, "spawn_process failed: %s", error.AsCString());
diff --git a/lldb/unittests/Host/HostTest.cpp b/lldb/unittests/Host/HostTest.cpp
index a1d8a3b7f485..ed1df6de001e 100644
--- a/lldb/unittests/Host/HostTest.cpp
+++ b/lldb/unittests/Host/HostTest.cpp
@@ -7,12 +7,24 @@
 //===----------------------------------------------------------------------===//
 
 #include "lldb/Host/Host.h"
+#include "TestingSupport/SubsystemRAII.h"
+#include "lldb/Host/FileSystem.h"
+#include "lldb/Host/ProcessLaunchInfo.h"
 #include "lldb/Utility/ProcessInfo.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/FileSystem.h"
+#include "llvm/Testing/Support/Error.h"
 #include "gtest/gtest.h"
+#include <future>
 
 using namespace lldb_private;
 using namespace llvm;
 
+// From TestMain.cpp.
+extern const char *TestMainArgv0;
+
+static cl::opt<uint64_t> test_arg("test-arg");
+
 TEST(Host, WaitStatusFormat) {
   EXPECT_EQ("W01", formatv("{0:g}", WaitStatus{WaitStatus::Exit, 1}).str());
   EXPECT_EQ("X02", formatv("{0:g}", WaitStatus{WaitStatus::Signal, 2}).str());
@@ -45,4 +57,33 @@ TEST(Host, ProcessInstanceInfoCumulativeSystemTimeIsValid) {
   EXPECT_TRUE(info.CumulativeSystemTimeIsValid());
   info.SetCumulativeSystemTime(ProcessInstanceInfo::timespec{1, 0});
   EXPECT_TRUE(info.CumulativeSystemTimeIsValid());
-}
\ No newline at end of file
+}
+
+TEST(Host, LaunchProcessSetsArgv0) {
+  SubsystemRAII<FileSystem> subsystems;
+
+  static constexpr StringLiteral TestArgv0 = "HelloArgv0";
+  if (test_arg != 0) {
+    // In subprocess
+    if (TestMainArgv0 != TestArgv0) {
+      errs() << formatv("Got '{0}' for argv[0]\n", TestMainArgv0);
+      exit(1);
+    }
+    exit(0);
+  }
+
+  ProcessLaunchInfo info;
+  info.SetExecutableFile(
+      FileSpec(llvm::sys::fs::getMainExecutable(TestMainArgv0, &test_arg)),
+      /*add_exe_file_as_first_arg=*/false);
+  info.GetArguments().AppendArgument("HelloArgv0");
+  info.GetArguments().AppendArgument(
+      "--gtest_filter=Host.LaunchProcessSetsArgv0");
+  info.GetArguments().AppendArgument("--test-arg=47");
+  std::promise<int> exit_status;
+  info.SetMonitorProcessCallback([&](lldb::pid_t pid, int signal, int status) {
+    exit_status.set_value(status);
+  });
+  ASSERT_THAT_ERROR(Host::LaunchProcess(info).takeError(), Succeeded());
+  ASSERT_THAT(exit_status.get_future().get(), 0);
+}
diff --git a/llvm/docs/LangRef.rst b/llvm/docs/LangRef.rst
index e002195cb7ed..1c8eaa60e1c8 100644
--- a/llvm/docs/LangRef.rst
+++ b/llvm/docs/LangRef.rst
@@ -5826,6 +5826,8 @@ Hexagon:
 
 LoongArch:
 
+- ``u``: Print an LASX register.
+- ``w``: Print an LSX register.
 - ``z``: Print $zero register if operand is zero, otherwise print it normally.
 
 MSP430:
diff --git a/llvm/docs/ReleaseNotes.md b/llvm/docs/ReleaseNotes.md
index 958b7adbc4c3..f34003eaf0fe 100644
--- a/llvm/docs/ReleaseNotes.md
+++ b/llvm/docs/ReleaseNotes.md
@@ -202,6 +202,10 @@ Changes to the DirectX Backend
 Changes to the Hexagon Backend
 ------------------------------
 
+* The default Hexagon architecture version in ELF object files produced by
+  the tools such as llvm-mc is changed to v68. This version will be set if
+  the user does not provide the CPU version in the command line.
+
 Changes to the LoongArch Backend
 --------------------------------
 
diff --git a/llvm/include/llvm/BinaryFormat/ELF.h b/llvm/include/llvm/BinaryFormat/ELF.h
index 48ae0db80f43..8853c4a88b0b 100644
--- a/llvm/include/llvm/BinaryFormat/ELF.h
+++ b/llvm/include/llvm/BinaryFormat/ELF.h
@@ -619,6 +619,7 @@ enum {
   EF_HEXAGON_MACH_V5 = 0x00000004,   // Hexagon V5
   EF_HEXAGON_MACH_V55 = 0x00000005,  // Hexagon V55
   EF_HEXAGON_MACH_V60 = 0x00000060,  // Hexagon V60
+  EF_HEXAGON_MACH_V61 = 0x00000061,  // Hexagon V61
   EF_HEXAGON_MACH_V62 = 0x00000062,  // Hexagon V62
   EF_HEXAGON_MACH_V65 = 0x00000065,  // Hexagon V65
   EF_HEXAGON_MACH_V66 = 0x00000066,  // Hexagon V66
@@ -630,7 +631,11 @@ enum {
   EF_HEXAGON_MACH_V71T = 0x00008071, // Hexagon V71T
   EF_HEXAGON_MACH_V73 = 0x00000073,  // Hexagon V73
   EF_HEXAGON_MACH_V75 = 0x00000075,  // Hexagon V75
+  EF_HEXAGON_MACH_V77 = 0x00000077,  // Hexagon V77
   EF_HEXAGON_MACH_V79 = 0x00000079,  // Hexagon V79
+  EF_HEXAGON_MACH_V81 = 0x00000081,  // Hexagon V81
+  EF_HEXAGON_MACH_V83 = 0x00000083,  // Hexagon V83
+  EF_HEXAGON_MACH_V85 = 0x00000085,  // Hexagon V85
   EF_HEXAGON_MACH = 0x000003ff,      // Hexagon V..
 
   // Highest ISA version flags
@@ -642,6 +647,7 @@ enum {
   EF_HEXAGON_ISA_V5 = 0x00000040,   // Hexagon V5 ISA
   EF_HEXAGON_ISA_V55 = 0x00000050,  // Hexagon V55 ISA
   EF_HEXAGON_ISA_V60 = 0x00000060,  // Hexagon V60 ISA
+  EF_HEXAGON_ISA_V61 = 0x00000061,  // Hexagon V61 ISA
   EF_HEXAGON_ISA_V62 = 0x00000062,  // Hexagon V62 ISA
   EF_HEXAGON_ISA_V65 = 0x00000065,  // Hexagon V65 ISA
   EF_HEXAGON_ISA_V66 = 0x00000066,  // Hexagon V66 ISA
@@ -651,7 +657,11 @@ enum {
   EF_HEXAGON_ISA_V71 = 0x00000071,  // Hexagon V71 ISA
   EF_HEXAGON_ISA_V73 = 0x00000073,  // Hexagon V73 ISA
   EF_HEXAGON_ISA_V75 = 0x00000075,  // Hexagon V75 ISA
+  EF_HEXAGON_ISA_V77 = 0x00000077,  // Hexagon V77 ISA
   EF_HEXAGON_ISA_V79 = 0x00000079,  // Hexagon V79 ISA
+  EF_HEXAGON_ISA_V81 = 0x00000081,  // Hexagon V81 ISA
+  EF_HEXAGON_ISA_V83 = 0x00000083,  // Hexagon V83 ISA
+  EF_HEXAGON_ISA_V85 = 0x00000085,  // Hexagon V85 ISA
   EF_HEXAGON_ISA = 0x000003ff,      // Hexagon V.. ISA
 };
 
diff --git a/llvm/include/llvm/CodeGen/MachineBasicBlock.h b/llvm/include/llvm/CodeGen/MachineBasicBlock.h
index 0b803a972474..11efb2f656a7 100644
--- a/llvm/include/llvm/CodeGen/MachineBasicBlock.h
+++ b/llvm/include/llvm/CodeGen/MachineBasicBlock.h
@@ -311,6 +311,15 @@ public:
   const MachineFunction *getParent() const { return xParent; }
   MachineFunction *getParent() { return xParent; }
 
+  /// Returns true if the original IR terminator is an `indirectbr`. This
+  /// typically corresponds to a `goto` in C, rather than jump tables.
+  bool terminatorIsComputedGoto() const {
+    return back().isIndirectBranch() &&
+           llvm::all_of(successors(), [](const MachineBasicBlock *Succ) {
+             return Succ->isIRBlockAddressTaken();
+           });
+  }
+
   using instr_iterator = Instructions::iterator;
   using const_instr_iterator = Instructions::const_iterator;
   using reverse_instr_iterator = Instructions::reverse_iterator;
diff --git a/llvm/include/llvm/CodeGen/MachineInstr.h b/llvm/include/llvm/CodeGen/MachineInstr.h
index 102b1eb07358..b26cabe801ee 100644
--- a/llvm/include/llvm/CodeGen/MachineInstr.h
+++ b/llvm/include/llvm/CodeGen/MachineInstr.h
@@ -994,8 +994,17 @@ public:
 
   /// Return true if this is an indirect branch, such as a
   /// branch through a register.
-  bool isIndirectBranch(QueryType Type = AnyInBundle) const {
-    return hasProperty(MCID::IndirectBranch, Type);
+  bool isIndirectBranch(QueryType Type = AnyInBundle,
+                        bool IncludeJumpTable = true) const {
+    return hasProperty(MCID::IndirectBranch, Type) &&
+           (IncludeJumpTable || !llvm::any_of(operands(), [](const auto &Op) {
+              return Op.isJTI();
+            }));
+  }
+
+  bool isComputedGoto(QueryType Type = AnyInBundle) const {
+    // Jump tables are not considered computed gotos.
+    return isIndirectBranch(Type, /*IncludeJumpTable=*/false);
   }
 
   /// Return true if this is a branch which may fall
diff --git a/llvm/include/llvm/Support/Compiler.h b/llvm/include/llvm/Support/Compiler.h
index f9c57b89f1f0..dc8b5389069e 100644
--- a/llvm/include/llvm/Support/Compiler.h
+++ b/llvm/include/llvm/Support/Compiler.h
@@ -203,7 +203,7 @@
 #define LLVM_TEMPLATE_ABI LLVM_ATTRIBUTE_VISIBILITY_DEFAULT
 #define LLVM_EXPORT_TEMPLATE
 #define LLVM_ABI_EXPORT LLVM_ATTRIBUTE_VISIBILITY_DEFAULT
-#elif defined(__MACH__) || defined(__WASM__)
+#elif defined(__MACH__) || defined(__WASM__) || defined(__EMSCRIPTEN__)
 #define LLVM_ABI LLVM_ATTRIBUTE_VISIBILITY_DEFAULT
 #define LLVM_TEMPLATE_ABI
 #define LLVM_EXPORT_TEMPLATE
diff --git a/llvm/lib/Analysis/BasicAliasAnalysis.cpp b/llvm/lib/Analysis/BasicAliasAnalysis.cpp
index b2a3f3390e00..06e8eb707291 100644
--- a/llvm/lib/Analysis/BasicAliasAnalysis.cpp
+++ b/llvm/lib/Analysis/BasicAliasAnalysis.cpp
@@ -1245,8 +1245,11 @@ AliasResult BasicAAResult::aliasGEP(
   if (V1Size.isScalable() || V2Size.isScalable())
     return AliasResult::MayAlias;
 
-  // We need to know both acess sizes for all the following heuristics.
-  if (!V1Size.hasValue() || !V2Size.hasValue())
+  // We need to know both access sizes for all the following heuristics. Don't
+  // try to reason about sizes larger than the index space.
+  unsigned BW = DecompGEP1.Offset.getBitWidth();
+  if (!V1Size.hasValue() || !V2Size.hasValue() ||
+      !isUIntN(BW, V1Size.getValue()) || !isUIntN(BW, V2Size.getValue()))
     return AliasResult::MayAlias;
 
   APInt GCD;
@@ -1301,7 +1304,6 @@ AliasResult BasicAAResult::aliasGEP(
 
   // Compute ranges of potentially accessed bytes for both accesses. If the
   // interseciton is empty, there can be no overlap.
-  unsigned BW = OffsetRange.getBitWidth();
   ConstantRange Range1 = OffsetRange.add(
       ConstantRange(APInt(BW, 0), APInt(BW, V1Size.getValue())));
   ConstantRange Range2 =
diff --git a/llvm/lib/Analysis/LoopAccessAnalysis.cpp b/llvm/lib/Analysis/LoopAccessAnalysis.cpp
index 697b40403902..dcd8a910d0f4 100644
--- a/llvm/lib/Analysis/LoopAccessAnalysis.cpp
+++ b/llvm/lib/Analysis/LoopAccessAnalysis.cpp
@@ -1283,8 +1283,10 @@ void AccessAnalysis::processMemAccesses() {
 
     bool SetHasWrite = false;
 
-    // Map of pointers to last access encountered.
-    typedef DenseMap<const Value*, MemAccessInfo> UnderlyingObjToAccessMap;
+    // Map of (pointer to underlying objects, accessed address space) to last
+    // access encountered.
+    typedef DenseMap<std::pair<const Value *, unsigned>, MemAccessInfo>
+        UnderlyingObjToAccessMap;
     UnderlyingObjToAccessMap ObjToLastAccess;
 
     // Set of access to check after all writes have been processed.
@@ -1364,12 +1366,14 @@ void AccessAnalysis::processMemAccesses() {
                     UnderlyingObj->getType()->getPointerAddressSpace()))
               continue;
 
+            unsigned AccessAS = cast<PointerType>(Ptr->getType())->getAddressSpace();
             UnderlyingObjToAccessMap::iterator Prev =
-                ObjToLastAccess.find(UnderlyingObj);
+                ObjToLastAccess.find({UnderlyingObj,AccessAS 
+                 });
             if (Prev != ObjToLastAccess.end())
               DepCands.unionSets(Access, Prev->second);
 
-            ObjToLastAccess[UnderlyingObj] = Access;
+            ObjToLastAccess[{UnderlyingObj, AccessAS}] = Access;
             LLVM_DEBUG(dbgs() << "  " << *UnderlyingObj << "\n");
           }
         }
diff --git a/llvm/lib/Analysis/ScalarEvolution.cpp b/llvm/lib/Analysis/ScalarEvolution.cpp
index c71202c8dd58..36fe036aa9e9 100644
--- a/llvm/lib/Analysis/ScalarEvolution.cpp
+++ b/llvm/lib/Analysis/ScalarEvolution.cpp
@@ -7854,7 +7854,7 @@ const SCEV *ScalarEvolution::createSCEV(Value *V) {
               unsigned GCD = std::min(MulZeros, TZ);
               APInt DivAmt = APInt::getOneBitSet(BitWidth, TZ - GCD);
               SmallVector<const SCEV*, 4> MulOps;
-              MulOps.push_back(getConstant(OpC->getAPInt().lshr(GCD)));
+              MulOps.push_back(getConstant(OpC->getAPInt().ashr(GCD)));
               append_range(MulOps, LHSMul->operands().drop_front());
               auto *NewMul = getMulExpr(MulOps, LHSMul->getNoWrapFlags());
               ShiftedLHS = getUDivExpr(NewMul, getConstant(DivAmt));
@@ -10635,10 +10635,11 @@ ScalarEvolution::ExitLimit ScalarEvolution::howFarToZero(const SCEV *V,
   if (ControlsOnlyExit && AddRec->hasNoSelfWrap() &&
       loopHasNoAbnormalExits(AddRec->getLoop())) {
 
-    // If the stride is zero, the loop must be infinite.  In C++, most loops
-    // are finite by assumption, in which case the step being zero implies
-    // UB must execute if the loop is entered.
-    if (!loopIsFiniteByAssumption(L) && !isKnownNonZero(StepWLG))
+    // If the stride is zero and the start is non-zero, the loop must be
+    // infinite. In C++, most loops are finite by assumption, in which case the
+    // step being zero implies UB must execute if the loop is entered.
+    if (!(loopIsFiniteByAssumption(L) && isKnownNonZero(Start)) &&
+        !isKnownNonZero(StepWLG))
       return getCouldNotCompute();
 
     const SCEV *Exact =
diff --git a/llvm/lib/Analysis/ValueTracking.cpp b/llvm/lib/Analysis/ValueTracking.cpp
index 8a674914641a..02a48d35a74a 100644
--- a/llvm/lib/Analysis/ValueTracking.cpp
+++ b/llvm/lib/Analysis/ValueTracking.cpp
@@ -6135,13 +6135,14 @@ void computeKnownFPClass(const Value *V, const APInt &DemandedElts,
     else if (Bits.isNegative())
       Known.signBitMustBeOne();
 
-    if (Ty->isIEEE()) {
+    if (Ty->isIEEELikeFPTy()) {
       // IEEE floats are NaN when all bits of the exponent plus at least one of
       // the fraction bits are 1. This means:
       //   - If we assume unknown bits are 0 and the value is NaN, it will
       //     always be NaN
       //   - If we assume unknown bits are 1 and the value is not NaN, it can
       //     never be NaN
+      // Note: They do not hold for x86_fp80 format.
       if (APFloat(Ty->getFltSemantics(), Bits.One).isNaN())
         Known.KnownFPClasses = fcNan;
       else if (!APFloat(Ty->getFltSemantics(), ~Bits.Zero).isNaN())
@@ -7776,6 +7777,8 @@ static bool isGuaranteedNotToBeUndefOrPoison(
       unsigned Num = PN->getNumIncomingValues();
       bool IsWellDefined = true;
       for (unsigned i = 0; i < Num; ++i) {
+        if (PN == PN->getIncomingValue(i))
+          continue;
         auto *TI = PN->getIncomingBlock(i)->getTerminator();
         if (!isGuaranteedNotToBeUndefOrPoison(PN->getIncomingValue(i), AC, TI,
                                               DT, Depth + 1, Kind)) {
diff --git a/llvm/lib/CodeGen/ComplexDeinterleavingPass.cpp b/llvm/lib/CodeGen/ComplexDeinterleavingPass.cpp
index 92053ed56190..4cd378f9aa59 100644
--- a/llvm/lib/CodeGen/ComplexDeinterleavingPass.cpp
+++ b/llvm/lib/CodeGen/ComplexDeinterleavingPass.cpp
@@ -1741,6 +1741,17 @@ void ComplexDeinterleavingGraph::identifyReductionNodes() {
       LLVM_DEBUG(
           dbgs() << "Identified single reduction starting from instruction: "
                  << *Real << "/" << *ReductionInfo[Real].second << "\n");
+
+      // Reducing to a single vector is not supported, only permit reducing down
+      // to scalar values.
+      // Doing this here will leave the prior node in the graph,
+      // however with no uses the node will be unreachable by the replacement
+      // process. That along with the usage outside the graph should prevent the
+      // replacement process from kicking off at all for this graph.
+      // TODO Add support for reducing to a single vector value
+      if (ReductionInfo[Real].second->getType()->isVectorTy())
+        continue;
+
       Processed[i] = true;
       auto RootNode = prepareCompositeNode(
           ComplexDeinterleavingOperation::ReductionSingle, Real, nullptr);
diff --git a/llvm/lib/CodeGen/GlobalMerge.cpp b/llvm/lib/CodeGen/GlobalMerge.cpp
index 5993fc939a08..b4650a4851c3 100644
--- a/llvm/lib/CodeGen/GlobalMerge.cpp
+++ b/llvm/lib/CodeGen/GlobalMerge.cpp
@@ -711,7 +711,8 @@ bool GlobalMergeImpl::run(Module &M) {
       continue;
 
     // Ignore all 'special' globals.
-    if (GV.getName().starts_with("llvm.") || GV.getName().starts_with(".llvm."))
+    if (GV.getName().starts_with("llvm.") ||
+        GV.getName().starts_with(".llvm.") || Section == "llvm.metadata")
       continue;
 
     // Ignore all "required" globals:
diff --git a/llvm/lib/CodeGen/ModuloSchedule.cpp b/llvm/lib/CodeGen/ModuloSchedule.cpp
index f9fe812f7e65..7792a0eaa285 100644
--- a/llvm/lib/CodeGen/ModuloSchedule.cpp
+++ b/llvm/lib/CodeGen/ModuloSchedule.cpp
@@ -141,6 +141,7 @@ void ModuloScheduleExpander::generatePipelinedLoop() {
     MachineInstr *NewMI = cloneInstr(CI, MaxStageCount, StageNum);
     updateInstruction(NewMI, false, MaxStageCount, StageNum, VRMap);
     KernelBB->push_back(NewMI);
+    LIS.InsertMachineInstrInMaps(*NewMI);
     InstrMap[NewMI] = CI;
   }
 
@@ -150,6 +151,7 @@ void ModuloScheduleExpander::generatePipelinedLoop() {
     MachineInstr *NewMI = MF.CloneMachineInstr(&MI);
     updateInstruction(NewMI, false, MaxStageCount, 0, VRMap);
     KernelBB->push_back(NewMI);
+    LIS.InsertMachineInstrInMaps(*NewMI);
     InstrMap[NewMI] = &MI;
   }
 
@@ -226,6 +228,7 @@ void ModuloScheduleExpander::generateProlog(unsigned LastStage,
               cloneAndChangeInstr(&*BBI, i, (unsigned)StageNum);
           updateInstruction(NewMI, false, i, (unsigned)StageNum, VRMap);
           NewBB->push_back(NewMI);
+          LIS.InsertMachineInstrInMaps(*NewMI);
           InstrMap[NewMI] = &*BBI;
         }
       }
@@ -303,6 +306,7 @@ void ModuloScheduleExpander::generateEpilog(
           MachineInstr *NewMI = cloneInstr(In, UINT_MAX, 0);
           updateInstruction(NewMI, i == 1, EpilogStage, 0, VRMap);
           NewBB->push_back(NewMI);
+          LIS.InsertMachineInstrInMaps(*NewMI);
           InstrMap[NewMI] = In;
         }
       }
@@ -343,14 +347,11 @@ void ModuloScheduleExpander::generateEpilog(
 /// basic block with ToReg.
 static void replaceRegUsesAfterLoop(unsigned FromReg, unsigned ToReg,
                                     MachineBasicBlock *MBB,
-                                    MachineRegisterInfo &MRI,
-                                    LiveIntervals &LIS) {
+                                    MachineRegisterInfo &MRI) {
   for (MachineOperand &O :
        llvm::make_early_inc_range(MRI.use_operands(FromReg)))
     if (O.getParent()->getParent() != MBB)
       O.setReg(ToReg);
-  if (!LIS.hasInterval(ToReg))
-    LIS.createEmptyInterval(ToReg);
 }
 
 /// Return true if the register has a use that occurs outside the
@@ -464,10 +465,12 @@ void ModuloScheduleExpander::generateExistingPhis(
           InstOp1 = MRI.getVRegDef(PhiOp1);
           int PhiOpStage = Schedule.getStage(InstOp1);
           int StageAdj = (PhiOpStage != -1 ? PhiStage - PhiOpStage : 0);
-          if (PhiOpStage != -1 && PrologStage - StageAdj >= Indirects + np &&
-              VRMap[PrologStage - StageAdj - Indirects - np].count(PhiOp1)) {
-            PhiOp1 = VRMap[PrologStage - StageAdj - Indirects - np][PhiOp1];
-            break;
+          if (PhiOpStage != -1 && PrologStage - StageAdj >= Indirects + np) {
+            auto &M = VRMap[PrologStage - StageAdj - Indirects - np];
+            if (auto It = M.find(PhiOp1); It != M.end()) {
+              PhiOp1 = It->second;
+              break;
+            }
           }
           ++Indirects;
         }
@@ -540,7 +543,7 @@ void ModuloScheduleExpander::generateExistingPhis(
                 PhiOp2 = VRMap[LastStageNum - np - 1][LoopVal];
 
               if (IsLast && np == NumPhis - 1)
-                replaceRegUsesAfterLoop(Def, NewReg, BB, MRI, LIS);
+                replaceRegUsesAfterLoop(Def, NewReg, BB, MRI);
               continue;
             }
           }
@@ -558,6 +561,7 @@ void ModuloScheduleExpander::generateExistingPhis(
                   TII->get(TargetOpcode::PHI), NewReg);
       NewPhi.addReg(PhiOp1).addMBB(BB1);
       NewPhi.addReg(PhiOp2).addMBB(BB2);
+      LIS.InsertMachineInstrInMaps(*NewPhi);
       if (np == 0)
         InstrMap[NewPhi] = &*BBI;
 
@@ -580,7 +584,7 @@ void ModuloScheduleExpander::generateExistingPhis(
       // register to replace depends on whether the Phi is scheduled in the
       // epilog.
       if (IsLast && np == NumPhis - 1)
-        replaceRegUsesAfterLoop(Def, NewReg, BB, MRI, LIS);
+        replaceRegUsesAfterLoop(Def, NewReg, BB, MRI);
 
       // In the kernel, a dependent Phi uses the value from this Phi.
       if (InKernel)
@@ -597,8 +601,12 @@ void ModuloScheduleExpander::generateExistingPhis(
 
     // Check if we need to rename a Phi that has been eliminated due to
     // scheduling.
-    if (NumStages == 0 && IsLast && VRMap[CurStageNum].count(LoopVal))
-      replaceRegUsesAfterLoop(Def, VRMap[CurStageNum][LoopVal], BB, MRI, LIS);
+    if (NumStages == 0 && IsLast) {
+      auto &CurStageMap = VRMap[CurStageNum];
+      auto It = CurStageMap.find(LoopVal);
+      if (It != CurStageMap.end())
+        replaceRegUsesAfterLoop(Def, It->second, BB, MRI);
+    }
   }
 }
 
@@ -697,6 +705,7 @@ void ModuloScheduleExpander::generatePhis(
                     TII->get(TargetOpcode::PHI), NewReg);
         NewPhi.addReg(PhiOp1).addMBB(BB1);
         NewPhi.addReg(PhiOp2).addMBB(BB2);
+        LIS.InsertMachineInstrInMaps(*NewPhi);
         if (np == 0)
           InstrMap[NewPhi] = &*BBI;
 
@@ -717,7 +726,7 @@ void ModuloScheduleExpander::generatePhis(
                                   NewReg);
         }
         if (IsLast && np == NumPhis - 1)
-          replaceRegUsesAfterLoop(Def, NewReg, BB, MRI, LIS);
+          replaceRegUsesAfterLoop(Def, NewReg, BB, MRI);
       }
     }
   }
@@ -826,9 +835,11 @@ void ModuloScheduleExpander::splitLifetimes(MachineBasicBlock *KernelBB,
             // We split the lifetime when we find the first use.
             if (SplitReg == 0) {
               SplitReg = MRI.createVirtualRegister(MRI.getRegClass(Def));
-              BuildMI(*KernelBB, MI, MI->getDebugLoc(),
-                      TII->get(TargetOpcode::COPY), SplitReg)
-                  .addReg(Def);
+              MachineInstr *newCopy =
+                  BuildMI(*KernelBB, MI, MI->getDebugLoc(),
+                          TII->get(TargetOpcode::COPY), SplitReg)
+                      .addReg(Def);
+              LIS.InsertMachineInstrInMaps(*newCopy);
             }
             BBJ.substituteRegister(Def, SplitReg, 0, *TRI);
           }
@@ -896,6 +907,8 @@ void ModuloScheduleExpander::addBranches(MachineBasicBlock &PreheaderBB,
       removePhis(Epilog, LastEpi);
       // Remove the blocks that are no longer referenced.
       if (LastPro != LastEpi) {
+        for (auto &MI : *LastEpi)
+          LIS.RemoveMachineInstrFromMaps(MI);
         LastEpi->clear();
         LastEpi->eraseFromParent();
       }
@@ -903,6 +916,8 @@ void ModuloScheduleExpander::addBranches(MachineBasicBlock &PreheaderBB,
         LoopInfo->disposed(&LIS);
         NewKernel = nullptr;
       }
+      for (auto &MI : *LastPro)
+        LIS.RemoveMachineInstrFromMaps(MI);
       LastPro->clear();
       LastPro->eraseFromParent();
     } else {
@@ -1044,7 +1059,7 @@ void ModuloScheduleExpander::updateInstruction(MachineInstr *NewMI,
       MO.setReg(NewReg);
       VRMap[CurStageNum][reg] = NewReg;
       if (LastDef)
-        replaceRegUsesAfterLoop(reg, NewReg, BB, MRI, LIS);
+        replaceRegUsesAfterLoop(reg, NewReg, BB, MRI);
     } else if (MO.isUse()) {
       MachineInstr *Def = MRI.getVRegDef(reg);
       // Compute the stage that contains the last definition for instruction.
@@ -1193,10 +1208,11 @@ void ModuloScheduleExpander::rewriteScheduledInstr(
         UseOp.setReg(ReplaceReg);
       else {
         Register SplitReg = MRI.createVirtualRegister(MRI.getRegClass(OldReg));
-        BuildMI(*BB, UseMI, UseMI->getDebugLoc(), TII->get(TargetOpcode::COPY),
-                SplitReg)
-            .addReg(ReplaceReg);
+        MachineInstr *newCopy = BuildMI(*BB, UseMI, UseMI->getDebugLoc(),
+                                        TII->get(TargetOpcode::COPY), SplitReg)
+                                    .addReg(ReplaceReg);
         UseOp.setReg(SplitReg);
+        LIS.InsertMachineInstrInMaps(*newCopy);
       }
     }
   }
diff --git a/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp b/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
index 9d0456848367..e921ced8326b 100644
--- a/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
@@ -12547,9 +12547,10 @@ SDValue DAGCombiner::foldVSelectOfConstants(SDNode *N) {
   for (unsigned i = 0; i != Elts; ++i) {
     SDValue N1Elt = N1.getOperand(i);
     SDValue N2Elt = N2.getOperand(i);
-    if (N1Elt.isUndef() || N2Elt.isUndef())
+    if (N1Elt.isUndef())
       continue;
-    if (N1Elt.getValueType() != N2Elt.getValueType()) {
+    // N2 should not contain undef values since it will be reused in the fold.
+    if (N2Elt.isUndef() || N1Elt.getValueType() != N2Elt.getValueType()) {
       AllAddOne = false;
       AllSubOne = false;
       break;
diff --git a/llvm/lib/CodeGen/TailDuplicator.cpp b/llvm/lib/CodeGen/TailDuplicator.cpp
index 6c6d38462484..b0de3c322ddd 100644
--- a/llvm/lib/CodeGen/TailDuplicator.cpp
+++ b/llvm/lib/CodeGen/TailDuplicator.cpp
@@ -601,8 +601,11 @@ bool TailDuplicator::shouldTailDuplicate(bool IsSimple,
   // that rearrange the predecessors of the indirect branch.
 
   bool HasIndirectbr = false;
-  if (!TailBB.empty())
+  bool HasComputedGoto = false;
+  if (!TailBB.empty()) {
     HasIndirectbr = TailBB.back().isIndirectBranch();
+    HasComputedGoto = TailBB.terminatorIsComputedGoto();
+  }
 
   if (HasIndirectbr && PreRegAlloc)
     MaxDuplicateCount = TailDupIndirectBranchSize;
@@ -660,7 +663,12 @@ bool TailDuplicator::shouldTailDuplicate(bool IsSimple,
   // Duplicating a BB which has both multiple predecessors and successors will
   // may cause huge amount of PHI nodes. If we want to remove this limitation,
   // we have to address https://github.com/llvm/llvm-project/issues/78578.
-  if (TailBB.pred_size() > TailDupPredSize &&
+  // NB. This basically unfactors computed gotos that were factored early on in
+  // the compilation process to speed up edge based data flow. If we do not
+  // unfactor them again, it can seriously pessimize code with many computed
+  // jumps in the source code, such as interpreters. Therefore we do not
+  // restrict the computed gotos.
+  if (!HasComputedGoto && TailBB.pred_size() > TailDupPredSize &&
       TailBB.succ_size() > TailDupSuccSize) {
     // If TailBB or any of its successors contains a phi, we may have to add a
     // large number of additional phis with additional incoming values.
diff --git a/llvm/lib/CodeGen/TargetLoweringBase.cpp b/llvm/lib/CodeGen/TargetLoweringBase.cpp
index 9c56912aa6ba..411f59e714b0 100644
--- a/llvm/lib/CodeGen/TargetLoweringBase.cpp
+++ b/llvm/lib/CodeGen/TargetLoweringBase.cpp
@@ -1987,6 +1987,9 @@ void TargetLoweringBase::insertSSPDeclarations(Module &M) const {
 // Currently only support "standard" __stack_chk_guard.
 // TODO: add LOAD_STACK_GUARD support.
 Value *TargetLoweringBase::getSDagStackGuard(const Module &M) const {
+  if (getTargetMachine().getTargetTriple().isOSOpenBSD()) {
+    return M.getNamedValue("__guard_local");
+  }
   return M.getNamedValue("__stack_chk_guard");
 }
 
diff --git a/llvm/lib/IR/RuntimeLibcalls.cpp b/llvm/lib/IR/RuntimeLibcalls.cpp
index e38fce764b64..085a3bc0586b 100644
--- a/llvm/lib/IR/RuntimeLibcalls.cpp
+++ b/llvm/lib/IR/RuntimeLibcalls.cpp
@@ -82,6 +82,7 @@ void RuntimeLibcallsInfo::initLibcalls(const Triple &TT) {
     setLibcallName(RTLIB::POWI_F128, "__powikf2");
     setLibcallName(RTLIB::FPEXT_F32_F128, "__extendsfkf2");
     setLibcallName(RTLIB::FPEXT_F64_F128, "__extenddfkf2");
+    setLibcallName(RTLIB::FPROUND_F128_F16, "__trunckfhf2");
     setLibcallName(RTLIB::FPROUND_F128_F32, "__trunckfsf2");
     setLibcallName(RTLIB::FPROUND_F128_F64, "__trunckfdf2");
     setLibcallName(RTLIB::FPTOSINT_F128_I32, "__fixkfsi");
diff --git a/llvm/lib/IR/User.cpp b/llvm/lib/IR/User.cpp
index b0aa785deb9a..ab44cb4b8a3f 100644
--- a/llvm/lib/IR/User.cpp
+++ b/llvm/lib/IR/User.cpp
@@ -146,6 +146,9 @@ void *User::allocateFixedOperandUser(size_t Size, unsigned Us,
   Use *Start = reinterpret_cast<Use *>(Storage + DescBytesToAllocate);
   Use *End = Start + Us;
   User *Obj = reinterpret_cast<User *>(End);
+  Obj->NumUserOperands = Us;
+  Obj->HasHungOffUses = false;
+  Obj->HasDescriptor = DescBytes != 0;
   for (; Start != End; Start++)
     new (Start) Use(Obj);
 
@@ -172,6 +175,9 @@ void *User::operator new(size_t Size, HungOffOperandsAllocMarker) {
   void *Storage = ::operator new(Size + sizeof(Use *));
   Use **HungOffOperandList = static_cast<Use **>(Storage);
   User *Obj = reinterpret_cast<User *>(HungOffOperandList + 1);
+  Obj->NumUserOperands = 0;
+  Obj->HasHungOffUses = true;
+  Obj->HasDescriptor = false;
   *HungOffOperandList = nullptr;
   return Obj;
 }
diff --git a/llvm/lib/IR/Verifier.cpp b/llvm/lib/IR/Verifier.cpp
index 8432779c107d..551c00a518b8 100644
--- a/llvm/lib/IR/Verifier.cpp
+++ b/llvm/lib/IR/Verifier.cpp
@@ -2818,6 +2818,9 @@ void Verifier::visitFunction(const Function &F) {
   Check(!Attrs.hasAttrSomewhere(Attribute::ElementType),
         "Attribute 'elementtype' can only be applied to a callsite.", &F);
 
+  Check(!Attrs.hasFnAttr("aarch64_zt0_undef"),
+        "Attribute 'aarch64_zt0_undef' can only be applied to a callsite.");
+
   if (Attrs.hasFnAttr(Attribute::Naked))
     for (const Argument &Arg : F.args())
       Check(Arg.use_empty(), "cannot use argument of naked function", &Arg);
diff --git a/llvm/lib/MC/MCWinCOFFStreamer.cpp b/llvm/lib/MC/MCWinCOFFStreamer.cpp
index 8fd46bc8b025..3720d6e26fe4 100644
--- a/llvm/lib/MC/MCWinCOFFStreamer.cpp
+++ b/llvm/lib/MC/MCWinCOFFStreamer.cpp
@@ -299,7 +299,8 @@ void MCWinCOFFStreamer::emitCOFFSafeSEH(MCSymbol const *Symbol) {
     return;
 
   MCSection *SXData = getContext().getObjectFileInfo()->getSXDataSection();
-  changeSection(SXData);
+  pushSection();
+  switchSection(SXData);
   SXData->ensureMinAlignment(Align(4));
 
   insert(getContext().allocFragment<MCSymbolIdFragment>(Symbol));
@@ -310,6 +311,7 @@ void MCWinCOFFStreamer::emitCOFFSafeSEH(MCSymbol const *Symbol) {
   // function. Go ahead and oblige it here.
   CSymbol->setType(COFF::IMAGE_SYM_DTYPE_FUNCTION
                    << COFF::SCT_COMPLEX_TYPE_SHIFT);
+  popSection();
 }
 
 void MCWinCOFFStreamer::emitCOFFSymbolIndex(MCSymbol const *Symbol) {
diff --git a/llvm/lib/ObjCopy/MachO/MachOLayoutBuilder.cpp b/llvm/lib/ObjCopy/MachO/MachOLayoutBuilder.cpp
index d4eb6a9b9fc0..8ecd669e6717 100644
--- a/llvm/lib/ObjCopy/MachO/MachOLayoutBuilder.cpp
+++ b/llvm/lib/ObjCopy/MachO/MachOLayoutBuilder.cpp
@@ -116,11 +116,10 @@ uint64_t MachOLayoutBuilder::layoutSegments() {
   const bool IsObjectFile =
       O.Header.FileType == MachO::HeaderFileType::MH_OBJECT;
   uint64_t Offset = IsObjectFile ? (HeaderSize + O.Header.SizeOfCmds) : 0;
-  if (O.EncryptionInfoCommandIndex) {
-    // If we are emitting an encryptable binary, our load commands must have a
-    // separate (non-encrypted) page to themselves.
-    Offset = alignToPowerOf2(HeaderSize + O.Header.SizeOfCmds, PageSize);
-  }
+  // If we are emitting an encryptable binary, our load commands must have a
+  // separate (non-encrypted) page to themselves.
+  bool RequiresFirstSectionOutsideFirstPage =
+      O.EncryptionInfoCommandIndex.has_value();
   for (LoadCommand &LC : O.LoadCommands) {
     auto &MLC = LC.MachOLoadCommand;
     StringRef Segname;
@@ -174,6 +173,10 @@ uint64_t MachOLayoutBuilder::layoutSegments() {
         if (!Sec->hasValidOffset()) {
           Sec->Offset = 0;
         } else {
+          if (RequiresFirstSectionOutsideFirstPage) {
+            SectOffset = alignToPowerOf2(SectOffset, PageSize);
+            RequiresFirstSectionOutsideFirstPage = false;
+          }
           Sec->Offset = SegOffset + SectOffset;
           Sec->Size = Sec->Content.size();
           SegFileSize = std::max(SegFileSize, SectOffset + Sec->Size);
diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
index 17ff3bd37884..d195619f32d0 100644
--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
+++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
@@ -1660,6 +1660,19 @@ PassBuilder::buildFatLTODefaultPipeline(OptimizationLevel Level, bool ThinLTO,
   if (ThinLTO && PGOOpt && PGOOpt->Action == PGOOptions::SampleUse)
     MPM.addPass(buildThinLTODefaultPipeline(Level, /*ImportSummary=*/nullptr));
   else {
+    // ModuleSimplification does not run the coroutine passes for
+    // ThinLTOPreLink, so we need the coroutine passes to run for ThinLTO
+    // builds, otherwise they will miscompile.
+    if (ThinLTO) {
+      // TODO: replace w/ buildCoroWrapper() when it takes phase and level into
+      // consideration.
+      CGSCCPassManager CGPM;
+      CGPM.addPass(CoroSplitPass(Level != OptimizationLevel::O0));
+      CGPM.addPass(CoroAnnotationElidePass());
+      MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(std::move(CGPM)));
+      MPM.addPass(CoroCleanupPass());
+    }
+
     // otherwise, just use module optimization
     MPM.addPass(
         buildModuleOptimizationPipeline(Level, ThinOrFullLTOPhase::None));
diff --git a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
index b5cca88b6b51..cfd0fc32357c 100644
--- a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
@@ -498,8 +498,10 @@ AArch64TargetLowering::AArch64TargetLowering(const TargetMachine &TM,
   setOperationAction(ISD::BR_CC, MVT::f64, Custom);
   setOperationAction(ISD::SELECT, MVT::i32, Custom);
   setOperationAction(ISD::SELECT, MVT::i64, Custom);
-  setOperationAction(ISD::SELECT, MVT::f16, Custom);
-  setOperationAction(ISD::SELECT, MVT::bf16, Custom);
+  if (Subtarget->hasFPARMv8()) {
+    setOperationAction(ISD::SELECT, MVT::f16, Custom);
+    setOperationAction(ISD::SELECT, MVT::bf16, Custom);
+  }
   setOperationAction(ISD::SELECT, MVT::f32, Custom);
   setOperationAction(ISD::SELECT, MVT::f64, Custom);
   setOperationAction(ISD::SELECT_CC, MVT::i32, Custom);
@@ -10681,6 +10683,25 @@ SDValue AArch64TargetLowering::LowerFCOPYSIGN(SDValue Op,
     return convertFromScalableVector(DAG, VT, Res);
   }
 
+  // With SVE, but without Neon, extend the scalars to scalable vectors and use
+  // a SVE FCOPYSIGN.
+  if (!VT.isVector() && !Subtarget->isNeonAvailable() &&
+      Subtarget->isSVEorStreamingSVEAvailable()) {
+    if (VT != MVT::f16 && VT != MVT::f32 && VT != MVT::f64)
+      return SDValue();
+    EVT SVT = getPackedSVEVectorVT(VT);
+
+    SDValue Ins1 =
+        DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, SVT, DAG.getUNDEF(SVT), In1,
+                    DAG.getConstant(0, DL, MVT::i64));
+    SDValue Ins2 =
+        DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, SVT, DAG.getUNDEF(SVT), In2,
+                    DAG.getConstant(0, DL, MVT::i64));
+    SDValue FCS = DAG.getNode(ISD::FCOPYSIGN, DL, SVT, Ins1, Ins2);
+    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, FCS,
+                       DAG.getConstant(0, DL, MVT::i64));
+  }
+
   auto BitCast = [this](EVT VT, SDValue Op, SelectionDAG &DAG) {
     if (VT.isScalableVector())
       return getSVESafeBitCast(VT, Op, DAG);
@@ -10783,7 +10804,10 @@ SDValue AArch64TargetLowering::LowerCTPOP_PARITY(SDValue Op,
     if (VT == MVT::i32)
       AddV = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, MVT::i32, AddV,
                          DAG.getConstant(0, DL, MVT::i64));
-    AddV = DAG.getNode(ISD::BITCAST, DL, VT, AddV);
+    else
+      AddV = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT,
+                         DAG.getNode(AArch64ISD::NVCAST, DL, MVT::v1i64, AddV),
+                         DAG.getConstant(0, DL, MVT::i64));
     if (IsParity)
       AddV = DAG.getNode(ISD::AND, DL, VT, AddV, DAG.getConstant(1, DL, VT));
     return AddV;
@@ -10792,7 +10816,10 @@ SDValue AArch64TargetLowering::LowerCTPOP_PARITY(SDValue Op,
 
     SDValue CtPop = DAG.getNode(ISD::CTPOP, DL, MVT::v16i8, Val);
     SDValue AddV = DAG.getNode(AArch64ISD::UADDV, DL, MVT::v16i8, CtPop);
-    AddV = DAG.getNode(ISD::BITCAST, DL, VT, AddV);
+    AddV = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, MVT::i64,
+                       DAG.getNode(AArch64ISD::NVCAST, DL, MVT::v2i64, AddV),
+                       DAG.getConstant(0, DL, MVT::i64));
+    AddV = DAG.getZExtOrTrunc(AddV, DL, VT);
     if (IsParity)
       AddV = DAG.getNode(ISD::AND, DL, VT, AddV, DAG.getConstant(1, DL, VT));
     return AddV;
diff --git a/llvm/lib/Target/AArch64/AArch64Processors.td b/llvm/lib/Target/AArch64/AArch64Processors.td
index d1d4986d1255..80454b4f72d0 100644
--- a/llvm/lib/Target/AArch64/AArch64Processors.td
+++ b/llvm/lib/Target/AArch64/AArch64Processors.td
@@ -868,7 +868,8 @@ def ProcessorFeatures {
                                    FeatureSSBS, FeatureLS64, FeatureCLRBHB,
                                    FeatureSPECRES2, FeatureSVEAES, FeatureSVE2SM4,
                                    FeatureSVE2SHA3, FeatureSVE2, FeatureSVEBitPerm, FeatureETE,
-                                   FeatureMEC, FeatureFAMINMAX, FeatureFP8DOT2, FeatureLUT];
+                                   FeatureMEC, FeatureFAMINMAX, FeatureFP8DOT2, FeatureFP8DOT4,
+                                   FeatureFP8FMA, FeatureLUT];
   list<SubtargetFeature> Carmel   = [HasV8_2aOps, FeatureNEON, FeatureSHA2, FeatureAES,
                                      FeatureFullFP16, FeatureCRC, FeatureLSE, FeatureRAS, FeatureRDM,
                                      FeatureFPARMv8];
diff --git a/llvm/lib/Target/AArch64/SMEABIPass.cpp b/llvm/lib/Target/AArch64/SMEABIPass.cpp
index bb885d86392f..b6685497e1fd 100644
--- a/llvm/lib/Target/AArch64/SMEABIPass.cpp
+++ b/llvm/lib/Target/AArch64/SMEABIPass.cpp
@@ -54,14 +54,22 @@ FunctionPass *llvm::createSMEABIPass() { return new SMEABI(); }
 //===----------------------------------------------------------------------===//
 
 // Utility function to emit a call to __arm_tpidr2_save and clear TPIDR2_EL0.
-void emitTPIDR2Save(Module *M, IRBuilder<> &Builder) {
+void emitTPIDR2Save(Module *M, IRBuilder<> &Builder, bool ZT0IsUndef = false) {
+  auto &Ctx = M->getContext();
   auto *TPIDR2SaveTy =
       FunctionType::get(Builder.getVoidTy(), {}, /*IsVarArgs=*/false);
-  auto Attrs = AttributeList().addFnAttribute(M->getContext(),
-                                              "aarch64_pstate_sm_compatible");
+  auto Attrs =
+      AttributeList().addFnAttribute(Ctx, "aarch64_pstate_sm_compatible");
   FunctionCallee Callee =
       M->getOrInsertFunction("__arm_tpidr2_save", TPIDR2SaveTy, Attrs);
   CallInst *Call = Builder.CreateCall(Callee);
+
+  // If ZT0 is undefined (i.e. we're at the entry of a "new_zt0" function), mark
+  // that on the __arm_tpidr2_save call. This prevents an unnecessary spill of
+  // ZT0 that can occur before ZA is enabled.
+  if (ZT0IsUndef)
+    Call->addFnAttr(Attribute::get(Ctx, "aarch64_zt0_undef"));
+
   Call->setCallingConv(
       CallingConv::AArch64_SME_ABI_Support_Routines_PreserveMost_From_X0);
 
@@ -119,7 +127,7 @@ bool SMEABI::updateNewStateFunctions(Module *M, Function *F,
 
     // Create a call __arm_tpidr2_save, which commits the lazy save.
     Builder.SetInsertPoint(&SaveBB->back());
-    emitTPIDR2Save(M, Builder);
+    emitTPIDR2Save(M, Builder, /*ZT0IsUndef=*/FnAttrs.isNewZT0());
 
     // Enable pstate.za at the start of the function.
     Builder.SetInsertPoint(&OrigBB->front());
diff --git a/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.cpp b/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.cpp
index bf16acd7f8f7..76d2ac6a601e 100644
--- a/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.cpp
+++ b/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.cpp
@@ -75,6 +75,8 @@ SMEAttrs::SMEAttrs(const AttributeList &Attrs) {
     Bitmask |= SM_Body;
   if (Attrs.hasFnAttr("aarch64_za_state_agnostic"))
     Bitmask |= ZA_State_Agnostic;
+  if (Attrs.hasFnAttr("aarch64_zt0_undef"))
+    Bitmask |= ZT0_Undef;
   if (Attrs.hasFnAttr("aarch64_in_za"))
     Bitmask |= encodeZAState(StateValue::In);
   if (Attrs.hasFnAttr("aarch64_out_za"))
diff --git a/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h b/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
index fb093da70c46..1691d4fec8b6 100644
--- a/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
+++ b/llvm/lib/Target/AArch64/Utils/AArch64SMEAttributes.h
@@ -43,9 +43,10 @@ public:
     SM_Body = 1 << 2,         // aarch64_pstate_sm_body
     SME_ABI_Routine = 1 << 3, // Used for SME ABI routines to avoid lazy saves
     ZA_State_Agnostic = 1 << 4,
-    ZA_Shift = 5,
+    ZT0_Undef = 1 << 5,       // Use to mark ZT0 as undef to avoid spills
+    ZA_Shift = 6,
     ZA_Mask = 0b111 << ZA_Shift,
-    ZT0_Shift = 8,
+    ZT0_Shift = 9,
     ZT0_Mask = 0b111 << ZT0_Shift
   };
 
@@ -125,6 +126,7 @@ public:
   bool isPreservesZT0() const {
     return decodeZT0State(Bitmask) == StateValue::Preserved;
   }
+  bool isUndefZT0() const { return Bitmask & ZT0_Undef; }
   bool sharesZT0() const {
     StateValue State = decodeZT0State(Bitmask);
     return State == StateValue::In || State == StateValue::Out ||
@@ -132,7 +134,7 @@ public:
   }
   bool hasZT0State() const { return isNewZT0() || sharesZT0(); }
   bool requiresPreservingZT0(const SMEAttrs &Callee) const {
-    return hasZT0State() && !Callee.sharesZT0() &&
+    return hasZT0State() && !Callee.isUndefZT0() && !Callee.sharesZT0() &&
            !Callee.hasAgnosticZAInterface();
   }
   bool requiresDisablingZABeforeCall(const SMEAttrs &Callee) const {
diff --git a/llvm/lib/Target/AMDGPU/FLATInstructions.td b/llvm/lib/Target/AMDGPU/FLATInstructions.td
index 8fa708b74dde..dbd5723142d2 100644
--- a/llvm/lib/Target/AMDGPU/FLATInstructions.td
+++ b/llvm/lib/Target/AMDGPU/FLATInstructions.td
@@ -1873,7 +1873,7 @@ multiclass FLAT_Real_AllAddr_LDS<bits<7> op, bits<7> pre_gfx940_op,
     }
   }
 
-  let SubtargetPredicate = isGFX940Plus in {
+  let AssemblerPredicate = isGFX940Plus in {
     def _gfx940 : FLAT_Real_gfx940<op, !cast<FLAT_Pseudo>(NAME)>;
     def _SADDR_gfx940 : FLAT_Real_gfx940<op, !cast<FLAT_Pseudo>(NAME#"_SADDR")>;
   }
diff --git a/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp b/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
index e41e02a560db..89eb49ed416a 100644
--- a/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
+++ b/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
@@ -1323,8 +1323,7 @@ bool ARMConstantIslands::findAvailableWater(CPUser &U, unsigned UserOffset,
   MachineBasicBlock *UserBB = U.MI->getParent();
   BBInfoVector &BBInfo = BBUtils->getBBInfo();
   const Align CPEAlign = getCPEAlign(U.CPEMI);
-  unsigned MinNoSplitDisp =
-      BBInfo[UserBB->getNumber()].postOffset(CPEAlign) - UserOffset;
+  unsigned MinNoSplitDisp = BBInfo[UserBB->getNumber()].postOffset(CPEAlign);
   if (CloserWater && MinNoSplitDisp > U.getMaxDisp() / 2)
     return false;
   for (water_iterator IP = std::prev(WaterList.end()), B = WaterList.begin();;
diff --git a/llvm/lib/Target/ARM/ARMISelLowering.cpp b/llvm/lib/Target/ARM/ARMISelLowering.cpp
index bd8d6079e1ba..d20115c84ea8 100644
--- a/llvm/lib/Target/ARM/ARMISelLowering.cpp
+++ b/llvm/lib/Target/ARM/ARMISelLowering.cpp
@@ -149,6 +149,11 @@ MVEMaxSupportedInterleaveFactor("mve-max-interleave-factor", cl::Hidden,
   cl::desc("Maximum interleave factor for MVE VLDn to generate."),
   cl::init(2));
 
+cl::opt<unsigned> ArmMaxBaseUpdatesToCheck(
+    "arm-max-base-updates-to-check", cl::Hidden,
+    cl::desc("Maximum number of base-updates to check generating postindex."),
+    cl::init(64));
+
 /// Value type used for "flags" operands / results (either CPSR or FPSCR_NZCV).
 constexpr MVT FlagsVT = MVT::i32;
 
@@ -15842,6 +15847,22 @@ struct BaseUpdateUser {
   unsigned ConstInc;
 };
 
+static bool isValidBaseUpdate(SDNode *N, SDNode *User) {
+  // Check that the add is independent of the load/store.
+  // Otherwise, folding it would create a cycle. Search through Addr
+  // as well, since the User may not be a direct user of Addr and
+  // only share a base pointer.
+  SmallPtrSet<const SDNode *, 32> Visited;
+  SmallVector<const SDNode *, 16> Worklist;
+  Worklist.push_back(N);
+  Worklist.push_back(User);
+  const unsigned MaxSteps = 1024;
+  if (SDNode::hasPredecessorHelper(N, Visited, Worklist, MaxSteps) ||
+      SDNode::hasPredecessorHelper(User, Visited, Worklist, MaxSteps))
+    return false;
+  return true;
+}
+
 static bool TryCombineBaseUpdate(struct BaseUpdateTarget &Target,
                                  struct BaseUpdateUser &User,
                                  bool SimpleConstIncOnly,
@@ -16043,6 +16064,9 @@ static bool TryCombineBaseUpdate(struct BaseUpdateTarget &Target,
   if (SimpleConstIncOnly && User.ConstInc != NumBytes)
     return false;
 
+  if (!isValidBaseUpdate(N, User.N))
+    return false;
+
   // OK, we found an ADD we can fold into the base update.
   // Now, create a _UPD node, taking care of not breaking alignment.
 
@@ -16191,21 +16215,6 @@ static bool findPointerConstIncrement(SDNode *N, SDValue *Ptr, SDValue *CInc) {
   }
 }
 
-static bool isValidBaseUpdate(SDNode *N, SDNode *User) {
-  // Check that the add is independent of the load/store.
-  // Otherwise, folding it would create a cycle. Search through Addr
-  // as well, since the User may not be a direct user of Addr and
-  // only share a base pointer.
-  SmallPtrSet<const SDNode *, 32> Visited;
-  SmallVector<const SDNode *, 16> Worklist;
-  Worklist.push_back(N);
-  Worklist.push_back(User);
-  if (SDNode::hasPredecessorHelper(N, Visited, Worklist) ||
-      SDNode::hasPredecessorHelper(User, Visited, Worklist))
-    return false;
-  return true;
-}
-
 /// CombineBaseUpdate - Target-specific DAG combine function for VLDDUP,
 /// NEON load/store intrinsics, and generic vector load/stores, to merge
 /// base address updates.
@@ -16219,6 +16228,10 @@ static SDValue CombineBaseUpdate(SDNode *N,
   const unsigned AddrOpIdx = ((isIntrinsic || isStore) ? 2 : 1);
   BaseUpdateTarget Target = {N, isIntrinsic, isStore, AddrOpIdx};
 
+  // Limit the number of possible base-updates we look at to prevent degenerate
+  // cases.
+  unsigned MaxBaseUpdates = ArmMaxBaseUpdatesToCheck;
+
   SDValue Addr = N->getOperand(AddrOpIdx);
 
   SmallVector<BaseUpdateUser, 8> BaseUpdates;
@@ -16233,8 +16246,11 @@ static SDValue CombineBaseUpdate(SDNode *N,
     unsigned ConstInc =
         getPointerConstIncrement(User->getOpcode(), Addr, Inc, DCI.DAG);
 
-    if (ConstInc || User->getOpcode() == ISD::ADD)
+    if (ConstInc || User->getOpcode() == ISD::ADD) {
       BaseUpdates.push_back({User, Inc, ConstInc});
+      if (BaseUpdates.size() >= MaxBaseUpdates)
+        break;
+    }
   }
 
   // If the address is a constant pointer increment itself, find
@@ -16261,27 +16277,19 @@ static SDValue CombineBaseUpdate(SDNode *N,
       unsigned NewConstInc = UserOffset - Offset;
       SDValue NewInc = DCI.DAG.getConstant(NewConstInc, SDLoc(N), MVT::i32);
       BaseUpdates.push_back({User, NewInc, NewConstInc});
+      if (BaseUpdates.size() >= MaxBaseUpdates)
+        break;
     }
   }
 
   // Try to fold the load/store with an update that matches memory
   // access size. This should work well for sequential loads.
-  //
-  // Filter out invalid updates as well.
   unsigned NumValidUpd = BaseUpdates.size();
-  for (unsigned I = 0; I < NumValidUpd;) {
+  for (unsigned I = 0; I < NumValidUpd; I++) {
     BaseUpdateUser &User = BaseUpdates[I];
-    if (!isValidBaseUpdate(N, User.N)) {
-      --NumValidUpd;
-      std::swap(BaseUpdates[I], BaseUpdates[NumValidUpd]);
-      continue;
-    }
-
     if (TryCombineBaseUpdate(Target, User, /*SimpleConstIncOnly=*/true, DCI))
       return SDValue();
-    ++I;
   }
-  BaseUpdates.resize(NumValidUpd);
 
   // Try to fold with other users. Non-constant updates are considered
   // first, and constant updates are sorted to not break a sequence of
@@ -16337,8 +16345,9 @@ static SDValue PerformMVEVLDCombine(SDNode *N,
     Visited.insert(Addr.getNode());
     Worklist.push_back(N);
     Worklist.push_back(User);
-    if (SDNode::hasPredecessorHelper(N, Visited, Worklist) ||
-        SDNode::hasPredecessorHelper(User, Visited, Worklist))
+    const unsigned MaxSteps = 1024;
+    if (SDNode::hasPredecessorHelper(N, Visited, Worklist, MaxSteps) ||
+        SDNode::hasPredecessorHelper(User, Visited, Worklist, MaxSteps))
       continue;
 
     // Find the new opcode for the updating load/store.
diff --git a/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp b/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp
index 646d57770164..77ed246edbad 100644
--- a/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp
+++ b/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp
@@ -1113,8 +1113,9 @@ bool BPFAbstractMemberAccess::transformGEPChain(CallInst *Call,
                               Call->getIterator());
 
   // Generate a BitCast
-  auto *BCInst =
-      new BitCastInst(Base, PointerType::getUnqual(BB->getContext()));
+  auto *BCInst = new BitCastInst(
+      Base, PointerType::get(BB->getContext(),
+                             Base->getType()->getPointerAddressSpace()));
   BCInst->insertBefore(Call->getIterator());
 
   // Generate a GetElementPtr
diff --git a/llvm/lib/Target/Hexagon/HexagonCallingConv.td b/llvm/lib/Target/Hexagon/HexagonCallingConv.td
index cc41b569e490..e0302b85fa94 100644
--- a/llvm/lib/Target/Hexagon/HexagonCallingConv.td
+++ b/llvm/lib/Target/Hexagon/HexagonCallingConv.td
@@ -65,6 +65,9 @@ def CC_Hexagon: CallingConv<[
   CCIfType<[i32],
     CCIfSplit<
       CCCustom<"CC_SkipOdd">>>,
+  CCIfType<[v2i1],  CCPromoteToType<v2i32>>,
+  CCIfType<[v4i1],  CCPromoteToType<v4i16>>,
+  CCIfType<[v8i1],  CCPromoteToType<v8i8>>,
 
   CCIfType<[i32,v2i16,v4i8],
     CCAssignToReg<[R0,R1,R2,R3,R4,R5]>>,
@@ -111,6 +114,14 @@ class CCIfHvx128<CCAction A>
 
 def CC_Hexagon_HVX: CallingConv<[
   // HVX 64-byte mode
+
+  CCIfHvx64<
+        CCIfType<[v16i1], CCPromoteToType<v16i32>>>,
+  CCIfHvx64<
+        CCIfType<[v32i1], CCPromoteToType<v32i16>>>,
+  CCIfHvx64<
+        CCIfType<[v64i1], CCPromoteToType<v64i8>>>,
+
   CCIfHvx64<
     CCIfType<[v16i32,v32i16,v64i8],
       CCAssignToReg<[V0,V1,V2,V3,V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15]>>>,
@@ -125,6 +136,14 @@ def CC_Hexagon_HVX: CallingConv<[
       CCAssignToStack<128,64>>>,
 
   // HVX 128-byte mode
+
+  CCIfHvx128<
+        CCIfType<[v32i1], CCPromoteToType<v32i32>>>,
+  CCIfHvx128<
+        CCIfType<[v64i1], CCPromoteToType<v64i16>>>,
+  CCIfHvx128<
+        CCIfType<[v128i1], CCPromoteToType<v128i8>>>,
+
   CCIfHvx128<
     CCIfType<[v32i32,v64i16,v128i8,v32f32,v64f16],
       CCAssignToReg<[V0,V1,V2,V3,V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15]>>>,
diff --git a/llvm/lib/Target/Hexagon/HexagonHardwareLoops.cpp b/llvm/lib/Target/Hexagon/HexagonHardwareLoops.cpp
index 933474634924..dd4b24045512 100644
--- a/llvm/lib/Target/Hexagon/HexagonHardwareLoops.cpp
+++ b/llvm/lib/Target/Hexagon/HexagonHardwareLoops.cpp
@@ -731,6 +731,11 @@ CountValue *HexagonHardwareLoops::computeCount(MachineLoop *Loop,
                                                Register IVReg,
                                                int64_t IVBump,
                                                Comparison::Kind Cmp) const {
+  LLVM_DEBUG(llvm::dbgs() << "Loop: " << *Loop << "\n");
+  LLVM_DEBUG(llvm::dbgs() << "Initial Value: " << *Start << "\n");
+  LLVM_DEBUG(llvm::dbgs() << "End Value: " << *End << "\n");
+  LLVM_DEBUG(llvm::dbgs() << "Inc/Dec Value: " << IVBump << "\n");
+  LLVM_DEBUG(llvm::dbgs() << "Comparison: " << Cmp << "\n");
   // Cannot handle comparison EQ, i.e. while (A == B).
   if (Cmp == Comparison::EQ)
     return nullptr;
@@ -846,6 +851,7 @@ CountValue *HexagonHardwareLoops::computeCount(MachineLoop *Loop,
   if (IVBump < 0) {
     std::swap(Start, End);
     IVBump = -IVBump;
+    std::swap(CmpLess, CmpGreater);
   }
   // Cmp may now have a wrong direction, e.g.  LEs may now be GEs.
   // Signedness, and "including equality" are preserved.
@@ -989,7 +995,45 @@ CountValue *HexagonHardwareLoops::computeCount(MachineLoop *Loop,
     CountSR = 0;
   }
 
-  return new CountValue(CountValue::CV_Register, CountR, CountSR);
+  const TargetRegisterClass *PredRC = &Hexagon::PredRegsRegClass;
+  Register MuxR = CountR;
+  unsigned MuxSR = CountSR;
+  // For the loop count to be valid unsigned number, CmpLess should imply
+  // Dist >= 0. Similarly, CmpGreater should imply Dist < 0. We can skip the
+  // check if the initial distance is zero and the comparison is LTu || LTEu.
+  if (!(Start->isImm() && StartV == 0 && Comparison::isUnsigned(Cmp) &&
+        CmpLess) &&
+      (CmpLess || CmpGreater)) {
+    // Generate:
+    //   DistCheck = CMP_GT DistR,  0   --> CmpLess
+    //   DistCheck = CMP_GT DistR, -1   --> CmpGreater
+    Register DistCheckR = MRI->createVirtualRegister(PredRC);
+    const MCInstrDesc &DistCheckD = TII->get(Hexagon::C2_cmpgti);
+    BuildMI(*PH, InsertPos, DL, DistCheckD, DistCheckR)
+        .addReg(DistR, 0, DistSR)
+        .addImm((CmpLess) ? 0 : -1);
+
+    // Generate:
+    //   MUXR = MUX DistCheck, CountR, 1   --> CmpLess
+    //   MUXR = MUX DistCheck, 1, CountR   --> CmpGreater
+    MuxR = MRI->createVirtualRegister(IntRC);
+    if (CmpLess) {
+      const MCInstrDesc &MuxD = TII->get(Hexagon::C2_muxir);
+      BuildMI(*PH, InsertPos, DL, MuxD, MuxR)
+          .addReg(DistCheckR)
+          .addReg(CountR, 0, CountSR)
+          .addImm(1);
+    } else {
+      const MCInstrDesc &MuxD = TII->get(Hexagon::C2_muxri);
+      BuildMI(*PH, InsertPos, DL, MuxD, MuxR)
+          .addReg(DistCheckR)
+          .addImm(1)
+          .addReg(CountR, 0, CountSR);
+    }
+    MuxSR = 0;
+  }
+
+  return new CountValue(CountValue::CV_Register, MuxR, MuxSR);
 }
 
 /// Return true if the operation is invalid within hardware loop.
diff --git a/llvm/lib/Target/Hexagon/HexagonISelLowering.h b/llvm/lib/Target/Hexagon/HexagonISelLowering.h
index aaa9c65c1e07..4df88b3a8abd 100644
--- a/llvm/lib/Target/Hexagon/HexagonISelLowering.h
+++ b/llvm/lib/Target/Hexagon/HexagonISelLowering.h
@@ -362,6 +362,7 @@ public:
   shouldExpandAtomicRMWInIR(AtomicRMWInst *AI) const override {
     return AtomicExpansionKind::LLSC;
   }
+  bool softPromoteHalfType() const override { return true; }
 
 private:
   void initializeHVXLowering();
diff --git a/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp b/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp
index 816e063f8dbb..a7eb20a3e5ff 100644
--- a/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp
+++ b/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp
@@ -1265,11 +1265,15 @@ HexagonTargetLowering::extractHvxSubvectorReg(SDValue OrigOp, SDValue VecV,
   // the subvector of interest. The subvector will never overlap two single
   // vectors.
   if (isHvxPairTy(VecTy)) {
-    if (Idx * ElemWidth >= 8*HwLen)
+    unsigned SubIdx = Hexagon::vsub_lo;
+    if (Idx * ElemWidth >= 8 * HwLen) {
+      SubIdx = Hexagon::vsub_hi;
       Idx -= VecTy.getVectorNumElements() / 2;
+    }
 
-    VecV = OrigOp;
-    if (typeSplit(VecTy).first == ResTy)
+    VecTy = typeSplit(VecTy).first;
+    VecV = DAG.getTargetExtractSubreg(SubIdx, dl, VecTy, VecV);
+    if (VecTy == ResTy)
       return VecV;
   }
 
@@ -1614,17 +1618,6 @@ HexagonTargetLowering::LowerHvxBuildVector(SDValue Op, SelectionDAG &DAG)
   for (unsigned i = 0; i != Size; ++i)
     Ops.push_back(Op.getOperand(i));
 
-  // First, split the BUILD_VECTOR for vector pairs. We could generate
-  // some pairs directly (via splat), but splats should be generated
-  // by the combiner prior to getting here.
-  if (VecTy.getSizeInBits() == 16*Subtarget.getVectorLength()) {
-    ArrayRef<SDValue> A(Ops);
-    MVT SingleTy = typeSplit(VecTy).first;
-    SDValue V0 = buildHvxVectorReg(A.take_front(Size/2), dl, SingleTy, DAG);
-    SDValue V1 = buildHvxVectorReg(A.drop_front(Size/2), dl, SingleTy, DAG);
-    return DAG.getNode(ISD::CONCAT_VECTORS, dl, VecTy, V0, V1);
-  }
-
   if (VecTy.getVectorElementType() == MVT::i1)
     return buildHvxVectorPred(Ops, dl, VecTy, DAG);
 
@@ -1641,6 +1634,17 @@ HexagonTargetLowering::LowerHvxBuildVector(SDValue Op, SelectionDAG &DAG)
     return DAG.getBitcast(tyVector(VecTy, MVT::f16), T0);
   }
 
+  // First, split the BUILD_VECTOR for vector pairs. We could generate
+  // some pairs directly (via splat), but splats should be generated
+  // by the combiner prior to getting here.
+  if (VecTy.getSizeInBits() == 16 * Subtarget.getVectorLength()) {
+    ArrayRef<SDValue> A(Ops);
+    MVT SingleTy = typeSplit(VecTy).first;
+    SDValue V0 = buildHvxVectorReg(A.take_front(Size / 2), dl, SingleTy, DAG);
+    SDValue V1 = buildHvxVectorReg(A.drop_front(Size / 2), dl, SingleTy, DAG);
+    return DAG.getNode(ISD::CONCAT_VECTORS, dl, VecTy, V0, V1);
+  }
+
   return buildHvxVectorReg(Ops, dl, VecTy, DAG);
 }
 
diff --git a/llvm/lib/Target/Hexagon/HexagonPatterns.td b/llvm/lib/Target/Hexagon/HexagonPatterns.td
index cba5ff1ab0d9..acf701b0f3e5 100644
--- a/llvm/lib/Target/Hexagon/HexagonPatterns.td
+++ b/llvm/lib/Target/Hexagon/HexagonPatterns.td
@@ -109,7 +109,12 @@ def pfalse: PatFrag<(ops), (HexagonPFALSE)>;
 def pnot:   PatFrag<(ops node:$Pu), (xor node:$Pu, ptrue)>;
 
 def: Pat<(v8i1 (HexagonPFALSE)), (C2_tfrrp (A2_tfrsi (i32 0)))>;
+def: Pat<(v4i1 (HexagonPFALSE)), (C2_tfrrp (A2_tfrsi (i32 0)))>;
+def: Pat<(v2i1 (HexagonPFALSE)), (C2_tfrrp (A2_tfrsi (i32 0)))>;
+
 def: Pat<(v8i1 (HexagonPTRUE)), (C2_tfrrp (A2_tfrsi (i32 -1)))>;
+def: Pat<(v4i1 (HexagonPTRUE)), (C2_tfrrp (A2_tfrsi (i32 -1)))>;
+def: Pat<(v2i1 (HexagonPTRUE)), (C2_tfrrp (A2_tfrsi (i32 -1)))>;
 
 def valign: PatFrag<(ops node:$Vt, node:$Vs, node:$Ru),
                     (HexagonVALIGN node:$Vt, node:$Vs, node:$Ru)>;
@@ -721,11 +726,6 @@ def: OpR_RR_pat<A2_vcmpwgtu,  setugt,         v2i1, V2I32>;
 def: OpR_RR_pat<F2_sfcmpeq,   seteq,          i1, F32>;
 def: OpR_RR_pat<F2_sfcmpgt,   setgt,          i1, F32>;
 def: OpR_RR_pat<F2_sfcmpge,   setge,          i1, F32>;
-def: OpR_RR_pat<F2_sfcmpeq,   setoeq,         i1, F32>;
-def: OpR_RR_pat<F2_sfcmpgt,   setogt,         i1, F32>;
-def: OpR_RR_pat<F2_sfcmpge,   setoge,         i1, F32>;
-def: OpR_RR_pat<F2_sfcmpgt,   RevCmp<setolt>, i1, F32>;
-def: OpR_RR_pat<F2_sfcmpge,   RevCmp<setole>, i1, F32>;
 def: OpR_RR_pat<F2_sfcmpgt,   RevCmp<setlt>,  i1, F32>;
 def: OpR_RR_pat<F2_sfcmpge,   RevCmp<setle>,  i1, F32>;
 def: OpR_RR_pat<F2_sfcmpuo,   setuo,          i1, F32>;
@@ -733,11 +733,6 @@ def: OpR_RR_pat<F2_sfcmpuo,   setuo,          i1, F32>;
 def: OpR_RR_pat<F2_dfcmpeq,   seteq,          i1, F64>;
 def: OpR_RR_pat<F2_dfcmpgt,   setgt,          i1, F64>;
 def: OpR_RR_pat<F2_dfcmpge,   setge,          i1, F64>;
-def: OpR_RR_pat<F2_dfcmpeq,   setoeq,         i1, F64>;
-def: OpR_RR_pat<F2_dfcmpgt,   setogt,         i1, F64>;
-def: OpR_RR_pat<F2_dfcmpge,   setoge,         i1, F64>;
-def: OpR_RR_pat<F2_dfcmpgt,   RevCmp<setolt>, i1, F64>;
-def: OpR_RR_pat<F2_dfcmpge,   RevCmp<setole>, i1, F64>;
 def: OpR_RR_pat<F2_dfcmpgt,   RevCmp<setlt>,  i1, F64>;
 def: OpR_RR_pat<F2_dfcmpge,   RevCmp<setle>,  i1, F64>;
 def: OpR_RR_pat<F2_dfcmpuo,   setuo,          i1, F64>;
@@ -900,15 +895,35 @@ def: OpmR_RR_pat<Cmpud<F2_dfcmpge>,  RevCmp<setule>, i1, F64>;
 def: OpmR_RR_pat<Cmpud<F2_dfcmpgt>,  RevCmp<setult>, i1, F64>;
 def: OpmR_RR_pat<Cmpudn<F2_dfcmpeq>, setune,         i1, F64>;
 
-def: OpmR_RR_pat<Outn<F2_sfcmpeq>, setone, i1, F32>;
-def: OpmR_RR_pat<Outn<F2_sfcmpeq>, setne,  i1, F32>;
+class T4<InstHexagon MI1, InstHexagon MI2, InstHexagon MI3, InstHexagon MI4>
+  : OutPatFrag<(ops node:$Rs, node:$Rt),
+               (MI1 (MI2 (MI3 $Rs, $Rt), (MI4 $Rs, $Rt)))>;
 
-def: OpmR_RR_pat<Outn<F2_dfcmpeq>, setone, i1, F64>;
-def: OpmR_RR_pat<Outn<F2_dfcmpeq>, setne,  i1, F64>;
+class Cmpof<InstHexagon MI>: T3<C2_andn, MI,  F2_sfcmpuo>;
+class Cmpod<InstHexagon MI>: T3<C2_andn, MI,  F2_dfcmpuo>;
+
+class Cmpofn<InstHexagon MI>: T4<C2_not,  C2_or, MI,  F2_sfcmpuo>;
+class Cmpodn<InstHexagon MI>: T4<C2_not,  C2_or, MI,  F2_dfcmpuo>;
+
+def: OpmR_RR_pat<Cmpof<F2_sfcmpeq>,  setoeq,         i1, F32>;
+def: OpmR_RR_pat<Cmpof<F2_sfcmpge>,  setoge,         i1, F32>;
+def: OpmR_RR_pat<Cmpof<F2_sfcmpgt>,  setogt,         i1, F32>;
+def: OpmR_RR_pat<Cmpof<F2_sfcmpge>,  RevCmp<setole>, i1, F32>;
+def: OpmR_RR_pat<Cmpof<F2_sfcmpgt>,  RevCmp<setolt>, i1, F32>;
+def: OpmR_RR_pat<Cmpofn<F2_sfcmpeq>, setone,         i1, F32>;
+
+def: OpmR_RR_pat<Cmpod<F2_dfcmpeq>,  setoeq,         i1, F64>;
+def: OpmR_RR_pat<Cmpod<F2_dfcmpge>,  setoge,         i1, F64>;
+def: OpmR_RR_pat<Cmpod<F2_dfcmpgt>,  setogt,         i1, F64>;
+def: OpmR_RR_pat<Cmpod<F2_dfcmpge>,  RevCmp<setole>, i1, F64>;
+def: OpmR_RR_pat<Cmpod<F2_dfcmpgt>,  RevCmp<setolt>, i1, F64>;
+def: OpmR_RR_pat<Cmpodn<F2_dfcmpeq>, setone,         i1, F64>;
 
 def: OpmR_RR_pat<Outn<F2_sfcmpuo>, seto,   i1, F32>;
 def: OpmR_RR_pat<Outn<F2_dfcmpuo>, seto,   i1, F64>;
 
+def: OpmR_RR_pat<Outn<F2_sfcmpeq>, setne,  i1, F32>;
+def: OpmR_RR_pat<Outn<F2_dfcmpeq>, setne,  i1, F64>;
 
 // --(6) Select ----------------------------------------------------------
 //
diff --git a/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonAsmBackend.cpp b/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonAsmBackend.cpp
index 98b1dde8fa3f..725067e0c9bd 100644
--- a/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonAsmBackend.cpp
+++ b/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonAsmBackend.cpp
@@ -728,6 +728,24 @@ public:
               MCContext &Context = Asm.getContext();
               auto &RF = cast<MCRelaxableFragment>(*Frags[K]);
               auto &Inst = const_cast<MCInst &>(RF.getInst());
+
+              const bool WouldTraverseLabel = llvm::any_of(
+                  Asm.symbols(), [&Asm, &RF, &Inst](MCSymbol const &sym) {
+                    uint64_t Offset = 0;
+                    const bool HasOffset = Asm.getSymbolOffset(sym, Offset);
+                    const unsigned PacketSizeBytes =
+                        HexagonMCInstrInfo::bundleSize(Inst) *
+                        HEXAGON_INSTR_SIZE;
+                    const bool OffsetPastSym =
+                        Offset <= (Asm.getFragmentOffset(RF) + PacketSizeBytes);
+                    return !sym.isVariable() && Offset != 0 && HasOffset &&
+                           OffsetPastSym;
+                  });
+              if (WouldTraverseLabel) {
+                Size = 0;
+                break;
+              }
+
               while (Size > 0 &&
                      HexagonMCInstrInfo::bundleSize(Inst) < MaxPacketSize) {
                 MCInst *Nop = Context.createMCInst();
diff --git a/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonMCTargetDesc.cpp b/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonMCTargetDesc.cpp
index a98f6048b051..8d18aade1a2b 100644
--- a/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonMCTargetDesc.cpp
+++ b/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonMCTargetDesc.cpp
@@ -125,7 +125,7 @@ static cl::opt<bool>
 static cl::opt<bool> EnableHexagonCabac
   ("mcabac", cl::desc("tbd"), cl::init(false));
 
-static StringRef DefaultArch = "hexagonv60";
+static constexpr StringRef DefaultArch = "hexagonv68";
 
 static StringRef HexagonGetArchVariant() {
   if (MV5)
diff --git a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
index 169f9568e536..895a8e264669 100644
--- a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
@@ -90,20 +90,29 @@ bool LoongArchAsmPrinter::PrintAsmOperand(const MachineInstr *MI, unsigned OpNo,
         return false;
       }
       break;
-    case 'w': // Print LSX registers.
-      if (MO.getReg().id() >= LoongArch::VR0 &&
-          MO.getReg().id() <= LoongArch::VR31)
-        break;
-      // The modifier is 'w' but the operand is not an LSX register; Report an
-      // unknown operand error.
-      return true;
     case 'u': // Print LASX registers.
-      if (MO.getReg().id() >= LoongArch::XR0 &&
-          MO.getReg().id() <= LoongArch::XR31)
-        break;
-      // The modifier is 'u' but the operand is not an LASX register; Report an
-      // unknown operand error.
-      return true;
+    case 'w': // Print LSX registers.
+    {
+      // If the operand is an LASX, LSX or floating point register, print the
+      // name of LASX or LSX register with the same index in that register
+      // class.
+      unsigned RegID = MO.getReg().id(), FirstReg;
+      if (RegID >= LoongArch::XR0 && RegID <= LoongArch::XR31)
+        FirstReg = LoongArch::XR0;
+      else if (RegID >= LoongArch::VR0 && RegID <= LoongArch::VR31)
+        FirstReg = LoongArch::VR0;
+      else if (RegID >= LoongArch::F0_64 && RegID <= LoongArch::F31_64)
+        FirstReg = LoongArch::F0_64;
+      else if (RegID >= LoongArch::F0 && RegID <= LoongArch::F31)
+        FirstReg = LoongArch::F0;
+      else
+        return true;
+      OS << '$'
+         << LoongArchInstPrinter::getRegisterName(
+                RegID - FirstReg +
+                (ExtraCode[0] == 'u' ? LoongArch::XR0 : LoongArch::VR0));
+      return false;
+    }
       // TODO: handle other extra codes if any.
     }
   }
diff --git a/llvm/lib/Target/LoongArch/LoongArchFloatInstrFormats.td b/llvm/lib/Target/LoongArch/LoongArchFloatInstrFormats.td
index f66f620ca8b2..ce42236895c7 100644
--- a/llvm/lib/Target/LoongArch/LoongArchFloatInstrFormats.td
+++ b/llvm/lib/Target/LoongArch/LoongArchFloatInstrFormats.td
@@ -206,7 +206,7 @@ class FP_LOAD_3R<bits<32> op, RegisterClass rc = FPR32>
     : FPFmtMEM<op, (outs rc:$fd), (ins GPR:$rj, GPR:$rk),
                "$fd, $rj, $rk">;
 class FP_LOAD_2RI12<bits<32> op, RegisterClass rc = FPR32>
-    : FPFmt2RI12<op, (outs rc:$fd), (ins GPR:$rj, simm12:$imm12),
+    : FPFmt2RI12<op, (outs rc:$fd), (ins GPR:$rj, simm12_addlike:$imm12),
                  "$fd, $rj, $imm12">;
 } // hasSideEffects = 0, mayLoad = 1, mayStore = 0
 
@@ -215,7 +215,7 @@ class FP_STORE_3R<bits<32> op, RegisterClass rc = FPR32>
     : FPFmtMEM<op, (outs), (ins rc:$fd, GPR:$rj, GPR:$rk),
                "$fd, $rj, $rk">;
 class FP_STORE_2RI12<bits<32> op, RegisterClass rc = FPR32>
-    : FPFmt2RI12<op, (outs), (ins rc:$fd, GPR:$rj, simm12:$imm12),
+    : FPFmt2RI12<op, (outs), (ins rc:$fd, GPR:$rj, simm12_addlike:$imm12),
                  "$fd, $rj, $imm12">;
 } // hasSideEffects = 0, mayLoad = 0, mayStore = 1
 
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp b/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
index 2282dc895561..4ed3c3cf92e3 100644
--- a/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
@@ -99,7 +99,7 @@ LoongArchTargetLowering::LoongArchTargetLowering(const TargetMachine &TM,
   setOperationAction(ISD::INTRINSIC_W_CHAIN, MVT::Other, Custom);
   setOperationAction(ISD::INTRINSIC_WO_CHAIN, MVT::Other, Custom);
 
-  setOperationAction(ISD::PREFETCH, MVT::Other, Legal);
+  setOperationAction(ISD::PREFETCH, MVT::Other, Custom);
 
   // Expand bitreverse.i16 with native-width bitrev and shift for now, before
   // we get to know which of sll and revb.2h is faster.
@@ -459,10 +459,24 @@ SDValue LoongArchTargetLowering::LowerOperation(SDValue Op,
     return lowerBITREVERSE(Op, DAG);
   case ISD::SCALAR_TO_VECTOR:
     return lowerSCALAR_TO_VECTOR(Op, DAG);
+  case ISD::PREFETCH:
+    return lowerPREFETCH(Op, DAG);
   }
   return SDValue();
 }
 
+SDValue LoongArchTargetLowering::lowerPREFETCH(SDValue Op,
+                                               SelectionDAG &DAG) const {
+  unsigned IsData = Op.getConstantOperandVal(4);
+
+  // We don't support non-data prefetch.
+  // Just preserve the chain.
+  if (!IsData)
+    return Op.getOperand(0);
+
+  return Op;
+}
+
 SDValue
 LoongArchTargetLowering::lowerSCALAR_TO_VECTOR(SDValue Op,
                                                SelectionDAG &DAG) const {
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelLowering.h b/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
index a215ab523874..3f44a720eca7 100644
--- a/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
+++ b/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
@@ -337,6 +337,7 @@ private:
   SDValue lowerVECTOR_SHUFFLE(SDValue Op, SelectionDAG &DAG) const;
   SDValue lowerBITREVERSE(SDValue Op, SelectionDAG &DAG) const;
   SDValue lowerSCALAR_TO_VECTOR(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerPREFETCH(SDValue Op, SelectionDAG &DAG) const;
 
   bool isFPImmLegal(const APFloat &Imm, EVT VT,
                     bool ForCodeSize) const override;
diff --git a/llvm/lib/Target/LoongArch/LoongArchLASXInstrInfo.td b/llvm/lib/Target/LoongArch/LoongArchLASXInstrInfo.td
index 24b5ed5a9344..7022fddf3410 100644
--- a/llvm/lib/Target/LoongArch/LoongArchLASXInstrInfo.td
+++ b/llvm/lib/Target/LoongArch/LoongArchLASXInstrInfo.td
@@ -186,10 +186,10 @@ class LASX2RI10_Load<bits<32> op, Operand ImmOpnd = simm10_lsl2>
 class LASX2RI11_Load<bits<32> op, Operand ImmOpnd = simm11_lsl1>
     : Fmt2RI11_XRI<op, (outs LASX256:$xd), (ins GPR:$rj, ImmOpnd:$imm11),
                   "$xd, $rj, $imm11">;
-class LASX2RI12_Load<bits<32> op, Operand ImmOpnd = simm12>
+class LASX2RI12_Load<bits<32> op, Operand ImmOpnd = simm12_addlike>
     : Fmt2RI12_XRI<op, (outs LASX256:$xd), (ins GPR:$rj, ImmOpnd:$imm12),
                   "$xd, $rj, $imm12">;
-class LASX2RI12_Store<bits<32> op, Operand ImmOpnd = simm12>
+class LASX2RI12_Store<bits<32> op, Operand ImmOpnd = simm12_addlike>
     : Fmt2RI12_XRI<op, (outs), (ins LASX256:$xd, GPR:$rj, ImmOpnd:$imm12),
                   "$xd, $rj, $imm12">;
 
diff --git a/llvm/lib/Target/LoongArch/LoongArchLSXInstrInfo.td b/llvm/lib/Target/LoongArch/LoongArchLSXInstrInfo.td
index d2063a8aaae9..e37de4f545a2 100644
--- a/llvm/lib/Target/LoongArch/LoongArchLSXInstrInfo.td
+++ b/llvm/lib/Target/LoongArch/LoongArchLSXInstrInfo.td
@@ -374,10 +374,10 @@ class LSX2RI10_Load<bits<32> op, Operand ImmOpnd = simm10_lsl2>
 class LSX2RI11_Load<bits<32> op, Operand ImmOpnd = simm11_lsl1>
     : Fmt2RI11_VRI<op, (outs LSX128:$vd), (ins GPR:$rj, ImmOpnd:$imm11),
                   "$vd, $rj, $imm11">;
-class LSX2RI12_Load<bits<32> op, Operand ImmOpnd = simm12>
+class LSX2RI12_Load<bits<32> op, Operand ImmOpnd = simm12_addlike>
     : Fmt2RI12_VRI<op, (outs LSX128:$vd), (ins GPR:$rj, ImmOpnd:$imm12),
                   "$vd, $rj, $imm12">;
-class LSX2RI12_Store<bits<32> op, Operand ImmOpnd = simm12>
+class LSX2RI12_Store<bits<32> op, Operand ImmOpnd = simm12_addlike>
     : Fmt2RI12_VRI<op, (outs), (ins LSX128:$vd, GPR:$rj, ImmOpnd:$imm12),
                   "$vd, $rj, $imm12">;
 
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.cpp
index 30d2d0c1184a..5698468c4754 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.cpp
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.cpp
@@ -275,6 +275,7 @@ void LoongArchMCExpr::fixELFSymbolsInTLSFixups(MCAssembler &Asm) const {
   case VK_LoongArch_TLS_GD_HI20:
   case VK_LoongArch_TLS_DESC_PC_HI20:
   case VK_LoongArch_TLS_DESC_HI20:
+  case VK_LoongArch_TLS_LE_HI20_R:
   case VK_LoongArch_TLS_LD_PCREL20_S2:
   case VK_LoongArch_TLS_GD_PCREL20_S2:
   case VK_LoongArch_TLS_DESC_PCREL20_S2:
diff --git a/llvm/lib/Target/PowerPC/PPCISelLowering.cpp b/llvm/lib/Target/PowerPC/PPCISelLowering.cpp
index 21ff6f050817..16491a145a5b 100644
--- a/llvm/lib/Target/PowerPC/PPCISelLowering.cpp
+++ b/llvm/lib/Target/PowerPC/PPCISelLowering.cpp
@@ -223,13 +223,19 @@ PPCTargetLowering::PPCTargetLowering(const PPCTargetMachine &TM,
     setLoadExtAction(ISD::SEXTLOAD, VT, MVT::i8, Expand);
   }
 
+  setTruncStoreAction(MVT::f128, MVT::f16, Expand);
+  setOperationAction(ISD::FP_TO_FP16, MVT::f128, Expand);
+
   if (Subtarget.isISA3_0()) {
+    setLoadExtAction(ISD::EXTLOAD, MVT::f128, MVT::f16, Legal);
     setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f16, Legal);
     setLoadExtAction(ISD::EXTLOAD, MVT::f32, MVT::f16, Legal);
     setTruncStoreAction(MVT::f64, MVT::f16, Legal);
     setTruncStoreAction(MVT::f32, MVT::f16, Legal);
   } else {
     // No extending loads from f16 or HW conversions back and forth.
+    setLoadExtAction(ISD::EXTLOAD, MVT::f128, MVT::f16, Expand);
+    setOperationAction(ISD::FP16_TO_FP, MVT::f128, Expand);
     setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f16, Expand);
     setOperationAction(ISD::FP16_TO_FP, MVT::f64, Expand);
     setOperationAction(ISD::FP_TO_FP16, MVT::f64, Expand);
diff --git a/llvm/lib/Target/PowerPC/PPCInstrVSX.td b/llvm/lib/Target/PowerPC/PPCInstrVSX.td
index 8e400bc63b78..a8724ea12514 100644
--- a/llvm/lib/Target/PowerPC/PPCInstrVSX.td
+++ b/llvm/lib/Target/PowerPC/PPCInstrVSX.td
@@ -3997,6 +3997,8 @@ defm : ScalToVecWPermute<
   (SUBREG_TO_REG (i64 1), (VEXTSH2Ds (LXSIHZX ForceXForm:$src)), sub_64)>;
 
 // Load/convert and convert/store patterns for f16.
+def : Pat<(f128 (extloadf16 ForceXForm:$src)),
+          (f128 (XSCVDPQP (XSCVHPDP (LXSIHZX ForceXForm:$src))))>;
 def : Pat<(f64 (extloadf16 ForceXForm:$src)),
           (f64 (XSCVHPDP (LXSIHZX ForceXForm:$src)))>;
 def : Pat<(truncstoref16 f64:$src, ForceXForm:$dst),
@@ -4005,6 +4007,8 @@ def : Pat<(f32 (extloadf16 ForceXForm:$src)),
           (f32 (COPY_TO_REGCLASS (XSCVHPDP (LXSIHZX ForceXForm:$src)), VSSRC))>;
 def : Pat<(truncstoref16 f32:$src, ForceXForm:$dst),
           (STXSIHX (XSCVDPHP (COPY_TO_REGCLASS $src, VSFRC)), ForceXForm:$dst)>;
+def : Pat<(f128 (f16_to_fp i32:$A)),
+          (f128 (XSCVDPQP (XSCVHPDP (MTVSRWZ $A))))>;
 def : Pat<(f64 (f16_to_fp i32:$A)),
           (f64 (XSCVHPDP (MTVSRWZ $A)))>;
 def : Pat<(f32 (f16_to_fp i32:$A)),
diff --git a/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp b/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp
index add82dc80c42..8f1094413a75 100644
--- a/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp
+++ b/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp
@@ -1658,9 +1658,8 @@ RISCVTTIImpl::getArithmeticReductionCost(unsigned Opcode, VectorType *Ty,
     break;
   case ISD::FADD:
     // We can't promote f16/bf16 fadd reductions.
-    if ((LT.second.getVectorElementType() == MVT::f16 &&
-         !ST->hasVInstructionsF16()) ||
-        LT.second.getVectorElementType() == MVT::bf16)
+    if ((LT.second.getScalarType() == MVT::f16 && !ST->hasVInstructionsF16()) ||
+        LT.second.getScalarType() == MVT::bf16)
       return BaseT::getArithmeticReductionCost(Opcode, Ty, FMF, CostKind);
     if (TTI::requiresOrderedReduction(FMF)) {
       Opcodes.push_back(RISCV::VFMV_S_F);
diff --git a/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp b/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp
index 1fb31c26e20d..049865c81667 100644
--- a/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp
+++ b/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp
@@ -254,7 +254,7 @@ SystemZTargetLowering::SystemZTargetLowering(const TargetMachine &TM,
     setOperationAction(ISD::ROTR,      MVT::i128, Expand);
     setOperationAction(ISD::ROTL,      MVT::i128, Expand);
 
-    // No special instructions for these before arch15.
+    // No special instructions for these before z17.
     if (!Subtarget.hasVectorEnhancements3()) {
       setOperationAction(ISD::MUL,   MVT::i128, Expand);
       setOperationAction(ISD::MULHS, MVT::i128, Expand);
@@ -281,7 +281,7 @@ SystemZTargetLowering::SystemZTargetLowering(const TargetMachine &TM,
     // Use VPOPCT and add up partial results.
     setOperationAction(ISD::CTPOP, MVT::i128, Custom);
 
-    // Additional instructions available with arch15.
+    // Additional instructions available with z17.
     if (Subtarget.hasVectorEnhancements3()) {
       setOperationAction(ISD::ABS, MVT::i128, Legal);
     }
@@ -353,7 +353,7 @@ SystemZTargetLowering::SystemZTargetLowering(const TargetMachine &TM,
   setOperationAction(ISD::CTLZ_ZERO_UNDEF, MVT::i32, Promote);
   setOperationAction(ISD::CTLZ, MVT::i64, Legal);
 
-  // On arch15 we have native support for a 64-bit CTTZ.
+  // On z17 we have native support for a 64-bit CTTZ.
   if (Subtarget.hasMiscellaneousExtensions4()) {
     setOperationAction(ISD::CTTZ, MVT::i32, Promote);
     setOperationAction(ISD::CTTZ_ZERO_UNDEF, MVT::i32, Promote);
@@ -4466,7 +4466,7 @@ SDValue SystemZTargetLowering::lowerMULH(SDValue Op,
   SDLoc DL(Op);
   SDValue Even, Odd;
 
-  // This custom expander is only used on arch15 and later for 64-bit types.
+  // This custom expander is only used on z17 and later for 64-bit types.
   assert(!is32Bit(VT));
   assert(Subtarget.hasMiscellaneousExtensions2());
 
@@ -10231,6 +10231,11 @@ static void printFunctionArgExts(const Function *F, raw_fd_ostream &OS) {
 void SystemZTargetLowering::
 verifyNarrowIntegerArgs_Call(const SmallVectorImpl<ISD::OutputArg> &Outs,
                              const Function *F, SDValue Callee) const {
+  // Temporarily only do the check when explicitly requested, until it can be
+  // enabled by default.
+  if (!EnableIntArgExtCheck)
+    return;
+
   bool IsInternal = false;
   const Function *CalleeFn = nullptr;
   if (auto *G = dyn_cast<GlobalAddressSDNode>(Callee))
@@ -10252,6 +10257,11 @@ verifyNarrowIntegerArgs_Call(const SmallVectorImpl<ISD::OutputArg> &Outs,
 void SystemZTargetLowering::
 verifyNarrowIntegerArgs_Ret(const SmallVectorImpl<ISD::OutputArg> &Outs,
                             const Function *F) const {
+  // Temporarily only do the check when explicitly requested, until it can be
+  // enabled by default.
+  if (!EnableIntArgExtCheck)
+    return;
+
   if (!verifyNarrowIntegerArgs(Outs, isFullyInternal(F))) {
     errs() << "ERROR: Missing extension attribute of returned "
            << "value from function:\n";
@@ -10268,11 +10278,6 @@ verifyNarrowIntegerArgs(const SmallVectorImpl<ISD::OutputArg> &Outs,
   if (IsInternal || !Subtarget.isTargetELF())
     return true;
 
-  // Temporarily only do the check when explicitly requested, until it can be
-  // enabled by default.
-  if (!EnableIntArgExtCheck)
-    return true;
-
   if (EnableIntArgExtCheck.getNumOccurrences()) {
     if (!EnableIntArgExtCheck)
       return true;
diff --git a/llvm/lib/Target/SystemZ/SystemZInstrVector.td b/llvm/lib/Target/SystemZ/SystemZInstrVector.td
index edd20a5de8c6..a4ece32c79d8 100644
--- a/llvm/lib/Target/SystemZ/SystemZInstrVector.td
+++ b/llvm/lib/Target/SystemZ/SystemZInstrVector.td
@@ -1973,7 +1973,7 @@ let Predicates = [FeatureVector] in {
             (VLEG (VGBM 0), bdxaddr12only:$addr, 1)>;
 }
 
-// In-register i128 sign-extensions on arch15.
+// In-register i128 sign-extensions on z17.
 let Predicates = [FeatureVectorEnhancements3] in {
   def : Pat<(i128 (sext_inreg VR128:$x, i8)), (VUPLG (VSEGB VR128:$x))>;
   def : Pat<(i128 (sext_inreg VR128:$x, i16)), (VUPLG (VSEGH VR128:$x))>;
@@ -1993,7 +1993,7 @@ let Predicates = [FeatureVector] in {
             (VSRAB (VREPG VR128:$x, 1), (VREPIB 64))>;
 }
 
-// Sign-extensions from GPR to i128 on arch15.
+// Sign-extensions from GPR to i128 on z17.
 let Predicates = [FeatureVectorEnhancements3] in {
   def : Pat<(i128 (sext_inreg (anyext GR32:$x), i8)),
             (VUPLG (VLVGP (LGBR (INSERT_SUBREG (i64 (IMPLICIT_DEF)), GR32:$x, subreg_l32)),
diff --git a/llvm/lib/Target/SystemZ/SystemZProcessors.td b/llvm/lib/Target/SystemZ/SystemZProcessors.td
index 75b6671dc772..0827701a48b5 100644
--- a/llvm/lib/Target/SystemZ/SystemZProcessors.td
+++ b/llvm/lib/Target/SystemZ/SystemZProcessors.td
@@ -41,4 +41,5 @@ def : ProcessorModel<"z15", Z15Model, Arch13SupportedFeatures.List>;
 def : ProcessorModel<"arch14", Z16Model, Arch14SupportedFeatures.List>;
 def : ProcessorModel<"z16", Z16Model, Arch14SupportedFeatures.List>;
 
-def : ProcessorModel<"arch15", Z16Model, Arch15SupportedFeatures.List>;
+def : ProcessorModel<"arch15", Z17Model, Arch15SupportedFeatures.List>;
+def : ProcessorModel<"z17", Z17Model, Arch15SupportedFeatures.List>;
diff --git a/llvm/lib/Target/SystemZ/SystemZSchedule.td b/llvm/lib/Target/SystemZ/SystemZSchedule.td
index d683cc042e5c..cc03a71d8a64 100644
--- a/llvm/lib/Target/SystemZ/SystemZSchedule.td
+++ b/llvm/lib/Target/SystemZ/SystemZSchedule.td
@@ -60,6 +60,7 @@ def VBU : SchedWrite; // Virtual branching unit
 
 def MCD : SchedWrite; // Millicode
 
+include "SystemZScheduleZ17.td"
 include "SystemZScheduleZ16.td"
 include "SystemZScheduleZ15.td"
 include "SystemZScheduleZ14.td"
diff --git a/llvm/lib/Target/SystemZ/SystemZScheduleZ16.td b/llvm/lib/Target/SystemZ/SystemZScheduleZ16.td
index 2c01691707cc..a9354ea76c72 100644
--- a/llvm/lib/Target/SystemZ/SystemZScheduleZ16.td
+++ b/llvm/lib/Target/SystemZ/SystemZScheduleZ16.td
@@ -1555,12 +1555,12 @@ def : InstRW<[WLat2, VecDFX, NormalGr], (instregex "V(T|C)P$")>;
 
 def : InstRW<[WLat30, VecDF2, NormalGr], (instregex "VSCH(S|D|X)?P$")>;
 def : InstRW<[WLat30, VecDF2, NormalGr], (instregex "VSCSHP$")>;
-def : InstRW<[WLat30, VecDF2, NormalGr], (instregex "VCSPH")>;
-def : InstRW<[WLat2, WLat2, VecXsPm, NormalGr], (instregex "VCLZDP")>;
-def : InstRW<[WLat10, WLat10, VecDF2, NormalGr], (instregex "VSRPR")>;
-def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VPKZR")>;
-def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VUPKZH")>;
-def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VUPKZL")>;
+def : InstRW<[WLat30, VecDF2, NormalGr], (instregex "VCSPH$")>;
+def : InstRW<[WLat2, WLat2, VecXsPm, NormalGr], (instregex "VCLZDP$")>;
+def : InstRW<[WLat10, WLat10, VecDF2, NormalGr], (instregex "VSRPR$")>;
+def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VPKZR$")>;
+def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VUPKZH$")>;
+def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VUPKZL$")>;
 
 // -------------------------------- System ---------------------------------- //
 
@@ -1597,8 +1597,8 @@ def : InstRW<[WLat30, MCD], (instregex "S(T)?PX$")>;
 // System: Breaking-Event-Address-Register Instructions
 //===----------------------------------------------------------------------===//
 
-def : InstRW<[WLat3LSU, LSU2, GroupAlone], (instregex "LBEAR")>;
-def : InstRW<[WLat1, LSU2, FXb, GroupAlone], (instregex "STBEAR")>;
+def : InstRW<[WLat3LSU, LSU2, GroupAlone], (instregex "LBEAR$")>;
+def : InstRW<[WLat1, LSU2, FXb, GroupAlone], (instregex "STBEAR$")>;
 
 //===----------------------------------------------------------------------===//
 // System: Storage-Key and Real Memory Instructions
diff --git a/llvm/lib/Target/SystemZ/SystemZScheduleZ17.td b/llvm/lib/Target/SystemZ/SystemZScheduleZ17.td
new file mode 100644
index 000000000000..bd52627f636a
--- /dev/null
+++ b/llvm/lib/Target/SystemZ/SystemZScheduleZ17.td
@@ -0,0 +1,1754 @@
+//--- SystemZScheduleZ17.td - SystemZ Scheduling Definitions ---*- tblgen -*-=//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the machine model for Z17 to support instruction
+// scheduling and other instruction cost heuristics.
+//
+// Pseudos expanded right after isel do not need to be modelled here.
+//
+//===----------------------------------------------------------------------===//
+
+def Z17Model : SchedMachineModel {
+
+    let UnsupportedFeatures = Arch15UnsupportedFeatures.List;
+
+    let IssueWidth = 6;             // Number of instructions decoded per cycle.
+    let MicroOpBufferSize = 60;     // Issue queues
+    let LoadLatency = 1;            // Optimistic load latency.
+
+    let PostRAScheduler = 1;
+
+    // Extra cycles for a mispredicted branch.
+    let MispredictPenalty = 20;
+}
+
+let SchedModel = Z17Model in  {
+// These definitions need the SchedModel value. They could be put in a
+// subtarget common include file, but it seems the include system in Tablegen
+// currently (2016) rejects multiple includes of same file.
+
+// Decoder grouping rules
+let NumMicroOps = 1 in {
+  def : WriteRes<NormalGr, []>;
+  def : WriteRes<BeginGroup, []> { let BeginGroup  = 1; }
+  def : WriteRes<EndGroup, []>   { let EndGroup    = 1; }
+}
+def : WriteRes<Cracked, []> {
+  let NumMicroOps = 2;
+  let BeginGroup  = 1;
+}
+def : WriteRes<GroupAlone, []> {
+  let NumMicroOps = 3;
+  let BeginGroup  = 1;
+  let EndGroup    = 1;
+}
+def : WriteRes<GroupAlone2, []> {
+  let NumMicroOps = 6;
+  let BeginGroup  = 1;
+  let EndGroup    = 1;
+}
+def : WriteRes<GroupAlone3, []> {
+  let NumMicroOps = 9;
+  let BeginGroup  = 1;
+  let EndGroup    = 1;
+}
+
+// Incoming latency removed from the register operand which is used together
+// with a memory operand by the instruction.
+def : ReadAdvance<RegReadAdv, 4>;
+
+// LoadLatency (above) is not used for instructions in this file. This is
+// instead the role of LSULatency, which is the latency value added to the
+// result of loads and instructions with folded memory operands.
+def : WriteRes<LSULatency, []> { let Latency = 4; let NumMicroOps = 0; }
+
+let NumMicroOps = 0 in {
+  foreach L = 1-30 in
+    def : WriteRes<!cast<SchedWrite>("WLat"#L), []> { let Latency = L; }
+}
+
+// Execution units.
+def Z17_FXaUnit     : ProcResource<2>;
+def Z17_FXbUnit     : ProcResource<2>;
+def Z17_LSUnit      : ProcResource<2>;
+def Z17_VecUnit     : ProcResource<2>;
+def Z17_VecFPdUnit  : ProcResource<2> { let BufferSize = 1; /* blocking */ }
+def Z17_VBUnit      : ProcResource<2>;
+def Z17_MCD         : ProcResource<1>;
+
+// Subtarget specific definitions of scheduling resources.
+let NumMicroOps = 0 in {
+  def : WriteRes<FXa, [Z17_FXaUnit]>;
+  def : WriteRes<FXb, [Z17_FXbUnit]>;
+  def : WriteRes<LSU, [Z17_LSUnit]>;
+  def : WriteRes<VecBF,  [Z17_VecUnit]>;
+  def : WriteRes<VecDF,  [Z17_VecUnit]>;
+  def : WriteRes<VecDFX, [Z17_VecUnit]>;
+  def : WriteRes<VecMul,  [Z17_VecUnit]>;
+  def : WriteRes<VecStr,  [Z17_VecUnit]>;
+  def : WriteRes<VecXsPm, [Z17_VecUnit]>;
+  foreach Num = 2-5 in { let ReleaseAtCycles = [Num] in {
+    def : WriteRes<!cast<SchedWrite>("FXa"#Num), [Z17_FXaUnit]>;
+    def : WriteRes<!cast<SchedWrite>("FXb"#Num), [Z17_FXbUnit]>;
+    def : WriteRes<!cast<SchedWrite>("LSU"#Num), [Z17_LSUnit]>;
+    def : WriteRes<!cast<SchedWrite>("VecBF"#Num), [Z17_VecUnit]>;
+    def : WriteRes<!cast<SchedWrite>("VecDF"#Num), [Z17_VecUnit]>;
+    def : WriteRes<!cast<SchedWrite>("VecDFX"#Num), [Z17_VecUnit]>;
+    def : WriteRes<!cast<SchedWrite>("VecMul"#Num), [Z17_VecUnit]>;
+    def : WriteRes<!cast<SchedWrite>("VecStr"#Num), [Z17_VecUnit]>;
+    def : WriteRes<!cast<SchedWrite>("VecXsPm"#Num), [Z17_VecUnit]>;
+  }}
+
+  def : WriteRes<VecFPd,   [Z17_VecFPdUnit]> { let ReleaseAtCycles = [30]; }
+  def : WriteRes<VecFPd20, [Z17_VecFPdUnit]> { let ReleaseAtCycles = [20]; }
+
+  def : WriteRes<VBU,     [Z17_VBUnit]>; // Virtual Branching Unit
+}
+
+def : WriteRes<MCD, [Z17_MCD]> { let NumMicroOps = 3;
+                                 let BeginGroup  = 1;
+                                 let EndGroup    = 1; }
+
+// -------------------------- INSTRUCTIONS ---------------------------------- //
+
+// InstRW constructs have been used in order to preserve the
+// readability of the InstrInfo files.
+
+// For each instruction, as matched by a regexp, provide a list of
+// resources that it needs. These will be combined into a SchedClass.
+
+//===----------------------------------------------------------------------===//
+// Stack allocation
+//===----------------------------------------------------------------------===//
+
+// Pseudo -> LA / LAY
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "ADJDYNALLOC$")>;
+
+//===----------------------------------------------------------------------===//
+// Branch instructions
+//===----------------------------------------------------------------------===//
+
+// Branch
+def : InstRW<[WLat1, VBU, NormalGr], (instregex "(Call)?BRC(L)?(Asm.*)?$")>;
+def : InstRW<[WLat1, VBU, NormalGr], (instregex "(Call)?J(G)?(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "(Call)?BC(R)?(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "(Call)?B(R)?(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "BI(C)?(Asm.*)?$")>;
+def : InstRW<[WLat1, FXa, EndGroup], (instregex "BRCT(G)?$")>;
+def : InstRW<[WLat1, FXa, FXb, GroupAlone], (instregex "BRCTH$")>;
+def : InstRW<[WLat1, FXa, FXb, GroupAlone], (instregex "BCT(G)?(R)?$")>;
+def : InstRW<[WLat1, FXa2, FXb, GroupAlone], (instregex "B(R)?X(H|L).*$")>;
+
+// Compare and branch
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "C(L)?(G)?(I|R)J(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb2, GroupAlone],
+             (instregex "C(L)?(G)?(I|R)B(Call|Return|Asm.*)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Trap instructions
+//===----------------------------------------------------------------------===//
+
+// Trap
+def : InstRW<[WLat1, VBU, NormalGr], (instregex "(Cond)?Trap$")>;
+
+// Compare and trap
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "C(G)?(I|R)T(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CL(G)?RT(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CL(F|G)IT(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "CL(G)?T(Asm.*)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Call and return instructions
+//===----------------------------------------------------------------------===//
+
+// Call
+def : InstRW<[WLat1, VBU, FXa2, GroupAlone], (instregex "(Call)?BRAS$")>;
+def : InstRW<[WLat1, FXa2, FXb, GroupAlone], (instregex "(Call)?BRASL(_XPLINK64)?$")>;
+def : InstRW<[WLat1, FXa2, FXb, GroupAlone], (instregex "(Call)?BAS(R)?(_XPLINK64|_STACKEXT)?$")>;
+def : InstRW<[WLat1, FXa2, FXb, GroupAlone], (instregex "TLS_(G|L)DCALL$")>;
+
+// Return
+def : InstRW<[WLat1, FXb, EndGroup], (instregex "Return(_XPLINK)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CondReturn(_XPLINK)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Move instructions
+//===----------------------------------------------------------------------===//
+
+// Moves
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "MV(G|H)?HI$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "MVI(Y)?$")>;
+
+// Move character
+def : InstRW<[WLat1, FXb, LSU3, GroupAlone], (instregex "MVC$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "MVCL(E|U)?$")>;
+def : InstRW<[WLat1, LSU2, GroupAlone], (instregex "MVCRL$")>;
+
+// Pseudo -> reg move
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "COPY(_TO_REGCLASS)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "EXTRACT_SUBREG$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "INSERT_SUBREG$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "REG_SEQUENCE$")>;
+
+// Loads
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "L(Y|FH|RL|Mux)?$")>;
+def : InstRW<[LSULatency, LSULatency, LSU, NormalGr], (instregex "LCBB$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LG(RL)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "L128$")>;
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LLIH(F|H|L)$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LLIL(F|H|L)$")>;
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LG(F|H)I$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LHI(Mux)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LR$")>;
+
+// Load and zero rightmost byte
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LZR(F|G)$")>;
+
+// Load and trap
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "L(FH|G)?AT$")>;
+
+// Load and test
+def : InstRW<[WLat1LSU, WLat1LSU, LSU, FXa, NormalGr], (instregex "LT(G)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LT(G)?R$")>;
+
+// Stores
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "STG(RL)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "ST128$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "ST(Y|FH|RL|Mux)?$")>;
+
+// String moves.
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "MVST$")>;
+
+//===----------------------------------------------------------------------===//
+// Conditional move instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, FXa, NormalGr], (instregex "LOCRMux$")>;
+def : InstRW<[WLat2, FXa, NormalGr], (instregex "LOC(G|FH)?R(Asm.*)?$")>;
+def : InstRW<[WLat2, FXa, NormalGr], (instregex "LOC(G|H)?HI(Mux|(Asm.*))?$")>;
+def : InstRW<[WLat2LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "LOC(G|FH|Mux)?(Asm.*)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr],
+             (instregex "STOC(G|FH|Mux)?(Asm.*)?$")>;
+
+def : InstRW<[WLat2, FXa, NormalGr], (instregex "SELRMux$")>;
+def : InstRW<[WLat2, FXa, NormalGr], (instregex "SEL(G|FH)?R(Asm.*)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Sign extensions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "L(B|H|G)R$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LG(B|H|F)R$")>;
+
+def : InstRW<[WLat1LSU, WLat1LSU, FXa, LSU, NormalGr], (instregex "LTGF$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LTGFR$")>;
+
+def : InstRW<[WLat1LSU, FXa, LSU, NormalGr], (instregex "LB(H|Mux)?$")>;
+def : InstRW<[WLat1LSU, FXa, LSU, NormalGr], (instregex "LH(Y)?$")>;
+def : InstRW<[WLat1LSU, FXa, LSU, NormalGr], (instregex "LH(H|Mux|RL)$")>;
+def : InstRW<[WLat1LSU, FXa, LSU, NormalGr], (instregex "LG(B|H|F)$")>;
+def : InstRW<[WLat1LSU, FXa, LSU, NormalGr], (instregex "LG(H|F)RL$")>;
+
+//===----------------------------------------------------------------------===//
+// Zero extensions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LLCR(Mux)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LLHR(Mux)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LLG(C|H|F|T)R$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LLC(Mux)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LLH(Mux)?$")>;
+def : InstRW<[WLat1LSU, FXa, LSU, NormalGr], (instregex "LL(C|H)H$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LLHRL$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LLG(C|H|F|T|HRL|FRL)$")>;
+
+// Load and zero rightmost byte
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LLZRGF$")>;
+
+// Load and trap
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "LLG(F|T)?AT$")>;
+
+//===----------------------------------------------------------------------===//
+// Truncations
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "STC(H|Y|Mux)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "STH(H|Y|RL|Mux)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "STCM(H|Y)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Multi-register moves
+//===----------------------------------------------------------------------===//
+
+// Load multiple (estimated average of 5 ops)
+def : InstRW<[WLat10, WLat10, LSU5, GroupAlone], (instregex "LM(H|Y|G)?$")>;
+
+// Load multiple disjoint
+def : InstRW<[WLat30, WLat30, MCD], (instregex "LMD$")>;
+
+// Store multiple
+def : InstRW<[WLat1, LSU2, FXb3, GroupAlone], (instregex "STM(G|H|Y)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Byte swaps
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LRV(G)?R$")>;
+def : InstRW<[WLat1LSU, FXa, LSU, NormalGr], (instregex "LRV(G|H)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "STRV(G|H)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "MVCIN$")>;
+
+//===----------------------------------------------------------------------===//
+// Load address instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LA(Y|RL)?$")>;
+
+// Load the Global Offset Table address ( -> larl )
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "GOT$")>;
+
+// Load (logical) indexed address.
+def : InstRW<[WLat2, FXa2, NormalGr], (instregex "(L)?LXA(B|H|F|G|Q)$")>;
+
+//===----------------------------------------------------------------------===//
+// Absolute and Negation
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, WLat1, FXa, NormalGr], (instregex "LP(G)?R$")>;
+def : InstRW<[WLat2, WLat2, FXa2, Cracked], (instregex "L(N|P)GFR$")>;
+def : InstRW<[WLat1, WLat1, FXa, NormalGr], (instregex "LN(R|GR)$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "LC(R|GR)$")>;
+def : InstRW<[WLat2, WLat2, FXa2, Cracked], (instregex "LCGFR$")>;
+
+//===----------------------------------------------------------------------===//
+// Insertion
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1LSU, RegReadAdv, FXa, LSU, NormalGr], (instregex "IC(Y)?$")>;
+def : InstRW<[WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "IC32(Y)?$")>;
+def : InstRW<[WLat1LSU, RegReadAdv, WLat1LSU, FXa, LSU, NormalGr],
+             (instregex "ICM(H|Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "II(F|H|L)Mux$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "IIHF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "IIHH(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "IIHL(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "IILF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "IILH(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "IILL(64)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Addition
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "A(Y)?$")>;
+def : InstRW<[WLat2LSU, WLat2LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "AH(Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AIH$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AFI(Mux)?$")>;
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "AG$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AGFI$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AGHI(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AGR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AHI(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AHIMux(K)?$")>;
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "AL(Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AL(FI|HSIK)$")>;
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "ALG(F)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "ALGHSIK$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "ALGF(I|R)$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "ALGR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "ALR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "AR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "A(L)?HHHR$")>;
+def : InstRW<[WLat2, WLat2, FXa, NormalGr], (instregex "A(L)?HHLR$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "ALSIH(N)?$")>;
+def : InstRW<[WLat2LSU, FXb, LSU, NormalGr], (instregex "A(L)?(G)?SI$")>;
+
+// Logical addition with carry
+def : InstRW<[WLat2LSU, WLat2LSU, RegReadAdv, FXa, LSU, GroupAlone],
+             (instregex "ALC(G)?$")>;
+def : InstRW<[WLat2, WLat2, FXa, GroupAlone], (instregex "ALC(G)?R$")>;
+
+// Add with sign extension (16/32 -> 64)
+def : InstRW<[WLat2LSU, WLat2LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "AG(F|H)$")>;
+def : InstRW<[WLat2, WLat2, FXa, NormalGr], (instregex "AGFR$")>;
+
+//===----------------------------------------------------------------------===//
+// Subtraction
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "S(G|Y)?$")>;
+def : InstRW<[WLat2LSU, WLat2LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "SH(Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SGR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SLFI$")>;
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "SL(G|GF|Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SLGF(I|R)$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SLGR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SLR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "S(L)?HHHR$")>;
+def : InstRW<[WLat2, WLat2, FXa, NormalGr], (instregex "S(L)?HHLR$")>;
+
+// Subtraction with borrow
+def : InstRW<[WLat2LSU, WLat2LSU, RegReadAdv, FXa, LSU, GroupAlone],
+             (instregex "SLB(G)?$")>;
+def : InstRW<[WLat2, WLat2, FXa, GroupAlone], (instregex "SLB(G)?R$")>;
+
+// Subtraction with sign extension (16/32 -> 64)
+def : InstRW<[WLat2LSU, WLat2LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "SG(F|H)$")>;
+def : InstRW<[WLat2, WLat2, FXa, NormalGr], (instregex "SGFR$")>;
+
+//===----------------------------------------------------------------------===//
+// AND
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "N(G|Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NGR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NI(FMux|HMux|LMux)$")>;
+def : InstRW<[WLat2LSU, FXb, LSU, NormalGr], (instregex "NI(Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NIHF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NIHH(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NIHL(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NILF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NILH(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NILL(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NR(K)?$")>;
+def : InstRW<[WLat3LSU, LSU2, FXb, Cracked], (instregex "NC$")>;
+
+//===----------------------------------------------------------------------===//
+// OR
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "O(G|Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OGR(K)?$")>;
+def : InstRW<[WLat2LSU, FXb, LSU, NormalGr], (instregex "OI(Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OI(FMux|HMux|LMux)$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OIHF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OIHH(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OIHL(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OILF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OILH(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OILL(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OR(K)?$")>;
+def : InstRW<[WLat3LSU, LSU2, FXb, Cracked], (instregex "OC$")>;
+
+//===----------------------------------------------------------------------===//
+// XOR
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1LSU, WLat1LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "X(G|Y)?$")>;
+def : InstRW<[WLat2LSU, FXb, LSU, NormalGr], (instregex "XI(Y)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "XIFMux$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "XGR(K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "XIHF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "XILF(64)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "XR(K)?$")>;
+def : InstRW<[WLat3LSU, LSU2, FXb, Cracked], (instregex "XC$")>;
+
+//===----------------------------------------------------------------------===//
+// Combined logical operations
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NC(G)?RK$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "OC(G)?RK$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NN(G)?RK$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NO(G)?RK$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NOT(G)?R$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "NX(G)?RK$")>;
+
+//===----------------------------------------------------------------------===//
+// Multiplication
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat4LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "MS(GF|Y)?$")>;
+def : InstRW<[WLat4, FXa, NormalGr], (instregex "MS(R|FI)$")>;
+def : InstRW<[WLat4LSU, RegReadAdv, FXa, LSU, NormalGr], (instregex "MSG$")>;
+def : InstRW<[WLat4, FXa, NormalGr], (instregex "MSGR$")>;
+def : InstRW<[WLat4, FXa, NormalGr], (instregex "MSGF(I|R)$")>;
+def : InstRW<[WLat5LSU, RegReadAdv, FXa2, LSU, GroupAlone], (instregex "MLG$")>;
+def : InstRW<[WLat5, FXa2, GroupAlone], (instregex "MLGR$")>;
+def : InstRW<[WLat4, FXa, NormalGr], (instregex "MGHI$")>;
+def : InstRW<[WLat4, FXa, NormalGr], (instregex "MHI$")>;
+def : InstRW<[WLat4LSU, RegReadAdv, FXa, LSU, NormalGr], (instregex "MH(Y)?$")>;
+def : InstRW<[WLat5, FXa2, GroupAlone], (instregex "M(L)?R$")>;
+def : InstRW<[WLat5LSU, RegReadAdv, FXa2, LSU, GroupAlone],
+             (instregex "M(FY|L)?$")>;
+def : InstRW<[WLat8, RegReadAdv, FXa, LSU, NormalGr], (instregex "MGH$")>;
+def : InstRW<[WLat9, RegReadAdv, FXa2, LSU, GroupAlone], (instregex "MG$")>;
+def : InstRW<[WLat5, FXa2, GroupAlone], (instregex "MGRK$")>;
+def : InstRW<[WLat4LSU, WLat4LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "MSC$")>;
+def : InstRW<[WLat4LSU, WLat4LSU, RegReadAdv, FXa, LSU, NormalGr],
+             (instregex "MSGC$")>;
+def : InstRW<[WLat4, WLat4, FXa, NormalGr], (instregex "MSRKC$")>;
+def : InstRW<[WLat4, WLat4, FXa, NormalGr], (instregex "MSGRKC$")>;
+
+//===----------------------------------------------------------------------===//
+// Division and remainder
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat20, FXa4, GroupAlone], (instregex "DR$")>;
+def : InstRW<[WLat30, RegReadAdv, FXa4, LSU, GroupAlone2], (instregex "D$")>;
+def : InstRW<[WLat30, FXa2, GroupAlone], (instregex "DSG(F)?R$")>;
+def : InstRW<[WLat30, RegReadAdv, FXa2, LSU, GroupAlone2],
+             (instregex "DSG(F)?$")>;
+def : InstRW<[WLat20, FXa4, GroupAlone], (instregex "DLR$")>;
+def : InstRW<[WLat30, FXa4, GroupAlone], (instregex "DLGR$")>;
+def : InstRW<[WLat30, RegReadAdv, FXa4, LSU, GroupAlone2],
+             (instregex "DL(G)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Shifts
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SLL(G|K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SRL(G|K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SRA(G|K)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "SLA(G|K)?$")>;
+def : InstRW<[WLat5LSU, WLat5LSU, FXa4, LSU, GroupAlone2],
+             (instregex "S(L|R)D(A|L)$")>;
+
+// Rotate
+def : InstRW<[WLat2LSU, FXa, LSU, NormalGr], (instregex "RLL(G)?$")>;
+
+// Rotate and insert
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "RISBH(G|H|L)(Opt)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "RISBL(G|H|L)(Opt)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "RISBG(N|32)?(Z)?(Opt)?$")>;
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "RISBMux$")>;
+
+// Rotate and Select
+def : InstRW<[WLat2, WLat2, FXa2, Cracked], (instregex "R(N|O|X)SBG(Opt)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Comparison
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1LSU, RegReadAdv, FXb, LSU, NormalGr],
+             (instregex "C(G|Y|Mux)?$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CRL$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "C(F|H)I(Mux)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CG(F|H)I$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CG(HSI|RL)$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "C(G)?R$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CIH$")>;
+def : InstRW<[WLat1LSU, RegReadAdv, FXb, LSU, NormalGr], (instregex "CHF$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CHSI$")>;
+def : InstRW<[WLat1LSU, RegReadAdv, FXb, LSU, NormalGr],
+             (instregex "CL(Y|Mux)?$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CLFHSI$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CLFI(Mux)?$")>;
+def : InstRW<[WLat1LSU, RegReadAdv, FXb, LSU, NormalGr], (instregex "CLG$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CLG(HRL|HSI)$")>;
+def : InstRW<[WLat1LSU, RegReadAdv, FXb, LSU, NormalGr], (instregex "CLGF$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CLGFRL$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CLGF(I|R)$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CLGR$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CLGRL$")>;
+def : InstRW<[WLat1LSU, RegReadAdv, FXb, LSU, NormalGr], (instregex "CLHF$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CLH(RL|HSI)$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CLIH$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CLI(Y)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "CLR$")>;
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "CLRL$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "C(L)?HHR$")>;
+def : InstRW<[WLat2, FXb, NormalGr], (instregex "C(L)?HLR$")>;
+
+// Compare halfword
+def : InstRW<[WLat2LSU, RegReadAdv, FXb, LSU, NormalGr], (instregex "CH(Y)?$")>;
+def : InstRW<[WLat2LSU, FXb, LSU, NormalGr], (instregex "CHRL$")>;
+def : InstRW<[WLat2LSU, RegReadAdv, FXb, LSU, NormalGr], (instregex "CGH$")>;
+def : InstRW<[WLat2LSU, FXb, LSU, NormalGr], (instregex "CGHRL$")>;
+def : InstRW<[WLat2LSU, FXa, FXb, LSU, Cracked], (instregex "CHHSI$")>;
+
+// Compare with sign extension (32 -> 64)
+def : InstRW<[WLat2LSU, RegReadAdv, FXb, LSU, NormalGr], (instregex "CGF$")>;
+def : InstRW<[WLat2LSU, FXb, LSU, NormalGr], (instregex "CGFRL$")>;
+def : InstRW<[WLat2, FXb, NormalGr], (instregex "CGFR$")>;
+
+// Compare logical character
+def : InstRW<[WLat6, FXb, LSU2, Cracked], (instregex "CLC$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "CLCL(E|U)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "CLST$")>;
+
+// Test under mask
+def : InstRW<[WLat1LSU, FXb, LSU, NormalGr], (instregex "TM(Y)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "TM(H|L)Mux$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "TMHH(64)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "TMHL(64)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "TMLH(64)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "TMLL(64)?$")>;
+
+// Compare logical characters under mask
+def : InstRW<[WLat2LSU, RegReadAdv, FXb, LSU, NormalGr],
+             (instregex "CLM(H|Y)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Prefetch and execution hint
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, LSU, NormalGr], (instregex "PFD(RL)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "BPP$")>;
+def : InstRW<[FXb, EndGroup], (instregex "BPRP$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "NIAI$")>;
+
+//===----------------------------------------------------------------------===//
+// Atomic operations
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, EndGroup], (instregex "Serialize$")>;
+
+def : InstRW<[WLat2LSU, WLat2LSU, FXb, LSU, NormalGr], (instregex "LAA(G)?$")>;
+def : InstRW<[WLat2LSU, WLat2LSU, FXb, LSU, NormalGr], (instregex "LAAL(G)?$")>;
+def : InstRW<[WLat2LSU, WLat2LSU, FXb, LSU, NormalGr], (instregex "LAN(G)?$")>;
+def : InstRW<[WLat2LSU, WLat2LSU, FXb, LSU, NormalGr], (instregex "LAO(G)?$")>;
+def : InstRW<[WLat2LSU, WLat2LSU, FXb, LSU, NormalGr], (instregex "LAX(G)?$")>;
+
+// Test and set
+def : InstRW<[WLat2LSU, FXb, LSU, EndGroup], (instregex "TS$")>;
+
+// Compare and swap
+def : InstRW<[WLat3LSU, WLat3LSU, FXa, FXb, LSU, GroupAlone],
+             (instregex "CS(G|Y)?$")>;
+
+// Compare double and swap
+def : InstRW<[WLat6LSU, WLat6LSU, FXa3, FXb2, LSU, GroupAlone2],
+             (instregex "CDS(Y)?$")>;
+def : InstRW<[WLat15, WLat15, FXa2, FXb4, LSU3,
+              GroupAlone3], (instregex "CDSG$")>;
+
+// Compare and swap and store
+def : InstRW<[WLat30, MCD], (instregex "CSST$")>;
+
+// Perform locked operation
+def : InstRW<[WLat30, MCD], (instregex "PLO$")>;
+
+// Load/store pair from/to quadword
+def : InstRW<[WLat4LSU, LSU2, GroupAlone], (instregex "LPQ$")>;
+def : InstRW<[WLat1, FXb2, LSU, GroupAlone], (instregex "STPQ$")>;
+
+// Load pair disjoint
+def : InstRW<[WLat1LSU, WLat1LSU, LSU2, GroupAlone], (instregex "LPD(G)?$")>;
+
+// Compare and load
+def : InstRW<[WLat30, MCD], (instregex "CAL(G|GF)?$")>;
+
+// Perform functions with concurrent results
+def : InstRW<[WLat30, MCD], (instregex "PFCR$")>;
+
+//===----------------------------------------------------------------------===//
+// Translate and convert
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, LSU5, GroupAlone], (instregex "TR$")>;
+def : InstRW<[WLat30, WLat30, WLat30, FXa3, LSU2, GroupAlone2],
+             (instregex "TRT$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "TRTR$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "TRE$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "TRT(R)?E(Opt)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "TR(T|O)(T|O)(Opt)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD],
+             (instregex "CU(12|14|21|24|41|42)(Opt)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "(CUUTF|CUTFU)(Opt)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Message-security assist
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, WLat30, WLat30, WLat30, MCD],
+             (instregex "KM(C|F|O|CTR|A)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD],
+             (instregex "(KIMD|KLMD|KMAC|KDSA)(Opt)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD],
+             (instregex "(PCC|PPNO|PRNO)$")>;
+
+//===----------------------------------------------------------------------===//
+// Guarded storage
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LGG$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LLGFSG$")>;
+def : InstRW<[WLat30, MCD], (instregex "(L|ST)GSC$")>;
+
+//===----------------------------------------------------------------------===//
+// Decimal arithmetic
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat20, RegReadAdv, FXb, VecDF2, LSU2, GroupAlone2],
+             (instregex "CVBG$")>;
+def : InstRW<[WLat20, RegReadAdv, FXb, VecDF, LSU, GroupAlone2],
+             (instregex "CVB(Y)?$")>;
+def : InstRW<[WLat1, FXb3, VecDF4, LSU, GroupAlone3], (instregex "CVDG$")>;
+def : InstRW<[WLat1, FXb2, VecDF, LSU, GroupAlone2], (instregex "CVD(Y)?$")>;
+def : InstRW<[WLat1, LSU5, GroupAlone], (instregex "MV(N|O|Z)$")>;
+def : InstRW<[WLat1, LSU5, GroupAlone], (instregex "(PACK|PKA|PKU)$")>;
+def : InstRW<[WLat12, LSU5, GroupAlone], (instregex "UNPK(A|U)$")>;
+def : InstRW<[WLat1, FXb, LSU2, Cracked], (instregex "UNPK$")>;
+
+def : InstRW<[WLat5LSU, FXb, VecDFX, LSU3, GroupAlone2],
+             (instregex "(A|S|ZA)P$")>;
+def : InstRW<[WLat1, FXb, VecDFX2, LSU3, GroupAlone2], (instregex "MP$")>;
+def : InstRW<[WLat1, FXb, VecDFX4, LSU3, GroupAlone2], (instregex "DP$")>;
+def : InstRW<[WLat15, FXb, VecDFX2, LSU2, GroupAlone3], (instregex "SRP$")>;
+def : InstRW<[WLat8, VecDFX, LSU, LSU, GroupAlone], (instregex "CP$")>;
+def : InstRW<[WLat3LSU, VecDFX, LSU, Cracked], (instregex "TP$")>;
+def : InstRW<[WLat30, MCD], (instregex "ED(MK)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Access registers
+//===----------------------------------------------------------------------===//
+
+// Extract/set/copy access register
+def : InstRW<[WLat3, LSU, NormalGr], (instregex "(EAR|SAR|CPYA)$")>;
+
+// Load address extended
+def : InstRW<[WLat5, LSU, FXa, Cracked], (instregex "LAE(Y)?$")>;
+
+// Load/store access multiple (not modeled precisely)
+def : InstRW<[WLat20, WLat20, LSU5, GroupAlone], (instregex "LAM(Y)?$")>;
+def : InstRW<[WLat1, LSU5, FXb, GroupAlone2], (instregex "STAM(Y)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Program mask and addressing mode
+//===----------------------------------------------------------------------===//
+
+// Insert Program Mask
+def : InstRW<[WLat3, FXa, EndGroup], (instregex "IPM$")>;
+
+// Set Program Mask
+def : InstRW<[WLat3, LSU, EndGroup], (instregex "SPM$")>;
+
+// Branch and link
+def : InstRW<[WLat1, FXa2, FXb, GroupAlone], (instregex "BAL(R)?$")>;
+
+// Test addressing mode
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "TAM$")>;
+
+// Set addressing mode
+def : InstRW<[WLat1, FXb, EndGroup], (instregex "SAM(24|31|64)$")>;
+
+// Branch (and save) and set mode.
+def : InstRW<[WLat1, FXa, FXb, GroupAlone], (instregex "BSM$")>;
+def : InstRW<[WLat1, FXa2, FXb, GroupAlone], (instregex "BASSM$")>;
+
+//===----------------------------------------------------------------------===//
+// Transactional execution
+//===----------------------------------------------------------------------===//
+
+// Transaction begin
+def : InstRW<[WLat9, LSU2, FXb5, GroupAlone2], (instregex "TBEGIN(C)?$")>;
+
+// Transaction end
+def : InstRW<[WLat1, FXb, GroupAlone], (instregex "TEND$")>;
+
+// Transaction abort
+def : InstRW<[WLat30, MCD], (instregex "TABORT$")>;
+
+// Extract Transaction Nesting Depth
+def : InstRW<[WLat1, FXa, NormalGr], (instregex "ETND$")>;
+
+// Nontransactional store
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "NTSTG$")>;
+
+//===----------------------------------------------------------------------===//
+// Processor assist
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, GroupAlone], (instregex "PPA$")>;
+
+//===----------------------------------------------------------------------===//
+// Miscellaneous Instructions.
+//===----------------------------------------------------------------------===//
+
+// Count leading/trailing zeros.
+def : InstRW<[WLat3, FXa, NormalGr], (instregex "C(L|T)ZG$")>;
+
+// Find leftmost one
+def : InstRW<[WLat5, WLat5, FXa2, GroupAlone], (instregex "FLOGR$")>;
+
+// Population count
+def : InstRW<[WLat3, WLat3, FXa, NormalGr], (instregex "POPCNT(Opt)?$")>;
+
+// Bit deposit and bit extract.
+def : InstRW<[WLat4, FXa, NormalGr], (instregex "(BDEPG|BEXTG)$")>;
+
+// String instructions
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "SRST(U)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "CUSE$")>;
+
+// Various complex instructions
+def : InstRW<[WLat30, WLat30, WLat30, WLat30, MCD], (instregex "CFC$")>;
+def : InstRW<[WLat30, WLat30, WLat30, WLat30, WLat30, WLat30, MCD],
+             (instregex "UPT$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "CKSM$")>;
+def : InstRW<[WLat30, WLat30, WLat30, WLat30, MCD], (instregex "CMPSC$")>;
+def : InstRW<[WLat30, WLat30, WLat30, WLat30, MCD], (instregex "SORTL$")>;
+def : InstRW<[WLat30, WLat30, WLat30, WLat30, MCD], (instregex "DFLTCC$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "NNPA$")>;
+
+// Execute
+def : InstRW<[WLat1, FXb, GroupAlone], (instregex "EX(RL)?$")>;
+
+//===----------------------------------------------------------------------===//
+// .insn directive instructions
+//===----------------------------------------------------------------------===//
+
+// An "empty" sched-class will be assigned instead of the "invalid sched-class".
+// getNumDecoderSlots() will then return 1 instead of 0.
+def : InstRW<[], (instregex "Insn.*")>;
+
+
+// ----------------------------- Floating point ----------------------------- //
+
+//===----------------------------------------------------------------------===//
+// FP: Move instructions
+//===----------------------------------------------------------------------===//
+
+// Load zero
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "LZ(DR|ER)$")>;
+def : InstRW<[WLat2, FXb2, Cracked], (instregex "LZXR$")>;
+
+// Load
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "LER$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "LD(R|R32|GR)$")>;
+def : InstRW<[WLat3, FXb, NormalGr], (instregex "LGDR$")>;
+def : InstRW<[WLat2, FXb2, GroupAlone], (instregex "LXR$")>;
+
+// Load and Test
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "LT(E|D)BR$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "LTXBR$")>;
+
+// Copy sign
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "CPSDR(d|s)(d|s)$")>;
+
+//===----------------------------------------------------------------------===//
+// FP: Load instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2LSU, VecXsPm, LSU, NormalGr], (instregex "LE(Y)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LD(Y|E32)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LX$")>;
+
+//===----------------------------------------------------------------------===//
+// FP: Store instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "ST(E|D)(Y)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "STX$")>;
+
+//===----------------------------------------------------------------------===//
+// FP: Conversion instructions
+//===----------------------------------------------------------------------===//
+
+// Load rounded
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "LEDBR(A)?$")>;
+def : InstRW<[WLat9, VecDF2, NormalGr], (instregex "L(E|D)XBR(A)?$")>;
+
+// Load lengthened
+def : InstRW<[WLat6LSU, VecBF, LSU, NormalGr], (instregex "LDEB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "LDEBR$")>;
+def : InstRW<[WLat7LSU, VecBF4, LSU, GroupAlone], (instregex "LX(E|D)B$")>;
+def : InstRW<[WLat7, VecBF4, GroupAlone], (instregex "LX(E|D)BR$")>;
+
+// Convert from fixed / logical
+def : InstRW<[WLat7, FXb, VecBF, Cracked], (instregex "C(E|D)(F|G)BR(A)?$")>;
+def : InstRW<[WLat11, FXb, VecDF4, GroupAlone2], (instregex "CX(F|G)BR(A)?$")>;
+def : InstRW<[WLat7, FXb, VecBF, Cracked], (instregex "C(E|D)L(F|G)BR$")>;
+def : InstRW<[WLat11, FXb, VecDF4, GroupAlone2], (instregex "CXL(F|G)BR$")>;
+
+// Convert to fixed / logical
+def : InstRW<[WLat9, WLat9, FXb, VecBF, Cracked],
+             (instregex "C(F|G)(E|D)BR(A)?$")>;
+def : InstRW<[WLat12, WLat12, FXb, VecDF2, Cracked],
+             (instregex "C(F|G)XBR(A)?$")>;
+def : InstRW<[WLat9, WLat9, FXb, VecBF, GroupAlone], (instregex "CLFEBR$")>;
+def : InstRW<[WLat9, WLat9, FXb, VecBF, Cracked], (instregex "CLFDBR$")>;
+def : InstRW<[WLat9, WLat9, FXb, VecBF, Cracked], (instregex "CLG(E|D)BR$")>;
+def : InstRW<[WLat12, WLat12, FXb, VecDF2, Cracked], (instregex "CL(F|G)XBR$")>;
+
+//===----------------------------------------------------------------------===//
+// FP: Unary arithmetic
+//===----------------------------------------------------------------------===//
+
+// Load Complement / Negative / Positive
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "L(C|N|P)(E|D)BR$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "L(C|N|P)DFR(_32)?$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "L(C|N|P)XBR$")>;
+
+// Square root
+def : InstRW<[WLat30, VecFPd, LSU, NormalGr], (instregex "SQ(E|D)B$")>;
+def : InstRW<[WLat20, VecFPd20, NormalGr], (instregex "SQEBR$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "SQDBR$")>;
+def : InstRW<[WLat30, VecFPd, GroupAlone], (instregex "SQXBR$")>;
+
+// Load FP integer
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "FI(E|D)BR(A)?$")>;
+def : InstRW<[WLat10, VecDF4, GroupAlone], (instregex "FIXBR(A)?$")>;
+
+//===----------------------------------------------------------------------===//
+// FP: Binary arithmetic
+//===----------------------------------------------------------------------===//
+
+// Addition
+def : InstRW<[WLat6LSU, WLat6LSU, RegReadAdv, VecBF, LSU, NormalGr],
+             (instregex "A(E|D)B$")>;
+def : InstRW<[WLat6, WLat6, VecBF, NormalGr], (instregex "A(E|D)BR$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "AXBR$")>;
+
+// Subtraction
+def : InstRW<[WLat6LSU, WLat6LSU, RegReadAdv, VecBF, LSU, NormalGr],
+             (instregex "S(E|D)B$")>;
+def : InstRW<[WLat6, WLat6, VecBF, NormalGr], (instregex "S(E|D)BR$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "SXBR$")>;
+
+// Multiply
+def : InstRW<[WLat6LSU, RegReadAdv, VecBF, LSU, NormalGr],
+             (instregex "M(D|DE|EE)B$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "M(D|DE|EE)BR$")>;
+def : InstRW<[WLat7LSU, RegReadAdv, VecBF4, LSU, GroupAlone],
+             (instregex "MXDB$")>;
+def : InstRW<[WLat7, VecBF4, GroupAlone], (instregex "MXDBR$")>;
+def : InstRW<[WLat20, VecDF4, GroupAlone], (instregex "MXBR$")>;
+
+// Multiply and add / subtract
+def : InstRW<[WLat6LSU, RegReadAdv, RegReadAdv, VecBF2, LSU, GroupAlone],
+             (instregex "M(A|S)EB$")>;
+def : InstRW<[WLat6, VecBF, GroupAlone], (instregex "M(A|S)EBR$")>;
+def : InstRW<[WLat6LSU, RegReadAdv, RegReadAdv, VecBF2, LSU, GroupAlone],
+             (instregex "M(A|S)DB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "M(A|S)DBR$")>;
+
+// Division
+def : InstRW<[WLat20, RegReadAdv, VecFPd20, LSU, NormalGr], (instregex "DEB$")>;
+def : InstRW<[WLat30, RegReadAdv, VecFPd, LSU, NormalGr], (instregex "DDB$")>;
+def : InstRW<[WLat20, VecFPd20, NormalGr], (instregex "DEBR$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "DDBR$")>;
+def : InstRW<[WLat30, VecFPd, GroupAlone], (instregex "DXBR$")>;
+
+// Divide to integer
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "DI(E|D)BR$")>;
+
+//===----------------------------------------------------------------------===//
+// FP: Comparisons
+//===----------------------------------------------------------------------===//
+
+// Compare
+def : InstRW<[WLat3LSU, RegReadAdv, VecXsPm, LSU, NormalGr],
+             (instregex "(K|C)(E|D)B$")>;
+def : InstRW<[WLat3, VecXsPm, NormalGr], (instregex "(K|C)(E|D)BR$")>;
+def : InstRW<[WLat9, VecDF2, GroupAlone], (instregex "(K|C)XBR$")>;
+
+// Test Data Class
+def : InstRW<[WLat5, LSU, VecXsPm, NormalGr], (instregex "TC(E|D)B$")>;
+def : InstRW<[WLat10, LSU, VecDF4, GroupAlone], (instregex "TCXB$")>;
+
+//===----------------------------------------------------------------------===//
+// FP: Floating-point control register instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat4, FXa, LSU, GroupAlone], (instregex "EFPC$")>;
+def : InstRW<[WLat1, FXb, LSU, GroupAlone], (instregex "STFPC$")>;
+def : InstRW<[WLat3, LSU, GroupAlone], (instregex "SFPC$")>;
+def : InstRW<[WLat3LSU, LSU2, GroupAlone], (instregex "LFPC$")>;
+def : InstRW<[WLat30, MCD], (instregex "SFASR$")>;
+def : InstRW<[WLat30, MCD], (instregex "LFAS$")>;
+def : InstRW<[WLat3, FXb, GroupAlone], (instregex "SRNM(B|T)?$")>;
+
+
+// --------------------- Hexadecimal floating point ------------------------- //
+
+//===----------------------------------------------------------------------===//
+// HFP: Move instructions
+//===----------------------------------------------------------------------===//
+
+// Load and Test
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "LT(E|D)R$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "LTXR$")>;
+
+//===----------------------------------------------------------------------===//
+// HFP: Conversion instructions
+//===----------------------------------------------------------------------===//
+
+// Load rounded
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "(LEDR|LRER)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "LEXR$")>;
+def : InstRW<[WLat9, VecDF2, NormalGr], (instregex "(LDXR|LRDR)$")>;
+
+// Load lengthened
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "LDE$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "LDER$")>;
+def : InstRW<[WLat7LSU, VecBF4, LSU, GroupAlone], (instregex "LX(E|D)$")>;
+def : InstRW<[WLat7, VecBF4, GroupAlone], (instregex "LX(E|D)R$")>;
+
+// Convert from fixed
+def : InstRW<[WLat7, FXb, VecBF, Cracked], (instregex "C(E|D)(F|G)R$")>;
+def : InstRW<[WLat11, FXb, VecDF4, GroupAlone2], (instregex "CX(F|G)R$")>;
+
+// Convert to fixed
+def : InstRW<[WLat9, WLat9, FXb, VecBF, Cracked], (instregex "C(F|G)(E|D)R$")>;
+def : InstRW<[WLat12, WLat12, FXb, VecDF2, Cracked], (instregex "C(F|G)XR$")>;
+
+// Convert BFP to HFP / HFP to BFP.
+def : InstRW<[WLat6, WLat6, VecBF, NormalGr], (instregex "THD(E)?R$")>;
+def : InstRW<[WLat6, WLat6, VecBF, NormalGr], (instregex "TB(E)?DR$")>;
+
+//===----------------------------------------------------------------------===//
+// HFP: Unary arithmetic
+//===----------------------------------------------------------------------===//
+
+// Load Complement / Negative / Positive
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "L(C|N|P)(E|D)R$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "L(C|N|P)XR$")>;
+
+// Halve
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "H(E|D)R$")>;
+
+// Square root
+def : InstRW<[WLat30, VecFPd, LSU, NormalGr], (instregex "SQ(E|D)$")>;
+def : InstRW<[WLat20, VecFPd20, NormalGr], (instregex "SQER$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "SQDR$")>;
+def : InstRW<[WLat30, VecFPd, GroupAlone], (instregex "SQXR$")>;
+
+// Load FP integer
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "FI(E|D)R$")>;
+def : InstRW<[WLat10, VecDF4, GroupAlone], (instregex "FIXR$")>;
+
+//===----------------------------------------------------------------------===//
+// HFP: Binary arithmetic
+//===----------------------------------------------------------------------===//
+
+// Addition
+def : InstRW<[WLat6LSU, WLat6LSU, RegReadAdv, VecBF, LSU, NormalGr],
+             (instregex "A(E|D|U|W)$")>;
+def : InstRW<[WLat6, WLat6, VecBF, NormalGr], (instregex "A(E|D|U|W)R$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "AXR$")>;
+
+// Subtraction
+def : InstRW<[WLat6LSU, WLat6LSU, RegReadAdv, VecBF, LSU, NormalGr],
+             (instregex "S(E|D|U|W)$")>;
+def : InstRW<[WLat6, WLat6, VecBF, NormalGr], (instregex "S(E|D|U|W)R$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "SXR$")>;
+
+// Multiply
+def : InstRW<[WLat6LSU, RegReadAdv, VecBF, LSU, NormalGr],
+             (instregex "M(D|DE|E|EE)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "M(D|DE|E|EE)R$")>;
+def : InstRW<[WLat7LSU, RegReadAdv, VecBF4, LSU, GroupAlone],
+             (instregex "MXD$")>;
+def : InstRW<[WLat7, VecBF4, GroupAlone], (instregex "MXDR$")>;
+def : InstRW<[WLat20, VecDF4, GroupAlone], (instregex "MXR$")>;
+def : InstRW<[WLat7LSU, RegReadAdv, VecBF4, LSU, GroupAlone], (instregex "MY$")>;
+def : InstRW<[WLat6LSU, RegReadAdv, VecBF2, LSU, GroupAlone],
+             (instregex "MY(H|L)$")>;
+def : InstRW<[WLat7, VecBF4, GroupAlone], (instregex "MYR$")>;
+def : InstRW<[WLat6, VecBF, GroupAlone], (instregex "MY(H|L)R$")>;
+
+// Multiply and add / subtract
+def : InstRW<[WLat6LSU, RegReadAdv, RegReadAdv, VecBF2, LSU, GroupAlone],
+             (instregex "M(A|S)(E|D)$")>;
+def : InstRW<[WLat6, VecBF, GroupAlone], (instregex "M(A|S)(E|D)R$")>;
+def : InstRW<[WLat7LSU, RegReadAdv, RegReadAdv, VecBF4, LSU, GroupAlone],
+             (instregex "MAY$")>;
+def : InstRW<[WLat6LSU, RegReadAdv, RegReadAdv, VecBF2, LSU, GroupAlone],
+             (instregex "MAY(H|L)$")>;
+def : InstRW<[WLat7, VecBF4, GroupAlone], (instregex "MAYR$")>;
+def : InstRW<[WLat6, VecBF, GroupAlone], (instregex "MAY(H|L)R$")>;
+
+// Division
+def : InstRW<[WLat20, RegReadAdv, VecFPd20, LSU, NormalGr], (instregex "DE$")>;
+def : InstRW<[WLat30, RegReadAdv, VecFPd, LSU, NormalGr], (instregex "DD$")>;
+def : InstRW<[WLat20, VecFPd20, NormalGr], (instregex "DER$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "DDR$")>;
+def : InstRW<[WLat30, VecFPd, GroupAlone], (instregex "DXR$")>;
+
+//===----------------------------------------------------------------------===//
+// HFP: Comparisons
+//===----------------------------------------------------------------------===//
+
+// Compare
+def : InstRW<[WLat6LSU, RegReadAdv, VecBF, LSU, NormalGr],
+             (instregex "C(E|D)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "C(E|D)R$")>;
+def : InstRW<[WLat10, VecDF2, GroupAlone], (instregex "CXR$")>;
+
+
+// ------------------------ Decimal floating point -------------------------- //
+
+//===----------------------------------------------------------------------===//
+// DFP: Move instructions
+//===----------------------------------------------------------------------===//
+
+// Load and Test
+def : InstRW<[WLat8, WLat8, VecDF, NormalGr], (instregex "LTDTR$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "LTXTR$")>;
+
+//===----------------------------------------------------------------------===//
+// DFP: Conversion instructions
+//===----------------------------------------------------------------------===//
+
+// Load rounded
+def : InstRW<[WLat15, VecDF, NormalGr], (instregex "LEDTR$")>;
+def : InstRW<[WLat15, VecDF2, NormalGr], (instregex "LDXTR$")>;
+
+// Load lengthened
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "LDETR$")>;
+def : InstRW<[WLat10, VecDF4, GroupAlone], (instregex "LXDTR$")>;
+
+// Convert from fixed / logical
+def : InstRW<[WLat15, FXb, VecDF, Cracked], (instregex "CDFTR(A)?$")>;
+def : InstRW<[WLat20, FXb, VecDF, Cracked], (instregex "CDGTR(A)?$")>;
+def : InstRW<[WLat15, FXb, VecDF4, GroupAlone2], (instregex "CXFTR(A)?$")>;
+def : InstRW<[WLat20, FXb, VecDF4, GroupAlone2], (instregex "CXGTR(A)?$")>;
+def : InstRW<[WLat15, FXb, VecDF, Cracked], (instregex "CDLFTR$")>;
+def : InstRW<[WLat20, FXb, VecDF, Cracked], (instregex "CDLGTR$")>;
+def : InstRW<[WLat15, FXb, VecDF4, GroupAlone2], (instregex "CXLFTR$")>;
+def : InstRW<[WLat20, FXb, VecDF4, GroupAlone2], (instregex "CXLGTR$")>;
+
+// Convert to fixed / logical
+def : InstRW<[WLat20, WLat20, FXb, VecDF, Cracked],
+             (instregex "C(F|G)DTR(A)?$")>;
+def : InstRW<[WLat20, WLat20, FXb, VecDF2, Cracked],
+             (instregex "C(F|G)XTR(A)?$")>;
+def : InstRW<[WLat20, WLat20, FXb, VecDF, Cracked], (instregex "CL(F|G)DTR$")>;
+def : InstRW<[WLat20, WLat20, FXb, VecDF2, Cracked], (instregex "CL(F|G)XTR$")>;
+
+// Convert from / to signed / unsigned packed
+def : InstRW<[WLat9, FXb, VecDF, Cracked], (instregex "CD(S|U)TR$")>;
+def : InstRW<[WLat12, FXb2, VecDF4, GroupAlone2], (instregex "CX(S|U)TR$")>;
+def : InstRW<[WLat11, FXb, VecDF, Cracked], (instregex "C(S|U)DTR$")>;
+def : InstRW<[WLat15, FXb2, VecDF4, GroupAlone2], (instregex "C(S|U)XTR$")>;
+
+// Convert from / to zoned
+def : InstRW<[WLat8LSU, LSU, VecDF, Cracked], (instregex "CDZT$")>;
+def : InstRW<[WLat16LSU, LSU2, VecDF4, GroupAlone3], (instregex "CXZT$")>;
+def : InstRW<[WLat1, FXb, LSU, VecDF, Cracked], (instregex "CZDT$")>;
+def : InstRW<[WLat1, FXb, LSU, VecDF2, GroupAlone], (instregex "CZXT$")>;
+
+// Convert from / to packed
+def : InstRW<[WLat8LSU, LSU, VecDF, Cracked], (instregex "CDPT$")>;
+def : InstRW<[WLat16LSU, LSU2, VecDF4, GroupAlone3], (instregex "CXPT$")>;
+def : InstRW<[WLat1, FXb, LSU, VecDF, Cracked], (instregex "CPDT$")>;
+def : InstRW<[WLat1, FXb, LSU, VecDF2, GroupAlone], (instregex "CPXT$")>;
+
+// Perform floating-point operation
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "PFPO$")>;
+
+//===----------------------------------------------------------------------===//
+// DFP: Unary arithmetic
+//===----------------------------------------------------------------------===//
+
+// Load FP integer
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "FIDTR$")>;
+def : InstRW<[WLat10, VecDF4, GroupAlone], (instregex "FIXTR$")>;
+
+// Extract biased exponent
+def : InstRW<[WLat11, FXb, VecDF, Cracked], (instregex "EEDTR$")>;
+def : InstRW<[WLat11, FXb, VecDF, Cracked], (instregex "EEXTR$")>;
+
+// Extract significance
+def : InstRW<[WLat11, FXb, VecDF, Cracked], (instregex "ESDTR$")>;
+def : InstRW<[WLat12, FXb, VecDF2, Cracked], (instregex "ESXTR$")>;
+
+//===----------------------------------------------------------------------===//
+// DFP: Binary arithmetic
+//===----------------------------------------------------------------------===//
+
+// Addition
+def : InstRW<[WLat8, WLat8, VecDF, NormalGr], (instregex "ADTR(A)?$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "AXTR(A)?$")>;
+
+// Subtraction
+def : InstRW<[WLat8, WLat8, VecDF, NormalGr], (instregex "SDTR(A)?$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "SXTR(A)?$")>;
+
+// Multiply
+def : InstRW<[WLat20, VecDF, NormalGr], (instregex "MDTR(A)?$")>;
+def : InstRW<[WLat30, VecDF4, GroupAlone], (instregex "MXTR(A)?$")>;
+
+// Division
+def : InstRW<[WLat30, VecDF, NormalGr], (instregex "DDTR(A)?$")>;
+def : InstRW<[WLat30, VecDF4, GroupAlone], (instregex "DXTR(A)?$")>;
+
+// Quantize
+def : InstRW<[WLat8, WLat8, VecDF, NormalGr], (instregex "QADTR$")>;
+def : InstRW<[WLat10, WLat10, VecDF4, GroupAlone], (instregex "QAXTR$")>;
+
+// Reround
+def : InstRW<[WLat9, WLat9, FXb, VecDF, Cracked], (instregex "RRDTR$")>;
+def : InstRW<[WLat11, WLat11, FXb, VecDF4, GroupAlone2], (instregex "RRXTR$")>;
+
+// Shift significand left/right
+def : InstRW<[WLat11LSU, LSU, VecDF, GroupAlone], (instregex "S(L|R)DT$")>;
+def : InstRW<[WLat11LSU, LSU, VecDF4, GroupAlone], (instregex "S(L|R)XT$")>;
+
+// Insert biased exponent
+def : InstRW<[WLat9, FXb, VecDF, Cracked], (instregex "IEDTR$")>;
+def : InstRW<[WLat11, FXb, VecDF4, GroupAlone2], (instregex "IEXTR$")>;
+
+//===----------------------------------------------------------------------===//
+// DFP: Comparisons
+//===----------------------------------------------------------------------===//
+
+// Compare
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "(K|C)DTR$")>;
+def : InstRW<[WLat9, VecDF2, GroupAlone], (instregex "(K|C)XTR$")>;
+
+// Compare biased exponent
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "CEDTR$")>;
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "CEXTR$")>;
+
+// Test Data Class/Group
+def : InstRW<[WLat15, LSU, VecDF, NormalGr], (instregex "TD(C|G)(E|D)T$")>;
+def : InstRW<[WLat15, LSU, VecDF2, GroupAlone], (instregex "TD(C|G)XT$")>;
+
+
+// --------------------------------- Vector --------------------------------- //
+
+//===----------------------------------------------------------------------===//
+// Vector: Move instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "VLR(32|64)?$")>;
+def : InstRW<[WLat3, FXb, NormalGr], (instregex "VLGV(B|F|G|H)?$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "VLVG(B|F|G|H)?$")>;
+def : InstRW<[WLat3, FXb, NormalGr], (instregex "VLVGP(32)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Immediate instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VZERO$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VONE$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VGBM$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VGM(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VREPI(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VLEI(B|F|G|H)$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Loads
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VL(Align)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VL(L|BB)$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VL(32|64)$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VLLEZ(B|F|G|H|LF)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VLREP(B|F|G|H)?$")>;
+def : InstRW<[WLat2LSU, RegReadAdv, VecXsPm, LSU, NormalGr],
+             (instregex "VLE(B|F|G|H)$")>;
+def : InstRW<[WLat5LSU, RegReadAdv, FXb, LSU, VecXsPm, Cracked],
+             (instregex "VGE(F|G)$")>;
+def : InstRW<[WLat4LSU, WLat4LSU, LSU5, GroupAlone],
+             (instregex "VLM(Align)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VLRL(R)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Stores
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "VST(Align|L|32|64)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "VSTE(F|G)$")>;
+def : InstRW<[WLat1, FXb, LSU, VecXsPm, Cracked], (instregex "VSTE(B|H)$")>;
+def : InstRW<[WLat1, LSU2, FXb3, GroupAlone2], (instregex "VSTM(Align)?$")>;
+def : InstRW<[WLat1, FXb2, LSU, Cracked], (instregex "VSCE(F|G)$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "VSTRL(R)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Byte swaps
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VLBR(H|F|G|Q)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VLER(H|F|G)?$")>;
+def : InstRW<[WLat2LSU, RegReadAdv, VecXsPm, LSU, NormalGr],
+             (instregex "VLEBR(H|F|G)$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VLLEBRZ(H|F|G|E)?$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "VLBRREP(H|F|G)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "VSTBR(H|F|G|Q)?$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "VSTER(H|F|G)?$")>;
+def : InstRW<[WLat1, FXb, LSU, VecXsPm, Cracked], (instregex "VSTEBRH$")>;
+def : InstRW<[WLat1, FXb, LSU, NormalGr], (instregex "VSTEBR(F|G)$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Selects and permutes
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VMRH(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VMRL(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VPERM$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VPDI$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VBPERM$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VREP(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSEL$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VBLEND(B|F|G|H|Q)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Widening and narrowing
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VPK(F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VPKS(F|G|H)?$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VPKS(F|G|H)S$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VPKLS(F|G|H)?$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VPKLS(F|G|H)S$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSEG(B|F|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VGEM(B|H|F|G|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VUPH(B|F|H|G)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VUPL(B|F|G)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VUPLH(B|F|H|G|W)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VUPLL(B|F|H|G)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Integer arithmetic
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VA(B|F|G|H|Q|C|CQ)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VACC(B|F|G|H|Q|C|CQ)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VAVG(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VAVGL(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VN(C|O|N|X)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VO(C)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VCKSM$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VCLZ(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VCTZ(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "VD(L)?(F|G|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VEVAL$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VX$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VGFM?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VGFMA(B|F|G|H)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VGFM(B|F|G|H)$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VLC(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VLP(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VMX(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VMXL(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VMN(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VMNL(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMAL(B|F|G|Q)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMALE(B|F|H|G)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMALH(B|F|H|G|Q|W)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMALO(B|F|H|G)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMAO(B|F|H|G)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMAE(B|F|H|G)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMAH(B|F|H|G|Q)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VME(B|F|H|G)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMH(B|F|H|G|Q)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VML(B|F|G|Q)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMLE(B|F|H|G)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMLH(B|F|H|G|Q|W)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMLO(B|F|H|G)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VMO(B|F|H|G)?$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VMSL(G)?$")>;
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VPOPCT(B|F|G|H)?$")>;
+
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "VR(L)?(F|G|Q)?$")>;
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VERLL(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VERLLV(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VERIM(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VESL(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VESLV(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VESRA(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VESRAV(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VESRL(B|F|G|H)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VESRLV(B|F|G|H)?$")>;
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSL(DB)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSLB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSR(A|L)$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSR(A|L)B$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSLD$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSRD$")>;
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSB(I|IQ|CBI|CBIQ)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VSCBI(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VS(F|G|H|Q)?$")>;
+
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VSUM(B|H)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VSUMG(F|H)?$")>;
+def : InstRW<[WLat4, VecMul, NormalGr], (instregex "VSUMQ(F|G)?$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Integer comparison
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat3, VecXsPm, NormalGr], (instregex "VEC(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat3, VecXsPm, NormalGr], (instregex "VECL(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VCEQ(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VCEQ(B|F|G|H|Q)S$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VCH(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VCH(B|F|G|H|Q)S$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VCHL(B|F|G|H|Q)?$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VCHL(B|F|G|H|Q)S$")>;
+def : InstRW<[WLat4, VecStr, NormalGr], (instregex "VTM$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Floating-point arithmetic
+//===----------------------------------------------------------------------===//
+
+// Conversion and rounding
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VCFP(S|L)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VCD(L)?G$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VCD(L)?GB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WCD(L)?GB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VCE(L)?FB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WCE(L)?FB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VC(S|L)FP$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VC(L)?GD$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VC(L)?GDB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WC(L)?GDB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VC(L)?FEB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WC(L)?FEB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VL(DE|ED)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VL(DE|ED)B$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WL(DE|ED)B$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VFL(L|R)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VFL(LS|RD)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WFL(LS|RD)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WFLLD$")>;
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "WFLRX$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VFI(DB)?$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WFIDB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VFISB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WFISB$")>;
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "WFIXB$")>;
+
+// Sign operations
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VFPSO$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "(V|W)FPSODB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "(V|W)FPSOSB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WFPSOXB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "(V|W)FL(C|N|P)DB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "(V|W)FL(C|N|P)SB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WFL(C|N|P)XB$")>;
+
+// Minimum / maximum
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VF(MAX|MIN)$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VF(MAX|MIN)DB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WF(MAX|MIN)DB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VF(MAX|MIN)SB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WF(MAX|MIN)SB$")>;
+def : InstRW<[WLat2, VecDFX, NormalGr], (instregex "WF(MAX|MIN)XB$")>;
+
+// Test data class
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VFTCI$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "(V|W)FTCIDB$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "(V|W)FTCISB$")>;
+def : InstRW<[WLat3, WLat3, VecDFX, NormalGr], (instregex "WFTCIXB$")>;
+
+// Add / subtract
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VF(A|S)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VF(A|S)DB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WF(A|S)DB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VF(A|S)SB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WF(A|S)SB$")>;
+def : InstRW<[WLat8, VecDF, NormalGr], (instregex "WF(A|S)XB$")>;
+
+// Multiply / multiply-and-add/subtract
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VFM(DB)?$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WFM(D|S)B$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VFMSB$")>;
+def : InstRW<[WLat20, VecDF, NormalGr], (instregex "WFMXB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VF(N)?M(A|S)$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VF(N)?M(A|S)DB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WF(N)?M(A|S)DB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "VF(N)?M(A|S)SB$")>;
+def : InstRW<[WLat6, VecBF, NormalGr], (instregex "WF(N)?M(A|S)SB$")>;
+def : InstRW<[WLat20, VecDF, NormalGr], (instregex "WF(N)?M(A|S)XB$")>;
+
+// Divide / square root
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "VFD$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "(V|W)FDDB$")>;
+def : InstRW<[WLat20, VecFPd20, NormalGr], (instregex "WFDSB$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "VFDSB$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "WFDXB$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "VFSQ$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "(V|W)FSQDB$")>;
+def : InstRW<[WLat20, VecFPd20, NormalGr], (instregex "WFSQSB$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "VFSQSB$")>;
+def : InstRW<[WLat30, VecFPd, NormalGr], (instregex "WFSQXB$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Floating-point comparison
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VF(C|K)(E|H|HE)$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VF(C|K)(E|H|HE)DB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WFC(E|H|HE)DB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WFK(E|H|HE)DB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VF(C|K)(E|H|HE)SB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WFC(E|H|HE)SB$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "WFK(E|H|HE)SB$")>;
+def : InstRW<[WLat2, VecDFX, NormalGr], (instregex "WFC(E|H|HE)XB$")>;
+def : InstRW<[WLat2, VecDFX, NormalGr], (instregex "WFK(E|H|HE)XB$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VFC(E|H|HE)DBS$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "VFK(E|H|HE)DBS$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr],
+             (instregex "WF(C|K)(E|H|HE)DBS$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr],
+             (instregex "VF(C|K)(E|H|HE)SBS$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "WFC(E|H|HE)SBS$")>;
+def : InstRW<[WLat3, WLat3, VecXsPm, NormalGr], (instregex "WFK(E|H|HE)SBS$")>;
+def : InstRW<[WLat3, WLat3, VecDFX, NormalGr], (instregex "WFC(E|H|HE)XBS$")>;
+def : InstRW<[WLat3, WLat3, VecDFX, NormalGr], (instregex "WFK(E|H|HE)XBS$")>;
+def : InstRW<[WLat3, VecXsPm, NormalGr], (instregex "WF(C|K)$")>;
+def : InstRW<[WLat3, VecXsPm, NormalGr], (instregex "WF(C|K)DB$")>;
+def : InstRW<[WLat3, VecXsPm, NormalGr], (instregex "WF(C|K)SB$")>;
+def : InstRW<[WLat3, VecDFX, NormalGr], (instregex "WF(C|K)XB$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Floating-point insertion and extraction
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "LEFR$")>;
+def : InstRW<[WLat3, FXb, NormalGr], (instregex "LFER$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: String instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VFAE(B)?$")>;
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VFAE(F|H)$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr], (instregex "VFAE(B|F|H)S$")>;
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VFAEZ(B|F|H)$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr], (instregex "VFAEZ(B|F|H)S$")>;
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VFEE(B|F|H|ZB|ZF|ZH)?$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr],
+             (instregex "VFEE(B|F|H|ZB|ZF|ZH)S$")>;
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VFENE(B|F|H|ZB|ZF|ZH)?$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr],
+             (instregex "VFENE(B|F|H|ZB|ZF|ZH)S$")>;
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VISTR(B|F|H)?$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr], (instregex "VISTR(B|F|H)S$")>;
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VSTRC(B|F|H)?$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr], (instregex "VSTRC(B|F|H)S$")>;
+def : InstRW<[WLat3, VecStr, NormalGr], (instregex "VSTRCZ(B|F|H)$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr], (instregex "VSTRCZ(B|F|H)S$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr], (instregex "VSTRS(B|F|H)?$")>;
+def : InstRW<[WLat4, WLat4, VecStr, NormalGr], (instregex "VSTRSZ(B|F|H)$")>;
+
+//===----------------------------------------------------------------------===//
+// NNP assist instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VCFN$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VCLFN(L|H)$")>;
+def : InstRW<[WLat2, VecXsPm, NormalGr], (instregex "VC(R)?NF$")>;
+
+//===----------------------------------------------------------------------===//
+// Vector: Packed-decimal instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat2, VecDFX, NormalGr], (instregex "VLIP$")>;
+def : InstRW<[WLat6, VecDFX, LSU, GroupAlone2], (instregex "VPKZ$")>;
+def : InstRW<[WLat1, VecDFX, FXb, LSU2, GroupAlone2], (instregex "VUPKZ$")>;
+def : InstRW<[WLat20, WLat20, VecDF, FXb, GroupAlone],
+             (instregex "VCVB(G|Q)?(Opt)?$")>;
+def : InstRW<[WLat15, WLat15, VecDF, FXb, GroupAlone],
+             (instregex "VCVD(G|Q)?$")>;
+def : InstRW<[WLat4, WLat4, VecDFX, NormalGr], (instregex "V(A|S)P$")>;
+def : InstRW<[WLat30, WLat30, VecDF, GroupAlone], (instregex "VM(S)?P$")>;
+def : InstRW<[WLat30, WLat30, VecDF, GroupAlone], (instregex "V(D|R)P$")>;
+def : InstRW<[WLat30, WLat30, VecDF, GroupAlone], (instregex "VSDP$")>;
+def : InstRW<[WLat8, WLat8, VecDF, NormalGr], (instregex "VSRP(R)?$")>;
+def : InstRW<[WLat4, WLat4, VecDFX, NormalGr], (instregex "VPSOP$")>;
+def : InstRW<[WLat2, VecDFX, NormalGr], (instregex "V(T|C)(P|Z)(Opt)?$")>;
+
+def : InstRW<[WLat20, VecDF, NormalGr], (instregex "VSCH(S|D|X)?P$")>;
+def : InstRW<[WLat30, VecDF, NormalGr], (instregex "VSCSHP$")>;
+def : InstRW<[WLat30, VecDF, NormalGr], (instregex "VCSPH$")>;
+def : InstRW<[WLat2, WLat2, VecXsPm, NormalGr], (instregex "VCLZDP$")>;
+def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VPKZR$")>;
+def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VUPKZH$")>;
+def : InstRW<[WLat2, WLat2, VecDFX, NormalGr], (instregex "VUPKZL$")>;
+
+// -------------------------------- System ---------------------------------- //
+
+//===----------------------------------------------------------------------===//
+// System: Program-Status Word Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, WLat30, MCD], (instregex "EPSW$")>;
+def : InstRW<[WLat20, GroupAlone3], (instregex "LPSW(E)?(Y)?$")>;
+def : InstRW<[WLat3, FXa, GroupAlone], (instregex "IPK$")>;
+def : InstRW<[WLat1, LSU, EndGroup], (instregex "SPKA$")>;
+def : InstRW<[WLat1, LSU, EndGroup], (instregex "SSM$")>;
+def : InstRW<[WLat1, FXb, LSU, GroupAlone], (instregex "ST(N|O)SM$")>;
+def : InstRW<[WLat3, FXa, NormalGr], (instregex "IAC$")>;
+def : InstRW<[WLat1, LSU, EndGroup], (instregex "SAC(F)?$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Control Register Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat4LSU, WLat4LSU, LSU2, GroupAlone], (instregex "LCTL(G)?$")>;
+def : InstRW<[WLat1, LSU5, FXb, GroupAlone2], (instregex "STCT(L|G)$")>;
+def : InstRW<[LSULatency, LSU, NormalGr], (instregex "E(P|S)A(I)?R$")>;
+def : InstRW<[WLat30, MCD], (instregex "SSA(I)?R$")>;
+def : InstRW<[WLat30, MCD], (instregex "ESEA$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Prefix-Register Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "S(T)?PX$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Breaking-Event-Address-Register Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat3LSU, LSU2, GroupAlone], (instregex "LBEAR$")>;
+def : InstRW<[WLat1, LSU2, FXb, GroupAlone], (instregex "STBEAR$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Storage-Key and Real Memory Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "ISKE$")>;
+def : InstRW<[WLat30, MCD], (instregex "IVSK$")>;
+def : InstRW<[WLat30, MCD], (instregex "SSKE(Opt)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "RRB(E|M)$")>;
+def : InstRW<[WLat30, MCD], (instregex "IRBM$")>;
+def : InstRW<[WLat30, MCD], (instregex "PFMF$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "TB$")>;
+def : InstRW<[WLat30, MCD], (instregex "PGIN$")>;
+def : InstRW<[WLat30, MCD], (instregex "PGOUT$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Dynamic-Address-Translation Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "IPTE(Opt)?(Opt)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "IDTE(Opt)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "RDP(Opt)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "CRDTE(Opt)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "PTLB$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "CSP(G)?$")>;
+def : InstRW<[WLat30, WLat30, WLat30, MCD], (instregex "LPTEA$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "LRA(Y|G)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "STRAG$")>;
+def : InstRW<[WLat30, MCD], (instregex "LURA(G)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "STUR(A|G)$")>;
+def : InstRW<[WLat30, MCD], (instregex "TPROT$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Memory-move Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat4LSU, FXa2, FXb, LSU5, GroupAlone2], (instregex "MVC(K|P|S)$")>;
+def : InstRW<[WLat1, FXa, LSU5, GroupAlone2], (instregex "MVC(S|D)K$")>;
+def : InstRW<[WLat30, MCD], (instregex "MVCOS$")>;
+def : InstRW<[WLat30, MCD], (instregex "MVPG$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Address-Space Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "LASP$")>;
+def : InstRW<[WLat1, LSU, GroupAlone], (instregex "PALB$")>;
+def : InstRW<[WLat30, MCD], (instregex "PC$")>;
+def : InstRW<[WLat30, MCD], (instregex "PR$")>;
+def : InstRW<[WLat30, MCD], (instregex "PT(I)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "RP$")>;
+def : InstRW<[WLat30, MCD], (instregex "BS(G|A)$")>;
+def : InstRW<[WLat30, MCD], (instregex "TAR$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Linkage-Stack Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "BAKR$")>;
+def : InstRW<[WLat30, MCD], (instregex "EREG(G)?$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "(E|M)STA$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Time-Related Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "PTFF$")>;
+def : InstRW<[WLat30, MCD], (instregex "SCK(PF|C)?$")>;
+def : InstRW<[WLat1, LSU2, GroupAlone], (instregex "SPT$")>;
+def : InstRW<[WLat15, LSU3, FXa2, FXb, GroupAlone2], (instregex "STCK(F)?$")>;
+def : InstRW<[WLat20, LSU4, FXa2, FXb2, GroupAlone3], (instregex "STCKE$")>;
+def : InstRW<[WLat30, MCD], (instregex "STCKC$")>;
+def : InstRW<[WLat1, LSU2, FXb, Cracked], (instregex "STPT$")>;
+
+//===----------------------------------------------------------------------===//
+// System: CPU-Related Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "STAP$")>;
+def : InstRW<[WLat30, MCD], (instregex "STIDP$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "STSI$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "STFL(E)?$")>;
+def : InstRW<[WLat30, MCD], (instregex "ECAG$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "ECTG$")>;
+def : InstRW<[WLat30, MCD], (instregex "PTF$")>;
+def : InstRW<[WLat30, MCD], (instregex "PCKMO$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "QPACI$")>;
+
+//===----------------------------------------------------------------------===//
+// System: Miscellaneous Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "SVC$")>;
+def : InstRW<[WLat1, FXb, GroupAlone], (instregex "MC$")>;
+def : InstRW<[WLat30, MCD], (instregex "DIAG$")>;
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "TRACE$")>;
+def : InstRW<[WLat1, FXb, GroupAlone], (instregex "TRACG$")>;
+def : InstRW<[WLat30, MCD], (instregex "TRAP(2|4)$")>;
+def : InstRW<[WLat30, MCD], (instregex "SIG(P|A)$")>;
+def : InstRW<[WLat30, MCD], (instregex "SIE$")>;
+
+//===----------------------------------------------------------------------===//
+// System: CPU-Measurement Facility Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "LPP$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "ECPGA$")>;
+def : InstRW<[WLat30, WLat30, MCD], (instregex "E(C|P)CTR$")>;
+def : InstRW<[WLat30, MCD], (instregex "LCCTL$")>;
+def : InstRW<[WLat30, MCD], (instregex "L(P|S)CTL$")>;
+def : InstRW<[WLat30, MCD], (instregex "Q(S|CTR)I$")>;
+def : InstRW<[WLat30, MCD], (instregex "S(C|P)CTR$")>;
+
+//===----------------------------------------------------------------------===//
+// System: I/O Instructions
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat30, MCD], (instregex "(C|H|R|X)SCH$")>;
+def : InstRW<[WLat30, MCD], (instregex "(M|S|ST|T)SCH$")>;
+def : InstRW<[WLat30, MCD], (instregex "RCHP$")>;
+def : InstRW<[WLat30, MCD], (instregex "SCHM$")>;
+def : InstRW<[WLat30, MCD], (instregex "STC(PS|RW)$")>;
+def : InstRW<[WLat30, MCD], (instregex "TPE?I$")>;
+def : InstRW<[WLat30, MCD], (instregex "SAL$")>;
+
+//===----------------------------------------------------------------------===//
+// NOPs
+//===----------------------------------------------------------------------===//
+
+def : InstRW<[WLat1, FXb, NormalGr], (instregex "NOP(R)?(Opt)?$")>;
+def : InstRW<[WLat1, VBU, NormalGr], (instregex "J(G)?NOP$")>;
+}
+
diff --git a/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp b/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp
index 2b9483293941..7eec38b79cb8 100644
--- a/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp
+++ b/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp
@@ -18,6 +18,7 @@
 #include "llvm/CodeGen/BasicTTIImpl.h"
 #include "llvm/CodeGen/TargetLowering.h"
 #include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/InstIterator.h"
 #include "llvm/IR/IntrinsicInst.h"
 #include "llvm/IR/Intrinsics.h"
 #include "llvm/Support/Debug.h"
@@ -80,7 +81,6 @@ unsigned SystemZTTIImpl::adjustInliningThreshold(const CallBase *CB) const {
   const Function *Callee = CB->getCalledFunction();
   if (!Callee)
     return 0;
-  const Module *M = Caller->getParent();
 
   // Increase the threshold if an incoming argument is used only as a memcpy
   // source.
@@ -92,25 +92,37 @@ unsigned SystemZTTIImpl::adjustInliningThreshold(const CallBase *CB) const {
     }
   }
 
-  // Give bonus for globals used much in both caller and callee.
-  std::set<const GlobalVariable *> CalleeGlobals;
-  std::set<const GlobalVariable *> CallerGlobals;
-  for (const GlobalVariable &Global : M->globals())
-    for (const User *U : Global.users())
-      if (const Instruction *User = dyn_cast<Instruction>(U)) {
-        if (User->getParent()->getParent() == Callee)
-          CalleeGlobals.insert(&Global);
-        if (User->getParent()->getParent() == Caller)
-          CallerGlobals.insert(&Global);
+  // Give bonus for globals used much in both caller and a relatively small
+  // callee.
+  unsigned InstrCount = 0;
+  SmallDenseMap<const Value *, unsigned> Ptr2NumUses;
+  for (auto &I : instructions(Callee)) {
+    if (++InstrCount == 200) {
+      Ptr2NumUses.clear();
+      break;
+    }
+    if (const auto *SI = dyn_cast<StoreInst>(&I)) {
+      if (!SI->isVolatile())
+        if (auto *GV = dyn_cast<GlobalVariable>(SI->getPointerOperand()))
+          Ptr2NumUses[GV]++;
+    } else if (const auto *LI = dyn_cast<LoadInst>(&I)) {
+      if (!LI->isVolatile())
+        if (auto *GV = dyn_cast<GlobalVariable>(LI->getPointerOperand()))
+          Ptr2NumUses[GV]++;
+    } else if (const auto *GEP = dyn_cast<GetElementPtrInst>(&I)) {
+      if (auto *GV = dyn_cast<GlobalVariable>(GEP->getPointerOperand())) {
+        unsigned NumStores = 0, NumLoads = 0;
+        countNumMemAccesses(GEP, NumStores, NumLoads, Callee);
+        Ptr2NumUses[GV] += NumLoads + NumStores;
       }
-  for (auto *GV : CalleeGlobals)
-    if (CallerGlobals.count(GV)) {
-      unsigned CalleeStores = 0, CalleeLoads = 0;
+    }
+  }
+
+  for (auto [Ptr, NumCalleeUses] : Ptr2NumUses)
+    if (NumCalleeUses > 10) {
       unsigned CallerStores = 0, CallerLoads = 0;
-      countNumMemAccesses(GV, CalleeStores, CalleeLoads, Callee);
-      countNumMemAccesses(GV, CallerStores, CallerLoads, Caller);
-      if ((CalleeStores + CalleeLoads) > 10 &&
-          (CallerStores + CallerLoads) > 10) {
+      countNumMemAccesses(Ptr, CallerStores, CallerLoads, Caller);
+      if (CallerStores + CallerLoads > 10) {
         Bonus = 1000;
         break;
       }
diff --git a/llvm/lib/Target/X86/X86.td b/llvm/lib/Target/X86/X86.td
index 38761e1fd7ee..577428cad6d6 100644
--- a/llvm/lib/Target/X86/X86.td
+++ b/llvm/lib/Target/X86/X86.td
@@ -338,7 +338,7 @@ def FeatureAVX10_1 : SubtargetFeature<"avx10.1-256", "HasAVX10_1", "true",
                                       "Support AVX10.1 up to 256-bit instruction",
                                       [FeatureCDI, FeatureVBMI, FeatureIFMA, FeatureVNNI,
                                        FeatureBF16, FeatureVPOPCNTDQ, FeatureVBMI2, FeatureBITALG,
-                                       FeatureVAES, FeatureVPCLMULQDQ, FeatureFP16]>;
+                                       FeatureFP16]>;
 def FeatureAVX10_1_512 : SubtargetFeature<"avx10.1-512", "HasAVX10_1_512", "true",
                                           "Support AVX10.1 up to 512-bit instruction",
                                           [FeatureAVX10_1, FeatureEVEX512]>;
diff --git a/llvm/lib/Target/X86/X86ExpandPseudo.cpp b/llvm/lib/Target/X86/X86ExpandPseudo.cpp
index 78db8413e62c..813f0f2542fa 100644
--- a/llvm/lib/Target/X86/X86ExpandPseudo.cpp
+++ b/llvm/lib/Target/X86/X86ExpandPseudo.cpp
@@ -439,8 +439,18 @@ bool X86ExpandPseudo::expandMI(MachineBasicBlock &MBB,
     TII->copyPhysReg(MBB, MBBI, DL, X86::RBX, InArg.getReg(), false);
     // Create the actual instruction.
     MachineInstr *NewInstr = BuildMI(MBB, MBBI, DL, TII->get(X86::LCMPXCHG16B));
-    // Copy the operands related to the address.
-    for (unsigned Idx = 1; Idx < 6; ++Idx)
+    // Copy the operands related to the address. If we access a frame variable,
+    // we need to replace the RBX base with SaveRbx, as RBX has another value.
+    const MachineOperand &Base = MBBI->getOperand(1);
+    if (Base.getReg() == X86::RBX || Base.getReg() == X86::EBX)
+      NewInstr->addOperand(MachineOperand::CreateReg(
+          Base.getReg() == X86::RBX
+              ? SaveRbx
+              : Register(TRI->getSubReg(SaveRbx, X86::sub_32bit)),
+          /*IsDef=*/false));
+    else
+      NewInstr->addOperand(Base);
+    for (unsigned Idx = 1 + 1; Idx < 1 + X86::AddrNumOperands; ++Idx)
       NewInstr->addOperand(MBBI->getOperand(Idx));
     // Finally, restore the value of RBX.
     TII->copyPhysReg(MBB, MBBI, DL, X86::RBX, SaveRbx,
diff --git a/llvm/lib/Target/X86/X86FixupVectorConstants.cpp b/llvm/lib/Target/X86/X86FixupVectorConstants.cpp
index 453898e132ca..9dc392d6e962 100644
--- a/llvm/lib/Target/X86/X86FixupVectorConstants.cpp
+++ b/llvm/lib/Target/X86/X86FixupVectorConstants.cpp
@@ -333,6 +333,7 @@ bool X86FixupVectorConstantsPass::processInstruction(MachineFunction &MF,
                                                      MachineInstr &MI) {
   unsigned Opc = MI.getOpcode();
   MachineConstantPool *CP = MI.getParent()->getParent()->getConstantPool();
+  bool HasSSE2 = ST->hasSSE2();
   bool HasSSE41 = ST->hasSSE41();
   bool HasAVX2 = ST->hasAVX2();
   bool HasDQI = ST->hasDQI();
@@ -394,11 +395,13 @@ bool X86FixupVectorConstantsPass::processInstruction(MachineFunction &MF,
   case X86::MOVAPDrm:
   case X86::MOVAPSrm:
   case X86::MOVUPDrm:
-  case X86::MOVUPSrm:
+  case X86::MOVUPSrm: {
     // TODO: SSE3 MOVDDUP Handling
-    return FixupConstant({{X86::MOVSSrm, 1, 32, rebuildZeroUpperCst},
-                          {X86::MOVSDrm, 1, 64, rebuildZeroUpperCst}},
-                         128, 1);
+    FixupEntry Fixups[] = {
+        {X86::MOVSSrm, 1, 32, rebuildZeroUpperCst},
+        {HasSSE2 ? X86::MOVSDrm : 0, 1, 64, rebuildZeroUpperCst}};
+    return FixupConstant(Fixups, 128, 1);
+  }
   case X86::VMOVAPDrm:
   case X86::VMOVAPSrm:
   case X86::VMOVUPDrm:
diff --git a/llvm/lib/Target/X86/X86ISelLowering.cpp b/llvm/lib/Target/X86/X86ISelLowering.cpp
index 627cef9ead7f..4413fbb77f41 100644
--- a/llvm/lib/Target/X86/X86ISelLowering.cpp
+++ b/llvm/lib/Target/X86/X86ISelLowering.cpp
@@ -54147,12 +54147,19 @@ SDValue X86TargetLowering::getNegatedExpression(SDValue Op, SelectionDAG &DAG,
     if (!Flags.hasNoSignedZeros())
       break;
 
+    // Because getCheaperNegatedExpression can delete nodes we need a handle to
+    // keep temporary nodes alive.
+    std::list<HandleSDNode> Handles;
+
     // This is always negatible for free but we might be able to remove some
     // extra operand negations as well.
     SmallVector<SDValue, 4> NewOps(Op.getNumOperands(), SDValue());
-    for (int i = 0; i != 3; ++i)
+    for (int i = 0; i != 3; ++i) {
       NewOps[i] = getCheaperNegatedExpression(
           Op.getOperand(i), DAG, LegalOperations, ForCodeSize, Depth + 1);
+      if (!!NewOps[i])
+        Handles.emplace_back(NewOps[i]);
+    }
 
     bool NegA = !!NewOps[0];
     bool NegB = !!NewOps[1];
diff --git a/llvm/lib/Target/X86/X86InstrAVX10.td b/llvm/lib/Target/X86/X86InstrAVX10.td
index 9bb3e364f7c6..37d3b0a67cd3 100644
--- a/llvm/lib/Target/X86/X86InstrAVX10.td
+++ b/llvm/lib/Target/X86/X86InstrAVX10.td
@@ -1468,7 +1468,7 @@ defm VRSQRT  : avx10_fp14_bf16<0x4E, "vrsqrt", X86rsqrt14, SchedWriteFRsqrt>,
 defm VRCP    : avx10_fp14_bf16<0x4C, "vrcp", X86rcp14, SchedWriteFRcp>,
                                 T_MAP6, PS, EVEX_CD8<16, CD8VF>;
 defm VGETEXP : avx10_fp14_bf16<0x42, "vgetexp", X86fgetexp, SchedWriteFRnd>,
-                                T_MAP5, EVEX_CD8<16, CD8VF>;
+                                T_MAP6, PS, EVEX_CD8<16, CD8VF>;
 
 // VSCALEFBF16
 multiclass avx10_fp_scalef_bf16<bits<8> opc, string OpcodeStr,
@@ -1665,31 +1665,31 @@ multiclass avx10_com_ef_int<bits<8> Opc, X86VectorVTInfo _, SDNode OpNode,
 let Defs = [EFLAGS], Uses = [MXCSR], Predicates = [HasAVX10_2] in {
   defm VUCOMXSDZ  :  avx10_com_ef<0x2e, FR64X, f64, X86ucomi512,
                                   "vucomxsd", f64mem, loadf64, SSEPackedDouble>,
-                                  TB, XS, VEX_LIG, REX_W, EVEX_CD8<64, CD8VT1>;
+                                  TB, XD, VEX_LIG, REX_W, EVEX_CD8<64, CD8VT1>;
   defm VUCOMXSHZ  :  avx10_com_ef<0x2e, FR16X, f16, X86ucomi512,
                                   "vucomxsh", f16mem, loadf16, SSEPackedSingle>,
-                                  T_MAP5, XD, EVEX_CD8<16, CD8VT1>;
+                                  T_MAP5, XS, EVEX_CD8<16, CD8VT1>;
   defm VUCOMXSSZ  :  avx10_com_ef<0x2e, FR32X, f32, X86ucomi512,
                                   "vucomxss", f32mem, loadf32, SSEPackedSingle>,
-                                  TB, XD, VEX_LIG, EVEX_CD8<32, CD8VT1>;
+                                  TB, XS, VEX_LIG, EVEX_CD8<32, CD8VT1>;
   defm VCOMXSDZ   :  avx10_com_ef_int<0x2f, v2f64x_info, X86comi512,
                                       "vcomxsd", SSEPackedDouble>,
-                                      TB, XS, VEX_LIG, REX_W, EVEX_CD8<64, CD8VT1>;
+                                      TB, XD, VEX_LIG, REX_W, EVEX_CD8<64, CD8VT1>;
   defm VCOMXSHZ   :  avx10_com_ef_int<0x2f, v8f16x_info, X86comi512,
                                       "vcomxsh", SSEPackedSingle>,
-                                      T_MAP5, XD, EVEX_CD8<16, CD8VT1>;
+                                      T_MAP5, XS, EVEX_CD8<16, CD8VT1>;
   defm VCOMXSSZ   :  avx10_com_ef_int<0x2f, v4f32x_info, X86comi512,
                                       "vcomxss", SSEPackedSingle>,
-                                      TB, XD, VEX_LIG, EVEX_CD8<32, CD8VT1>;
+                                      TB, XS, VEX_LIG, EVEX_CD8<32, CD8VT1>;
   defm VUCOMXSDZ  :  avx10_com_ef_int<0x2e, v2f64x_info, X86ucomi512,
                                       "vucomxsd", SSEPackedDouble>,
-                                      TB, XS, VEX_LIG, REX_W, EVEX_CD8<64, CD8VT1>;
+                                      TB, XD, VEX_LIG, REX_W, EVEX_CD8<64, CD8VT1>;
   defm VUCOMXSHZ  :  avx10_com_ef_int<0x2e, v8f16x_info, X86ucomi512,
                                       "vucomxsh", SSEPackedSingle>,
-                                      T_MAP5, XD, EVEX_CD8<16, CD8VT1>;
+                                      T_MAP5, XS, EVEX_CD8<16, CD8VT1>;
   defm VUCOMXSSZ  :  avx10_com_ef_int<0x2e, v4f32x_info, X86ucomi512,
                                       "vucomxss", SSEPackedSingle>,
-                                      TB, XD, VEX_LIG, EVEX_CD8<32, CD8VT1>;
+                                      TB, XS, VEX_LIG, EVEX_CD8<32, CD8VT1>;
 }
 
 //-------------------------------------------------
diff --git a/llvm/lib/Target/X86/X86InstrSSE.td b/llvm/lib/Target/X86/X86InstrSSE.td
index 6aadb788c851..2a7ab1e31061 100644
--- a/llvm/lib/Target/X86/X86InstrSSE.td
+++ b/llvm/lib/Target/X86/X86InstrSSE.td
@@ -6121,8 +6121,9 @@ let Predicates = [HasAVX, NoAVX10_2] in {
                                     v8i16, VR128, load, i128mem, 0,
                                     SchedWriteMPSAD.XMM>, VEX, VVVV, WIG;
   }
+}
 
-let Uses = [MXCSR], mayRaiseFPException = 1 in {
+let Predicates = [HasAVX], Uses = [MXCSR], mayRaiseFPException = 1 in {
   let ExeDomain = SSEPackedSingle in
   defm VDPPS : SS41I_binop_rmi_int<0x40, "vdpps", int_x86_sse41_dpps,
                                    VR128, load, f128mem, 0,
@@ -6136,7 +6137,6 @@ let Uses = [MXCSR], mayRaiseFPException = 1 in {
                                     VR256, load, i256mem, 0,
                                     SchedWriteDPPS.YMM>, VEX, VVVV, VEX_L, WIG;
 }
-}
 
 let Predicates = [HasAVX2, NoAVX10_2] in {
   let isCommutable = 0 in {
diff --git a/llvm/lib/Target/X86/X86MCInstLower.cpp b/llvm/lib/Target/X86/X86MCInstLower.cpp
index 645a9baeba65..680bf4286da0 100644
--- a/llvm/lib/Target/X86/X86MCInstLower.cpp
+++ b/llvm/lib/Target/X86/X86MCInstLower.cpp
@@ -348,12 +348,8 @@ MCOperand X86MCInstLower::LowerMachineOperand(const MachineInstr *MI,
     return MCOperand::createImm(MO.getImm());
   case MachineOperand::MO_MachineBasicBlock:
   case MachineOperand::MO_GlobalAddress:
+  case MachineOperand::MO_ExternalSymbol:
     return LowerSymbolOperand(MO, GetSymbolFromOperand(MO));
-  case MachineOperand::MO_ExternalSymbol: {
-    MCSymbol *Sym = GetSymbolFromOperand(MO);
-    Sym->setExternal(true);
-    return LowerSymbolOperand(MO, Sym);
-  }
   case MachineOperand::MO_MCSymbol:
     return LowerSymbolOperand(MO, MO.getMCSymbol());
   case MachineOperand::MO_JumpTableIndex:
diff --git a/llvm/lib/TargetParser/Host.cpp b/llvm/lib/TargetParser/Host.cpp
index fa57ae183bb8..4c60698a63ef 100644
--- a/llvm/lib/TargetParser/Host.cpp
+++ b/llvm/lib/TargetParser/Host.cpp
@@ -428,7 +428,7 @@ StringRef getCPUNameFromS390Model(unsigned int Id, bool HaveVectorSupport) {
     case 9175:
     case 9176:
     default:
-      return HaveVectorSupport? "arch15" : "zEC12";
+      return HaveVectorSupport? "z17" : "zEC12";
   }
 }
 } // end anonymous namespace
diff --git a/llvm/lib/TargetParser/RISCVISAInfo.cpp b/llvm/lib/TargetParser/RISCVISAInfo.cpp
index c78d60fd86b3..64ec411cb06e 100644
--- a/llvm/lib/TargetParser/RISCVISAInfo.cpp
+++ b/llvm/lib/TargetParser/RISCVISAInfo.cpp
@@ -45,9 +45,8 @@ struct RISCVProfile {
 
 } // end anonymous namespace
 
-static const char *RISCVGImplications[] = {
-  "i", "m", "a", "f", "d", "zicsr", "zifencei"
-};
+static const char *RISCVGImplications[] = {"i", "m", "a", "f", "d"};
+static const char *RISCVGImplicationsZi[] = {"zicsr", "zifencei"};
 
 #define GET_SUPPORTED_EXTENSIONS
 #include "llvm/TargetParser/RISCVTargetParserDef.inc"
@@ -718,6 +717,19 @@ RISCVISAInfo::parseArchString(StringRef Arch, bool EnableExperimentalExtension,
     } while (!Ext.empty());
   }
 
+  // We add Zicsr/Zifenci as final to allow duplicated "zicsr"/"zifencei" like
+  // "rv64g_zicsr_zifencei".
+  if (Baseline == 'g') {
+    for (const char *Ext : RISCVGImplicationsZi) {
+      if (ISAInfo->Exts.count(Ext))
+        continue;
+
+      auto Version = findDefaultVersion(Ext);
+      assert(Version && "Default extension version not found?");
+      ISAInfo->Exts[std::string(Ext)] = {Version->Major, Version->Minor};
+    }
+  }
+
   return RISCVISAInfo::postProcessAndChecking(std::move(ISAInfo));
 }
 
diff --git a/llvm/lib/TargetParser/X86TargetParser.cpp b/llvm/lib/TargetParser/X86TargetParser.cpp
index e4b7ed7cf9b6..2ae6dd6b3d1e 100644
--- a/llvm/lib/TargetParser/X86TargetParser.cpp
+++ b/llvm/lib/TargetParser/X86TargetParser.cpp
@@ -637,8 +637,7 @@ constexpr FeatureBitset ImpliedFeaturesAVXVNNI = FeatureAVX2;
 constexpr FeatureBitset ImpliedFeaturesAVX10_1 =
     FeatureAVX512CD | FeatureAVX512VBMI | FeatureAVX512IFMA |
     FeatureAVX512VNNI | FeatureAVX512BF16 | FeatureAVX512VPOPCNTDQ |
-    FeatureAVX512VBMI2 | FeatureAVX512BITALG | FeatureVAES | FeatureVPCLMULQDQ |
-    FeatureAVX512FP16;
+    FeatureAVX512VBMI2 | FeatureAVX512BITALG | FeatureAVX512FP16;
 constexpr FeatureBitset ImpliedFeaturesAVX10_1_512 =
     FeatureAVX10_1 | FeatureEVEX512;
 constexpr FeatureBitset ImpliedFeaturesAVX10_2 = FeatureAVX10_1;
diff --git a/llvm/lib/ToolDrivers/llvm-dlltool/CMakeLists.txt b/llvm/lib/ToolDrivers/llvm-dlltool/CMakeLists.txt
index 855ae5f048ff..5db08e7852d0 100644
--- a/llvm/lib/ToolDrivers/llvm-dlltool/CMakeLists.txt
+++ b/llvm/lib/ToolDrivers/llvm-dlltool/CMakeLists.txt
@@ -6,6 +6,7 @@ add_llvm_component_library(LLVMDlltoolDriver
   DlltoolDriver.cpp
 
   LINK_COMPONENTS
+  BinaryFormat
   Object
   Option
   Support
diff --git a/llvm/lib/ToolDrivers/llvm-dlltool/DlltoolDriver.cpp b/llvm/lib/ToolDrivers/llvm-dlltool/DlltoolDriver.cpp
index 1782e2428786..380fbd8b6fc6 100644
--- a/llvm/lib/ToolDrivers/llvm-dlltool/DlltoolDriver.cpp
+++ b/llvm/lib/ToolDrivers/llvm-dlltool/DlltoolDriver.cpp
@@ -12,6 +12,7 @@
 
 #include "llvm/ToolDrivers/llvm-dlltool/DlltoolDriver.h"
 #include "llvm/ADT/StringSwitch.h"
+#include "llvm/Object/Archive.h"
 #include "llvm/Object/COFF.h"
 #include "llvm/Object/COFFImportFile.h"
 #include "llvm/Object/COFFModuleDefinition.h"
@@ -158,6 +159,143 @@ bool parseModuleDefinition(StringRef DefFileName, MachineTypes Machine,
   return true;
 }
 
+int printError(llvm::Error E, Twine File) {
+  if (!E)
+    return 0;
+  handleAllErrors(std::move(E), [&](const llvm::ErrorInfoBase &EIB) {
+    llvm::errs() << "error opening " << File << ": " << EIB.message() << "\n";
+  });
+  return 1;
+}
+
+template <typename Callable>
+int forEachCoff(object::Archive &Archive, StringRef Name, Callable Callback) {
+  Error Err = Error::success();
+  for (auto &C : Archive.children(Err)) {
+    Expected<StringRef> NameOrErr = C.getName();
+    if (!NameOrErr)
+      return printError(NameOrErr.takeError(), Name);
+    StringRef Name = *NameOrErr;
+
+    Expected<MemoryBufferRef> ChildMB = C.getMemoryBufferRef();
+    if (!ChildMB)
+      return printError(ChildMB.takeError(), Name);
+
+    if (identify_magic(ChildMB->getBuffer()) == file_magic::coff_object) {
+      auto Obj = object::COFFObjectFile::create(*ChildMB);
+      if (!Obj)
+        return printError(Obj.takeError(), Name);
+      if (!Callback(*Obj->get(), Name))
+        return 1;
+    }
+  }
+  if (Err)
+    return printError(std::move(Err), Name);
+  return 0;
+}
+
+// To find the named of the imported DLL from an import library, we can either
+// inspect the object files that form the import table entries, or we could
+// just look at the archive member names, for MSVC style import libraries.
+// Looking at the archive member names doesn't work for GNU style import
+// libraries though, while inspecting the import table entries works for
+// both. (MSVC style import libraries contain a couple regular object files
+// for the header/trailers.)
+//
+// This implementation does the same as GNU dlltool does; look at the
+// content of ".idata$7" sections, or for MSVC style libraries, look
+// at ".idata$6" sections.
+//
+// For GNU style import libraries, there are also other data chunks in sections
+// named ".idata$7" (entries to the IAT or ILT); these are distinguished
+// by seeing that they contain relocations. (They also look like an empty
+// string when looking for null termination.)
+//
+// Alternatively, we could do things differently - look for any .idata$2
+// section; this would be import directory entries. At offset 0xc in them
+// there is the RVA of the import DLL name; look for a relocation at this
+// spot and locate the symbol that it points at. That symbol may either
+// be within the same object file (in the case of MSVC style import libraries)
+// or another object file (in the case of GNU import libraries).
+bool identifyImportName(const COFFObjectFile &Obj, StringRef ObjName,
+                        std::vector<StringRef> &Names, bool IsMsStyleImplib) {
+  StringRef TargetName = IsMsStyleImplib ? ".idata$6" : ".idata$7";
+  for (const auto &S : Obj.sections()) {
+    Expected<StringRef> NameOrErr = S.getName();
+    if (!NameOrErr) {
+      printError(NameOrErr.takeError(), ObjName);
+      return false;
+    }
+    StringRef Name = *NameOrErr;
+    if (Name != TargetName)
+      continue;
+
+    // GNU import libraries contain .idata$7 section in the per function
+    // objects too, but they contain relocations.
+    if (!IsMsStyleImplib && !S.relocations().empty())
+      continue;
+
+    Expected<StringRef> ContentsOrErr = S.getContents();
+    if (!ContentsOrErr) {
+      printError(ContentsOrErr.takeError(), ObjName);
+      return false;
+    }
+    StringRef Contents = *ContentsOrErr;
+    Contents = Contents.substr(0, Contents.find('\0'));
+    if (Contents.empty())
+      continue;
+    Names.push_back(Contents);
+    return true;
+  }
+  return true;
+}
+
+int doIdentify(StringRef File, bool IdentifyStrict) {
+  ErrorOr<std::unique_ptr<MemoryBuffer>> MaybeBuf = MemoryBuffer::getFile(
+      File, /*IsText=*/false, /*RequiredNullTerminator=*/false);
+  if (!MaybeBuf)
+    return printError(errorCodeToError(MaybeBuf.getError()), File);
+  if (identify_magic(MaybeBuf.get()->getBuffer()) != file_magic::archive) {
+    llvm::errs() << File << " is not a library\n";
+    return 1;
+  }
+
+  std::unique_ptr<MemoryBuffer> B = std::move(MaybeBuf.get());
+  Error Err = Error::success();
+  object::Archive Archive(B->getMemBufferRef(), Err);
+  if (Err)
+    return printError(std::move(Err), B->getBufferIdentifier());
+
+  bool IsMsStyleImplib = false;
+  for (const auto &S : Archive.symbols()) {
+    if (S.getName() == "__NULL_IMPORT_DESCRIPTOR") {
+      IsMsStyleImplib = true;
+      break;
+    }
+  }
+  std::vector<StringRef> Names;
+  if (forEachCoff(Archive, B->getBufferIdentifier(),
+                  [&](const COFFObjectFile &Obj, StringRef ObjName) -> bool {
+                    return identifyImportName(Obj, ObjName, Names,
+                                              IsMsStyleImplib);
+                  }))
+    return 1;
+
+  if (Names.empty()) {
+    llvm::errs() << "No DLL import name found in " << File << "\n";
+    return 1;
+  }
+  if (Names.size() > 1 && IdentifyStrict) {
+    llvm::errs() << File << "contains imports for two or more DLLs\n";
+    return 1;
+  }
+
+  for (StringRef S : Names)
+    llvm::outs() << S << "\n";
+
+  return 0;
+}
+
 } // namespace
 
 int llvm::dlltoolDriverMain(llvm::ArrayRef<const char *> ArgsArr) {
@@ -173,7 +311,8 @@ int llvm::dlltoolDriverMain(llvm::ArrayRef<const char *> ArgsArr) {
 
   // Handle when no input or output is specified
   if (Args.hasArgNoClaim(OPT_INPUT) ||
-      (!Args.hasArgNoClaim(OPT_d) && !Args.hasArgNoClaim(OPT_l))) {
+      (!Args.hasArgNoClaim(OPT_d) && !Args.hasArgNoClaim(OPT_l) &&
+       !Args.hasArgNoClaim(OPT_I))) {
     Table.printHelp(outs(), "llvm-dlltool [options] file...", "llvm-dlltool",
                     false);
     llvm::outs()
@@ -185,6 +324,11 @@ int llvm::dlltoolDriverMain(llvm::ArrayRef<const char *> ArgsArr) {
     llvm::errs() << "ignoring unknown argument: " << Arg->getAsString(Args)
                  << "\n";
 
+  if (Args.hasArg(OPT_I)) {
+    return doIdentify(Args.getLastArg(OPT_I)->getValue(),
+                      Args.hasArg(OPT_identify_strict));
+  }
+
   if (!Args.hasArg(OPT_d)) {
     llvm::errs() << "no definition file specified\n";
     return 1;
diff --git a/llvm/lib/ToolDrivers/llvm-dlltool/Options.td b/llvm/lib/ToolDrivers/llvm-dlltool/Options.td
index 7810694c98e3..4fd80189aff2 100644
--- a/llvm/lib/ToolDrivers/llvm-dlltool/Options.td
+++ b/llvm/lib/ToolDrivers/llvm-dlltool/Options.td
@@ -21,6 +21,11 @@ def k_alias: Flag<["--"], "kill-at">, Alias<k>;
 def no_leading_underscore: Flag<["--"], "no-leading-underscore">,
     HelpText<"Don't add leading underscores on symbols">;
 
+def I: JoinedOrSeparate<["-"], "I">, HelpText<"Identify DLL name from import library">;
+def I_long : JoinedOrSeparate<["--"], "identify">, Alias<I>;
+
+def identify_strict : Flag<["--"], "identify-strict">, HelpText<"Error out if the --identify option detects more than one DLL">;
+
 //==============================================================================
 // The flags below do nothing. They are defined only for dlltool compatibility.
 //==============================================================================
diff --git a/llvm/lib/Transforms/IPO/GlobalOpt.cpp b/llvm/lib/Transforms/IPO/GlobalOpt.cpp
index 9586fc97a39f..236a53131767 100644
--- a/llvm/lib/Transforms/IPO/GlobalOpt.cpp
+++ b/llvm/lib/Transforms/IPO/GlobalOpt.cpp
@@ -719,10 +719,14 @@ static bool allUsesOfLoadedValueWillTrapIfNull(const GlobalVariable *GV) {
     const Value *P = Worklist.pop_back_val();
     for (const auto *U : P->users()) {
       if (auto *LI = dyn_cast<LoadInst>(U)) {
+        if (!LI->isSimple())
+          return false;
         SmallPtrSet<const PHINode *, 8> PHIs;
         if (!AllUsesOfValueWillTrapIfNull(LI, PHIs))
           return false;
       } else if (auto *SI = dyn_cast<StoreInst>(U)) {
+        if (!SI->isSimple())
+          return false;
         // Ignore stores to the global.
         if (SI->getPointerOperand() != P)
           return false;
diff --git a/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp b/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp
index ca8a20b4b731..ebb84d177a83 100644
--- a/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp
+++ b/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp
@@ -1475,7 +1475,9 @@ Value *InstCombinerImpl::foldLogicOfFCmps(FCmpInst *LHS, FCmpInst *RHS,
     }
   }
 
-  if (IsAnd && stripSignOnlyFPOps(LHS0) == stripSignOnlyFPOps(RHS0)) {
+  // This transform is not valid for a logical select.
+  if (!IsLogicalSelect && IsAnd &&
+      stripSignOnlyFPOps(LHS0) == stripSignOnlyFPOps(RHS0)) {
     // and (fcmp ord x, 0), (fcmp u* x, inf) -> fcmp o* x, inf
     // and (fcmp ord x, 0), (fcmp u* fabs(x), inf) -> fcmp o* x, inf
     if (Value *Left = matchIsFiniteTest(Builder, LHS, RHS))
diff --git a/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp b/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp
index 29c5cef84ccd..9cd234dd3bab 100644
--- a/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp
+++ b/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp
@@ -2838,7 +2838,14 @@ static Instruction *foldSelectWithFCmpToFabs(SelectInst &SI,
 
     // fold (X <= +/-0.0) ? (0.0 - X) : X to fabs(X), when 'Swap' is false
     // fold (X >  +/-0.0) ? X : (0.0 - X) to fabs(X), when 'Swap' is true
-    if (match(TrueVal, m_FSub(m_PosZeroFP(), m_Specific(X)))) {
+    // Note: We require "nnan" for this fold because fcmp ignores the signbit
+    //       of NAN, but IEEE-754 specifies the signbit of NAN values with
+    //       fneg/fabs operations.
+    if (match(TrueVal, m_FSub(m_PosZeroFP(), m_Specific(X))) &&
+        (cast<FPMathOperator>(CondVal)->hasNoNaNs() || SI.hasNoNaNs() ||
+         isKnownNeverNaN(X, /*Depth=*/0,
+                         IC.getSimplifyQuery().getWithInstruction(
+                             cast<Instruction>(CondVal))))) {
       if (!Swap && (Pred == FCmpInst::FCMP_OLE || Pred == FCmpInst::FCMP_ULE)) {
         Value *Fabs = IC.Builder.CreateUnaryIntrinsic(Intrinsic::fabs, X, &SI);
         return IC.replaceInstUsesWith(SI, Fabs);
diff --git a/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp b/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp
index 6860a7cd07b7..118d2d4be828 100644
--- a/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp
+++ b/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp
@@ -3029,10 +3029,18 @@ Instruction *InstCombinerImpl::visitShuffleVectorInst(ShuffleVectorInst &SVI) {
     SmallVector<BitCastInst *, 8> BCs;
     DenseMap<Type *, Value *> NewBCs;
     for (User *U : SVI.users())
-      if (BitCastInst *BC = dyn_cast<BitCastInst>(U))
-        if (!BC->use_empty())
-          // Only visit bitcasts that weren't previously handled.
-          BCs.push_back(BC);
+      if (BitCastInst *BC = dyn_cast<BitCastInst>(U)) {
+        // Only visit bitcasts that weren't previously handled.
+        if (BC->use_empty())
+          continue;
+        // Prefer to combine bitcasts of bitcasts before attempting this fold.
+        if (BC->hasOneUse()) {
+          auto *BC2 = dyn_cast<BitCastInst>(BC->user_back());
+          if (BC2 && isEliminableCastPair(BC, BC2))
+            continue;
+        }
+        BCs.push_back(BC);
+      }
     for (BitCastInst *BC : BCs) {
       unsigned BegIdx = Mask.front();
       Type *TgtTy = BC->getDestTy();
diff --git a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
index a80a85f38e74..9202c341da92 100644
--- a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
+++ b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
@@ -638,17 +638,19 @@ bool MemCpyOptPass::processStoreOfLoad(StoreInst *SI, LoadInst *LI,
       (EnableMemCpyOptWithoutLibcalls ||
        (TLI->has(LibFunc_memcpy) && TLI->has(LibFunc_memmove)))) {
     MemoryLocation LoadLoc = MemoryLocation::get(LI);
-    MemoryUseOrDef *LoadAccess = MSSA->getMemoryAccess(LI),
-                   *StoreAccess = MSSA->getMemoryAccess(SI);
-
-    // We use MSSA to check if an instruction may store to the memory we load
-    // from in between the load and the store. If such an instruction is found,
-    // we try to promote there instead of at the store position.
-    auto *Clobber = MSSA->getWalker()->getClobberingMemoryAccess(
-        StoreAccess->getDefiningAccess(), LoadLoc, BAA);
-    Instruction *P = MSSA->dominates(LoadAccess, Clobber)
-                         ? cast<MemoryUseOrDef>(Clobber)->getMemoryInst()
-                         : SI;
+
+    // We use alias analysis to check if an instruction may store to
+    // the memory we load from in between the load and the store. If
+    // such an instruction is found, we try to promote there instead
+    // of at the store position.
+    // TODO: Can use MSSA for this.
+    Instruction *P = SI;
+    for (auto &I : make_range(++LI->getIterator(), SI->getIterator())) {
+      if (isModSet(BAA.getModRefInfo(&I, LoadLoc))) {
+        P = &I;
+        break;
+      }
+    }
 
     // If we found an instruction that may write to the loaded memory,
     // we can try to promote at this position instead of the store
@@ -1516,7 +1518,7 @@ bool MemCpyOptPass::performStackMoveOptzn(Instruction *Load, Instruction *Store,
   // to remove them.
 
   SmallVector<Instruction *, 4> LifetimeMarkers;
-  SmallSet<Instruction *, 4> NoAliasInstrs;
+  SmallSet<Instruction *, 4> AAMetadataInstrs;
   bool SrcNotDom = false;
 
   // Recursively track the user and check whether modified alias exist.
@@ -1571,8 +1573,8 @@ bool MemCpyOptPass::performStackMoveOptzn(Instruction *Load, Instruction *Store,
               continue;
             }
           }
-          if (UI->hasMetadata(LLVMContext::MD_noalias))
-            NoAliasInstrs.insert(UI);
+          AAMetadataInstrs.insert(UI);
+
           if (!ModRefCallback(UI))
             return false;
         }
@@ -1677,11 +1679,16 @@ bool MemCpyOptPass::performStackMoveOptzn(Instruction *Load, Instruction *Store,
   }
 
   // As this transformation can cause memory accesses that didn't previously
-  // alias to begin to alias one another, we remove !noalias metadata from any
-  // uses of either alloca. This is conservative, but more precision doesn't
-  // seem worthwhile right now.
-  for (Instruction *I : NoAliasInstrs)
+  // alias to begin to alias one another, we remove !alias.scope, !noalias,
+  // !tbaa and !tbaa_struct metadata from any uses of either alloca.
+  // This is conservative, but more precision doesn't seem worthwhile
+  // right now.
+  for (Instruction *I : AAMetadataInstrs) {
+    I->setMetadata(LLVMContext::MD_alias_scope, nullptr);
     I->setMetadata(LLVMContext::MD_noalias, nullptr);
+    I->setMetadata(LLVMContext::MD_tbaa, nullptr);
+    I->setMetadata(LLVMContext::MD_tbaa_struct, nullptr);
+  }
 
   LLVM_DEBUG(dbgs() << "Stack Move: Performed staack-move optimization\n");
   NumStackMove++;
diff --git a/llvm/lib/Transforms/Utils/SimplifyIndVar.cpp b/llvm/lib/Transforms/Utils/SimplifyIndVar.cpp
index f05d32d980e5..7e30a67397f5 100644
--- a/llvm/lib/Transforms/Utils/SimplifyIndVar.cpp
+++ b/llvm/lib/Transforms/Utils/SimplifyIndVar.cpp
@@ -1742,6 +1742,9 @@ bool WidenIV::widenWithVariantUse(WidenIV::NarrowIVDefUse DU) {
     // TODO: Support case for NarrowDef = NarrowUse->getOperand(1).
     if (NarrowUse->getOperand(0) != NarrowDef)
       return false;
+    // We cannot use a different extend kind for the same operand.
+    if (NarrowUse->getOperand(1) == NarrowDef)
+      return false;
     if (!SE->isKnownNegative(RHS))
       return false;
     bool ProvedSubNUW = SE->isKnownPredicateAt(ICmpInst::ICMP_UGE, LHS,
diff --git a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
index 7be14353f722..1a3576534c3e 100644
--- a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
+++ b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
@@ -4643,6 +4643,16 @@ bool LoopVectorizationPlanner::isCandidateForEpilogueVectorization(
         return false;
   }
 
+  // Don't vectorize the epilogue, if there are FindLastIV recurrences with a
+  // start value may be undef or poison. Such start values would need freezing.
+  if (any_of(Legal->getReductionVars(), [](const auto &P) {
+        return RecurrenceDescriptor::isFindLastIVRecurrenceKind(
+                   P.second.getRecurrenceKind()) &&
+               !isGuaranteedNotToBeUndefOrPoison(
+                   P.second.getRecurrenceStartValue());
+      }))
+    return false;
+
   // Epilogue vectorization code has not been auditted to ensure it handles
   // non-latch exits properly.  It may be fine, but it needs auditted and
   // tested.
@@ -7275,7 +7285,7 @@ LoopVectorizationPlanner::precomputeCosts(VPlan &Plan, ElementCount VF,
   // Collect all exit conditions.
   for (BasicBlock *EB : Exiting) {
     auto *Term = dyn_cast<BranchInst>(EB->getTerminator());
-    if (!Term)
+    if (!Term || CostCtx.skipCostComputation(Term, VF.isVector()))
       continue;
     if (auto *CondI = dyn_cast<Instruction>(Term->getOperand(0))) {
       ExitInstrs.insert(CondI);
@@ -7295,7 +7305,8 @@ LoopVectorizationPlanner::precomputeCosts(VPlan &Plan, ElementCount VF,
     Cost += CondICost;
     for (Value *Op : CondI->operands()) {
       auto *OpI = dyn_cast<Instruction>(Op);
-      if (!OpI || any_of(OpI->users(), [&ExitInstrs, this](User *U) {
+      if (!OpI || CostCtx.skipCostComputation(OpI, VF.isVector()) ||
+          any_of(OpI->users(), [&ExitInstrs, this](User *U) {
             return OrigLoop->contains(cast<Instruction>(U)->getParent()) &&
                    !ExitInstrs.contains(cast<Instruction>(U));
           }))
diff --git a/llvm/lib/Transforms/Vectorize/VectorCombine.cpp b/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
index 59920b5a4dd2..1deaaca05d98 100644
--- a/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
+++ b/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
@@ -1403,6 +1403,11 @@ bool VectorCombine::scalarizeLoadExtract(Instruction &I) {
     if (!UI || UI->getParent() != LI->getParent())
       return false;
 
+    // If any extract is waiting to be erased, then bail out as this will
+    // distort the cost calculation and possibly lead to infinite loops.
+    if (UI->use_empty())
+      return false;
+
     // Check if any instruction between the load and the extract may modify
     // memory.
     if (LastCheckedInst->comesBefore(UI)) {
diff --git a/llvm/test/Analysis/BasicAA/size-overflow.ll b/llvm/test/Analysis/BasicAA/size-overflow.ll
new file mode 100644
index 000000000000..2a390d29e472
--- /dev/null
+++ b/llvm/test/Analysis/BasicAA/size-overflow.ll
@@ -0,0 +1,14 @@
+; RUN: opt -passes=aa-eval -print-all-alias-modref-info -disable-output < %s 2>&1 | FileCheck %s
+
+target datalayout = "p:32:32"
+
+; Make sure that using a LocationSize larget than the index space does not
+; assert.
+
+; CHECK: Just Mod:  Ptr: i32* %gep	<->  call void @llvm.memset.p0.i64(ptr %p, i8 0, i64 4294967296, i1 false)
+define void @test(ptr %p, i32 %idx) {
+  %gep = getelementptr i8, ptr %p, i32 %idx
+  load i32, ptr %gep
+  call void @llvm.memset.i64(ptr %p, i8 0, i64 u0x100000000, i1 false)
+  ret void
+}
diff --git a/llvm/test/Analysis/CostModel/RISCV/reduce-fadd.ll b/llvm/test/Analysis/CostModel/RISCV/reduce-fadd.ll
index 1762f701a9b2..71685b4acc82 100644
--- a/llvm/test/Analysis/CostModel/RISCV/reduce-fadd.ll
+++ b/llvm/test/Analysis/CostModel/RISCV/reduce-fadd.ll
@@ -1,25 +1,60 @@
 ; NOTE: Assertions have been autogenerated by utils/update_analyze_test_checks.py
 ; RUN: opt < %s -mtriple=riscv64 -mattr=+v,+zfh,+zvfh,+zfbfmin,+zvfbfmin -passes="print<cost-model>" -cost-kind=throughput 2>&1 -disable-output | FileCheck %s --check-prefixes=FP-REDUCE,FP-REDUCE-ZVFH
 ; RUN: opt < %s -mtriple=riscv64 -mattr=+v,+zfh,+zvfhmin,+zfbfmin,+zvfbfmin -passes="print<cost-model>" -cost-kind=throughput 2>&1 -disable-output | FileCheck %s --check-prefixes=FP-REDUCE,FP-REDUCE-ZVFHMIN
+; RUN: opt < %s -mtriple=riscv64 -mattr=+v -passes="print<cost-model>" -cost-kind=throughput 2>&1 -disable-output | FileCheck %s --check-prefixes=FP-REDUCE,FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN
 ; RUN: opt < %s -mtriple=riscv64 -mattr=+v,+zfh,+zvfh,+zfbfmin,+zvfbfmin -passes="print<cost-model>" -cost-kind=code-size 2>&1 -disable-output | FileCheck %s  --check-prefix=SIZE
 
 define void @reduce_fadd_bfloat() {
-; FP-REDUCE-LABEL: 'reduce_fadd_bfloat'
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %V1 = call fast bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 8 for instruction: %V2 = call fast bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 15 for instruction: %V4 = call fast bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 28 for instruction: %V8 = call fast bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 73 for instruction: %V16 = call fast bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 211 for instruction: %v32 = call fast bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 541 for instruction: %V64 = call fast bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 573 for instruction: %V128 = call fast bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call fast bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call fast bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call fast bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call fast bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call fast bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call fast bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+; FP-REDUCE-ZVFH-LABEL: 'reduce_fadd_bfloat'
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %V1 = call fast bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 8 for instruction: %V2 = call fast bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 15 for instruction: %V4 = call fast bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 28 for instruction: %V8 = call fast bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 73 for instruction: %V16 = call fast bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 211 for instruction: %v32 = call fast bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 541 for instruction: %V64 = call fast bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 573 for instruction: %V128 = call fast bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call fast bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call fast bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call fast bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call fast bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call fast bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call fast bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+;
+; FP-REDUCE-ZVFHMIN-LABEL: 'reduce_fadd_bfloat'
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %V1 = call fast bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 8 for instruction: %V2 = call fast bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 15 for instruction: %V4 = call fast bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 28 for instruction: %V8 = call fast bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 73 for instruction: %V16 = call fast bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 211 for instruction: %v32 = call fast bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 541 for instruction: %V64 = call fast bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 573 for instruction: %V128 = call fast bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call fast bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call fast bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call fast bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call fast bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call fast bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call fast bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+;
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-LABEL: 'reduce_fadd_bfloat'
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V1 = call fast bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V2 = call fast bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V4 = call fast bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V8 = call fast bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V16 = call fast bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %v32 = call fast bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V64 = call fast bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V128 = call fast bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call fast bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call fast bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call fast bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call fast bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call fast bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call fast bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
 ;
 ; SIZE-LABEL: 'reduce_fadd_bfloat'
 ; SIZE-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %V1 = call fast bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
@@ -90,6 +125,23 @@ define void @reduce_fadd_half() {
 ; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call fast half @llvm.vector.reduce.fadd.nxv32f16(half 0xH0000, <vscale x 32 x half> undef)
 ; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
 ;
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-LABEL: 'reduce_fadd_half'
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V1 = call fast half @llvm.vector.reduce.fadd.v1f16(half 0xH0000, <1 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V2 = call fast half @llvm.vector.reduce.fadd.v2f16(half 0xH0000, <2 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V4 = call fast half @llvm.vector.reduce.fadd.v4f16(half 0xH0000, <4 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V8 = call fast half @llvm.vector.reduce.fadd.v8f16(half 0xH0000, <8 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V16 = call fast half @llvm.vector.reduce.fadd.v16f16(half 0xH0000, <16 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %v32 = call fast half @llvm.vector.reduce.fadd.v32f16(half 0xH0000, <32 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V64 = call fast half @llvm.vector.reduce.fadd.v64f16(half 0xH0000, <64 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V128 = call fast half @llvm.vector.reduce.fadd.v128f16(half 0xH0000, <128 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call fast half @llvm.vector.reduce.fadd.nxv1f16(half 0xH0000, <vscale x 1 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call fast half @llvm.vector.reduce.fadd.nxv2f16(half 0xH0000, <vscale x 2 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call fast half @llvm.vector.reduce.fadd.nxv4f16(half 0xH0000, <vscale x 4 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call fast half @llvm.vector.reduce.fadd.nxv8f16(half 0xH0000, <vscale x 8 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call fast half @llvm.vector.reduce.fadd.nxv16f16(half 0xH0000, <vscale x 16 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call fast half @llvm.vector.reduce.fadd.nxv32f16(half 0xH0000, <vscale x 32 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+;
 ; SIZE-LABEL: 'reduce_fadd_half'
 ; SIZE-NEXT:  Cost Model: Found an estimated cost of 3 for instruction: %V1 = call fast half @llvm.vector.reduce.fadd.v1f16(half 0xH0000, <1 x half> undef)
 ; SIZE-NEXT:  Cost Model: Found an estimated cost of 3 for instruction: %V2 = call fast half @llvm.vector.reduce.fadd.v2f16(half 0xH0000, <2 x half> undef)
@@ -220,22 +272,56 @@ define void @reduce_fadd_double() {
 }
 
 define void @reduce_ordered_fadd_bfloat() {
-; FP-REDUCE-LABEL: 'reduce_ordered_fadd_bfloat'
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 3 for instruction: %V1 = call bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 7 for instruction: %V2 = call bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 15 for instruction: %V4 = call bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 31 for instruction: %V8 = call bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 63 for instruction: %V16 = call bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 127 for instruction: %v32 = call bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 255 for instruction: %V64 = call bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 510 for instruction: %V128 = call bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
-; FP-REDUCE-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+; FP-REDUCE-ZVFH-LABEL: 'reduce_ordered_fadd_bfloat'
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 3 for instruction: %V1 = call bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 7 for instruction: %V2 = call bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 15 for instruction: %V4 = call bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 31 for instruction: %V8 = call bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 63 for instruction: %V16 = call bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 127 for instruction: %v32 = call bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 255 for instruction: %V64 = call bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 510 for instruction: %V128 = call bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
+; FP-REDUCE-ZVFH-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+;
+; FP-REDUCE-ZVFHMIN-LABEL: 'reduce_ordered_fadd_bfloat'
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 3 for instruction: %V1 = call bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 7 for instruction: %V2 = call bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 15 for instruction: %V4 = call bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 31 for instruction: %V8 = call bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 63 for instruction: %V16 = call bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 127 for instruction: %v32 = call bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 255 for instruction: %V64 = call bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 510 for instruction: %V128 = call bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
+; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+;
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-LABEL: 'reduce_ordered_fadd_bfloat'
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V1 = call bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V2 = call bfloat @llvm.vector.reduce.fadd.v2bf16(bfloat 0xR0000, <2 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V4 = call bfloat @llvm.vector.reduce.fadd.v4bf16(bfloat 0xR0000, <4 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V8 = call bfloat @llvm.vector.reduce.fadd.v8bf16(bfloat 0xR0000, <8 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V16 = call bfloat @llvm.vector.reduce.fadd.v16bf16(bfloat 0xR0000, <16 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %v32 = call bfloat @llvm.vector.reduce.fadd.v32bf16(bfloat 0xR0000, <32 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V64 = call bfloat @llvm.vector.reduce.fadd.v64bf16(bfloat 0xR0000, <64 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V128 = call bfloat @llvm.vector.reduce.fadd.v128bf16(bfloat 0xR0000, <128 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call bfloat @llvm.vector.reduce.fadd.nxv1bf16(bfloat 0xR0000, <vscale x 1 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call bfloat @llvm.vector.reduce.fadd.nxv2bf16(bfloat 0xR0000, <vscale x 2 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call bfloat @llvm.vector.reduce.fadd.nxv4bf16(bfloat 0xR0000, <vscale x 4 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call bfloat @llvm.vector.reduce.fadd.nxv8bf16(bfloat 0xR0000, <vscale x 8 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call bfloat @llvm.vector.reduce.fadd.nxv16bf16(bfloat 0xR0000, <vscale x 16 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call bfloat @llvm.vector.reduce.fadd.nxv32bf16(bfloat 0xR0000, <vscale x 32 x bfloat> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
 ;
 ; SIZE-LABEL: 'reduce_ordered_fadd_bfloat'
 ; SIZE-NEXT:  Cost Model: Found an estimated cost of 2 for instruction: %V1 = call bfloat @llvm.vector.reduce.fadd.v1bf16(bfloat 0xR0000, <1 x bfloat> undef)
@@ -306,6 +392,23 @@ define void @reduce_ordered_fadd_half() {
 ; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call half @llvm.vector.reduce.fadd.nxv32f16(half 0xH0000, <vscale x 32 x half> undef)
 ; FP-REDUCE-ZVFHMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
 ;
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-LABEL: 'reduce_ordered_fadd_half'
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V1 = call half @llvm.vector.reduce.fadd.v1f16(half 0xH0000, <1 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V2 = call half @llvm.vector.reduce.fadd.v2f16(half 0xH0000, <2 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V4 = call half @llvm.vector.reduce.fadd.v4f16(half 0xH0000, <4 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V8 = call half @llvm.vector.reduce.fadd.v8f16(half 0xH0000, <8 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V16 = call half @llvm.vector.reduce.fadd.v16f16(half 0xH0000, <16 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %v32 = call half @llvm.vector.reduce.fadd.v32f16(half 0xH0000, <32 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V64 = call half @llvm.vector.reduce.fadd.v64f16(half 0xH0000, <64 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %V128 = call half @llvm.vector.reduce.fadd.v128f16(half 0xH0000, <128 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV1 = call half @llvm.vector.reduce.fadd.nxv1f16(half 0xH0000, <vscale x 1 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV2 = call half @llvm.vector.reduce.fadd.nxv2f16(half 0xH0000, <vscale x 2 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV4 = call half @llvm.vector.reduce.fadd.nxv4f16(half 0xH0000, <vscale x 4 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV8 = call half @llvm.vector.reduce.fadd.nxv8f16(half 0xH0000, <vscale x 8 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV16 = call half @llvm.vector.reduce.fadd.nxv16f16(half 0xH0000, <vscale x 16 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Invalid cost for instruction: %NXV32 = call half @llvm.vector.reduce.fadd.nxv32f16(half 0xH0000, <vscale x 32 x half> undef)
+; FP-REDUCE-NO-ZFHMIN-NO-ZFBFMIN-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
+;
 ; SIZE-LABEL: 'reduce_ordered_fadd_half'
 ; SIZE-NEXT:  Cost Model: Found an estimated cost of 3 for instruction: %V1 = call half @llvm.vector.reduce.fadd.v1f16(half 0xH0000, <1 x half> undef)
 ; SIZE-NEXT:  Cost Model: Found an estimated cost of 3 for instruction: %V2 = call half @llvm.vector.reduce.fadd.v2f16(half 0xH0000, <2 x half> undef)
diff --git a/llvm/test/Analysis/CostModel/SystemZ/divrem-reg.ll b/llvm/test/Analysis/CostModel/SystemZ/divrem-reg.ll
index 2f13d7e3ef9b..68ffe5759e13 100644
--- a/llvm/test/Analysis/CostModel/SystemZ/divrem-reg.ll
+++ b/llvm/test/Analysis/CostModel/SystemZ/divrem-reg.ll
@@ -1,6 +1,6 @@
 ; NOTE: Assertions have been autogenerated by utils/update_analyze_test_checks.py UTC_ARGS: --version 4
 ; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=z13 | FileCheck %s --check-prefixes=CHECK,Z13
-; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=arch15 | FileCheck %s --check-prefixes=CHECK,ARC15
+; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=z17 | FileCheck %s --check-prefixes=CHECK,Z17
 
 ; Check costs of divisions by register
 ;
@@ -52,9 +52,9 @@ define <2 x i64> @fun4(<2 x i64> %a, <2 x i64> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 47 for instruction: %r = sdiv <2 x i64> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
-; ARC15-LABEL: 'fun4'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = sdiv <2 x i64> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
+; Z17-LABEL: 'fun4'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = sdiv <2 x i64> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
   %r = sdiv <2 x i64> %a, %b
   ret <2 x i64> %r
@@ -65,9 +65,9 @@ define <4 x i32> @fun5(<4 x i32> %a, <4 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 94 for instruction: %r = sdiv <4 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
-; ARC15-LABEL: 'fun5'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = sdiv <4 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
+; Z17-LABEL: 'fun5'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = sdiv <4 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
   %r = sdiv <4 x i32> %a, %b
   ret <4 x i32> %r
@@ -78,9 +78,9 @@ define <2 x i32> @fun6(<2 x i32> %a, <2 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 48 for instruction: %r = sdiv <2 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
-; ARC15-LABEL: 'fun6'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = sdiv <2 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
+; Z17-LABEL: 'fun6'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = sdiv <2 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
   %r = sdiv <2 x i32> %a, %b
   ret <2 x i32> %r
@@ -167,9 +167,9 @@ define <2 x i64> @fun15(<2 x i64> %a, <2 x i64> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 47 for instruction: %r = udiv <2 x i64> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
-; ARC15-LABEL: 'fun15'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = udiv <2 x i64> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
+; Z17-LABEL: 'fun15'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = udiv <2 x i64> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
   %r = udiv <2 x i64> %a, %b
   ret <2 x i64> %r
@@ -180,9 +180,9 @@ define <4 x i32> @fun16(<4 x i32> %a, <4 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 94 for instruction: %r = udiv <4 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
-; ARC15-LABEL: 'fun16'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = udiv <4 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
+; Z17-LABEL: 'fun16'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = udiv <4 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
   %r = udiv <4 x i32> %a, %b
   ret <4 x i32> %r
@@ -193,9 +193,9 @@ define <2 x i32> @fun17(<2 x i32> %a, <2 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 48 for instruction: %r = udiv <2 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
-; ARC15-LABEL: 'fun17'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = udiv <2 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
+; Z17-LABEL: 'fun17'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = udiv <2 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
   %r = udiv <2 x i32> %a, %b
   ret <2 x i32> %r
@@ -282,9 +282,9 @@ define <2 x i64> @fun26(<2 x i64> %a, <2 x i64> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 47 for instruction: %r = srem <2 x i64> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
-; ARC15-LABEL: 'fun26'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = srem <2 x i64> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
+; Z17-LABEL: 'fun26'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = srem <2 x i64> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
   %r = srem <2 x i64> %a, %b
   ret <2 x i64> %r
@@ -295,9 +295,9 @@ define <4 x i32> @fun27(<4 x i32> %a, <4 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 94 for instruction: %r = srem <4 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
-; ARC15-LABEL: 'fun27'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = srem <4 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
+; Z17-LABEL: 'fun27'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = srem <4 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
   %r = srem <4 x i32> %a, %b
   ret <4 x i32> %r
@@ -308,9 +308,9 @@ define <2 x i32> @fun28(<2 x i32> %a, <2 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 48 for instruction: %r = srem <2 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
-; ARC15-LABEL: 'fun28'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = srem <2 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
+; Z17-LABEL: 'fun28'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = srem <2 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
   %r = srem <2 x i32> %a, %b
   ret <2 x i32> %r
@@ -397,9 +397,9 @@ define <2 x i64> @fun37(<2 x i64> %a, <2 x i64> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 47 for instruction: %r = urem <2 x i64> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
-; ARC15-LABEL: 'fun37'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = urem <2 x i64> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
+; Z17-LABEL: 'fun37'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = urem <2 x i64> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i64> %r
 ;
   %r = urem <2 x i64> %a, %b
   ret <2 x i64> %r
@@ -410,9 +410,9 @@ define <4 x i32> @fun38(<4 x i32> %a, <4 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 94 for instruction: %r = urem <4 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
-; ARC15-LABEL: 'fun38'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = urem <4 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
+; Z17-LABEL: 'fun38'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = urem <4 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <4 x i32> %r
 ;
   %r = urem <4 x i32> %a, %b
   ret <4 x i32> %r
@@ -423,9 +423,9 @@ define <2 x i32> @fun39(<2 x i32> %a, <2 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 48 for instruction: %r = urem <2 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
-; ARC15-LABEL: 'fun39'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = urem <2 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
+; Z17-LABEL: 'fun39'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 20 for instruction: %r = urem <2 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <2 x i32> %r
 ;
   %r = urem <2 x i32> %a, %b
   ret <2 x i32> %r
@@ -473,9 +473,9 @@ define <8 x i64> @fun44(<8 x i64> %a, <8 x i64> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1000 for instruction: %r = sdiv <8 x i64> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <8 x i64> %r
 ;
-; ARC15-LABEL: 'fun44'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 80 for instruction: %r = sdiv <8 x i64> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <8 x i64> %r
+; Z17-LABEL: 'fun44'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 80 for instruction: %r = sdiv <8 x i64> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <8 x i64> %r
 ;
   %r = sdiv <8 x i64> %a, %b
   ret <8 x i64> %r
@@ -486,9 +486,9 @@ define <8 x i32> @fun45(<8 x i32> %a, <8 x i32> %b) {
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1000 for instruction: %r = urem <8 x i32> %a, %b
 ; Z13-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <8 x i32> %r
 ;
-; ARC15-LABEL: 'fun45'
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 40 for instruction: %r = urem <8 x i32> %a, %b
-; ARC15-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <8 x i32> %r
+; Z17-LABEL: 'fun45'
+; Z17-NEXT:  Cost Model: Found an estimated cost of 40 for instruction: %r = urem <8 x i32> %a, %b
+; Z17-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: ret <8 x i32> %r
 ;
   %r = urem <8 x i32> %a, %b
   ret <8 x i32> %r
diff --git a/llvm/test/Analysis/CostModel/SystemZ/i128-cmp-ext-conv.ll b/llvm/test/Analysis/CostModel/SystemZ/i128-cmp-ext-conv.ll
index 105e634cea1a..ba86c9ab1d70 100644
--- a/llvm/test/Analysis/CostModel/SystemZ/i128-cmp-ext-conv.ll
+++ b/llvm/test/Analysis/CostModel/SystemZ/i128-cmp-ext-conv.ll
@@ -1,12 +1,12 @@
 ; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=z13 | FileCheck %s --check-prefixes=CHECK,Z13
-; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=arch15 | FileCheck %s --check-prefixes=CHECK,ARC15
+; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=z17 | FileCheck %s --check-prefixes=CHECK,Z17
 ;
 
 define i128 @fun1(i128 %val1, i128 %val2) {
 ; CHECK-LABEL: 'fun1'
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %cmp = icmp eq i128 %val1, %val2
 ; Z13:   Cost Model: Found an estimated cost of 5 for instruction:   %v128 = sext i1 %cmp to i128
-; ARC15: Cost Model: Found an estimated cost of 0 for instruction:   %v128 = sext i1 %cmp to i128
+; Z17:   Cost Model: Found an estimated cost of 0 for instruction:   %v128 = sext i1 %cmp to i128
   %cmp = icmp eq i128 %val1, %val2
   %v128 = sext i1 %cmp to i128
   ret i128 %v128
@@ -27,7 +27,7 @@ define i128 @fun3(i128 %val1, i128 %val2,
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %cmp = icmp eq i128 %val1, %val2
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %add = add i128 %val3, %val4
 ; Z13:   Cost Model: Found an estimated cost of 4 for instruction:   %sel = select i1 %cmp, i128 %val3, i128 %add
-; ARC15: Cost Model: Found an estimated cost of 1 for instruction:   %sel = select i1 %cmp, i128 %val3, i128 %add
+; Z17:   Cost Model: Found an estimated cost of 1 for instruction:   %sel = select i1 %cmp, i128 %val3, i128 %add
   %cmp = icmp eq i128 %val1, %val2
   %add = add i128 %val3, %val4
   %sel = select i1 %cmp, i128 %val3, i128 %add
@@ -40,7 +40,7 @@ define i64 @fun3_sel64(i128 %val1, i128 %val2,
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %cmp = icmp ugt i128 %val1, %val2
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %add = add i64 %val3, %val4
 ; Z13:   Cost Model: Found an estimated cost of 4 for instruction:   %sel = select i1 %cmp, i64 %val3, i64 %add
-; ARC15: Cost Model: Found an estimated cost of 1 for instruction:   %sel = select i1 %cmp, i64 %val3, i64 %add
+; Z17:   Cost Model: Found an estimated cost of 1 for instruction:   %sel = select i1 %cmp, i64 %val3, i64 %add
   %cmp = icmp ugt i128 %val1, %val2
   %add = add i64 %val3, %val4
   %sel = select i1 %cmp, i64 %val3, i64 %add
diff --git a/llvm/test/Analysis/CostModel/SystemZ/int-arith.ll b/llvm/test/Analysis/CostModel/SystemZ/int-arith.ll
index bf5cbfb48a77..ebeb2df28123 100644
--- a/llvm/test/Analysis/CostModel/SystemZ/int-arith.ll
+++ b/llvm/test/Analysis/CostModel/SystemZ/int-arith.ll
@@ -1,5 +1,5 @@
 ; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=z13 | FileCheck %s
-; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=arch15 | FileCheck %s -check-prefix=ARC15
+; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -mtriple=systemz-unknown -mcpu=z17 | FileCheck %s -check-prefix=Z17
 ;
 ; Note: The scalarized vector instructions costs are not including any
 ; extracts, due to the undef operands.
@@ -132,22 +132,22 @@ define void @mul() {
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res5 = mul <2 x i16> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res6 = mul <2 x i32> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 3 for instruction:   %res7 = mul <2 x i64> undef, undef
-; ARC15: Cost Model: Found an estimated cost of 1 for instruction:   %res7 = mul <2 x i64> undef, undef
+; Z17:   Cost Model: Found an estimated cost of 1 for instruction:   %res7 = mul <2 x i64> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res8 = mul <4 x i8> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res9 = mul <4 x i16> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res10 = mul <4 x i32> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 6 for instruction:   %res11 = mul <4 x i64> undef, undef
-; ARC15: Cost Model: Found an estimated cost of 2 for instruction:   %res11 = mul <4 x i64> undef, undef
+; Z17:   Cost Model: Found an estimated cost of 2 for instruction:   %res11 = mul <4 x i64> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res12 = mul <8 x i8> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res13 = mul <8 x i16> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 2 for instruction:   %res14 = mul <8 x i32> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 12 for instruction:   %res15 = mul <8 x i64> undef, undef
-; ARC15: Cost Model: Found an estimated cost of 4 for instruction:   %res15 = mul <8 x i64> undef, undef
+; Z17:   Cost Model: Found an estimated cost of 4 for instruction:   %res15 = mul <8 x i64> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 1 for instruction:   %res16 = mul <16 x i8> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 2 for instruction:   %res17 = mul <16 x i16> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 4 for instruction:   %res18 = mul <16 x i32> undef, undef
 ; CHECK: Cost Model: Found an estimated cost of 24 for instruction:   %res19 = mul <16 x i64> undef, undef
-; ARC15: Cost Model: Found an estimated cost of 8 for instruction:   %res19 = mul <16 x i64> undef, undef
+; Z17:   Cost Model: Found an estimated cost of 8 for instruction:   %res19 = mul <16 x i64> undef, undef
 
   ret void;
 }
diff --git a/llvm/test/Analysis/LoopAccessAnalysis/underlying-object-different-address-spaces.ll b/llvm/test/Analysis/LoopAccessAnalysis/underlying-object-different-address-spaces.ll
new file mode 100644
index 000000000000..adf73c091be0
--- /dev/null
+++ b/llvm/test/Analysis/LoopAccessAnalysis/underlying-object-different-address-spaces.ll
@@ -0,0 +1,39 @@
+; NOTE: Assertions have been autogenerated by utils/update_analyze_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -passes='print<access-info>' -disable-output %s 2>&1 | FileCheck %s
+
+target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
+
+; Test case for https://github.com/llvm/llvm-project/issues/124759. The same
+; underlying object is access through pointers with different address spaces.
+define void @same_underlying_object_different_address_spaces(ptr %dst1.as1, ptr %dst2.as1) {
+; CHECK-LABEL: 'same_underlying_object_different_address_spaces'
+; CHECK-NEXT:    loop:
+; CHECK-NEXT:      Report: cannot identify array bounds
+; CHECK-NEXT:      Dependences:
+; CHECK-NEXT:      Run-time memory checks:
+; CHECK-NEXT:      Grouped accesses:
+; CHECK-EMPTY:
+; CHECK-NEXT:      Non vectorizable stores to invariant address were not found in loop.
+; CHECK-NEXT:      SCEV assumptions:
+; CHECK-EMPTY:
+; CHECK-NEXT:      Expressions re-written:
+;
+entry:
+  %alloc = alloca i8, i64 0, align 128
+  %as3 = addrspacecast ptr %alloc to ptr addrspace(3)
+  %as4 = addrspacecast ptr %alloc to ptr addrspace(4)
+  br label %loop
+
+loop:
+  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop ]
+  store i32 0, ptr addrspace(4) %as4, align 4
+  store i32 0, ptr %dst1.as1, align 4
+  %l = load i64, ptr addrspace(3) %as3, align 4
+  store i64 %l, ptr %dst2.as1, align 4
+  %iv.next = add i64 %iv, 1
+  %c = icmp eq i64 %iv.next, 100
+  br i1 %c, label %loop, label %exit
+
+exit:
+  ret void
+}
diff --git a/llvm/test/Analysis/ScalarEvolution/pr135531.ll b/llvm/test/Analysis/ScalarEvolution/pr135531.ll
new file mode 100644
index 000000000000..e172d56d3a51
--- /dev/null
+++ b/llvm/test/Analysis/ScalarEvolution/pr135531.ll
@@ -0,0 +1,19 @@
+; NOTE: Assertions have been autogenerated by utils/update_analyze_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -disable-output -passes='print<scalar-evolution>' < %s 2>&1 | FileCheck %s
+
+define i32 @pr135511(i32 %x) {
+; CHECK-LABEL: 'pr135511'
+; CHECK-NEXT:  Classifying expressions for: @pr135511
+; CHECK-NEXT:    %and = and i32 %x, 16382
+; CHECK-NEXT:    --> (2 * (zext i13 (trunc i32 (%x /u 2) to i13) to i32))<nuw><nsw> U: [0,16383) S: [0,16383)
+; CHECK-NEXT:    %neg = sub nsw i32 0, %and
+; CHECK-NEXT:    --> (-2 * (zext i13 (trunc i32 (%x /u 2) to i13) to i32))<nsw> U: [0,-1) S: [-16382,1)
+; CHECK-NEXT:    %res = and i32 %neg, 268431360
+; CHECK-NEXT:    --> (4096 * (zext i16 (trunc i32 ((-1 * (zext i13 (trunc i32 (%x /u 2) to i13) to i32))<nsw> /u 2048) to i16) to i32))<nuw><nsw> U: [0,268431361) S: [0,268431361)
+; CHECK-NEXT:  Determining loop execution counts for: @pr135511
+;
+  %and = and i32 %x, 16382
+  %neg = sub nsw i32 0, %and
+  %res = and i32 %neg, 268431360
+  ret i32 %res
+}
diff --git a/llvm/test/Analysis/ScalarEvolution/trip-count-unknown-stride.ll b/llvm/test/Analysis/ScalarEvolution/trip-count-unknown-stride.ll
index 2d02cb6194f4..1f08a620b2e1 100644
--- a/llvm/test/Analysis/ScalarEvolution/trip-count-unknown-stride.ll
+++ b/llvm/test/Analysis/ScalarEvolution/trip-count-unknown-stride.ll
@@ -329,10 +329,9 @@ define void @ne_nsw_nonneg_step(ptr nocapture %A, i32 %n, i32 %s) mustprogress {
 ;
 ; CHECK-LABEL: 'ne_nsw_nonneg_step'
 ; CHECK-NEXT:  Determining loop execution counts for: @ne_nsw_nonneg_step
-; CHECK-NEXT:  Loop %for.body: backedge-taken count is (((-1 * %s) + %n) /u %s)
-; CHECK-NEXT:  Loop %for.body: constant max backedge-taken count is i32 -1
-; CHECK-NEXT:  Loop %for.body: symbolic max backedge-taken count is (((-1 * %s) + %n) /u %s)
-; CHECK-NEXT:  Loop %for.body: Trip multiple is 1
+; CHECK-NEXT:  Loop %for.body: Unpredictable backedge-taken count.
+; CHECK-NEXT:  Loop %for.body: Unpredictable constant max backedge-taken count.
+; CHECK-NEXT:  Loop %for.body: Unpredictable symbolic max backedge-taken count.
 ;
 entry:
   %nonneg_step = icmp sge i32 %s, 0
@@ -442,10 +441,9 @@ define void @ne_nuw_nonneg_step(ptr nocapture %A, i32 %n, i32 %s) mustprogress {
 ;
 ; CHECK-LABEL: 'ne_nuw_nonneg_step'
 ; CHECK-NEXT:  Determining loop execution counts for: @ne_nuw_nonneg_step
-; CHECK-NEXT:  Loop %for.body: backedge-taken count is (((-1 * %s) + %n) /u %s)
-; CHECK-NEXT:  Loop %for.body: constant max backedge-taken count is i32 -1
-; CHECK-NEXT:  Loop %for.body: symbolic max backedge-taken count is (((-1 * %s) + %n) /u %s)
-; CHECK-NEXT:  Loop %for.body: Trip multiple is 1
+; CHECK-NEXT:  Loop %for.body: Unpredictable backedge-taken count.
+; CHECK-NEXT:  Loop %for.body: Unpredictable constant max backedge-taken count.
+; CHECK-NEXT:  Loop %for.body: Unpredictable symbolic max backedge-taken count.
 ;
 entry:
   %nonneg_step = icmp sge i32 %s, 0
@@ -493,6 +491,26 @@ for.end:                                          ; preds = %for.body, %entry
   ret void
 }
 
+define i32 @pr131465(i1 %x) mustprogress {
+; CHECK-LABEL: 'pr131465'
+; CHECK-NEXT:  Determining loop execution counts for: @pr131465
+; CHECK-NEXT:  Loop %for.body: Unpredictable backedge-taken count.
+; CHECK-NEXT:  Loop %for.body: Unpredictable constant max backedge-taken count.
+; CHECK-NEXT:  Loop %for.body: Unpredictable symbolic max backedge-taken count.
+;
+entry:
+  %inc = zext i1 %x to i32
+  br label %for.body
+
+for.body:
+  %indvar = phi i32 [ 2, %entry ], [ %next, %for.body ]
+  %next = add nsw i32 %indvar, %inc
+  %exitcond = icmp eq i32 %next, 2
+  br i1 %exitcond, label %for.end, label %for.body
+
+for.end:
+  ret i32 0
+}
 
 declare void @llvm.assume(i1)
 
diff --git a/llvm/test/Analysis/ValueTracking/phi-self.ll b/llvm/test/Analysis/ValueTracking/phi-self.ll
new file mode 100644
index 000000000000..17afd872cab0
--- /dev/null
+++ b/llvm/test/Analysis/ValueTracking/phi-self.ll
@@ -0,0 +1,89 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -S -passes=instsimplify < %s | FileCheck %s
+
+; Test `%r` can be replaced by `%nonpoison`.
+
+define i32 @other_noundef(i32 noundef %arg) {
+; CHECK-LABEL: define i32 @other_noundef(
+; CHECK-SAME: i32 noundef [[ARG:%.*]]) {
+; CHECK-NEXT:  [[START:.*]]:
+; CHECK-NEXT:    br label %[[LOOP:.*]]
+; CHECK:       [[LOOP]]:
+; CHECK-NEXT:    [[NONPOISON:%.*]] = phi i32 [ 0, %[[START]] ], [ [[NONPOISON]], %[[BB0:.*]] ], [ [[ARG]], %[[BB1:.*]] ]
+; CHECK-NEXT:    [[I:%.*]] = call i32 @opaque()
+; CHECK-NEXT:    switch i32 [[I]], label %[[EXIT:.*]] [
+; CHECK-NEXT:      i32 0, label %[[BB0]]
+; CHECK-NEXT:      i32 1, label %[[BB1]]
+; CHECK-NEXT:    ]
+; CHECK:       [[EXIT]]:
+; CHECK-NEXT:    ret i32 [[NONPOISON]]
+; CHECK:       [[BB0]]:
+; CHECK-NEXT:    br label %[[LOOP]]
+; CHECK:       [[BB1]]:
+; CHECK-NEXT:    br label %[[LOOP]]
+;
+start:
+  br label %loop
+
+loop:
+  %nonpoison = phi i32 [ 0, %start ], [ %nonpoison, %bb0 ], [ %arg, %bb1 ]
+  %i = call i32 @opaque()
+  switch i32 %i, label %exit [
+  i32 0, label %bb0
+  i32 1, label %bb1
+  ]
+
+exit:
+  %r = freeze i32 %nonpoison
+  ret i32 %r
+
+bb0:
+  br label %loop
+
+bb1:
+  br label %loop
+}
+
+define i32 @other_poison(i32 %arg) {
+; CHECK-LABEL: define i32 @other_poison(
+; CHECK-SAME: i32 [[ARG:%.*]]) {
+; CHECK-NEXT:  [[START:.*]]:
+; CHECK-NEXT:    br label %[[LOOP:.*]]
+; CHECK:       [[LOOP]]:
+; CHECK-NEXT:    [[MAYPOISON:%.*]] = phi i32 [ 0, %[[START]] ], [ [[MAYPOISON]], %[[BB0:.*]] ], [ [[ARG]], %[[BB1:.*]] ]
+; CHECK-NEXT:    [[I:%.*]] = call i32 @opaque()
+; CHECK-NEXT:    switch i32 [[I]], label %[[EXIT:.*]] [
+; CHECK-NEXT:      i32 0, label %[[BB0]]
+; CHECK-NEXT:      i32 1, label %[[BB1]]
+; CHECK-NEXT:    ]
+; CHECK:       [[EXIT]]:
+; CHECK-NEXT:    [[R:%.*]] = freeze i32 [[MAYPOISON]]
+; CHECK-NEXT:    ret i32 [[R]]
+; CHECK:       [[BB0]]:
+; CHECK-NEXT:    br label %[[LOOP]]
+; CHECK:       [[BB1]]:
+; CHECK-NEXT:    br label %[[LOOP]]
+;
+start:
+  br label %loop
+
+loop:
+  %maypoison = phi i32 [ 0, %start ], [ %maypoison, %bb0 ], [ %arg, %bb1 ]
+  %i = call i32 @opaque()
+  switch i32 %i, label %exit [
+  i32 0, label %bb0
+  i32 1, label %bb1
+  ]
+
+exit:
+  %r = freeze i32 %maypoison
+  ret i32 %r
+
+bb0:
+  br label %loop
+
+bb1:
+  br label %loop
+}
+
+declare i32 @opaque()
diff --git a/llvm/test/CodeGen/AArch64/16bit-float-promotion-with-nofp.ll b/llvm/test/CodeGen/AArch64/16bit-float-promotion-with-nofp.ll
index bfe9ab8424bb..f560420e2c92 100644
--- a/llvm/test/CodeGen/AArch64/16bit-float-promotion-with-nofp.ll
+++ b/llvm/test/CodeGen/AArch64/16bit-float-promotion-with-nofp.ll
@@ -29,3 +29,94 @@ entry:
   ret bfloat %0
 }
 
+define double @select_f64(double %a, double %b, i1 %c) {
+; CHECK-LABEL: select_f64:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    tst w2, #0x1
+; CHECK-NEXT:    csel x0, x0, x1, ne
+; CHECK-NEXT:    ret
+entry:
+  %0 = select i1 %c, double %a, double %b
+  ret double %0
+}
+
+define float @select_f32(float %a, float %b, i1 %c) {
+; CHECK-LABEL: select_f32:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    tst w2, #0x1
+; CHECK-NEXT:    csel w0, w0, w1, ne
+; CHECK-NEXT:    ret
+entry:
+  %0 = select i1 %c, float %a, float %b
+  ret float %0
+}
+
+define half @select_f16(half %a, half %b, i1 %c) {
+; CHECK-LABEL: select_f16:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    tst w2, #0x1
+; CHECK-NEXT:    csel w0, w0, w1, ne
+; CHECK-NEXT:    ret
+entry:
+  %0 = select i1 %c, half %a, half %b
+  ret half %0
+}
+
+define bfloat @select_bf16(bfloat %a, bfloat %b, i1 %c) {
+; CHECK-LABEL: select_bf16:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    tst w2, #0x1
+; CHECK-NEXT:    csel w0, w0, w1, ne
+; CHECK-NEXT:    ret
+entry:
+  %0 = select i1 %c, bfloat %a, bfloat %b
+  ret bfloat %0
+}
+
+define double @selectcc_f64(double %a, double %b, i32 %d) {
+; CHECK-LABEL: selectcc_f64:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    cmp w2, #0
+; CHECK-NEXT:    csel x0, x0, x1, lt
+; CHECK-NEXT:    ret
+entry:
+  %c = icmp slt i32 %d, 0
+  %0 = select i1 %c, double %a, double %b
+  ret double %0
+}
+
+define float @selectcc_f32(float %a, float %b, i32 %d) {
+; CHECK-LABEL: selectcc_f32:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    cmp w2, #0
+; CHECK-NEXT:    csel w0, w0, w1, lt
+; CHECK-NEXT:    ret
+entry:
+  %c = icmp slt i32 %d, 0
+  %0 = select i1 %c, float %a, float %b
+  ret float %0
+}
+
+define half @selectcc_f16(half %a, half %b, i32 %d) {
+; CHECK-LABEL: selectcc_f16:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    cmp w2, #0
+; CHECK-NEXT:    csel w0, w0, w1, lt
+; CHECK-NEXT:    ret
+entry:
+  %c = icmp slt i32 %d, 0
+  %0 = select i1 %c, half %a, half %b
+  ret half %0
+}
+
+define bfloat @selectcc_bf16(bfloat %a, bfloat %b, i32 %d) {
+; CHECK-LABEL: selectcc_bf16:
+; CHECK:       // %bb.0: // %entry
+; CHECK-NEXT:    cmp w2, #0
+; CHECK-NEXT:    csel w0, w0, w1, lt
+; CHECK-NEXT:    ret
+entry:
+  %c = icmp slt i32 %d, 0
+  %0 = select i1 %c, bfloat %a, bfloat %b
+  ret bfloat %0
+}
diff --git a/llvm/test/CodeGen/AArch64/aarch64-swp-ws-live-intervals.mir b/llvm/test/CodeGen/AArch64/aarch64-swp-ws-live-intervals.mir
new file mode 100644
index 000000000000..48f02452e359
--- /dev/null
+++ b/llvm/test/CodeGen/AArch64/aarch64-swp-ws-live-intervals.mir
@@ -0,0 +1,103 @@
+# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
+# RUN: llc --mtriple=aarch64 %s -run-pass=pipeliner -o - | FileCheck %s
+
+...
+---
+name:            foo
+tracksRegLiveness: true
+body:             |
+  ; CHECK-LABEL: name: foo
+  ; CHECK: bb.0:
+  ; CHECK-NEXT:   successors: %bb.3(0x80000000)
+  ; CHECK-NEXT:   liveins: $x0
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr64common = COPY $x0
+  ; CHECK-NEXT:   [[FMOVD0_:%[0-9]+]]:fpr64 = FMOVD0
+  ; CHECK-NEXT:   [[MOVi32imm:%[0-9]+]]:gpr32 = MOVi32imm 1
+  ; CHECK-NEXT:   [[SUBREG_TO_REG:%[0-9]+]]:gpr64sp = SUBREG_TO_REG 0, [[MOVi32imm]], %subreg.sub_32
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.3:
+  ; CHECK-NEXT:   successors: %bb.4(0x40000000), %bb.7(0x40000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[FADDDrr:%[0-9]+]]:fpr64 = nofpexcept FADDDrr [[FMOVD0_]], [[FMOVD0_]], implicit $fpcr
+  ; CHECK-NEXT:   [[SUBSXri:%[0-9]+]]:gpr64 = nsw SUBSXri [[SUBREG_TO_REG]], 1, 0, implicit-def $nzcv
+  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr64sp = COPY [[SUBSXri]]
+  ; CHECK-NEXT:   [[FMOVDi:%[0-9]+]]:fpr64 = FMOVDi 112
+  ; CHECK-NEXT:   Bcc 0, %bb.7, implicit $nzcv
+  ; CHECK-NEXT:   B %bb.4
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.4:
+  ; CHECK-NEXT:   successors: %bb.5(0x80000000), %bb.6(0x00000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[FADDDrr1:%[0-9]+]]:fpr64 = nofpexcept FADDDrr [[FADDDrr]], [[FMOVD0_]], implicit $fpcr
+  ; CHECK-NEXT:   [[FADDDrr2:%[0-9]+]]:fpr64 = nofpexcept FADDDrr [[FMOVD0_]], [[FMOVD0_]], implicit $fpcr
+  ; CHECK-NEXT:   [[SUBSXri1:%[0-9]+]]:gpr64 = nsw SUBSXri [[COPY1]], 1, 0, implicit-def $nzcv
+  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr64all = COPY [[SUBSXri1]]
+  ; CHECK-NEXT:   [[FMOVDi1:%[0-9]+]]:fpr64 = FMOVDi 112
+  ; CHECK-NEXT:   Bcc 0, %bb.6, implicit $nzcv
+  ; CHECK-NEXT:   B %bb.5
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.5:
+  ; CHECK-NEXT:   successors: %bb.6(0x04000000), %bb.5(0x7c000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:gpr64sp = PHI [[COPY2]], %bb.4, %24, %bb.5
+  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:fpr64 = PHI [[FMOVDi1]], %bb.4, %25, %bb.5
+  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:fpr64 = PHI [[FMOVDi]], %bb.4, [[PHI1]], %bb.5
+  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:fpr64 = PHI [[FADDDrr2]], %bb.4, %22, %bb.5
+  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:fpr64 = PHI [[FADDDrr1]], %bb.4, %23, %bb.5
+  ; CHECK-NEXT:   [[SUBSXri2:%[0-9]+]]:gpr64 = nsw SUBSXri [[PHI]], 1, 0, implicit-def $nzcv
+  ; CHECK-NEXT:   [[FADDDrr3:%[0-9]+]]:fpr64 = nofpexcept FADDDrr [[PHI2]], [[FMOVD0_]], implicit $fpcr
+  ; CHECK-NEXT:   [[FADDDrr4:%[0-9]+]]:fpr64 = nofpexcept FADDDrr [[PHI3]], [[PHI2]], implicit $fpcr
+  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr64all = COPY [[SUBSXri2]]
+  ; CHECK-NEXT:   STRDui [[PHI4]], [[COPY]], 0
+  ; CHECK-NEXT:   [[FMOVDi2:%[0-9]+]]:fpr64 = FMOVDi 112
+  ; CHECK-NEXT:   Bcc 1, %bb.5, implicit $nzcv
+  ; CHECK-NEXT:   B %bb.6
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.6:
+  ; CHECK-NEXT:   successors: %bb.7(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:fpr64 = PHI [[FMOVDi]], %bb.4, [[PHI1]], %bb.5
+  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:fpr64 = PHI [[FADDDrr2]], %bb.4, [[FADDDrr3]], %bb.5
+  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:fpr64 = PHI [[FADDDrr1]], %bb.4, [[FADDDrr4]], %bb.5
+  ; CHECK-NEXT:   STRDui [[PHI7]], [[COPY]], 0
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.7:
+  ; CHECK-NEXT:   successors: %bb.2(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:fpr64 = PHI [[FMOVD0_]], %bb.3, [[PHI5]], %bb.6
+  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:fpr64 = PHI [[FADDDrr]], %bb.3, [[PHI6]], %bb.6
+  ; CHECK-NEXT:   [[FADDDrr5:%[0-9]+]]:fpr64 = nofpexcept FADDDrr [[PHI9]], [[PHI8]], implicit $fpcr
+  ; CHECK-NEXT:   STRDui [[FADDDrr5]], [[COPY]], 0
+  ; CHECK-NEXT:   B %bb.2
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.2:
+  ; CHECK-NEXT:   RET_ReallyLR
+  bb.0:
+    successors: %bb.1(0x80000000)
+    liveins: $x0
+
+    %0:gpr64common = COPY $x0
+    %1:fpr64 = FMOVD0
+    %2:gpr32 = MOVi32imm 1
+    %3:gpr64all = SUBREG_TO_REG 0, killed %2, %subreg.sub_32
+
+  bb.1:
+    successors: %bb.2(0x04000000), %bb.1(0x7c000000)
+
+    %4:gpr64sp = PHI %3, %bb.0, %5, %bb.1
+    %6:fpr64 = PHI %1, %bb.0, %7, %bb.1
+    %8:fpr64 = PHI %1, %bb.0, %6, %bb.1
+    %9:fpr64 = nofpexcept FADDDrr %8, %1, implicit $fpcr
+    %10:fpr64 = nofpexcept FADDDrr killed %9, %6, implicit $fpcr
+    STRDui killed %10, %0, 0
+    %11:gpr64 = nsw SUBSXri %4, 1, 0, implicit-def $nzcv
+    %5:gpr64all = COPY %11
+    %7:fpr64 = FMOVDi 112
+    Bcc 1, %bb.1, implicit $nzcv
+    B %bb.2
+
+  bb.2:
+    RET_ReallyLR
+
+...
diff --git a/llvm/test/CodeGen/AArch64/arm64-popcnt.ll b/llvm/test/CodeGen/AArch64/arm64-popcnt.ll
index ad0904ff9808..d06e42f5405e 100644
--- a/llvm/test/CodeGen/AArch64/arm64-popcnt.ll
+++ b/llvm/test/CodeGen/AArch64/arm64-popcnt.ll
@@ -2,6 +2,7 @@
 ; RUN: llc < %s -mtriple=arm64-eabi -aarch64-neon-syntax=apple | FileCheck %s
 ; RUN: llc < %s -mtriple=aarch64 -mattr -neon -aarch64-neon-syntax=apple | FileCheck -check-prefix=CHECK-NONEON %s
 ; RUN: llc < %s -mtriple=aarch64 -mattr +cssc -aarch64-neon-syntax=apple | FileCheck -check-prefix=CHECK-CSSC %s
+; RUN: llc < %s -mtriple=aarch64_be-none-eabi | FileCheck %s --check-prefix=CHECK-BE
 
 define i32 @cnt32_advsimd(i32 %x) nounwind readnone {
 ; CHECK-LABEL: cnt32_advsimd:
@@ -32,6 +33,14 @@ define i32 @cnt32_advsimd(i32 %x) nounwind readnone {
 ; CHECK-CSSC:       // %bb.0:
 ; CHECK-CSSC-NEXT:    cnt w0, w0
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: cnt32_advsimd:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    fmov s0, w0
+; CHECK-BE-NEXT:    cnt v0.8b, v0.8b
+; CHECK-BE-NEXT:    addv b0, v0.8b
+; CHECK-BE-NEXT:    fmov w0, s0
+; CHECK-BE-NEXT:    ret
   %cnt = tail call i32 @llvm.ctpop.i32(i32 %x)
   ret i32 %cnt
 }
@@ -69,6 +78,16 @@ define i32 @cnt32_advsimd_2(<2 x i32> %x) {
 ; CHECK-CSSC-NEXT:    fmov w8, s0
 ; CHECK-CSSC-NEXT:    cnt w0, w8
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: cnt32_advsimd_2:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
+; CHECK-BE-NEXT:    fmov w8, s0
+; CHECK-BE-NEXT:    fmov s0, w8
+; CHECK-BE-NEXT:    cnt v0.8b, v0.8b
+; CHECK-BE-NEXT:    addv b0, v0.8b
+; CHECK-BE-NEXT:    fmov w0, s0
+; CHECK-BE-NEXT:    ret
   %1 = extractelement <2 x i32> %x, i64 0
   %2 = tail call i32 @llvm.ctpop.i32(i32 %1)
   ret i32 %2
@@ -103,6 +122,15 @@ define i64 @cnt64_advsimd(i64 %x) nounwind readnone {
 ; CHECK-CSSC:       // %bb.0:
 ; CHECK-CSSC-NEXT:    cnt x0, x0
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: cnt64_advsimd:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    fmov d0, x0
+; CHECK-BE-NEXT:    rev64 v0.8b, v0.8b
+; CHECK-BE-NEXT:    cnt v0.8b, v0.8b
+; CHECK-BE-NEXT:    addv b0, v0.8b
+; CHECK-BE-NEXT:    fmov x0, d0
+; CHECK-BE-NEXT:    ret
   %cnt = tail call i64 @llvm.ctpop.i64(i64 %x)
   ret i64 %cnt
 }
@@ -147,6 +175,22 @@ define i32 @cnt32(i32 %x) nounwind readnone noimplicitfloat {
 ; CHECK-CSSC:       // %bb.0:
 ; CHECK-CSSC-NEXT:    cnt w0, w0
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: cnt32:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    lsr w9, w0, #1
+; CHECK-BE-NEXT:    mov w8, #16843009 // =0x1010101
+; CHECK-BE-NEXT:    and w9, w9, #0x55555555
+; CHECK-BE-NEXT:    sub w9, w0, w9
+; CHECK-BE-NEXT:    lsr w10, w9, #2
+; CHECK-BE-NEXT:    and w9, w9, #0x33333333
+; CHECK-BE-NEXT:    and w10, w10, #0x33333333
+; CHECK-BE-NEXT:    add w9, w9, w10
+; CHECK-BE-NEXT:    add w9, w9, w9, lsr #4
+; CHECK-BE-NEXT:    and w9, w9, #0xf0f0f0f
+; CHECK-BE-NEXT:    mul w8, w9, w8
+; CHECK-BE-NEXT:    lsr w0, w8, #24
+; CHECK-BE-NEXT:    ret
   %cnt = tail call i32 @llvm.ctpop.i32(i32 %x)
   ret i32 %cnt
 }
@@ -188,6 +232,22 @@ define i64 @cnt64(i64 %x) nounwind readnone noimplicitfloat {
 ; CHECK-CSSC:       // %bb.0:
 ; CHECK-CSSC-NEXT:    cnt x0, x0
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: cnt64:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    lsr x9, x0, #1
+; CHECK-BE-NEXT:    mov x8, #72340172838076673 // =0x101010101010101
+; CHECK-BE-NEXT:    and x9, x9, #0x5555555555555555
+; CHECK-BE-NEXT:    sub x9, x0, x9
+; CHECK-BE-NEXT:    lsr x10, x9, #2
+; CHECK-BE-NEXT:    and x9, x9, #0x3333333333333333
+; CHECK-BE-NEXT:    and x10, x10, #0x3333333333333333
+; CHECK-BE-NEXT:    add x9, x9, x10
+; CHECK-BE-NEXT:    add x9, x9, x9, lsr #4
+; CHECK-BE-NEXT:    and x9, x9, #0xf0f0f0f0f0f0f0f
+; CHECK-BE-NEXT:    mul x8, x9, x8
+; CHECK-BE-NEXT:    lsr x0, x8, #56
+; CHECK-BE-NEXT:    ret
   %cnt = tail call i64 @llvm.ctpop.i64(i64 %x)
   ret i64 %cnt
 }
@@ -215,6 +275,14 @@ define i32 @ctpop_eq_one(i64 %x) nounwind readnone {
 ; CHECK-CSSC-NEXT:    cmp x8, #1
 ; CHECK-CSSC-NEXT:    cset w0, eq
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: ctpop_eq_one:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    sub x8, x0, #1
+; CHECK-BE-NEXT:    eor x9, x0, x8
+; CHECK-BE-NEXT:    cmp x9, x8
+; CHECK-BE-NEXT:    cset w0, hi
+; CHECK-BE-NEXT:    ret
   %count = tail call i64 @llvm.ctpop.i64(i64 %x)
   %cmp = icmp eq i64 %count, 1
   %conv = zext i1 %cmp to i32
@@ -244,6 +312,14 @@ define i32 @ctpop_ne_one(i64 %x) nounwind readnone {
 ; CHECK-CSSC-NEXT:    cmp x8, #1
 ; CHECK-CSSC-NEXT:    cset w0, ne
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: ctpop_ne_one:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    sub x8, x0, #1
+; CHECK-BE-NEXT:    eor x9, x0, x8
+; CHECK-BE-NEXT:    cmp x9, x8
+; CHECK-BE-NEXT:    cset w0, ls
+; CHECK-BE-NEXT:    ret
   %count = tail call i64 @llvm.ctpop.i64(i64 %x)
   %cmp = icmp ne i64 %count, 1
   %conv = zext i1 %cmp to i32
@@ -273,6 +349,14 @@ define i1 @ctpop32_ne_one(i32 %x) nounwind readnone {
 ; CHECK-CSSC-NEXT:    cmp w8, #1
 ; CHECK-CSSC-NEXT:    cset w0, ne
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: ctpop32_ne_one:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    sub w8, w0, #1
+; CHECK-BE-NEXT:    eor w9, w0, w8
+; CHECK-BE-NEXT:    cmp w9, w8
+; CHECK-BE-NEXT:    cset w0, ls
+; CHECK-BE-NEXT:    ret
   %count = tail call i32 @llvm.ctpop.i32(i32 %x)
   %cmp = icmp ne i32 %count, 1
   ret i1 %cmp
@@ -299,6 +383,13 @@ define i1 @ctpop32_eq_one_nonzero(i32 %x) {
 ; CHECK-CSSC-NEXT:    tst w0, w8
 ; CHECK-CSSC-NEXT:    cset w0, eq
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: ctpop32_eq_one_nonzero:
+; CHECK-BE:       // %bb.0: // %entry
+; CHECK-BE-NEXT:    sub w8, w0, #1
+; CHECK-BE-NEXT:    tst w0, w8
+; CHECK-BE-NEXT:    cset w0, eq
+; CHECK-BE-NEXT:    ret
 entry:
   %popcnt = call range(i32 1, 33) i32 @llvm.ctpop.i32(i32 %x)
   %cmp = icmp eq i32 %popcnt, 1
@@ -326,11 +417,79 @@ define i1 @ctpop32_ne_one_nonzero(i32 %x) {
 ; CHECK-CSSC-NEXT:    tst w0, w8
 ; CHECK-CSSC-NEXT:    cset w0, ne
 ; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: ctpop32_ne_one_nonzero:
+; CHECK-BE:       // %bb.0: // %entry
+; CHECK-BE-NEXT:    sub w8, w0, #1
+; CHECK-BE-NEXT:    tst w0, w8
+; CHECK-BE-NEXT:    cset w0, ne
+; CHECK-BE-NEXT:    ret
 entry:
   %popcnt = tail call range(i32 1, 33) i32 @llvm.ctpop.i32(i32 %x)
   %cmp = icmp ne i32 %popcnt, 1
   ret i1 %cmp
 }
 
+define i128 @cnt128(i128 %x) nounwind readnone {
+; CHECK-LABEL: cnt128:
+; CHECK:       // %bb.0:
+; CHECK-NEXT:    fmov d0, x0
+; CHECK-NEXT:    mov.d v0[1], x1
+; CHECK-NEXT:    mov x1, xzr
+; CHECK-NEXT:    cnt.16b v0, v0
+; CHECK-NEXT:    addv.16b b0, v0
+; CHECK-NEXT:    fmov x0, d0
+; CHECK-NEXT:    ret
+;
+; CHECK-NONEON-LABEL: cnt128:
+; CHECK-NONEON:       // %bb.0:
+; CHECK-NONEON-NEXT:    lsr x9, x0, #1
+; CHECK-NONEON-NEXT:    lsr x10, x1, #1
+; CHECK-NONEON-NEXT:    mov x8, #72340172838076673 // =0x101010101010101
+; CHECK-NONEON-NEXT:    and x9, x9, #0x5555555555555555
+; CHECK-NONEON-NEXT:    and x10, x10, #0x5555555555555555
+; CHECK-NONEON-NEXT:    sub x9, x0, x9
+; CHECK-NONEON-NEXT:    sub x10, x1, x10
+; CHECK-NONEON-NEXT:    mov x1, xzr
+; CHECK-NONEON-NEXT:    lsr x11, x9, #2
+; CHECK-NONEON-NEXT:    lsr x12, x10, #2
+; CHECK-NONEON-NEXT:    and x9, x9, #0x3333333333333333
+; CHECK-NONEON-NEXT:    and x10, x10, #0x3333333333333333
+; CHECK-NONEON-NEXT:    and x11, x11, #0x3333333333333333
+; CHECK-NONEON-NEXT:    add x9, x9, x11
+; CHECK-NONEON-NEXT:    and x11, x12, #0x3333333333333333
+; CHECK-NONEON-NEXT:    add x9, x9, x9, lsr #4
+; CHECK-NONEON-NEXT:    add x10, x10, x11
+; CHECK-NONEON-NEXT:    add x10, x10, x10, lsr #4
+; CHECK-NONEON-NEXT:    and x9, x9, #0xf0f0f0f0f0f0f0f
+; CHECK-NONEON-NEXT:    mul x9, x9, x8
+; CHECK-NONEON-NEXT:    and x10, x10, #0xf0f0f0f0f0f0f0f
+; CHECK-NONEON-NEXT:    mul x8, x10, x8
+; CHECK-NONEON-NEXT:    lsr x9, x9, #56
+; CHECK-NONEON-NEXT:    add x0, x9, x8, lsr #56
+; CHECK-NONEON-NEXT:    ret
+;
+; CHECK-CSSC-LABEL: cnt128:
+; CHECK-CSSC:       // %bb.0:
+; CHECK-CSSC-NEXT:    cnt x8, x1
+; CHECK-CSSC-NEXT:    cnt x9, x0
+; CHECK-CSSC-NEXT:    mov x1, xzr
+; CHECK-CSSC-NEXT:    add x0, x9, x8
+; CHECK-CSSC-NEXT:    ret
+;
+; CHECK-BE-LABEL: cnt128:
+; CHECK-BE:       // %bb.0:
+; CHECK-BE-NEXT:    fmov d0, x0
+; CHECK-BE-NEXT:    mov x0, xzr
+; CHECK-BE-NEXT:    mov v0.d[1], x1
+; CHECK-BE-NEXT:    rev64 v0.16b, v0.16b
+; CHECK-BE-NEXT:    cnt v0.16b, v0.16b
+; CHECK-BE-NEXT:    addv b0, v0.16b
+; CHECK-BE-NEXT:    fmov x1, d0
+; CHECK-BE-NEXT:    ret
+  %cnt = tail call i128 @llvm.ctpop.i128(i128 %x)
+  ret i128 %cnt
+}
+
 declare i32 @llvm.ctpop.i32(i32) nounwind readnone
 declare i64 @llvm.ctpop.i64(i64) nounwind readnone
diff --git a/llvm/test/CodeGen/AArch64/complex-deinterleaving-unrolled-cdot.ll b/llvm/test/CodeGen/AArch64/complex-deinterleaving-unrolled-cdot.ll
new file mode 100644
index 000000000000..faefaf9bad7b
--- /dev/null
+++ b/llvm/test/CodeGen/AArch64/complex-deinterleaving-unrolled-cdot.ll
@@ -0,0 +1,191 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -S --passes=complex-deinterleaving %s --mattr=+sve2 -o - | FileCheck %s --check-prefix=CHECK-SVE2
+; RUN: opt -S --passes=complex-deinterleaving %s --mattr=+sve -o - | FileCheck %s --check-prefix=CHECK-SVE
+; RUN: opt -S --passes=complex-deinterleaving %s -o - | FileCheck %s --check-prefix=CHECK-NOSVE
+
+target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
+target triple = "aarch64-none-unknown-elf"
+
+define i32 @cdotp_i8_rot0(<vscale x 32 x i8> %a0, <vscale x 32 x i8> %b0, <vscale x 32 x i8> %a1, <vscale x 32 x i8> %b1) {
+; CHECK-SVE2-LABEL: define i32 @cdotp_i8_rot0(
+; CHECK-SVE2-SAME: <vscale x 32 x i8> [[A0:%.*]], <vscale x 32 x i8> [[B0:%.*]], <vscale x 32 x i8> [[A1:%.*]], <vscale x 32 x i8> [[B1:%.*]]) #[[ATTR0:[0-9]+]] {
+; CHECK-SVE2-NEXT:  [[ENTRY:.*]]:
+; CHECK-SVE2-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK-SVE2:       [[VECTOR_BODY]]:
+; CHECK-SVE2-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[ENTRY]] ], [ [[PARTIAL_REDUCE33:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-SVE2-NEXT:    [[VEC_PHI25:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[ENTRY]] ], [ [[PARTIAL_REDUCE34:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-SVE2-NEXT:    [[A0_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[A0]])
+; CHECK-SVE2-NEXT:    [[A0_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A0_DEINTERLEAVED]], 0
+; CHECK-SVE2-NEXT:    [[A0_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A0_DEINTERLEAVED]], 1
+; CHECK-SVE2-NEXT:    [[A1_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[A1]])
+; CHECK-SVE2-NEXT:    [[A1_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A1_DEINTERLEAVED]], 0
+; CHECK-SVE2-NEXT:    [[A1_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A1_DEINTERLEAVED]], 1
+; CHECK-SVE2-NEXT:    [[A0_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[A0_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[A1_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[A1_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[B0_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[B0]])
+; CHECK-SVE2-NEXT:    [[B0_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B0_DEINTERLEAVED]], 0
+; CHECK-SVE2-NEXT:    [[B0_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B0_DEINTERLEAVED]], 1
+; CHECK-SVE2-NEXT:    [[B1_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[B1]])
+; CHECK-SVE2-NEXT:    [[B1_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B1_DEINTERLEAVED]], 0
+; CHECK-SVE2-NEXT:    [[B1_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B1_DEINTERLEAVED]], 1
+; CHECK-SVE2-NEXT:    [[B0_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[B0_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[B1_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[B1_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[TMP0:%.*]] = mul nsw <vscale x 16 x i32> [[B0_REAL_EXT]], [[A0_REAL_EXT]]
+; CHECK-SVE2-NEXT:    [[TMP1:%.*]] = mul nsw <vscale x 16 x i32> [[B1_REAL_EXT]], [[A1_REAL_EXT]]
+; CHECK-SVE2-NEXT:    [[A0_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[A0_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[A1_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[A1_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[B0_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[B0_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[B1_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[B1_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE2-NEXT:    [[TMP2:%.*]] = mul nsw <vscale x 16 x i32> [[B0_IMAG_EXT]], [[A0_IMAG_EXT]]
+; CHECK-SVE2-NEXT:    [[TMP3:%.*]] = mul nsw <vscale x 16 x i32> [[B1_IMAG_EXT]], [[A1_IMAG_EXT]]
+; CHECK-SVE2-NEXT:    [[PARTIAL_REDUCE:%.*]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI]], <vscale x 16 x i32> [[TMP0]])
+; CHECK-SVE2-NEXT:    [[PARTIAL_REDUCE32:%.*]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI25]], <vscale x 16 x i32> [[TMP1]])
+; CHECK-SVE2-NEXT:    [[TMP4:%.*]] = sub nsw <vscale x 16 x i32> zeroinitializer, [[TMP2]]
+; CHECK-SVE2-NEXT:    [[TMP5:%.*]] = sub nsw <vscale x 16 x i32> zeroinitializer, [[TMP3]]
+; CHECK-SVE2-NEXT:    [[PARTIAL_REDUCE33]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[PARTIAL_REDUCE]], <vscale x 16 x i32> [[TMP4]])
+; CHECK-SVE2-NEXT:    [[PARTIAL_REDUCE34]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[PARTIAL_REDUCE32]], <vscale x 16 x i32> [[TMP5]])
+; CHECK-SVE2-NEXT:    br i1 true, label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]]
+; CHECK-SVE2:       [[MIDDLE_BLOCK]]:
+; CHECK-SVE2-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 4 x i32> [[PARTIAL_REDUCE34]], [[PARTIAL_REDUCE33]]
+; CHECK-SVE2-NEXT:    [[TMP23:%.*]] = tail call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[BIN_RDX]])
+; CHECK-SVE2-NEXT:    ret i32 [[TMP23]]
+;
+; CHECK-SVE-LABEL: define i32 @cdotp_i8_rot0(
+; CHECK-SVE-SAME: <vscale x 32 x i8> [[A0:%.*]], <vscale x 32 x i8> [[B0:%.*]], <vscale x 32 x i8> [[A1:%.*]], <vscale x 32 x i8> [[B1:%.*]]) #[[ATTR0:[0-9]+]] {
+; CHECK-SVE-NEXT:  [[ENTRY:.*]]:
+; CHECK-SVE-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK-SVE:       [[VECTOR_BODY]]:
+; CHECK-SVE-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[ENTRY]] ], [ [[PARTIAL_REDUCE33:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-SVE-NEXT:    [[VEC_PHI25:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[ENTRY]] ], [ [[PARTIAL_REDUCE34:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-SVE-NEXT:    [[A0_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[A0]])
+; CHECK-SVE-NEXT:    [[A0_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A0_DEINTERLEAVED]], 0
+; CHECK-SVE-NEXT:    [[A0_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A0_DEINTERLEAVED]], 1
+; CHECK-SVE-NEXT:    [[A1_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[A1]])
+; CHECK-SVE-NEXT:    [[A1_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A1_DEINTERLEAVED]], 0
+; CHECK-SVE-NEXT:    [[A1_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A1_DEINTERLEAVED]], 1
+; CHECK-SVE-NEXT:    [[A0_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[A0_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[A1_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[A1_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[B0_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[B0]])
+; CHECK-SVE-NEXT:    [[B0_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B0_DEINTERLEAVED]], 0
+; CHECK-SVE-NEXT:    [[B0_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B0_DEINTERLEAVED]], 1
+; CHECK-SVE-NEXT:    [[B1_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[B1]])
+; CHECK-SVE-NEXT:    [[B1_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B1_DEINTERLEAVED]], 0
+; CHECK-SVE-NEXT:    [[B1_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B1_DEINTERLEAVED]], 1
+; CHECK-SVE-NEXT:    [[B0_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[B0_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[B1_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[B1_REAL]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[TMP0:%.*]] = mul nsw <vscale x 16 x i32> [[B0_REAL_EXT]], [[A0_REAL_EXT]]
+; CHECK-SVE-NEXT:    [[TMP1:%.*]] = mul nsw <vscale x 16 x i32> [[B1_REAL_EXT]], [[A1_REAL_EXT]]
+; CHECK-SVE-NEXT:    [[A0_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[A0_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[A1_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[A1_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[B0_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[B0_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[B1_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[B1_IMAG]] to <vscale x 16 x i32>
+; CHECK-SVE-NEXT:    [[TMP2:%.*]] = mul nsw <vscale x 16 x i32> [[B0_IMAG_EXT]], [[A0_IMAG_EXT]]
+; CHECK-SVE-NEXT:    [[TMP3:%.*]] = mul nsw <vscale x 16 x i32> [[B1_IMAG_EXT]], [[A1_IMAG_EXT]]
+; CHECK-SVE-NEXT:    [[PARTIAL_REDUCE:%.*]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI]], <vscale x 16 x i32> [[TMP0]])
+; CHECK-SVE-NEXT:    [[PARTIAL_REDUCE32:%.*]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI25]], <vscale x 16 x i32> [[TMP1]])
+; CHECK-SVE-NEXT:    [[TMP4:%.*]] = sub nsw <vscale x 16 x i32> zeroinitializer, [[TMP2]]
+; CHECK-SVE-NEXT:    [[TMP5:%.*]] = sub nsw <vscale x 16 x i32> zeroinitializer, [[TMP3]]
+; CHECK-SVE-NEXT:    [[PARTIAL_REDUCE33]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[PARTIAL_REDUCE]], <vscale x 16 x i32> [[TMP4]])
+; CHECK-SVE-NEXT:    [[PARTIAL_REDUCE34]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[PARTIAL_REDUCE32]], <vscale x 16 x i32> [[TMP5]])
+; CHECK-SVE-NEXT:    br i1 true, label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]]
+; CHECK-SVE:       [[MIDDLE_BLOCK]]:
+; CHECK-SVE-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 4 x i32> [[PARTIAL_REDUCE34]], [[PARTIAL_REDUCE33]]
+; CHECK-SVE-NEXT:    [[TMP6:%.*]] = tail call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[BIN_RDX]])
+; CHECK-SVE-NEXT:    ret i32 [[TMP6]]
+;
+; CHECK-NOSVE-LABEL: define i32 @cdotp_i8_rot0(
+; CHECK-NOSVE-SAME: <vscale x 32 x i8> [[A0:%.*]], <vscale x 32 x i8> [[B0:%.*]], <vscale x 32 x i8> [[A1:%.*]], <vscale x 32 x i8> [[B1:%.*]]) {
+; CHECK-NOSVE-NEXT:  [[ENTRY:.*]]:
+; CHECK-NOSVE-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK-NOSVE:       [[VECTOR_BODY]]:
+; CHECK-NOSVE-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[ENTRY]] ], [ [[PARTIAL_REDUCE33:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NOSVE-NEXT:    [[VEC_PHI25:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[ENTRY]] ], [ [[PARTIAL_REDUCE34:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NOSVE-NEXT:    [[A0_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[A0]])
+; CHECK-NOSVE-NEXT:    [[A0_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A0_DEINTERLEAVED]], 0
+; CHECK-NOSVE-NEXT:    [[A0_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A0_DEINTERLEAVED]], 1
+; CHECK-NOSVE-NEXT:    [[A1_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[A1]])
+; CHECK-NOSVE-NEXT:    [[A1_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A1_DEINTERLEAVED]], 0
+; CHECK-NOSVE-NEXT:    [[A1_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[A1_DEINTERLEAVED]], 1
+; CHECK-NOSVE-NEXT:    [[A0_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[A0_REAL]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[A1_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[A1_REAL]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[B0_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[B0]])
+; CHECK-NOSVE-NEXT:    [[B0_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B0_DEINTERLEAVED]], 0
+; CHECK-NOSVE-NEXT:    [[B0_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B0_DEINTERLEAVED]], 1
+; CHECK-NOSVE-NEXT:    [[B1_DEINTERLEAVED:%.*]] = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> [[B1]])
+; CHECK-NOSVE-NEXT:    [[B1_REAL:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B1_DEINTERLEAVED]], 0
+; CHECK-NOSVE-NEXT:    [[B1_IMAG:%.*]] = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } [[B1_DEINTERLEAVED]], 1
+; CHECK-NOSVE-NEXT:    [[B0_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[B0_REAL]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[B1_REAL_EXT:%.*]] = sext <vscale x 16 x i8> [[B1_REAL]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[TMP0:%.*]] = mul nsw <vscale x 16 x i32> [[B0_REAL_EXT]], [[A0_REAL_EXT]]
+; CHECK-NOSVE-NEXT:    [[TMP1:%.*]] = mul nsw <vscale x 16 x i32> [[B1_REAL_EXT]], [[A1_REAL_EXT]]
+; CHECK-NOSVE-NEXT:    [[A0_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[A0_IMAG]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[A1_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[A1_IMAG]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[B0_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[B0_IMAG]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[B1_IMAG_EXT:%.*]] = sext <vscale x 16 x i8> [[B1_IMAG]] to <vscale x 16 x i32>
+; CHECK-NOSVE-NEXT:    [[TMP2:%.*]] = mul nsw <vscale x 16 x i32> [[B0_IMAG_EXT]], [[A0_IMAG_EXT]]
+; CHECK-NOSVE-NEXT:    [[TMP3:%.*]] = mul nsw <vscale x 16 x i32> [[B1_IMAG_EXT]], [[A1_IMAG_EXT]]
+; CHECK-NOSVE-NEXT:    [[PARTIAL_REDUCE:%.*]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI]], <vscale x 16 x i32> [[TMP0]])
+; CHECK-NOSVE-NEXT:    [[PARTIAL_REDUCE32:%.*]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[VEC_PHI25]], <vscale x 16 x i32> [[TMP1]])
+; CHECK-NOSVE-NEXT:    [[TMP4:%.*]] = sub nsw <vscale x 16 x i32> zeroinitializer, [[TMP2]]
+; CHECK-NOSVE-NEXT:    [[TMP5:%.*]] = sub nsw <vscale x 16 x i32> zeroinitializer, [[TMP3]]
+; CHECK-NOSVE-NEXT:    [[PARTIAL_REDUCE33]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[PARTIAL_REDUCE]], <vscale x 16 x i32> [[TMP4]])
+; CHECK-NOSVE-NEXT:    [[PARTIAL_REDUCE34]] = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> [[PARTIAL_REDUCE32]], <vscale x 16 x i32> [[TMP5]])
+; CHECK-NOSVE-NEXT:    br i1 true, label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]]
+; CHECK-NOSVE:       [[MIDDLE_BLOCK]]:
+; CHECK-NOSVE-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 4 x i32> [[PARTIAL_REDUCE34]], [[PARTIAL_REDUCE33]]
+; CHECK-NOSVE-NEXT:    [[TMP6:%.*]] = tail call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[BIN_RDX]])
+; CHECK-NOSVE-NEXT:    ret i32 [[TMP6]]
+;
+entry:
+  br label %vector.body
+
+vector.body:                                      ; preds = %vector.body, %entry
+  %vec.phi = phi <vscale x 4 x i32> [ zeroinitializer, %entry ], [ %partial.reduce33, %vector.body ]
+  %vec.phi25 = phi <vscale x 4 x i32> [ zeroinitializer, %entry ], [ %partial.reduce34, %vector.body ]
+  %a0.deinterleaved = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> %a0)
+  %a0.real = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %a0.deinterleaved, 0
+  %a0.imag = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %a0.deinterleaved, 1
+  %a1.deinterleaved = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> %a1)
+  %a1.real = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %a1.deinterleaved, 0
+  %a1.imag = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %a1.deinterleaved, 1
+  %a0.real.ext = sext <vscale x 16 x i8> %a0.real to <vscale x 16 x i32>
+  %a1.real.ext = sext <vscale x 16 x i8> %a1.real to <vscale x 16 x i32>
+  %b0.deinterleaved = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> %b0)
+  %b0.real = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %b0.deinterleaved, 0
+  %b0.imag = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %b0.deinterleaved, 1
+  %b1.deinterleaved = tail call { <vscale x 16 x i8>, <vscale x 16 x i8> } @llvm.vector.deinterleave2.nxv32i8(<vscale x 32 x i8> %b1)
+  %b1.real = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %b1.deinterleaved, 0
+  %b1.imag = extractvalue { <vscale x 16 x i8>, <vscale x 16 x i8> } %b1.deinterleaved, 1
+  %b0.real.ext = sext <vscale x 16 x i8> %b0.real to <vscale x 16 x i32>
+  %b1.real.ext = sext <vscale x 16 x i8> %b1.real to <vscale x 16 x i32>
+  %18 = mul nsw <vscale x 16 x i32> %b0.real.ext, %a0.real.ext
+  %19 = mul nsw <vscale x 16 x i32> %b1.real.ext, %a1.real.ext
+  %a0.imag.ext = sext <vscale x 16 x i8> %a0.imag to <vscale x 16 x i32>
+  %a1.imag.ext = sext <vscale x 16 x i8> %a1.imag to <vscale x 16 x i32>
+  %b0.imag.ext = sext <vscale x 16 x i8> %b0.imag to <vscale x 16 x i32>
+  %b1.imag.ext = sext <vscale x 16 x i8> %b1.imag to <vscale x 16 x i32>
+  %24 = mul nsw <vscale x 16 x i32> %b0.imag.ext, %a0.imag.ext
+  %25 = mul nsw <vscale x 16 x i32> %b1.imag.ext, %a1.imag.ext
+  %partial.reduce = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> %vec.phi, <vscale x 16 x i32> %18)
+  %partial.reduce32 = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> %vec.phi25, <vscale x 16 x i32> %19)
+  %26 = sub nsw <vscale x 16 x i32> zeroinitializer, %24
+  %27 = sub nsw <vscale x 16 x i32> zeroinitializer, %25
+  %partial.reduce33 = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> %partial.reduce, <vscale x 16 x i32> %26)
+  %partial.reduce34 = tail call <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32> %partial.reduce32, <vscale x 16 x i32> %27)
+  br i1 true, label %middle.block, label %vector.body
+
+middle.block:                                     ; preds = %vector.body
+  %bin.rdx = add <vscale x 4 x i32> %partial.reduce34, %partial.reduce33
+  %29 = tail call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> %bin.rdx)
+  ret i32 %29
+}
+
+declare <vscale x 8 x i16> @llvm.experimental.vector.partial.reduce.add.nxv8i16.nxv16i32(<vscale x 8 x i16>, <vscale x 16 x i32>)
+declare <vscale x 4 x i32> @llvm.experimental.vector.partial.reduce.add.nxv4i32.nxv16i32(<vscale x 4 x i32>, <vscale x 16 x i32>)
+declare <vscale x 2 x i64> @llvm.experimental.vector.partial.reduce.add.nxv2i64.nxv8i32(<vscale x 2 x i64>, <vscale x 16 x i32>)
+
+declare <4 x i32> @llvm.experimental.vector.partial.reduce.add.v4i32.v16i32(<4 x i32>, <16 x i32>)
+declare i32 @llvm.vector.reduce.add.v4i32(<4 x i32>)
+
+declare i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32>)
+declare i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64>)
diff --git a/llvm/test/CodeGen/AArch64/parity.ll b/llvm/test/CodeGen/AArch64/parity.ll
index 1e51793fb5f9..91515277cb3f 100644
--- a/llvm/test/CodeGen/AArch64/parity.ll
+++ b/llvm/test/CodeGen/AArch64/parity.ll
@@ -159,7 +159,7 @@ define i32 @parity_64_trunc(i64 %x) {
 ; CHECK-NEXT:    fmov d0, x0
 ; CHECK-NEXT:    cnt v0.8b, v0.8b
 ; CHECK-NEXT:    addv b0, v0.8b
-; CHECK-NEXT:    fmov x8, d0
+; CHECK-NEXT:    fmov w8, s0
 ; CHECK-NEXT:    and w0, w8, #0x1
 ; CHECK-NEXT:    ret
 ;
diff --git a/llvm/test/CodeGen/AArch64/popcount.ll b/llvm/test/CodeGen/AArch64/popcount.ll
index 89b1ac0a0edf..e664e7359492 100644
--- a/llvm/test/CodeGen/AArch64/popcount.ll
+++ b/llvm/test/CodeGen/AArch64/popcount.ll
@@ -3,6 +3,7 @@
 ; RUN: llc < %s -mtriple=aarch64-unknown-unknown -mattr=+neon | FileCheck %s --check-prefixes=CHECK,NEON
 ; RUN: llc < %s -mtriple=aarch64-unknown-unknown -mattr=+neon,+dotprod | FileCheck %s --check-prefixes=CHECK,DOT
 ; RUN: llc < %s -mtriple=aarch64-unknown-unknown -mattr=+sve | FileCheck %s --check-prefixes=CHECK,SVE
+; RUN: llc < %s -mtriple=aarch64_be-unknown-unknown | FileCheck %s --check-prefixes=BE
 ; RUN: llc < %s -global-isel -mtriple=aarch64-unknown-unknown | FileCheck %s --check-prefix=GISEL
 ; RUN: llc < %s -O0 -global-isel -mtriple=aarch64-unknown-unknown | FileCheck %s --check-prefix=GISELO0
 ; RUN: llc < %s -global-isel -mtriple=aarch64-unknown-unknown -mattr=+neon | FileCheck %s --check-prefixes=GISEL,NEON-GISEL
@@ -32,6 +33,18 @@ define i8 @popcount128(ptr nocapture nonnull readonly %0) {
 ; CHECK-NEXT:    fmov w0, s0
 ; CHECK-NEXT:    ret
 ;
+; BE-LABEL: popcount128:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    ldr d0, [x0]
+; BE-NEXT:    add x8, x0, #8
+; BE-NEXT:    ld1 { v0.d }[1], [x8]
+; BE-NEXT:    rev64 v0.16b, v0.16b
+; BE-NEXT:    cnt v0.16b, v0.16b
+; BE-NEXT:    addv b0, v0.16b
+; BE-NEXT:    rev64 v0.4s, v0.4s
+; BE-NEXT:    mov w0, v0.s[1]
+; BE-NEXT:    ret
+;
 ; GISEL-LABEL: popcount128:
 ; GISEL:       // %bb.0: // %Entry
 ; GISEL-NEXT:    ldr q0, [x0]
@@ -111,6 +124,27 @@ define i16 @popcount256(ptr nocapture nonnull readonly %0) {
 ; CHECK-NEXT:    add w0, w9, w8
 ; CHECK-NEXT:    ret
 ;
+; BE-LABEL: popcount256:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    ldr d0, [x0]
+; BE-NEXT:    ldr d1, [x0, #16]
+; BE-NEXT:    add x8, x0, #24
+; BE-NEXT:    add x9, x0, #8
+; BE-NEXT:    ld1 { v0.d }[1], [x9]
+; BE-NEXT:    ld1 { v1.d }[1], [x8]
+; BE-NEXT:    rev64 v0.16b, v0.16b
+; BE-NEXT:    rev64 v1.16b, v1.16b
+; BE-NEXT:    cnt v0.16b, v0.16b
+; BE-NEXT:    cnt v1.16b, v1.16b
+; BE-NEXT:    addv b0, v0.16b
+; BE-NEXT:    addv b1, v1.16b
+; BE-NEXT:    rev64 v0.4s, v0.4s
+; BE-NEXT:    rev64 v1.4s, v1.4s
+; BE-NEXT:    mov w8, v0.s[1]
+; BE-NEXT:    mov w9, v1.s[1]
+; BE-NEXT:    add w0, w9, w8
+; BE-NEXT:    ret
+;
 ; GISEL-LABEL: popcount256:
 ; GISEL:       // %bb.0: // %Entry
 ; GISEL-NEXT:    ldp x8, x9, [x0]
@@ -193,12 +227,23 @@ define <1 x i128> @popcount1x128(<1 x i128> %0) {
 ; CHECK:       // %bb.0: // %Entry
 ; CHECK-NEXT:    fmov d0, x0
 ; CHECK-NEXT:    mov v0.d[1], x1
+; CHECK-NEXT:    mov x1, xzr
 ; CHECK-NEXT:    cnt v0.16b, v0.16b
 ; CHECK-NEXT:    addv b0, v0.16b
-; CHECK-NEXT:    mov x1, v0.d[1]
 ; CHECK-NEXT:    fmov x0, d0
 ; CHECK-NEXT:    ret
 ;
+; BE-LABEL: popcount1x128:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    fmov d0, x0
+; BE-NEXT:    mov x0, xzr
+; BE-NEXT:    mov v0.d[1], x1
+; BE-NEXT:    rev64 v0.16b, v0.16b
+; BE-NEXT:    cnt v0.16b, v0.16b
+; BE-NEXT:    addv b0, v0.16b
+; BE-NEXT:    fmov x1, d0
+; BE-NEXT:    ret
+;
 ; GISEL-LABEL: popcount1x128:
 ; GISEL:       // %bb.0: // %Entry
 ; GISEL-NEXT:    mov v0.d[0], x0
@@ -266,6 +311,17 @@ define <2 x i64> @popcount2x64(<2 x i64> %0) {
 ; SVE-NEXT:    uaddlp v0.2d, v0.4s
 ; SVE-NEXT:    ret
 ;
+; BE-LABEL: popcount2x64:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    rev64 v0.16b, v0.16b
+; BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
+; BE-NEXT:    cnt v0.16b, v0.16b
+; BE-NEXT:    uaddlp v0.8h, v0.16b
+; BE-NEXT:    uaddlp v0.4s, v0.8h
+; BE-NEXT:    uaddlp v0.2d, v0.4s
+; BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
+; BE-NEXT:    ret
+;
 ; GISELO0-LABEL: popcount2x64:
 ; GISELO0:       // %bb.0: // %Entry
 ; GISELO0-NEXT:    cnt v0.16b, v0.16b
@@ -326,6 +382,15 @@ define <1 x i64> @popcount1x64(<1 x i64> %0) {
 ; CHECK-NEXT:    uaddlp v0.1d, v0.2s
 ; CHECK-NEXT:    ret
 ;
+; BE-LABEL: popcount1x64:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    rev64 v0.8b, v0.8b
+; BE-NEXT:    cnt v0.8b, v0.8b
+; BE-NEXT:    uaddlp v0.4h, v0.8b
+; BE-NEXT:    uaddlp v0.2s, v0.4h
+; BE-NEXT:    uaddlp v0.1d, v0.2s
+; BE-NEXT:    ret
+;
 ; GISEL-LABEL: popcount1x64:
 ; GISEL:       // %bb.0: // %Entry
 ; GISEL-NEXT:    cnt v0.8b, v0.8b
@@ -382,6 +447,17 @@ define <4 x i32> @popcount4x32(<4 x i32> %0) {
 ; SVE-NEXT:    uaddlp v0.4s, v0.8h
 ; SVE-NEXT:    ret
 ;
+; BE-LABEL: popcount4x32:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    rev64 v0.16b, v0.16b
+; BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
+; BE-NEXT:    cnt v0.16b, v0.16b
+; BE-NEXT:    uaddlp v0.8h, v0.16b
+; BE-NEXT:    uaddlp v0.4s, v0.8h
+; BE-NEXT:    rev64 v0.4s, v0.4s
+; BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
+; BE-NEXT:    ret
+;
 ; GISELO0-LABEL: popcount4x32:
 ; GISELO0:       // %bb.0: // %Entry
 ; GISELO0-NEXT:    cnt v0.16b, v0.16b
@@ -449,6 +525,15 @@ define <2 x i32> @popcount2x32(<2 x i32> %0) {
 ; SVE-NEXT:    uaddlp v0.2s, v0.4h
 ; SVE-NEXT:    ret
 ;
+; BE-LABEL: popcount2x32:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    rev64 v0.8b, v0.8b
+; BE-NEXT:    cnt v0.8b, v0.8b
+; BE-NEXT:    uaddlp v0.4h, v0.8b
+; BE-NEXT:    uaddlp v0.2s, v0.4h
+; BE-NEXT:    rev64 v0.2s, v0.2s
+; BE-NEXT:    ret
+;
 ; GISELO0-LABEL: popcount2x32:
 ; GISELO0:       // %bb.0: // %Entry
 ; GISELO0-NEXT:    cnt v0.8b, v0.8b
@@ -498,6 +583,16 @@ define <8 x i16> @popcount8x16(<8 x i16> %0) {
 ; CHECK-NEXT:    uaddlp v0.8h, v0.16b
 ; CHECK-NEXT:    ret
 ;
+; BE-LABEL: popcount8x16:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    rev64 v0.16b, v0.16b
+; BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
+; BE-NEXT:    cnt v0.16b, v0.16b
+; BE-NEXT:    uaddlp v0.8h, v0.16b
+; BE-NEXT:    rev64 v0.8h, v0.8h
+; BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
+; BE-NEXT:    ret
+;
 ; GISEL-LABEL: popcount8x16:
 ; GISEL:       // %bb.0: // %Entry
 ; GISEL-NEXT:    cnt v0.16b, v0.16b
@@ -529,6 +624,14 @@ define <4 x i16> @popcount4x16(<4 x i16> %0) {
 ; CHECK-NEXT:    uaddlp v0.4h, v0.8b
 ; CHECK-NEXT:    ret
 ;
+; BE-LABEL: popcount4x16:
+; BE:       // %bb.0: // %Entry
+; BE-NEXT:    rev64 v0.8b, v0.8b
+; BE-NEXT:    cnt v0.8b, v0.8b
+; BE-NEXT:    uaddlp v0.4h, v0.8b
+; BE-NEXT:    rev64 v0.4h, v0.4h
+; BE-NEXT:    ret
+;
 ; GISEL-LABEL: popcount4x16:
 ; GISEL:       // %bb.0: // %Entry
 ; GISEL-NEXT:    cnt v0.8b, v0.8b
diff --git a/llvm/test/CodeGen/AArch64/sme-disable-gisel-fisel.ll b/llvm/test/CodeGen/AArch64/sme-disable-gisel-fisel.ll
index 33d08beae2ca..4a52bf27a759 100644
--- a/llvm/test/CodeGen/AArch64/sme-disable-gisel-fisel.ll
+++ b/llvm/test/CodeGen/AArch64/sme-disable-gisel-fisel.ll
@@ -475,16 +475,12 @@ declare double @zt0_shared_callee(double) "aarch64_inout_zt0"
 define double  @zt0_new_caller_to_zt0_shared_callee(double %x) nounwind noinline optnone "aarch64_new_zt0" {
 ; CHECK-COMMON-LABEL: zt0_new_caller_to_zt0_shared_callee:
 ; CHECK-COMMON:       // %bb.0: // %prelude
-; CHECK-COMMON-NEXT:    sub sp, sp, #80
-; CHECK-COMMON-NEXT:    str x30, [sp, #64] // 8-byte Folded Spill
+; CHECK-COMMON-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
 ; CHECK-COMMON-NEXT:    mrs x8, TPIDR2_EL0
 ; CHECK-COMMON-NEXT:    cbz x8, .LBB13_2
 ; CHECK-COMMON-NEXT:    b .LBB13_1
 ; CHECK-COMMON-NEXT:  .LBB13_1: // %save.za
-; CHECK-COMMON-NEXT:    mov x8, sp
-; CHECK-COMMON-NEXT:    str zt0, [x8]
 ; CHECK-COMMON-NEXT:    bl __arm_tpidr2_save
-; CHECK-COMMON-NEXT:    ldr zt0, [x8]
 ; CHECK-COMMON-NEXT:    msr TPIDR2_EL0, xzr
 ; CHECK-COMMON-NEXT:    b .LBB13_2
 ; CHECK-COMMON-NEXT:  .LBB13_2: // %entry
@@ -495,8 +491,7 @@ define double  @zt0_new_caller_to_zt0_shared_callee(double %x) nounwind noinline
 ; CHECK-COMMON-NEXT:    fmov d1, x8
 ; CHECK-COMMON-NEXT:    fadd d0, d0, d1
 ; CHECK-COMMON-NEXT:    smstop za
-; CHECK-COMMON-NEXT:    ldr x30, [sp, #64] // 8-byte Folded Reload
-; CHECK-COMMON-NEXT:    add sp, sp, #80
+; CHECK-COMMON-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
 ; CHECK-COMMON-NEXT:    ret
 entry:
   %call = call double @zt0_shared_callee(double %x)
diff --git a/llvm/test/CodeGen/AArch64/sme-new-zt0-function.ll b/llvm/test/CodeGen/AArch64/sme-new-zt0-function.ll
new file mode 100644
index 000000000000..94968ab4fd9a
--- /dev/null
+++ b/llvm/test/CodeGen/AArch64/sme-new-zt0-function.ll
@@ -0,0 +1,14 @@
+; RUN: opt -S -mtriple=aarch64-linux-gnu -aarch64-sme-abi %s | FileCheck %s
+
+declare void @callee();
+
+define void @private_za() "aarch64_new_zt0" {
+  call void @callee()
+  ret void
+}
+
+; CHECK: call aarch64_sme_preservemost_from_x0 void @__arm_tpidr2_save() #[[TPIDR2_SAVE_CALL_ATTR:[0-9]+]]
+; CHECK: declare void @__arm_tpidr2_save() #[[TPIDR2_SAVE_DECL_ATTR:[0-9]+]]
+
+; CHECK: attributes #[[TPIDR2_SAVE_DECL_ATTR]] = { "aarch64_pstate_sm_compatible" }
+; CHECK: attributes #[[TPIDR2_SAVE_CALL_ATTR]] = { "aarch64_zt0_undef" }
diff --git a/llvm/test/CodeGen/AArch64/sme-zt0-state.ll b/llvm/test/CodeGen/AArch64/sme-zt0-state.ll
index 312537630e77..7361e850d713 100644
--- a/llvm/test/CodeGen/AArch64/sme-zt0-state.ll
+++ b/llvm/test/CodeGen/AArch64/sme-zt0-state.ll
@@ -112,7 +112,7 @@ define void @za_zt0_shared_caller_za_zt0_shared_callee() "aarch64_inout_za" "aar
   ret void;
 }
 
-; New-ZA Callee
+; New-ZT0 Callee
 
 ; Expect spill & fill of ZT0 around call
 ; Expect smstop/smstart za around call
@@ -134,6 +134,72 @@ define void @zt0_in_caller_zt0_new_callee() "aarch64_in_zt0" nounwind {
   ret void;
 }
 
+; New-ZT0 Callee
+
+; Expect commit of lazy-save if ZA is dormant
+; Expect smstart ZA & clear ZT0
+; Expect spill & fill of ZT0 around call
+; Before return, expect smstop ZA
+define void @zt0_new_caller_zt0_new_callee() "aarch64_new_zt0" nounwind {
+; CHECK-LABEL: zt0_new_caller_zt0_new_callee:
+; CHECK:       // %bb.0: // %prelude
+; CHECK-NEXT:    sub sp, sp, #80
+; CHECK-NEXT:    stp x30, x19, [sp, #64] // 16-byte Folded Spill
+; CHECK-NEXT:    mrs x8, TPIDR2_EL0
+; CHECK-NEXT:    cbz x8, .LBB6_2
+; CHECK-NEXT:  // %bb.1: // %save.za
+; CHECK-NEXT:    bl __arm_tpidr2_save
+; CHECK-NEXT:    msr TPIDR2_EL0, xzr
+; CHECK-NEXT:  .LBB6_2:
+; CHECK-NEXT:    smstart za
+; CHECK-NEXT:    zero { zt0 }
+; CHECK-NEXT:    mov x19, sp
+; CHECK-NEXT:    str zt0, [x19]
+; CHECK-NEXT:    smstop za
+; CHECK-NEXT:    bl callee
+; CHECK-NEXT:    smstart za
+; CHECK-NEXT:    ldr zt0, [x19]
+; CHECK-NEXT:    smstop za
+; CHECK-NEXT:    ldp x30, x19, [sp, #64] // 16-byte Folded Reload
+; CHECK-NEXT:    add sp, sp, #80
+; CHECK-NEXT:    ret
+  call void @callee() "aarch64_new_zt0";
+  ret void;
+}
+
+; Expect commit of lazy-save if ZA is dormant
+; Expect smstart ZA & clear ZT0
+; No spill & fill of ZT0 around __arm_tpidr2_save
+; Expect spill & fill of ZT0 around __arm_sme_state call
+; Before return, expect smstop ZA
+define i64 @zt0_new_caller_abi_routine_callee() "aarch64_new_zt0" nounwind {
+; CHECK-LABEL: zt0_new_caller_abi_routine_callee:
+; CHECK:       // %bb.0: // %prelude
+; CHECK-NEXT:    sub sp, sp, #80
+; CHECK-NEXT:    stp x30, x19, [sp, #64] // 16-byte Folded Spill
+; CHECK-NEXT:    mrs x8, TPIDR2_EL0
+; CHECK-NEXT:    cbz x8, .LBB7_2
+; CHECK-NEXT:  // %bb.1: // %save.za
+; CHECK-NEXT:    bl __arm_tpidr2_save
+; CHECK-NEXT:    msr TPIDR2_EL0, xzr
+; CHECK-NEXT:  .LBB7_2:
+; CHECK-NEXT:    smstart za
+; CHECK-NEXT:    zero { zt0 }
+; CHECK-NEXT:    mov x19, sp
+; CHECK-NEXT:    str zt0, [x19]
+; CHECK-NEXT:    bl __arm_sme_state
+; CHECK-NEXT:    ldr zt0, [x19]
+; CHECK-NEXT:    smstop za
+; CHECK-NEXT:    ldp x30, x19, [sp, #64] // 16-byte Folded Reload
+; CHECK-NEXT:    add sp, sp, #80
+; CHECK-NEXT:    ret
+  %res = call {i64, i64} @__arm_sme_state()
+  %res.0 = extractvalue {i64, i64} %res, 0
+  ret i64 %res.0
+}
+
+declare {i64, i64} @__arm_sme_state()
+
 ;
 ; New-ZA Caller
 ;
@@ -144,23 +210,18 @@ define void @zt0_in_caller_zt0_new_callee() "aarch64_in_zt0" nounwind {
 define void @zt0_new_caller() "aarch64_new_zt0" nounwind {
 ; CHECK-LABEL: zt0_new_caller:
 ; CHECK:       // %bb.0: // %prelude
-; CHECK-NEXT:    sub sp, sp, #80
-; CHECK-NEXT:    str x30, [sp, #64] // 8-byte Folded Spill
+; CHECK-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
 ; CHECK-NEXT:    mrs x8, TPIDR2_EL0
-; CHECK-NEXT:    cbz x8, .LBB6_2
+; CHECK-NEXT:    cbz x8, .LBB8_2
 ; CHECK-NEXT:  // %bb.1: // %save.za
-; CHECK-NEXT:    mov x8, sp
-; CHECK-NEXT:    str zt0, [x8]
 ; CHECK-NEXT:    bl __arm_tpidr2_save
-; CHECK-NEXT:    ldr zt0, [x8]
 ; CHECK-NEXT:    msr TPIDR2_EL0, xzr
-; CHECK-NEXT:  .LBB6_2:
+; CHECK-NEXT:  .LBB8_2:
 ; CHECK-NEXT:    smstart za
 ; CHECK-NEXT:    zero { zt0 }
 ; CHECK-NEXT:    bl callee
 ; CHECK-NEXT:    smstop za
-; CHECK-NEXT:    ldr x30, [sp, #64] // 8-byte Folded Reload
-; CHECK-NEXT:    add sp, sp, #80
+; CHECK-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
 ; CHECK-NEXT:    ret
   call void @callee() "aarch64_in_zt0";
   ret void;
@@ -172,24 +233,19 @@ define void @zt0_new_caller() "aarch64_new_zt0" nounwind {
 define void @new_za_zt0_caller() "aarch64_new_za" "aarch64_new_zt0" nounwind {
 ; CHECK-LABEL: new_za_zt0_caller:
 ; CHECK:       // %bb.0: // %prelude
-; CHECK-NEXT:    sub sp, sp, #80
-; CHECK-NEXT:    str x30, [sp, #64] // 8-byte Folded Spill
+; CHECK-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
 ; CHECK-NEXT:    mrs x8, TPIDR2_EL0
-; CHECK-NEXT:    cbz x8, .LBB7_2
+; CHECK-NEXT:    cbz x8, .LBB9_2
 ; CHECK-NEXT:  // %bb.1: // %save.za
-; CHECK-NEXT:    mov x8, sp
-; CHECK-NEXT:    str zt0, [x8]
 ; CHECK-NEXT:    bl __arm_tpidr2_save
-; CHECK-NEXT:    ldr zt0, [x8]
 ; CHECK-NEXT:    msr TPIDR2_EL0, xzr
-; CHECK-NEXT:  .LBB7_2:
+; CHECK-NEXT:  .LBB9_2:
 ; CHECK-NEXT:    smstart za
 ; CHECK-NEXT:    zero {za}
 ; CHECK-NEXT:    zero { zt0 }
 ; CHECK-NEXT:    bl callee
 ; CHECK-NEXT:    smstop za
-; CHECK-NEXT:    ldr x30, [sp, #64] // 8-byte Folded Reload
-; CHECK-NEXT:    add sp, sp, #80
+; CHECK-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
 ; CHECK-NEXT:    ret
   call void @callee() "aarch64_inout_za" "aarch64_in_zt0";
   ret void;
diff --git a/llvm/test/CodeGen/AArch64/sve-streaming-mode-fixed-length-fcopysign.ll b/llvm/test/CodeGen/AArch64/sve-streaming-mode-fixed-length-fcopysign.ll
index 2282e74af5d0..79921e25caf5 100644
--- a/llvm/test/CodeGen/AArch64/sve-streaming-mode-fixed-length-fcopysign.ll
+++ b/llvm/test/CodeGen/AArch64/sve-streaming-mode-fixed-length-fcopysign.ll
@@ -8,6 +8,205 @@ target datalayout = "e-m:o-i64:64-i128:128-n32:64-S128"
 
 target triple = "aarch64-unknown-linux-gnu"
 
+define void @test_copysign_f16(ptr %ap, ptr %bp) {
+; SVE-LABEL: test_copysign_f16:
+; SVE:       // %bb.0:
+; SVE-NEXT:    ldr h0, [x1]
+; SVE-NEXT:    ldr h1, [x0]
+; SVE-NEXT:    and z0.h, z0.h, #0x8000
+; SVE-NEXT:    and z1.h, z1.h, #0x7fff
+; SVE-NEXT:    orr z0.d, z1.d, z0.d
+; SVE-NEXT:    str h0, [x0]
+; SVE-NEXT:    ret
+;
+; SVE2-LABEL: test_copysign_f16:
+; SVE2:       // %bb.0:
+; SVE2-NEXT:    mov z0.h, #32767 // =0x7fff
+; SVE2-NEXT:    ldr h1, [x1]
+; SVE2-NEXT:    ldr h2, [x0]
+; SVE2-NEXT:    bsl z2.d, z2.d, z1.d, z0.d
+; SVE2-NEXT:    str h2, [x0]
+; SVE2-NEXT:    ret
+;
+; NONEON-NOSVE-LABEL: test_copysign_f16:
+; NONEON-NOSVE:       // %bb.0:
+; NONEON-NOSVE-NEXT:    sub sp, sp, #16
+; NONEON-NOSVE-NEXT:    .cfi_def_cfa_offset 16
+; NONEON-NOSVE-NEXT:    ldr h0, [x0]
+; NONEON-NOSVE-NEXT:    ldr h1, [x1]
+; NONEON-NOSVE-NEXT:    fcvt s0, h0
+; NONEON-NOSVE-NEXT:    str h1, [sp, #12]
+; NONEON-NOSVE-NEXT:    ldrb w8, [sp, #13]
+; NONEON-NOSVE-NEXT:    tst w8, #0x80
+; NONEON-NOSVE-NEXT:    fabs s0, s0
+; NONEON-NOSVE-NEXT:    fneg s1, s0
+; NONEON-NOSVE-NEXT:    fcsel s0, s1, s0, ne
+; NONEON-NOSVE-NEXT:    fcvt h0, s0
+; NONEON-NOSVE-NEXT:    str h0, [x0]
+; NONEON-NOSVE-NEXT:    add sp, sp, #16
+; NONEON-NOSVE-NEXT:    ret
+  %a = load half, ptr %ap
+  %b = load half, ptr %bp
+  %r = call half @llvm.copysign.f16(half %a, half %b)
+  store half %r, ptr %ap
+  ret void
+}
+
+define void @test_copysign_bf16(ptr %ap, ptr %bp) {
+; SVE-LABEL: test_copysign_bf16:
+; SVE:       // %bb.0:
+; SVE-NEXT:    sub sp, sp, #16
+; SVE-NEXT:    .cfi_def_cfa_offset 16
+; SVE-NEXT:    ldr h0, [x0]
+; SVE-NEXT:    ldr h1, [x1]
+; SVE-NEXT:    fmov w8, s0
+; SVE-NEXT:    str h1, [sp, #12]
+; SVE-NEXT:    ldrb w9, [sp, #13]
+; SVE-NEXT:    and w8, w8, #0x7fff
+; SVE-NEXT:    tst w9, #0x80
+; SVE-NEXT:    fmov s0, w8
+; SVE-NEXT:    eor w8, w8, #0x8000
+; SVE-NEXT:    fmov s1, w8
+; SVE-NEXT:    fcsel h0, h1, h0, ne
+; SVE-NEXT:    str h0, [x0]
+; SVE-NEXT:    add sp, sp, #16
+; SVE-NEXT:    ret
+;
+; SVE2-LABEL: test_copysign_bf16:
+; SVE2:       // %bb.0:
+; SVE2-NEXT:    sub sp, sp, #16
+; SVE2-NEXT:    .cfi_def_cfa_offset 16
+; SVE2-NEXT:    ldr h0, [x0]
+; SVE2-NEXT:    ldr h1, [x1]
+; SVE2-NEXT:    fmov w8, s0
+; SVE2-NEXT:    str h1, [sp, #12]
+; SVE2-NEXT:    ldrb w9, [sp, #13]
+; SVE2-NEXT:    and w8, w8, #0x7fff
+; SVE2-NEXT:    tst w9, #0x80
+; SVE2-NEXT:    fmov s0, w8
+; SVE2-NEXT:    eor w8, w8, #0x8000
+; SVE2-NEXT:    fmov s1, w8
+; SVE2-NEXT:    fcsel h0, h1, h0, ne
+; SVE2-NEXT:    str h0, [x0]
+; SVE2-NEXT:    add sp, sp, #16
+; SVE2-NEXT:    ret
+;
+; NONEON-NOSVE-LABEL: test_copysign_bf16:
+; NONEON-NOSVE:       // %bb.0:
+; NONEON-NOSVE-NEXT:    sub sp, sp, #80
+; NONEON-NOSVE-NEXT:    .cfi_def_cfa_offset 80
+; NONEON-NOSVE-NEXT:    ldr h0, [x0]
+; NONEON-NOSVE-NEXT:    ldr h1, [x1]
+; NONEON-NOSVE-NEXT:    str h0, [sp, #40]
+; NONEON-NOSVE-NEXT:    ldr d0, [sp, #40]
+; NONEON-NOSVE-NEXT:    str h1, [sp, #76]
+; NONEON-NOSVE-NEXT:    ushll v0.4s, v0.4h, #0
+; NONEON-NOSVE-NEXT:    str q0, [sp]
+; NONEON-NOSVE-NEXT:    ldr w8, [sp, #12]
+; NONEON-NOSVE-NEXT:    lsl w9, w8, #16
+; NONEON-NOSVE-NEXT:    ldr w8, [sp, #8]
+; NONEON-NOSVE-NEXT:    lsl w8, w8, #16
+; NONEON-NOSVE-NEXT:    stp w8, w9, [sp, #24]
+; NONEON-NOSVE-NEXT:    ldr w8, [sp, #4]
+; NONEON-NOSVE-NEXT:    lsl w9, w8, #16
+; NONEON-NOSVE-NEXT:    ldr w8, [sp]
+; NONEON-NOSVE-NEXT:    lsl w8, w8, #16
+; NONEON-NOSVE-NEXT:    stp w8, w9, [sp, #16]
+; NONEON-NOSVE-NEXT:    ldrb w8, [sp, #77]
+; NONEON-NOSVE-NEXT:    ldr q0, [sp, #16]
+; NONEON-NOSVE-NEXT:    tst w8, #0x80
+; NONEON-NOSVE-NEXT:    str q0, [sp, #48]
+; NONEON-NOSVE-NEXT:    ldr s0, [sp, #48]
+; NONEON-NOSVE-NEXT:    fabs s0, s0
+; NONEON-NOSVE-NEXT:    fneg s1, s0
+; NONEON-NOSVE-NEXT:    fcsel s0, s1, s0, ne
+; NONEON-NOSVE-NEXT:    fmov w8, s0
+; NONEON-NOSVE-NEXT:    lsr w8, w8, #16
+; NONEON-NOSVE-NEXT:    fmov s0, w8
+; NONEON-NOSVE-NEXT:    str h0, [x0]
+; NONEON-NOSVE-NEXT:    add sp, sp, #80
+; NONEON-NOSVE-NEXT:    ret
+  %a = load bfloat, ptr %ap
+  %b = load bfloat, ptr %bp
+  %r = call bfloat @llvm.copysign.bf16(bfloat %a, bfloat %b)
+  store bfloat %r, ptr %ap
+  ret void
+}
+
+define void @test_copysign_f32(ptr %ap, ptr %bp) {
+; SVE-LABEL: test_copysign_f32:
+; SVE:       // %bb.0:
+; SVE-NEXT:    ldr s0, [x1]
+; SVE-NEXT:    ldr s1, [x0]
+; SVE-NEXT:    and z0.s, z0.s, #0x80000000
+; SVE-NEXT:    and z1.s, z1.s, #0x7fffffff
+; SVE-NEXT:    orr z0.d, z1.d, z0.d
+; SVE-NEXT:    str s0, [x0]
+; SVE-NEXT:    ret
+;
+; SVE2-LABEL: test_copysign_f32:
+; SVE2:       // %bb.0:
+; SVE2-NEXT:    mov z0.s, #0x7fffffff
+; SVE2-NEXT:    ldr s1, [x1]
+; SVE2-NEXT:    ldr s2, [x0]
+; SVE2-NEXT:    bsl z2.d, z2.d, z1.d, z0.d
+; SVE2-NEXT:    str s2, [x0]
+; SVE2-NEXT:    ret
+;
+; NONEON-NOSVE-LABEL: test_copysign_f32:
+; NONEON-NOSVE:       // %bb.0:
+; NONEON-NOSVE-NEXT:    ldr s0, [x0]
+; NONEON-NOSVE-NEXT:    ldr w8, [x1]
+; NONEON-NOSVE-NEXT:    fabs s0, s0
+; NONEON-NOSVE-NEXT:    tst w8, #0x80000000
+; NONEON-NOSVE-NEXT:    fneg s1, s0
+; NONEON-NOSVE-NEXT:    fcsel s0, s1, s0, ne
+; NONEON-NOSVE-NEXT:    str s0, [x0]
+; NONEON-NOSVE-NEXT:    ret
+  %a = load float, ptr %ap
+  %b = load float, ptr %bp
+  %r = call float @llvm.copysign.f32(float %a, float %b)
+  store float %r, ptr %ap
+  ret void
+}
+
+define void @test_copysign_f64(ptr %ap, ptr %bp) {
+; SVE-LABEL: test_copysign_f64:
+; SVE:       // %bb.0:
+; SVE-NEXT:    ldr d0, [x1]
+; SVE-NEXT:    ldr d1, [x0]
+; SVE-NEXT:    and z0.d, z0.d, #0x8000000000000000
+; SVE-NEXT:    and z1.d, z1.d, #0x7fffffffffffffff
+; SVE-NEXT:    orr z0.d, z1.d, z0.d
+; SVE-NEXT:    str d0, [x0]
+; SVE-NEXT:    ret
+;
+; SVE2-LABEL: test_copysign_f64:
+; SVE2:       // %bb.0:
+; SVE2-NEXT:    mov z0.d, #0x7fffffffffffffff
+; SVE2-NEXT:    ldr d1, [x1]
+; SVE2-NEXT:    ldr d2, [x0]
+; SVE2-NEXT:    bsl z2.d, z2.d, z1.d, z0.d
+; SVE2-NEXT:    str d2, [x0]
+; SVE2-NEXT:    ret
+;
+; NONEON-NOSVE-LABEL: test_copysign_f64:
+; NONEON-NOSVE:       // %bb.0:
+; NONEON-NOSVE-NEXT:    ldr d0, [x0]
+; NONEON-NOSVE-NEXT:    ldr x8, [x1]
+; NONEON-NOSVE-NEXT:    fabs d0, d0
+; NONEON-NOSVE-NEXT:    tst x8, #0x8000000000000000
+; NONEON-NOSVE-NEXT:    fneg d1, d0
+; NONEON-NOSVE-NEXT:    fcsel d0, d1, d0, ne
+; NONEON-NOSVE-NEXT:    str d0, [x0]
+; NONEON-NOSVE-NEXT:    ret
+  %a = load double, ptr %ap
+  %b = load double, ptr %bp
+  %r = call double @llvm.copysign.f64(double %a, double %b)
+  store double %r, ptr %ap
+  ret void
+}
+
 ;============ f16
 
 define void @test_copysign_v4f16_v4f16(ptr %ap, ptr %bp) {
diff --git a/llvm/test/CodeGen/BPF/CORE/arena_bitcast.ll b/llvm/test/CodeGen/BPF/CORE/arena_bitcast.ll
new file mode 100644
index 000000000000..bcd71c04d264
--- /dev/null
+++ b/llvm/test/CodeGen/BPF/CORE/arena_bitcast.ll
@@ -0,0 +1,80 @@
+; RUN: opt -O2 %s | llvm-dis > %t1
+; RUN: llc -mcpu=v3 -filetype=asm -o - %t1 | FileCheck %s
+; Source code:
+;   struct lock_t {
+;     int counter;
+;   } __attribute__((preserve_access_index));
+;
+;   #define __arena __attribute__((address_space(1)))
+;   int test(struct lock_t __arena *lock, unsigned val)
+;   {
+;     return __sync_val_compare_and_swap((&lock->counter), val, 1);
+;   }
+; Compilation flag:
+;   clang -target bpf -O2 -g -S -emit-llvm -Xclang -disable-llvm-passes arena_bitcast.c
+
+target triple = "bpf"
+
+%struct.lock_t = type { i32 }
+
+; Function Attrs: nounwind
+define dso_local i32 @test(ptr addrspace(1) noundef %lock, i32 noundef %val) #0 !dbg !7 {
+entry:
+  %lock.addr = alloca ptr addrspace(1), align 8
+  %val.addr = alloca i32, align 4
+  store ptr addrspace(1) %lock, ptr %lock.addr, align 8, !tbaa !19
+    #dbg_declare(ptr %lock.addr, !17, !DIExpression(), !24)
+  store i32 %val, ptr %val.addr, align 4, !tbaa !25
+    #dbg_declare(ptr %val.addr, !18, !DIExpression(), !27)
+  %0 = load ptr addrspace(1), ptr %lock.addr, align 8, !dbg !28, !tbaa !19
+  %1 = call ptr addrspace(1) @llvm.preserve.struct.access.index.p1.p1(ptr addrspace(1) elementtype(%struct.lock_t) %0, i32 0, i32 0), !dbg !29, !llvm.preserve.access.index !12
+  %2 = load i32, ptr %val.addr, align 4, !dbg !30, !tbaa !25
+  %3 = cmpxchg ptr addrspace(1) %1, i32 %2, i32 1 seq_cst seq_cst, align 4, !dbg !31
+  %4 = extractvalue { i32, i1 } %3, 0, !dbg !31
+  ret i32 %4, !dbg !32
+}
+; CHECK:  r1 = addr_space_cast(r1, 0, 1)
+
+; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
+declare ptr addrspace(1) @llvm.preserve.struct.access.index.p1.p1(ptr addrspace(1), i32 immarg, i32 immarg) #1
+
+attributes #0 = { nounwind "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
+attributes #1 = { nocallback nofree nosync nounwind willreturn memory(none) }
+
+!llvm.dbg.cu = !{!0}
+!llvm.module.flags = !{!2, !3, !4, !5}
+!llvm.ident = !{!6}
+
+!0 = distinct !DICompileUnit(language: DW_LANG_C11, file: !1, producer: "clang version 21.0.0git (https://github.com/llvm/llvm-project.git 7d4d8509cbec7eecd8aaf2510015b54bc5c173e1)", isOptimized: true, runtimeVersion: 0, emissionKind: FullDebug, splitDebugInlining: false, nameTableKind: None)
+!1 = !DIFile(filename: "arena_bitcast.c", directory: "/root/home/yhs/tests/arena/simple", checksumkind: CSK_MD5, checksum: "51cb51c1fc09d3033dbd9aee9044dc9b")
+!2 = !{i32 7, !"Dwarf Version", i32 5}
+!3 = !{i32 2, !"Debug Info Version", i32 3}
+!4 = !{i32 1, !"wchar_size", i32 4}
+!5 = !{i32 7, !"frame-pointer", i32 2}
+!6 = !{!"clang version 21.0.0git (https://github.com/llvm/llvm-project.git 7d4d8509cbec7eecd8aaf2510015b54bc5c173e1)"}
+!7 = distinct !DISubprogram(name: "test", scope: !1, file: !1, line: 6, type: !8, scopeLine: 7, flags: DIFlagPrototyped | DIFlagAllCallsDescribed, spFlags: DISPFlagDefinition | DISPFlagOptimized, unit: !0, retainedNodes: !16)
+!8 = !DISubroutineType(types: !9)
+!9 = !{!10, !11, !15}
+!10 = !DIBasicType(name: "int", size: 32, encoding: DW_ATE_signed)
+!11 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !12, size: 64)
+!12 = distinct !DICompositeType(tag: DW_TAG_structure_type, name: "lock_t", file: !1, line: 1, size: 32, elements: !13)
+!13 = !{!14}
+!14 = !DIDerivedType(tag: DW_TAG_member, name: "counter", scope: !12, file: !1, line: 2, baseType: !10, size: 32)
+!15 = !DIBasicType(name: "unsigned int", size: 32, encoding: DW_ATE_unsigned)
+!16 = !{!17, !18}
+!17 = !DILocalVariable(name: "lock", arg: 1, scope: !7, file: !1, line: 6, type: !11)
+!18 = !DILocalVariable(name: "val", arg: 2, scope: !7, file: !1, line: 6, type: !15)
+!19 = !{!20, !20, i64 0}
+!20 = !{!"p1 _ZTS6lock_t", !21, i64 0}
+!21 = !{!"any pointer", !22, i64 0}
+!22 = !{!"omnipotent char", !23, i64 0}
+!23 = !{!"Simple C/C++ TBAA"}
+!24 = !DILocation(line: 6, column: 33, scope: !7)
+!25 = !{!26, !26, i64 0}
+!26 = !{!"int", !22, i64 0}
+!27 = !DILocation(line: 6, column: 48, scope: !7)
+!28 = !DILocation(line: 8, column: 40, scope: !7)
+!29 = !DILocation(line: 8, column: 46, scope: !7)
+!30 = !DILocation(line: 8, column: 56, scope: !7)
+!31 = !DILocation(line: 8, column: 10, scope: !7)
+!32 = !DILocation(line: 8, column: 3, scope: !7)
diff --git a/llvm/test/CodeGen/Hexagon/arg-copy-elison.ll b/llvm/test/CodeGen/Hexagon/arg-copy-elison.ll
index f0c30c301f44..52f29eefa5ce 100644
--- a/llvm/test/CodeGen/Hexagon/arg-copy-elison.ll
+++ b/llvm/test/CodeGen/Hexagon/arg-copy-elison.ll
@@ -1,8 +1,7 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --include-generated-funcs --version 4
 ; RUN: llc -mtriple hexagon-- -o - %s | FileCheck %s
 
 ; Reproducer for https://github.com/llvm/llvm-project/issues/89060
-;
 ; Problem was a bug in argument copy elison. Given that the %alloca is
 ; eliminated, the same frame index will be used for accessing %alloca and %a
 ; on the fixed stack. Care must be taken when setting up
@@ -11,8 +10,15 @@
 ; ir.alloca name), or make sure that we still detect that they alias each
 ; other if using different kinds of MemOperands to identify the same fixed
 ; stack entry.
-;
 define i32 @f(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32 %q1, i32 %a, i32 %q2) {
+  %alloca = alloca i32
+  store i32 %a, ptr %alloca     ; Should be elided.
+  store i32 666, ptr %alloca
+  %x = sub i32 %q1, %q2
+  %y = xor i32 %x, %a           ; Results in a load of %a from fixed stack.
+                                ; Using same frame index as elided %alloca.
+  ret i32 %y
+}
 ; CHECK-LABEL: f:
 ; CHECK:         .cfi_startproc
 ; CHECK-NEXT:  // %bb.0:
@@ -24,16 +30,9 @@ define i32 @f(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i
 ; CHECK-NEXT:     r0 = sub(r1,r0)
 ; CHECK-NEXT:     r2 = memw(r29+#32)
 ; CHECK-NEXT:     memw(r29+#32) = ##666
-; CHECK-NEXT:    }
+; CHECK-EMPTY:
+; CHECK-NEXT:    } :mem_noshuf
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r0 = xor(r0,r2)
 ; CHECK-NEXT:     jumpr r31
 ; CHECK-NEXT:    }
-  %alloca = alloca i32
-  store i32 %a, ptr %alloca     ; Should be elided.
-  store i32 666, ptr %alloca
-  %x = sub i32 %q1, %q2
-  %y = xor i32 %x, %a           ; Results in a load of %a from fixed stack.
-                                ; Using same frame index as elided %alloca.
-  ret i32 %y
-}
diff --git a/llvm/test/CodeGen/Hexagon/atomicrmw-cond-sub-clamp.ll b/llvm/test/CodeGen/Hexagon/atomicrmw-cond-sub-clamp.ll
index ba09c3e2852d..0e0b64aac4f6 100644
--- a/llvm/test/CodeGen/Hexagon/atomicrmw-cond-sub-clamp.ll
+++ b/llvm/test/CodeGen/Hexagon/atomicrmw-cond-sub-clamp.ll
@@ -152,10 +152,8 @@ define i64 @atomicrmw_usub_cond_i64(ptr %ptr, i64 %val) {
 ; CHECK-NEXT:     r5:4 = memd_locked(r0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7:6 = sub(r5:4,r3:2)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = cmp.gtu(r3:2,r5:4)
+; CHECK-NEXT:     r7:6 = sub(r5:4,r3:2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r8 = mux(p0,r4,r6)
diff --git a/llvm/test/CodeGen/Hexagon/atomicrmw-uinc-udec-wrap.ll b/llvm/test/CodeGen/Hexagon/atomicrmw-uinc-udec-wrap.ll
index 6b6946d0dbb0..8e673c1bb06b 100644
--- a/llvm/test/CodeGen/Hexagon/atomicrmw-uinc-udec-wrap.ll
+++ b/llvm/test/CodeGen/Hexagon/atomicrmw-uinc-udec-wrap.ll
@@ -156,12 +156,12 @@ define i64 @atomicrmw_uinc_wrap_i64(ptr %ptr, i64 %val) {
 ; CHECK-NEXT:     r5:4 = memd_locked(r0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     p0 = cmp.gtu(r3:2,r5:4)
 ; CHECK-NEXT:     r9:8 = add(r5:4,r7:6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     p0 = cmp.gtu(r3:2,r5:4)
-; CHECK-NEXT:     if (!p0.new) r8 = add(r1,#0)
-; CHECK-NEXT:     if (!p0.new) r9 = add(r1,#0)
+; CHECK-NEXT:     if (!p0) r8 = add(r1,#0)
+; CHECK-NEXT:     if (!p0) r9 = add(r1,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     memd_locked(r0,p0) = r9:8
@@ -345,13 +345,13 @@ define i64 @atomicrmw_udec_wrap_i64(ptr %ptr, i64 %val) {
 ; CHECK-NEXT:     r5:4 = memd_locked(r0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r13:12 = add(r5:4,r7:6)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
 ; CHECK-NEXT:     p1 = cmp.gtu(r5:4,r3:2)
 ; CHECK-NEXT:     p0 = cmp.eq(r5:4,r9:8)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     r13:12 = add(r5:4,r7:6)
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
 ; CHECK-NEXT:     r1 = mux(p1,r2,r12)
 ; CHECK-NEXT:     r14 = mux(p1,r3,r13)
 ; CHECK-NEXT:    }
diff --git a/llvm/test/CodeGen/Hexagon/autohvx/fp-to-int.ll b/llvm/test/CodeGen/Hexagon/autohvx/fp-to-int.ll
index ac51662242de..196b37678be6 100644
--- a/llvm/test/CodeGen/Hexagon/autohvx/fp-to-int.ll
+++ b/llvm/test/CodeGen/Hexagon/autohvx/fp-to-int.ll
@@ -13,13 +13,13 @@ define void @f16s8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(##32768,#1)
 ; CHECK-NEXT:     r4 = #14
-; CHECK-NEXT:     v1 = vmem(r0+#0)
+; CHECK-NEXT:     v0 = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2.h = vsplat(r3)
 ; CHECK-NEXT:     r6 = #5
 ; CHECK-NEXT:     v3.h = vasl(v0.h,r2)
-; CHECK-NEXT:     v0.cur = vmem(r0+#1)
+; CHECK-NEXT:     v1 = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v4.h = vsplat(r4)
@@ -33,55 +33,55 @@ define void @f16s8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3 = #16
-; CHECK-NEXT:     v5.h = vasl(v1.h,r6)
-; CHECK-NEXT:     q1 = vcmp.gt(v7.h,v0.h)
+; CHECK-NEXT:     v5.h = vasl(v0.h,r6)
+; CHECK-NEXT:     q1 = vcmp.gt(v7.h,v1.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v6.h = vsplat(r3)
-; CHECK-NEXT:     v27.h = vasr(v3.h,r5)
+; CHECK-NEXT:     v28.h = vasr(v3.h,r5)
 ; CHECK-NEXT:     v5 = vor(v5,v2)
-; CHECK-NEXT:     q0 = vcmp.gt(v7.h,v1.h)
+; CHECK-NEXT:     q0 = vcmp.gt(v7.h,v0.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v9.h = vsplat(r4)
 ; CHECK-NEXT:     v8.h = vasr(v8.h,r5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v26.h = vasl(v0.h,r6)
-; CHECK-NEXT:     v0.h = vsub(v4.h,v27.h)
+; CHECK-NEXT:     v27.h = vasl(v1.h,r6)
+; CHECK-NEXT:     v1.h = vsub(v4.h,v28.h)
 ; CHECK-NEXT:     v4.h = vsub(v4.h,v8.h)
-; CHECK-NEXT:     v28 = vmux(q0,v2,v9)
+; CHECK-NEXT:     v29 = vmux(q0,v2,v9)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     v1.h = vmin(v1.h,v6.h)
+; CHECK-NEXT:     v0 = vor(v27,v2)
 ; CHECK-NEXT:     v4.h = vmin(v4.h,v6.h)
-; CHECK-NEXT:     v1 = vor(v26,v2)
-; CHECK-NEXT:     v0.h = vmin(v0.h,v6.h)
 ; CHECK-NEXT:     v2 = vmux(q1,v2,v9)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     q2 = vcmp.gt(v4.h,v7.h)
-; CHECK-NEXT:     q3 = vcmp.gt(v0.h,v7.h)
+; CHECK-NEXT:     q2 = vcmp.gt(v1.h,v7.h)
+; CHECK-NEXT:     q3 = vcmp.gt(v4.h,v7.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v5.h = vlsr(v5.h,v4.h)
+; CHECK-NEXT:     v5.h = vlsr(v5.h,v1.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1.h = vlsr(v1.h,v0.h)
-; CHECK-NEXT:     v29.h = vsub(v7.h,v5.h)
+; CHECK-NEXT:     v0.h = vlsr(v0.h,v4.h)
+; CHECK-NEXT:     v30.h = vsub(v7.h,v5.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v30.h = vsub(v7.h,v1.h)
-; CHECK-NEXT:     v5 = vmux(q0,v29,v5)
+; CHECK-NEXT:     v31.h = vsub(v7.h,v0.h)
+; CHECK-NEXT:     v5 = vmux(q0,v30,v5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1 = vmux(q1,v30,v1)
-; CHECK-NEXT:     v31 = vmux(q2,v5,v28)
+; CHECK-NEXT:     v0 = vmux(q1,v31,v0)
+; CHECK-NEXT:     v1 = vmux(q2,v5,v29)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1 = vmux(q3,v1,v2)
+; CHECK-NEXT:     v0 = vmux(q3,v0,v2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v0.b = vpack(v1.h,v31.h):sat
+; CHECK-NEXT:     v0.b = vpack(v0.h,v1.h):sat
 ; CHECK-NEXT:     jumpr r31
 ; CHECK-NEXT:     vmem(r1+#0) = v0.new
 ; CHECK-NEXT:    }
@@ -491,127 +491,127 @@ define void @f32s8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK:         .cfi_startproc
 ; CHECK-NEXT:  // %bb.0:
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = ##-2147483648
 ; CHECK-NEXT:     r3:2 = combine(#1,#8)
-; CHECK-NEXT:     v5 = vmem(r0+#0)
+; CHECK-NEXT:     r4 = ##-2147483648
+; CHECK-NEXT:     v5 = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1 = vsplat(r4)
+; CHECK-NEXT:     v0 = vsplat(r4)
 ; CHECK-NEXT:     r7 = #30
 ; CHECK-NEXT:     r6 = #24
-; CHECK-NEXT:     v2 = vmem(r0+#2)
+; CHECK-NEXT:     v4 = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v10 = vsplat(r7)
 ; CHECK-NEXT:     r5 = #32
-; CHECK-NEXT:     v8.w = vasl(v4.w,r3)
-; CHECK-NEXT:     v4.cur = vmem(r0+#1)
+; CHECK-NEXT:     v9.w = vasl(v5.w,r3)
+; CHECK-NEXT:     v1 = vmem(r0+#3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v7.w = vasl(v5.w,r3)
-; CHECK-NEXT:     v12 = vxor(v12,v12)
-; CHECK-NEXT:     v8.w = vsub(v8.w,v1.w)
-; CHECK-NEXT:     v0 = vmem(r0+#3)
+; CHECK-NEXT:     v8.w = vasl(v4.w,r3)
+; CHECK-NEXT:     v14 = vxor(v14,v14)
+; CHECK-NEXT:     v9.w = vsub(v9.w,v0.w)
+; CHECK-NEXT:     v2 = vmem(r0+#2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v13 = vsplat(r5)
-; CHECK-NEXT:     v11.w = vasl(v0.w,r3)
-; CHECK-NEXT:     v7.w = vsub(v7.w,v1.w)
-; CHECK-NEXT:     q0 = vcmp.gt(v12.w,v5.w)
+; CHECK-NEXT:     v11.w = vasl(v2.w,r3)
+; CHECK-NEXT:     v8.w = vsub(v8.w,v0.w)
+; CHECK-NEXT:     q1 = vcmp.gt(v14.w,v5.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v9.w = vasl(v2.w,r3)
-; CHECK-NEXT:     q1 = vcmp.gt(v12.w,v4.w)
-; CHECK-NEXT:     v11.w = vsub(v11.w,v1.w)
+; CHECK-NEXT:     v12.w = vasl(v1.w,r3)
+; CHECK-NEXT:     q0 = vcmp.gt(v14.w,v4.w)
+; CHECK-NEXT:     v11.w = vsub(v11.w,v0.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3 = ##2147483647
 ; CHECK-NEXT:     r7 = #64
-; CHECK-NEXT:     v8.w = vasr(v8.w,r6)
+; CHECK-NEXT:     v9.w = vasr(v9.w,r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v22 = vsplat(r3)
-; CHECK-NEXT:     v7.w = vasr(v7.w,r6)
-; CHECK-NEXT:     v19.w = vsub(v9.w,v1.w)
-; CHECK-NEXT:     v8.w = vsub(v10.w,v8.w)
+; CHECK-NEXT:     v18 = vsplat(r3)
+; CHECK-NEXT:     v7.w = vasl(v5.w,r2)
+; CHECK-NEXT:     v19.w = vsub(v12.w,v0.w)
+; CHECK-NEXT:     v9.w = vsub(v10.w,v9.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v20.w = vasl(v4.w,r2)
-; CHECK-NEXT:     v27 = vmux(q1,v1,v22)
-; CHECK-NEXT:     v25 = vmux(q0,v1,v22)
-; CHECK-NEXT:     v7.w = vsub(v10.w,v7.w)
+; CHECK-NEXT:     v8.w = vasr(v8.w,r6)
+; CHECK-NEXT:     v25 = vmux(q1,v0,v18)
+; CHECK-NEXT:     v23 = vmux(q0,v0,v18)
+; CHECK-NEXT:     v9.w = vmin(v9.w,v13.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v6.w = vasl(v5.w,r2)
+; CHECK-NEXT:     v6.w = vasl(v4.w,r2)
+; CHECK-NEXT:     v7 = vor(v7,v0)
+; CHECK-NEXT:     v8.w = vsub(v10.w,v8.w)
+; CHECK-NEXT:     q3 = vcmp.gt(v9.w,v14.w)
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
+; CHECK-NEXT:     v11.w = vasr(v11.w,r6)
 ; CHECK-NEXT:     v8.w = vmin(v8.w,v13.w)
-; CHECK-NEXT:     v9 = vor(v20,v1)
-; CHECK-NEXT:     v21.w = vmin(v7.w,v13.w)
+; CHECK-NEXT:     v6 = vor(v6,v0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v5.w = vasr(v19.w,r6)
-; CHECK-NEXT:     q3 = vcmp.gt(v8.w,v12.w)
-; CHECK-NEXT:     v6 = vor(v6,v1)
-; CHECK-NEXT:     q2 = vcmp.gt(v21.w,v12.w)
+; CHECK-NEXT:     v11.w = vsub(v10.w,v11.w)
+; CHECK-NEXT:     q2 = vcmp.gt(v8.w,v14.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v11.w = vasr(v11.w,r6)
+; CHECK-NEXT:     v3.w = vasl(v1.w,r2)
 ; CHECK-NEXT:     v5.w = vsub(v10.w,v5.w)
+; CHECK-NEXT:     v21.w = vmin(v11.w,v13.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v3.w = vasl(v2.w,r2)
-; CHECK-NEXT:     v10.w = vsub(v10.w,v11.w)
+; CHECK-NEXT:     v20.w = vasl(v2.w,r2)
+; CHECK-NEXT:     v3 = vor(v3,v0)
 ; CHECK-NEXT:     v5.w = vmin(v5.w,v13.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v23.w = vasl(v0.w,r2)
-; CHECK-NEXT:     v3 = vor(v3,v1)
-; CHECK-NEXT:     v10.w = vmin(v10.w,v13.w)
+; CHECK-NEXT:     v7.w = vlsr(v7.w,v9.w)
+; CHECK-NEXT:     v12 = vor(v20,v0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v8.w = vlsr(v9.w,v8.w)
-; CHECK-NEXT:     v4 = vor(v23,v1)
+; CHECK-NEXT:     v6.w = vlsr(v6.w,v8.w)
+; CHECK-NEXT:     v24.w = vsub(v14.w,v7.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v6.w = vlsr(v6.w,v21.w)
-; CHECK-NEXT:     v26.w = vsub(v12.w,v8.w)
+; CHECK-NEXT:     v26.w = vlsr(v12.w,v21.w)
+; CHECK-NEXT:     v22.w = vsub(v14.w,v6.w)
+; CHECK-NEXT:     v7 = vmux(q1,v24,v7)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v3.w = vlsr(v3.w,v5.w)
-; CHECK-NEXT:     v24.w = vsub(v12.w,v6.w)
-; CHECK-NEXT:     v8 = vmux(q1,v26,v8)
+; CHECK-NEXT:     v6 = vmux(q0,v22,v6)
+; CHECK-NEXT:     q0 = vcmp.gt(v14.w,v2.w)
+; CHECK-NEXT:     v27.w = vsub(v14.w,v26.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v4.w = vlsr(v4.w,v10.w)
-; CHECK-NEXT:     v6 = vmux(q0,v24,v6)
-; CHECK-NEXT:     q0 = vcmp.gt(v12.w,v2.w)
-; CHECK-NEXT:     v28.w = vsub(v12.w,v3.w)
+; CHECK-NEXT:     v2 = vmux(q3,v7,v25)
+; CHECK-NEXT:     v29.w = vsub(v14.w,v3.w)
+; CHECK-NEXT:     q3 = vcmp.gt(v14.w,v1.w)
+; CHECK-NEXT:     v6 = vmux(q2,v6,v23)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v2 = vmux(q3,v8,v27)
-; CHECK-NEXT:     v29.w = vsub(v12.w,v4.w)
-; CHECK-NEXT:     q3 = vcmp.gt(v12.w,v0.w)
-; CHECK-NEXT:     v6 = vmux(q2,v6,v25)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
-; CHECK-NEXT:     v30 = vmux(q0,v1,v22)
-; CHECK-NEXT:     v3 = vmux(q0,v28,v3)
-; CHECK-NEXT:     q2 = vcmp.gt(v5.w,v12.w)
-; CHECK-NEXT:     v4 = vmux(q3,v29,v4)
+; CHECK-NEXT:     v30 = vmux(q0,v0,v18)
+; CHECK-NEXT:     v28 = vmux(q0,v27,v26)
+; CHECK-NEXT:     q2 = vcmp.gt(v21.w,v14.w)
+; CHECK-NEXT:     v3 = vmux(q3,v29,v3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2.h = vpack(v2.w,v6.w):sat
-; CHECK-NEXT:     v1 = vmux(q3,v1,v22)
-; CHECK-NEXT:     q3 = vcmp.gt(v10.w,v12.w)
-; CHECK-NEXT:     v0 = vmux(q2,v3,v30)
+; CHECK-NEXT:     v0 = vmux(q3,v0,v18)
+; CHECK-NEXT:     q3 = vcmp.gt(v5.w,v14.w)
+; CHECK-NEXT:     v1 = vmux(q2,v28,v30)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1 = vmux(q3,v4,v1)
+; CHECK-NEXT:     v0 = vmux(q3,v3,v0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v3.h = vpack(v1.w,v0.w):sat
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v0.h = vpack(v1.w,v0.w):sat
+; CHECK-NEXT:     v0.h = vpack(v0.w,v1.w):sat
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v31.b = vpack(v3.h,v2.h):sat
@@ -638,13 +638,13 @@ define void @f32s8_1(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(##-2147483648,#8)
 ; CHECK-NEXT:     r4 = #1
-; CHECK-NEXT:     v1 = vmem(r0+#0)
+; CHECK-NEXT:     v1 = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v3 = vsplat(r3)
 ; CHECK-NEXT:     r5 = #30
 ; CHECK-NEXT:     v4.w = vasl(v0.w,r4)
-; CHECK-NEXT:     v0.cur = vmem(r0+#1)
+; CHECK-NEXT:     v0.cur = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v5.w = vasl(v1.w,r4)
@@ -653,64 +653,64 @@ define void @f32s8_1(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:     r4 = #32
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v6 = vsplat(r5)
-; CHECK-NEXT:     v7 = vsplat(r4)
+; CHECK-NEXT:     v7 = vsplat(r5)
+; CHECK-NEXT:     v8 = vsplat(r4)
 ; CHECK-NEXT:     v2.w = vasl(v1.w,r2)
 ; CHECK-NEXT:     v5.w = vsub(v5.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v4.w = vasr(v4.w,r6)
-; CHECK-NEXT:     v26 = vxor(v26,v26)
+; CHECK-NEXT:     v27 = vxor(v27,v27)
 ; CHECK-NEXT:     v2 = vor(v2,v3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3 = ##2147483647
 ; CHECK-NEXT:     v5.w = vasr(v5.w,r6)
-; CHECK-NEXT:     q0 = vcmp.gt(v26.w,v1.w)
+; CHECK-NEXT:     q0 = vcmp.gt(v27.w,v0.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v27 = vsplat(r3)
-; CHECK-NEXT:     v4.w = vsub(v6.w,v4.w)
-; CHECK-NEXT:     q2 = vcmp.gt(v26.w,v0.w)
-; CHECK-NEXT:     v5.w = vsub(v6.w,v5.w)
+; CHECK-NEXT:     v28 = vsplat(r3)
+; CHECK-NEXT:     v6.w = vasl(v0.w,r2)
+; CHECK-NEXT:     v4.w = vsub(v7.w,v4.w)
+; CHECK-NEXT:     q2 = vcmp.gt(v27.w,v1.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v8.w = vasl(v0.w,r2)
-; CHECK-NEXT:     v4.w = vmin(v4.w,v7.w)
-; CHECK-NEXT:     v30 = vmux(q0,v3,v27)
-; CHECK-NEXT:     v5.w = vmin(v5.w,v7.w)
+; CHECK-NEXT:     v5.w = vsub(v7.w,v5.w)
+; CHECK-NEXT:     v4.w = vmin(v4.w,v8.w)
+; CHECK-NEXT:     v31 = vmux(q0,v3,v28)
+; CHECK-NEXT:     v6 = vor(v6,v3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v25 = vor(v8,v3)
-; CHECK-NEXT:     v1 = vmux(q2,v3,v27)
-; CHECK-NEXT:     q3 = vcmp.gt(v4.w,v26.w)
-; CHECK-NEXT:     q1 = vcmp.gt(v5.w,v26.w)
+; CHECK-NEXT:     v5.w = vmin(v5.w,v8.w)
+; CHECK-NEXT:     q1 = vcmp.gt(v4.w,v27.w)
+; CHECK-NEXT:     v0 = vmux(q2,v3,v28)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r2 = #64
-; CHECK-NEXT:     v2.w = vlsr(v2.w,v5.w)
+; CHECK-NEXT:     v6.w = vlsr(v6.w,v4.w)
+; CHECK-NEXT:     q3 = vcmp.gt(v5.w,v27.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v28.w = vlsr(v25.w,v4.w)
-; CHECK-NEXT:     v29.w = vsub(v26.w,v2.w)
+; CHECK-NEXT:     v2.w = vlsr(v2.w,v5.w)
+; CHECK-NEXT:     v29.w = vsub(v27.w,v6.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v6.w = vsub(v26.w,v28.w)
-; CHECK-NEXT:     v0 = vmux(q0,v29,v2)
+; CHECK-NEXT:     v30.w = vsub(v27.w,v2.w)
+; CHECK-NEXT:     v1 = vmux(q0,v29,v6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v31 = vmux(q2,v6,v28)
-; CHECK-NEXT:     v0 = vmux(q1,v0,v30)
+; CHECK-NEXT:     v2 = vmux(q2,v30,v2)
+; CHECK-NEXT:     v1 = vmux(q1,v1,v31)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     q3 = vsetq(r2)
-; CHECK-NEXT:     v1 = vmux(q3,v31,v1)
+; CHECK-NEXT:     v0 = vmux(q3,v2,v0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2.h = vpack(v1.w,v0.w):sat
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v0.h = vpack(v1.w,v0.w):sat
+; CHECK-NEXT:     v0.h = vpack(v0.w,v1.w):sat
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v0.b = vpack(v2.h,v0.h):sat
@@ -808,13 +808,13 @@ define void @f32s16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(##-2147483648,#1)
 ; CHECK-NEXT:     r4 = #30
-; CHECK-NEXT:     v1 = vmem(r0+#0)
+; CHECK-NEXT:     v0 = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2 = vsplat(r3)
 ; CHECK-NEXT:     r6 = #8
 ; CHECK-NEXT:     v3.w = vasl(v0.w,r2)
-; CHECK-NEXT:     v0.cur = vmem(r0+#1)
+; CHECK-NEXT:     v1 = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v4 = vsplat(r4)
@@ -828,55 +828,55 @@ define void @f32s16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3 = #32
-; CHECK-NEXT:     v5.w = vasl(v1.w,r6)
-; CHECK-NEXT:     q1 = vcmp.gt(v7.w,v0.w)
+; CHECK-NEXT:     v5.w = vasl(v0.w,r6)
+; CHECK-NEXT:     q1 = vcmp.gt(v7.w,v1.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v6 = vsplat(r3)
-; CHECK-NEXT:     v27.w = vasr(v3.w,r5)
+; CHECK-NEXT:     v28.w = vasr(v3.w,r5)
 ; CHECK-NEXT:     v5 = vor(v5,v2)
-; CHECK-NEXT:     q0 = vcmp.gt(v7.w,v1.w)
+; CHECK-NEXT:     q0 = vcmp.gt(v7.w,v0.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v9 = vsplat(r4)
 ; CHECK-NEXT:     v8.w = vasr(v8.w,r5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v26.w = vasl(v0.w,r6)
-; CHECK-NEXT:     v0.w = vsub(v4.w,v27.w)
+; CHECK-NEXT:     v27.w = vasl(v1.w,r6)
+; CHECK-NEXT:     v1.w = vsub(v4.w,v28.w)
 ; CHECK-NEXT:     v4.w = vsub(v4.w,v8.w)
-; CHECK-NEXT:     v28 = vmux(q0,v2,v9)
+; CHECK-NEXT:     v29 = vmux(q0,v2,v9)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     v1.w = vmin(v1.w,v6.w)
+; CHECK-NEXT:     v0 = vor(v27,v2)
 ; CHECK-NEXT:     v4.w = vmin(v4.w,v6.w)
-; CHECK-NEXT:     v1 = vor(v26,v2)
-; CHECK-NEXT:     v0.w = vmin(v0.w,v6.w)
 ; CHECK-NEXT:     v2 = vmux(q1,v2,v9)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     q2 = vcmp.gt(v4.w,v7.w)
-; CHECK-NEXT:     q3 = vcmp.gt(v0.w,v7.w)
+; CHECK-NEXT:     q2 = vcmp.gt(v1.w,v7.w)
+; CHECK-NEXT:     q3 = vcmp.gt(v4.w,v7.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v5.w = vlsr(v5.w,v4.w)
+; CHECK-NEXT:     v5.w = vlsr(v5.w,v1.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1.w = vlsr(v1.w,v0.w)
-; CHECK-NEXT:     v29.w = vsub(v7.w,v5.w)
+; CHECK-NEXT:     v0.w = vlsr(v0.w,v4.w)
+; CHECK-NEXT:     v30.w = vsub(v7.w,v5.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v30.w = vsub(v7.w,v1.w)
-; CHECK-NEXT:     v5 = vmux(q0,v29,v5)
+; CHECK-NEXT:     v31.w = vsub(v7.w,v0.w)
+; CHECK-NEXT:     v5 = vmux(q0,v30,v5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1 = vmux(q1,v30,v1)
-; CHECK-NEXT:     v31 = vmux(q2,v5,v28)
+; CHECK-NEXT:     v0 = vmux(q1,v31,v0)
+; CHECK-NEXT:     v1 = vmux(q2,v5,v29)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v1 = vmux(q3,v1,v2)
+; CHECK-NEXT:     v0 = vmux(q3,v0,v2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v0.h = vpack(v1.w,v31.w):sat
+; CHECK-NEXT:     v0.h = vpack(v0.w,v1.w):sat
 ; CHECK-NEXT:     jumpr r31
 ; CHECK-NEXT:     vmem(r1+#0) = v0.new
 ; CHECK-NEXT:    }
@@ -1097,13 +1097,13 @@ define void @f16u8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(##32768,#1)
 ; CHECK-NEXT:     r4 = #14
-; CHECK-NEXT:     v0 = vmem(r0+#1)
+; CHECK-NEXT:     v0 = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2.h = vsplat(r3)
 ; CHECK-NEXT:     r7:6 = combine(#11,#16)
 ; CHECK-NEXT:     v3.h = vasl(v0.h,r2)
-; CHECK-NEXT:     v1 = vmem(r0+#0)
+; CHECK-NEXT:     v1 = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v6.h = vsplat(r4)
@@ -1113,7 +1113,7 @@ define void @f16u8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v7.h = vsplat(r6)
-; CHECK-NEXT:     v5.h = vasl(v1.h,r5)
+; CHECK-NEXT:     v5.h = vasl(v0.h,r5)
 ; CHECK-NEXT:     v4.h = vsub(v4.h,v2.h)
 ; CHECK-NEXT:     v28 = vxor(v28,v28)
 ; CHECK-NEXT:    }
@@ -1125,28 +1125,26 @@ define void @f16u8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v29.h = vsplat(r2)
 ; CHECK-NEXT:     v4.h = vasr(v4.h,r7)
-; CHECK-NEXT:     q2 = vcmp.gt(v28.h,v1.h)
+; CHECK-NEXT:     q2 = vcmp.gt(v28.h,v0.h)
 ; CHECK-NEXT:     v3.h = vsub(v6.h,v3.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v8.h = vasl(v0.h,r5)
-; CHECK-NEXT:     q3 = vcmp.gt(v28.h,v0.h)
+; CHECK-NEXT:     v8.h = vasl(v1.h,r5)
+; CHECK-NEXT:     q3 = vcmp.gt(v28.h,v1.h)
 ; CHECK-NEXT:     v4.h = vsub(v6.h,v4.h)
 ; CHECK-NEXT:     v3.h = vmin(v3.h,v7.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v4.h = vmin(v4.h,v7.h)
 ; CHECK-NEXT:     v2 = vor(v8,v2)
-; CHECK-NEXT:     q1 = vcmp.gt(v28.h,v3.h)
+; CHECK-NEXT:     q0 = vcmp.gt(v28.h,v3.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     q0 = vcmp.gt(v28.h,v4.h)
+; CHECK-NEXT:     v5.h = vlsr(v5.h,v3.h)
+; CHECK-NEXT:     q1 = vcmp.gt(v28.h,v4.h)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v5.h = vlsr(v5.h,v4.h)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
-; CHECK-NEXT:     v2.h = vlsr(v2.h,v3.h)
+; CHECK-NEXT:     v2.h = vlsr(v2.h,v4.h)
 ; CHECK-NEXT:     v30 = vmux(q0,v29,v5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -1552,7 +1550,7 @@ define void @f32u8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:     v5 = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v3 = vsplat(r4)
+; CHECK-NEXT:     v4 = vsplat(r4)
 ; CHECK-NEXT:     r5 = #30
 ; CHECK-NEXT:     r6 = #24
 ; CHECK-NEXT:     v2 = vmem(r0+#1)
@@ -1561,32 +1559,32 @@ define void @f32u8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:     v14 = vsplat(r5)
 ; CHECK-NEXT:     r4 = #32
 ; CHECK-NEXT:     v8.w = vasl(v5.w,r2)
-; CHECK-NEXT:     v0 = vmem(r0+#3)
+; CHECK-NEXT:     v0 = vmem(r0+#2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v9.w = vasl(v2.w,r2)
 ; CHECK-NEXT:     v13 = vxor(v13,v13)
-; CHECK-NEXT:     v8.w = vsub(v8.w,v3.w)
-; CHECK-NEXT:     v1 = vmem(r0+#2)
+; CHECK-NEXT:     v8.w = vsub(v8.w,v4.w)
+; CHECK-NEXT:     v1 = vmem(r0+#3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v20 = vsplat(r4)
-; CHECK-NEXT:     v12.w = vasl(v0.w,r2)
-; CHECK-NEXT:     v9.w = vsub(v9.w,v3.w)
+; CHECK-NEXT:     v21 = vsplat(r4)
+; CHECK-NEXT:     v11.w = vasl(v0.w,r2)
+; CHECK-NEXT:     v9.w = vsub(v9.w,v4.w)
 ; CHECK-NEXT:     q0 = vcmp.gt(v13.w,v5.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v11.w = vasl(v1.w,r2)
+; CHECK-NEXT:     v12.w = vasl(v1.w,r2)
 ; CHECK-NEXT:     q3 = vcmp.gt(v13.w,v2.w)
-; CHECK-NEXT:     v12.w = vsub(v12.w,v3.w)
+; CHECK-NEXT:     v11.w = vsub(v11.w,v4.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r2 = ##2147483647
 ; CHECK-NEXT:     r7 = #64
-; CHECK-NEXT:     v11.w = vsub(v11.w,v3.w)
+; CHECK-NEXT:     v12.w = vsub(v12.w,v4.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v22 = vsplat(r2)
+; CHECK-NEXT:     v23 = vsplat(r2)
 ; CHECK-NEXT:     v8.w = vasr(v8.w,r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -1596,68 +1594,68 @@ define void @f32u8_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v6.w = vasl(v5.w,r3)
 ; CHECK-NEXT:     v9.w = vsub(v14.w,v9.w)
-; CHECK-NEXT:     v8.w = vmin(v8.w,v20.w)
+; CHECK-NEXT:     v8.w = vmin(v8.w,v21.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v7.w = vasl(v2.w,r3)
-; CHECK-NEXT:     v6 = vor(v6,v3)
-; CHECK-NEXT:     v9.w = vmin(v9.w,v20.w)
+; CHECK-NEXT:     v6 = vor(v6,v4)
+; CHECK-NEXT:     v9.w = vmin(v9.w,v21.w)
 ; CHECK-NEXT:     q1 = vcmp.gt(v13.w,v8.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v19.w = vasr(v11.w,r6)
-; CHECK-NEXT:     v7 = vor(v7,v3)
+; CHECK-NEXT:     v20.w = vasr(v11.w,r6)
+; CHECK-NEXT:     v7 = vor(v7,v4)
 ; CHECK-NEXT:     q2 = vcmp.gt(v13.w,v9.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v12.w = vasr(v12.w,r6)
-; CHECK-NEXT:     v5.w = vsub(v14.w,v19.w)
+; CHECK-NEXT:     v5.w = vsub(v14.w,v20.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v4.w = vasl(v1.w,r3)
-; CHECK-NEXT:     v21.w = vsub(v14.w,v12.w)
-; CHECK-NEXT:     v5.w = vmin(v5.w,v20.w)
+; CHECK-NEXT:     v3.w = vasl(v1.w,r3)
+; CHECK-NEXT:     v22.w = vsub(v14.w,v12.w)
+; CHECK-NEXT:     v5.w = vmin(v5.w,v21.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v10.w = vasl(v0.w,r3)
-; CHECK-NEXT:     v4 = vor(v4,v3)
+; CHECK-NEXT:     v3 = vor(v3,v4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v6.w = vlsr(v6.w,v8.w)
-; CHECK-NEXT:     v3 = vor(v10,v3)
-; CHECK-NEXT:     v10.w = vmin(v21.w,v20.w)
+; CHECK-NEXT:     v10 = vor(v10,v4)
+; CHECK-NEXT:     v4.w = vmin(v22.w,v21.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v7.w = vlsr(v7.w,v9.w)
-; CHECK-NEXT:     v24 = vmux(q1,v22,v6)
+; CHECK-NEXT:     v6 = vmux(q1,v23,v6)
 ; CHECK-NEXT:     q1 = vcmp.gt(v13.w,v5.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v23.w = vlsr(v4.w,v5.w)
-; CHECK-NEXT:     v25 = vmux(q2,v22,v7)
-; CHECK-NEXT:     q2 = vcmp.gt(v13.w,v10.w)
-; CHECK-NEXT:     v4 = vmux(q0,v13,v24)
+; CHECK-NEXT:     v24.w = vlsr(v10.w,v5.w)
+; CHECK-NEXT:     v7 = vmux(q2,v23,v7)
+; CHECK-NEXT:     q2 = vcmp.gt(v13.w,v4.w)
+; CHECK-NEXT:     v25 = vmux(q0,v13,v6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v3.w = vlsr(v3.w,v10.w)
-; CHECK-NEXT:     v26 = vmux(q3,v13,v25)
-; CHECK-NEXT:     v2 = vmux(q1,v22,v23)
-; CHECK-NEXT:     q1 = vcmp.gt(v13.w,v1.w)
+; CHECK-NEXT:     v3.w = vlsr(v3.w,v4.w)
+; CHECK-NEXT:     v26 = vmux(q3,v13,v7)
+; CHECK-NEXT:     v2 = vmux(q1,v23,v24)
+; CHECK-NEXT:     q1 = vcmp.gt(v13.w,v0.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v27 = vmux(q2,v22,v3)
-; CHECK-NEXT:     q3 = vcmp.gt(v13.w,v0.w)
+; CHECK-NEXT:     v27 = vmux(q2,v23,v3)
+; CHECK-NEXT:     q3 = vcmp.gt(v13.w,v1.w)
 ; CHECK-NEXT:     v28 = vmux(q1,v13,v2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v29.uh = vpack(v26.w,v4.w):sat
-; CHECK-NEXT:     v1 = vmux(q3,v13,v27)
+; CHECK-NEXT:     v29.uh = vpack(v26.w,v25.w):sat
+; CHECK-NEXT:     v0 = vmux(q3,v13,v27)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v30.uh = vpack(v1.w,v28.w):sat
+; CHECK-NEXT:     v30.uh = vpack(v28.w,v0.w):sat
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v0.uh = vpack(v1.w,v28.w):sat
+; CHECK-NEXT:     v0.uh = vpack(v0.w,v28.w):sat
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v31.ub = vpack(v30.h,v29.h):sat
@@ -1684,13 +1682,13 @@ define void @f32u8_1(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(##-2147483648,#1)
 ; CHECK-NEXT:     r4 = #30
-; CHECK-NEXT:     v0 = vmem(r0+#1)
+; CHECK-NEXT:     v0 = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2 = vsplat(r3)
 ; CHECK-NEXT:     r7:6 = combine(#24,#32)
 ; CHECK-NEXT:     v3.w = vasl(v0.w,r2)
-; CHECK-NEXT:     v1 = vmem(r0+#0)
+; CHECK-NEXT:     v1 = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v6 = vsplat(r4)
@@ -1700,7 +1698,7 @@ define void @f32u8_1(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v7 = vsplat(r6)
-; CHECK-NEXT:     v5.w = vasl(v1.w,r5)
+; CHECK-NEXT:     v5.w = vasl(v0.w,r5)
 ; CHECK-NEXT:     v4.w = vsub(v4.w,v2.w)
 ; CHECK-NEXT:     v27 = vxor(v27,v27)
 ; CHECK-NEXT:    }
@@ -1712,13 +1710,13 @@ define void @f32u8_1(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v28 = vsplat(r3)
 ; CHECK-NEXT:     v4.w = vasr(v4.w,r7)
-; CHECK-NEXT:     q2 = vcmp.gt(v27.w,v1.w)
+; CHECK-NEXT:     q2 = vcmp.gt(v27.w,v0.w)
 ; CHECK-NEXT:     v3.w = vsub(v6.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r2 = #64
-; CHECK-NEXT:     v8.w = vasl(v0.w,r5)
-; CHECK-NEXT:     q3 = vcmp.gt(v27.w,v0.w)
+; CHECK-NEXT:     v8.w = vasl(v1.w,r5)
+; CHECK-NEXT:     q3 = vcmp.gt(v27.w,v1.w)
 ; CHECK-NEXT:     v4.w = vsub(v6.w,v4.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -1727,14 +1725,14 @@ define void @f32u8_1(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:     v2 = vor(v8,v2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     q1 = vcmp.gt(v27.w,v3.w)
-; CHECK-NEXT:     q0 = vcmp.gt(v27.w,v4.w)
+; CHECK-NEXT:     q0 = vcmp.gt(v27.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v5.w = vlsr(v5.w,v4.w)
+; CHECK-NEXT:     v5.w = vlsr(v5.w,v3.w)
+; CHECK-NEXT:     q1 = vcmp.gt(v27.w,v4.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v2.w = vlsr(v2.w,v3.w)
+; CHECK-NEXT:     v2.w = vlsr(v2.w,v4.w)
 ; CHECK-NEXT:     v29 = vmux(q0,v28,v5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -1843,13 +1841,13 @@ define void @f32u16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(##-2147483648,#1)
 ; CHECK-NEXT:     r4 = #30
-; CHECK-NEXT:     v0 = vmem(r0+#1)
+; CHECK-NEXT:     v0 = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2 = vsplat(r3)
 ; CHECK-NEXT:     r7:6 = combine(#24,#32)
 ; CHECK-NEXT:     v3.w = vasl(v0.w,r2)
-; CHECK-NEXT:     v1 = vmem(r0+#0)
+; CHECK-NEXT:     v1 = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v6 = vsplat(r4)
@@ -1859,7 +1857,7 @@ define void @f32u16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v7 = vsplat(r6)
-; CHECK-NEXT:     v5.w = vasl(v1.w,r5)
+; CHECK-NEXT:     v5.w = vasl(v0.w,r5)
 ; CHECK-NEXT:     v4.w = vsub(v4.w,v2.w)
 ; CHECK-NEXT:     v28 = vxor(v28,v28)
 ; CHECK-NEXT:    }
@@ -1871,28 +1869,26 @@ define void @f32u16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v29 = vsplat(r2)
 ; CHECK-NEXT:     v4.w = vasr(v4.w,r7)
-; CHECK-NEXT:     q2 = vcmp.gt(v28.w,v1.w)
+; CHECK-NEXT:     q2 = vcmp.gt(v28.w,v0.w)
 ; CHECK-NEXT:     v3.w = vsub(v6.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v8.w = vasl(v0.w,r5)
-; CHECK-NEXT:     q3 = vcmp.gt(v28.w,v0.w)
+; CHECK-NEXT:     v8.w = vasl(v1.w,r5)
+; CHECK-NEXT:     q3 = vcmp.gt(v28.w,v1.w)
 ; CHECK-NEXT:     v4.w = vsub(v6.w,v4.w)
 ; CHECK-NEXT:     v3.w = vmin(v3.w,v7.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v4.w = vmin(v4.w,v7.w)
 ; CHECK-NEXT:     v2 = vor(v8,v2)
-; CHECK-NEXT:     q1 = vcmp.gt(v28.w,v3.w)
+; CHECK-NEXT:     q0 = vcmp.gt(v28.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     q0 = vcmp.gt(v28.w,v4.w)
+; CHECK-NEXT:     v5.w = vlsr(v5.w,v3.w)
+; CHECK-NEXT:     q1 = vcmp.gt(v28.w,v4.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v5.w = vlsr(v5.w,v4.w)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
-; CHECK-NEXT:     v2.w = vlsr(v2.w,v3.w)
+; CHECK-NEXT:     v2.w = vlsr(v2.w,v4.w)
 ; CHECK-NEXT:     v30 = vmux(q0,v29,v5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
diff --git a/llvm/test/CodeGen/Hexagon/autohvx/int-to-fp.ll b/llvm/test/CodeGen/Hexagon/autohvx/int-to-fp.ll
index c0e38b924303..c3308ec19399 100644
--- a/llvm/test/CodeGen/Hexagon/autohvx/int-to-fp.ll
+++ b/llvm/test/CodeGen/Hexagon/autohvx/int-to-fp.ll
@@ -1042,7 +1042,7 @@ define void @s32f16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(#8,#1)
 ; CHECK-NEXT:     r6 = #255
-; CHECK-NEXT:     v6.w = vabs(v1.w)
+; CHECK-NEXT:     v4.w = vabs(v1.w)
 ; CHECK-NEXT:     v1.cur = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -1054,102 +1054,102 @@ define void @s32f16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v9 = vsplat(r4)
 ; CHECK-NEXT:     v8 = vsplat(r6)
-; CHECK-NEXT:     v3.uw = vcl0(v6.uw)
-; CHECK-NEXT:     v20 = vxor(v20,v20)
+; CHECK-NEXT:     r4 = #159
+; CHECK-NEXT:     v3.uw = vcl0(v4.uw)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = #159
-; CHECK-NEXT:     v4.uw = vcl0(v5.uw)
-; CHECK-NEXT:     v3.w = vadd(v3.w,v2.w)
+; CHECK-NEXT:     v6.uw = vcl0(v5.uw)
+; CHECK-NEXT:     v7.w = vadd(v3.w,v2.w)
+; CHECK-NEXT:     v3 = vxor(v3,v3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v27 = vsplat(r4)
+; CHECK-NEXT:     v26 = vsplat(r4)
 ; CHECK-NEXT:     r5 = ##-2147483648
-; CHECK-NEXT:     v7.w = vadd(v4.w,v2.w)
+; CHECK-NEXT:     v6.w = vadd(v6.w,v2.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v13 = vsplat(r5)
-; CHECK-NEXT:     v6.w = vasl(v6.w,v3.w)
-; CHECK-NEXT:     q0 = vcmp.gt(v20.w,v1.w)
+; CHECK-NEXT:     v4.w = vasl(v4.w,v7.w)
+; CHECK-NEXT:     q0 = vcmp.gt(v3.w,v1.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v5.w = vasl(v5.w,v7.w)
-; CHECK-NEXT:     v26 = vmux(q0,v13,v20)
-; CHECK-NEXT:     v10.w = vadd(v6.w,v8.w)
-; CHECK-NEXT:     v11 = vand(v6,v9)
+; CHECK-NEXT:     v5.w = vasl(v5.w,v6.w)
+; CHECK-NEXT:     v25 = vmux(q0,v13,v3)
+; CHECK-NEXT:     v10.w = vadd(v4.w,v8.w)
+; CHECK-NEXT:     v11 = vand(v4,v9)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v9 = vand(v5,v9)
-; CHECK-NEXT:     q3 = vcmp.eq(v11.w,v20.w)
+; CHECK-NEXT:     q3 = vcmp.eq(v11.w,v3.w)
 ; CHECK-NEXT:     v8.w = vadd(v5.w,v8.w)
-; CHECK-NEXT:     q1 = vcmp.gt(v6.uw,v10.uw)
+; CHECK-NEXT:     q1 = vcmp.gt(v4.uw,v10.uw)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v21.uw = vlsr(v10.uw,r3)
-; CHECK-NEXT:     q2 = vcmp.eq(v9.w,v20.w)
-; CHECK-NEXT:     v22 = vmux(q3,v20,v2)
+; CHECK-NEXT:     v20.uw = vlsr(v10.uw,r3)
+; CHECK-NEXT:     q2 = vcmp.eq(v9.w,v3.w)
+; CHECK-NEXT:     v21 = vmux(q3,v3,v2)
 ; CHECK-NEXT:     q3 = vcmp.gt(v5.uw,v8.uw)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v8.uw = vlsr(v8.uw,r3)
-; CHECK-NEXT:     v9.w = vadd(v21.w,v22.w)
-; CHECK-NEXT:     v24 = vmux(q2,v20,v2)
-; CHECK-NEXT:     v23 = vmux(q1,v2,v20)
+; CHECK-NEXT:     v9.w = vadd(v20.w,v21.w)
+; CHECK-NEXT:     v23 = vmux(q2,v3,v2)
+; CHECK-NEXT:     v22 = vmux(q1,v2,v3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v12.uw = vlsr(v6.uw,r3)
-; CHECK-NEXT:     v2 = vmux(q3,v2,v20)
-; CHECK-NEXT:     v25.w = vadd(v8.w,v24.w)
-; CHECK-NEXT:     v3.w = vsub(v23.w,v3.w)
+; CHECK-NEXT:     v12.uw = vlsr(v4.uw,r3)
+; CHECK-NEXT:     v2 = vmux(q3,v2,v3)
+; CHECK-NEXT:     v24.w = vadd(v8.w,v23.w)
+; CHECK-NEXT:     v7.w = vsub(v22.w,v7.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v5.uw = vlsr(v5.uw,r3)
-; CHECK-NEXT:     v2.w = vsub(v2.w,v7.w)
-; CHECK-NEXT:     q3 = vcmp.eq(v12.w,v21.w)
-; CHECK-NEXT:     v3.w = vadd(v3.w,v27.w)
+; CHECK-NEXT:     v2.w = vsub(v2.w,v6.w)
+; CHECK-NEXT:     q3 = vcmp.eq(v12.w,v20.w)
+; CHECK-NEXT:     v7.w = vadd(v7.w,v26.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3 = #23
-; CHECK-NEXT:     v6.uw = vlsr(v21.uw,r2)
+; CHECK-NEXT:     v4.uw = vlsr(v20.uw,r2)
 ; CHECK-NEXT:     q2 = vcmp.eq(v5.w,v8.w)
-; CHECK-NEXT:     v2.w = vadd(v2.w,v27.w)
+; CHECK-NEXT:     v2.w = vadd(v2.w,v26.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v9.uw = vlsr(v9.uw,r2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v28.uw = vlsr(v25.uw,r2)
-; CHECK-NEXT:     v6 = vmux(q3,v9,v6)
-; CHECK-NEXT:     q3 = vcmp.gt(v20.w,v0.w)
+; CHECK-NEXT:     v27.uw = vlsr(v24.uw,r2)
+; CHECK-NEXT:     v4 = vmux(q3,v9,v4)
+; CHECK-NEXT:     q3 = vcmp.gt(v3.w,v0.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v29.uw = vlsr(v8.uw,r2)
-; CHECK-NEXT:     v30 = vmux(q3,v13,v20)
-; CHECK-NEXT:     v6 = vor(v26,v6)
-; CHECK-NEXT:     q3 = vcmp.eq(v0.w,v20.w)
+; CHECK-NEXT:     v28.uw = vlsr(v8.uw,r2)
+; CHECK-NEXT:     v30 = vmux(q3,v13,v3)
+; CHECK-NEXT:     v4 = vor(v25,v4)
+; CHECK-NEXT:     q3 = vcmp.eq(v0.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v3.w = vasl(v3.w,r3)
-; CHECK-NEXT:     v5 = vmux(q2,v28,v29)
-; CHECK-NEXT:     q2 = vcmp.eq(v1.w,v20.w)
+; CHECK-NEXT:     v29.w = vasl(v7.w,r3)
+; CHECK-NEXT:     v5 = vmux(q2,v27,v28)
+; CHECK-NEXT:     q2 = vcmp.eq(v1.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2.w = vasl(v2.w,r3)
 ; CHECK-NEXT:     v31 = vor(v30,v5)
-; CHECK-NEXT:     v3 = vor(v6,v3)
+; CHECK-NEXT:     v4 = vor(v4,v29)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v1 = vor(v31,v2)
-; CHECK-NEXT:     v3 = vmux(q2,v20,v3)
+; CHECK-NEXT:     v4 = vmux(q2,v3,v4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v0 = vmux(q3,v20,v1)
+; CHECK-NEXT:     v0 = vmux(q3,v3,v1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v2.qf32 = vadd(v3.sf,v20.sf)
+; CHECK-NEXT:     v2.qf32 = vadd(v4.sf,v3.sf)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v3.qf32 = vadd(v0.sf,v20.sf)
+; CHECK-NEXT:     v3.qf32 = vadd(v0.sf,v3.sf)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v0.hf = v3:2.qf32
@@ -2369,20 +2369,20 @@ define void @u32f16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r3:2 = combine(#8,#1)
 ; CHECK-NEXT:     r6 = #255
-; CHECK-NEXT:     v3.uw = vcl0(v0.uw)
-; CHECK-NEXT:     v0.cur = vmem(r0+#1)
+; CHECK-NEXT:     v3.uw = vcl0(v1.uw)
+; CHECK-NEXT:     v1.cur = vmem(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v2 = vsplat(r2)
 ; CHECK-NEXT:     r4 = #512
-; CHECK-NEXT:     v4.uw = vcl0(v1.uw)
-; CHECK-NEXT:     v1.cur = vmem(r0+#0)
+; CHECK-NEXT:     v4.uw = vcl0(v0.uw)
+; CHECK-NEXT:     v0.cur = vmem(r0+#1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v7 = vsplat(r4)
 ; CHECK-NEXT:     v6 = vsplat(r6)
-; CHECK-NEXT:     v4.w = vadd(v4.w,v2.w)
 ; CHECK-NEXT:     v3.w = vadd(v3.w,v2.w)
+; CHECK-NEXT:     v4.w = vadd(v4.w,v2.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r4 = #159
@@ -2390,10 +2390,10 @@ define void @u32f16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v10 = vsplat(r4)
-; CHECK-NEXT:     v5.w = vasl(v1.w,v4.w)
+; CHECK-NEXT:     v5.w = vasl(v1.w,v3.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v8.w = vasl(v0.w,v3.w)
+; CHECK-NEXT:     v8.w = vasl(v0.w,v4.w)
 ; CHECK-NEXT:     v11.w = vadd(v5.w,v6.w)
 ; CHECK-NEXT:     v13 = vand(v5,v7)
 ; CHECK-NEXT:    }
@@ -2416,15 +2416,15 @@ define void @u32f16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:     v2 = vmux(q0,v9,v2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v4.w = vsub(v29.w,v4.w)
+; CHECK-NEXT:     v3.w = vsub(v29.w,v3.w)
 ; CHECK-NEXT:     v7.w = vadd(v27.w,v28.w)
-; CHECK-NEXT:     v3.w = vsub(v30.w,v3.w)
+; CHECK-NEXT:     v4.w = vsub(v30.w,v4.w)
 ; CHECK-NEXT:     v2.w = vadd(v6.w,v2.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v12.uw = vlsr(v5.uw,r3)
-; CHECK-NEXT:     v4.w = vadd(v4.w,v10.w)
 ; CHECK-NEXT:     v3.w = vadd(v3.w,v10.w)
+; CHECK-NEXT:     v4.w = vadd(v4.w,v10.w)
 ; CHECK-NEXT:     q2 = vcmp.eq(v1.w,v9.w)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -2448,16 +2448,16 @@ define void @u32f16_0(ptr %a0, ptr %a1) #0 {
 ; CHECK-NEXT:     v6.uw = vlsr(v6.uw,r2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v4.w = vasl(v4.w,r3)
+; CHECK-NEXT:     v3.w = vasl(v3.w,r3)
 ; CHECK-NEXT:     v31 = vmux(q1,v2,v6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     v2.w = vasl(v3.w,r3)
-; CHECK-NEXT:     v4 = vor(v5,v4)
+; CHECK-NEXT:     v2.w = vasl(v4.w,r3)
+; CHECK-NEXT:     v3 = vor(v5,v3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v1 = vor(v31,v2)
-; CHECK-NEXT:     v3 = vmux(q2,v9,v4)
+; CHECK-NEXT:     v3 = vmux(q2,v9,v3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     v0 = vmux(q3,v9,v1)
diff --git a/llvm/test/CodeGen/Hexagon/bank-conflict.mir b/llvm/test/CodeGen/Hexagon/bank-conflict.mir
index 12d7838b8372..f32c3868dcf0 100644
--- a/llvm/test/CodeGen/Hexagon/bank-conflict.mir
+++ b/llvm/test/CodeGen/Hexagon/bank-conflict.mir
@@ -8,9 +8,9 @@
 # CHECK: = A2_tfr
 # CHECK: = L2_loadrigp
 
-# CHECK: = L4_loadri_rr
 # CHECK: = S2_tstbit_i
 # CHECK: = L4_loadri_rr
+# CHECK: = L4_loadri_rr
 
 --- |
   %s.0 = type { [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [24 x i32], [3 x i32], [24 x i32], [8 x %s.1], [5 x i32] }
diff --git a/llvm/test/CodeGen/Hexagon/calloperand-v128i1.ll b/llvm/test/CodeGen/Hexagon/calloperand-v128i1.ll
new file mode 100644
index 000000000000..ddac8c1cd827
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/calloperand-v128i1.ll
@@ -0,0 +1,39 @@
+;RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length128b < %s -o - | FileCheck %s
+
+; CHECK-LABEL: compare_vectors
+; CHECK: [[REG1:(q[0-9]+)]] = vcmp.eq(v{{[0-9]+}}.b,v{{[0-9]+}}.b)
+; CHECK: [[REG2:(r[0-9]+)]] = #-1
+; CHECK: v0 = vand([[REG1]],[[REG2]])
+
+define void @compare_vectors(<128 x i8> %a, <128 x i8> %b) {
+entry:
+  %result = icmp eq <128 x i8> %a, %b
+  call i32 @f.1(<128 x i1> %result)
+  ret void
+}
+
+; CHECK-LABEL: f.1:
+; CHECK: [[REG3:(q[0-9]+)]] = vand(v0,r{{[0-9]+}})
+; CHECK: [[REG4:(v[0-9]+)]] = vand([[REG3]],r{{[0-9]+}})
+; CHECK: r{{[0-9]+}} = vextract([[REG4]],r{{[0-9]+}})
+
+define i32 @f.1(<128 x i1> %vec) {
+  %element = extractelement <128 x i1> %vec, i32 6
+  %is_true = icmp eq i1 %element, true
+  br i1 %is_true, label %if_true, label %if_false
+
+if_true:
+  call void @action_if_true()
+  br label %end
+
+if_false:
+  call void @action_if_false()
+  br label %end
+
+end:
+  %result = phi i32 [1, %if_true], [0, %if_false]
+  ret i32 %result
+}
+
+declare void @action_if_true()
+declare void @action_if_false()
diff --git a/llvm/test/CodeGen/Hexagon/calloperand-v16i1.ll b/llvm/test/CodeGen/Hexagon/calloperand-v16i1.ll
new file mode 100644
index 000000000000..bbb2697246df
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/calloperand-v16i1.ll
@@ -0,0 +1,40 @@
+;RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length64b < %s -o - | FileCheck %s --check-prefix=CHECK
+;RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length128b < %s -o - | FileCheck %s --check-prefix=CHECK
+
+; CHECK-LABEL: compare_vectors
+; CHECK: [[REG1:(q[0-9]+)]] = vcmp.eq(v{{[0-9]+}}.w,v{{[0-9]+}}.w)
+; CHECK: [[REG2:(r[0-9]+)]] = #-1
+; CHECK: v0  = vand([[REG1]],[[REG2]])
+
+define void @compare_vectors(<16 x i32> %a, <16 x i32> %b) {
+entry:
+  %result = icmp eq <16 x i32> %a, %b
+  call i32 @f.1(<16 x i1> %result)
+  ret void
+}
+
+; CHECK-LABEL: f.1:
+; CHECK: [[REG3:(q[0-9]+)]] = vand(v0,r{{[0-9]+}})
+; CHECK: [[REG4:(v[0-9]+)]] = vand([[REG3]],r{{[0-9]+}})
+; CHECK: r{{[0-9]+}} = vextract([[REG4]],r{{[0-9]+}})
+
+define i32 @f.1(<16 x i1> %vec) {
+  %element = extractelement <16 x i1> %vec, i32 6
+  %is_true = icmp eq i1 %element, true
+  br i1 %is_true, label %if_true, label %if_false
+
+if_true:
+  call void @action_if_true()
+  br label %end
+
+if_false:
+  call void @action_if_false()
+  br label %end
+
+end:
+  %result = phi i32 [1, %if_true], [0, %if_false]
+  ret i32 %result
+}
+
+declare void @action_if_true()
+declare void @action_if_false()
diff --git a/llvm/test/CodeGen/Hexagon/calloperand-v2i1.ll b/llvm/test/CodeGen/Hexagon/calloperand-v2i1.ll
new file mode 100644
index 000000000000..2dcd5fe571e3
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/calloperand-v2i1.ll
@@ -0,0 +1,14 @@
+;RUN: llc -mtriple=hexagon  < %s | FileCheck %s
+
+; Check that v2i1 type is promoted to v2i32.
+; CHECK: call f
+; CHECK: r{{[0-9]+}}:{{[0-9]+}} = memd(r29+#8)
+
+define  <2 x i1> @test(<2 x i1> %1) {
+Entry:
+   %2 = call <2 x i1> @f(<2 x i1> %1)
+  ret <2 x i1> %2
+
+  }
+
+declare <2 x i1> @f(<2 x i1>)
diff --git a/llvm/test/CodeGen/Hexagon/calloperand-v32i1.ll b/llvm/test/CodeGen/Hexagon/calloperand-v32i1.ll
new file mode 100644
index 000000000000..a73478728d91
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/calloperand-v32i1.ll
@@ -0,0 +1,50 @@
+; RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length64b < %s -o - | FileCheck %s --check-prefix=CHECK-64
+; RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length128b < %s -o - | FileCheck %s --check-prefix=CHECK-128
+
+; CHECK-LABEL: compare_vectors
+; CHECK-64: [[REG1:(q[0-9]+)]] = vcmp.eq(v{{[0-9]+}}.h,v{{[0-9]+}}.h)
+; CHECK-64: [[REG2:(r[0-9]+)]] = #-1
+; CHECK-64: v0 = vand([[REG1]],[[REG2]])
+; CHECK-128: r{{[0-9]+}}:{{[0-9]+}} = combine(##.LCPI0_0,#-1)
+; CHECK-128: [[REG1:(q[0-9]+)]] = vcmp.eq(v0.h,v1.h)
+; CHECK-128: [[REG2:(v[0-9]+)]] = vand([[REG1]],r{{[0-9]+}})
+; CHECK-128: [[REG3:(v[0-9]+)]] = vmem(r{{[0-9]+}}+#0)
+; CHECK-128: [[REG4:(v[0-9]+)]] = vdelta([[REG2]],[[REG3]])
+; CHECK-128: [[REG5:(q[0-9]+)]] = vand([[REG4]],r{{[0-9]+}})
+; CHECK-128: v0 = vand([[REG5]],r{{[0-9]+}})
+
+define void @compare_vectors(<32 x i16> %a, <32 x i16> %b) {
+entry:
+  %result = icmp eq <32 x i16> %a, %b
+  call i32 @f.1(<32 x i1> %result)
+  ret void
+}
+
+; CHECK-LABEL: f.1:
+; CHECK-64: [[REG3:(q[0-9]+)]] = vand(v0,r{{[0-9]+}})
+; CHECK-64: [[REG4:(v[0-9]+)]] = vand([[REG3]],r{{[0-9]+}})
+; CHECK-64: r{{[0-9]+}} = vextract([[REG4]],r{{[0-9]+}})
+; CHECK-128: [[REG6:(q[0-9]+)]] = vand(v0,r{{[0-9]+}})
+; CHECK-128: [[REG7:(v[0-9]+)]] = vand([[REG6]],r{{[0-9]+}})
+; CHECK-128: r{{[0-9]+}} = vextract([[REG7]],r{{[0-9]+}})
+
+define i32 @f.1(<32 x i1> %vec) {
+  %element = extractelement <32 x i1> %vec, i32 6
+  %is_true = icmp eq i1 %element, true
+  br i1 %is_true, label %if_true, label %if_false
+
+if_true:
+  call void @action_if_true()
+  br label %end
+
+if_false:
+  call void @action_if_false()
+  br label %end
+
+end:
+  %result = phi i32 [1, %if_true], [0, %if_false]
+  ret i32 %result
+}
+
+declare void @action_if_true()
+declare void @action_if_false()
diff --git a/llvm/test/CodeGen/Hexagon/calloperand-v4i1.ll b/llvm/test/CodeGen/Hexagon/calloperand-v4i1.ll
new file mode 100644
index 000000000000..af23e6c788ea
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/calloperand-v4i1.ll
@@ -0,0 +1,39 @@
+;RUN: llc -mtriple=hexagon  < %s -o - | FileCheck %s --check-prefix=CHECK
+;RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length64b < %s -o - | FileCheck %s --check-prefix=CHECK
+;RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length128b < %s -o - | FileCheck %s --check-prefix=CHECK
+
+; CHECK-LABEL: compare_vectors
+; CHECK: [[REG0:(p[0-9]+)]] = vcmph.eq([[REG1:(r[0-9]+):[0-9]]],[[REG2:(r[0-9]+):[0-9]]])
+; CHECK: [[REG1:(r[0-9]+):[0-9]]] = CONST64(#281479271743489)
+; CHECK: [[REG2:(r[0-9]+):[0-9]]] = mask([[REG0]])
+; CHECK: r{{[0-9]+}}:{{[0-9]+}} = and([[REG2]],[[REG1]])
+
+define void @compare_vectors(<4 x i16> %a, <4 x i16> %b) {
+entry:
+  %result = icmp eq <4 x i16> %a, %b
+  call i32 @f.1(<4 x i1> %result)
+  ret void
+}
+; CHECK-LABEL: f.1:
+; CHECK: [[REG3:(r[0-9]+)]] = and([[REG3]],##65537)
+; CHECK: [[REG4:(r[0-9]+)]] = and([[REG4]],##65537)
+define i32 @f.1(<4 x i1> %vec) {
+  %element = extractelement <4 x i1> %vec, i32 2
+  %is_true = icmp eq i1 %element, true
+  br i1 %is_true, label %if_true, label %if_false
+
+if_true:
+  call void @action_if_true()
+  br label %end
+
+if_false:
+  call void @action_if_false()
+  br label %end
+
+end:
+  %result = phi i32 [1, %if_true], [0, %if_false]
+  ret i32 %result
+}
+
+declare void @action_if_true()
+declare void @action_if_false()
diff --git a/llvm/test/CodeGen/Hexagon/calloperand-v64i1.ll b/llvm/test/CodeGen/Hexagon/calloperand-v64i1.ll
new file mode 100644
index 000000000000..7cc562085a7e
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/calloperand-v64i1.ll
@@ -0,0 +1,50 @@
+; RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length64b < %s -o - | FileCheck %s --check-prefix=CHECK-64
+; RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length128b < %s -o - | FileCheck %s --check-prefix=CHECK-128
+
+; CHECK-LABEL: compare_vectors
+; CHECK-64: [[REG1:(q[0-9]+)]] = vcmp.eq(v{{[0-9]+}}.b,v{{[0-9]+}}.b)
+; CHECK-64: [[REG2:(r[0-9]+)]] = #-1
+; CHECK-64: v0 = vand([[REG1]],[[REG2]])
+; CHECK-128: r{{[0-9]+}}:{{[0-9]+}} = combine(##.LCPI0_0,#-1)
+; CHECK-128: [[REG1:(q[0-9]+)]] = vcmp.eq(v0.b,v1.b)
+; CHECK-128: [[REG2:(v[0-9]+)]] = vand([[REG1]],r{{[0-9]+}})
+; CHECK-128: [[REG3:(v[0-9]+)]] = vmem(r{{[0-9]+}}+#0)
+; CHECK-128: [[REG4:(v[0-9]+)]] = vdelta([[REG2]],[[REG3]])
+; CHECK-128: [[REG5:(q[0-9]+)]] = vand([[REG4]],r{{[0-9]+}})
+; CHECK-128: v0 = vand([[REG5]],r{{[0-9]+}})
+
+define void @compare_vectors(<64 x i8> %a, <64 x i8> %b) {
+entry:
+  %result = icmp eq <64 x i8> %a, %b
+  call i32 @f.1(<64 x i1> %result)
+  ret void
+}
+
+; CHECK-LABEL: f.1:
+; CHECK-64: [[REG3:(q[0-9]+)]] = vand(v0,r{{[0-9]+}})
+; CHECK-64: [[REG4:(v[0-9]+)]] = vand([[REG3]],r{{[0-9]+}})
+; CHECK-64: r{{[0-9]+}} = vextract([[REG4]],r{{[0-9]+}})
+; CHECK-128: [[REG6:(q[0-9]+)]] = vand(v0,r{{[0-9]+}})
+; CHECK-128: [[REG7:(v[0-9]+)]] = vand([[REG6]],r{{[0-9]+}})
+; CHECK-128: r{{[0-9]+}} = vextract([[REG7]],r{{[0-9]+}})
+
+define i32 @f.1(<64 x i1> %vec) {
+  %element = extractelement <64 x i1> %vec, i32 6
+  %is_true = icmp eq i1 %element, true
+  br i1 %is_true, label %if_true, label %if_false
+
+if_true:
+  call void @action_if_true()
+  br label %end
+
+if_false:
+  call void @action_if_false()
+  br label %end
+
+end:
+  %result = phi i32 [1, %if_true], [0, %if_false]
+  ret i32 %result
+}
+
+declare void @action_if_true()
+declare void @action_if_false()
diff --git a/llvm/test/CodeGen/Hexagon/calloperand-v8i1.ll b/llvm/test/CodeGen/Hexagon/calloperand-v8i1.ll
new file mode 100644
index 000000000000..2ec163acd6ae
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/calloperand-v8i1.ll
@@ -0,0 +1,39 @@
+;RUN: llc -mtriple=hexagon  < %s -o - | FileCheck %s --check-prefix=CHECK
+;RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length64b < %s -o - | FileCheck %s --check-prefix=CHECK
+;RUN: llc -mtriple=hexagon -mattr=+hvxv79,+hvx-length128b < %s -o - | FileCheck %s --check-prefix=CHECK
+
+; CHECK-LABEL: compare_vectors
+; CHECK: [[REG0:(p[0-9]+)]] = vcmpb.eq([[REG1:(r[0-9]+):[0-9]]],[[REG2:(r[0-9]+):[0-9]]])
+; CHECK: [[REG1:(r[0-9]+):[0-9]]] = CONST64(#72340172838076673)
+; CHECK: [[REG2:(r[0-9]+):[0-9]]] = mask([[REG0]])
+; CHECK: r{{[0-9]+}}:{{[0-9]+}} = and([[REG2]],[[REG1]])
+
+define void @compare_vectors(<8 x i8> %a, <8 x i8> %b) {
+entry:
+  %result = icmp eq <8 x i8> %a, %b
+  call i32 @f.1(<8 x i1> %result)
+  ret void
+}
+; CHECK-LABEL: f.1:
+; CHECK: [[REG3:(r[0-9]+)]] = and([[REG3]],##16843009)
+; CHECK: [[REG4:(r[0-9]+)]] = and([[REG4]],##16843009)
+define i32 @f.1(<8 x i1> %vec) {
+  %element = extractelement <8 x i1> %vec, i32 6
+  %is_true = icmp eq i1 %element, true
+  br i1 %is_true, label %if_true, label %if_false
+
+if_true:
+  call void @action_if_true()
+  br label %end
+
+if_false:
+  call void @action_if_false()
+  br label %end
+
+end:
+  %result = phi i32 [1, %if_true], [0, %if_false]
+  ret i32 %result
+}
+
+declare void @action_if_true()
+declare void @action_if_false()
diff --git a/llvm/test/CodeGen/Hexagon/fcmp-nan.ll b/llvm/test/CodeGen/Hexagon/fcmp-nan.ll
new file mode 100644
index 000000000000..146940291160
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/fcmp-nan.ll
@@ -0,0 +1,189 @@
+; RUN: llc -march=hexagon < %s | FileCheck %s
+;
+; Test that all FP ordered compare instructions generate the correct
+; post-processing to accommodate NaNs.
+;
+; Specifically for ordered FP compares, we have to check if one of
+; the operands was a NaN to comform to the semantics of the ordered
+; fcmp bitcode instruction
+;
+target triple = "hexagon"
+
+;
+; Functions for float:
+;
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = sfcmp.eq(r0,r1)
+; CHECK-DAG: [[REG1:p([0-3])]] = sfcmp.uo(r0,r1)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_oeq_f(float %val, float %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp oeq float %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = sfcmp.eq(r0,r1)
+; CHECK-DAG: [[REG1:p([0-3])]] = sfcmp.uo(r0,r1)
+; CHECK: [[REG2:p([0-3])]] = or([[REG0]],[[REG1]])
+; CHECK: r0 = mux([[REG2]],#0,#1)
+;
+define i32 @compare_one_f(float %val, float %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp one float %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = sfcmp.gt(r0,r1)
+; CHECK-DAG: [[REG1:p([0-3])]] = sfcmp.uo(r0,r1)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_ogt_f(float %val, float %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp ogt float %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = sfcmp.ge(r1,r0)
+; CHECK-DAG: [[REG1:p([0-3])]] = sfcmp.uo(r1,r0)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_ole_f(float %val, float %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp ole float %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = sfcmp.ge(r0,r1)
+; CHECK-DAG: [[REG1:p([0-3])]] = sfcmp.uo(r0,r1)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_oge_f(float %val, float %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp oge float %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = sfcmp.gt(r1,r0)
+; CHECK-DAG: [[REG1:p([0-3])]] = sfcmp.uo(r1,r0)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_olt_f(float %val, float %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp olt float %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+
+;
+; Functions for double:
+;
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = dfcmp.eq(r1:0,r3:2)
+; CHECK-DAG: [[REG1:p([0-3])]] = dfcmp.uo(r1:0,r3:2)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_oeq_d(double %val, double %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp oeq double %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = dfcmp.eq(r1:0,r3:2)
+; CHECK-DAG: [[REG1:p([0-3])]] = dfcmp.uo(r1:0,r3:2)
+; CHECK: [[REG2:p([0-3])]] = or([[REG0]],[[REG1]])
+; CHECK: r0 = mux([[REG2]],#0,#1)
+;
+define i32 @compare_one_d(double %val, double %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp one double %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = dfcmp.gt(r1:0,r3:2)
+; CHECK-DAG: [[REG1:p([0-3])]] = dfcmp.uo(r1:0,r3:2)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_ogt_d(double %val, double %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp ogt double %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = dfcmp.ge(r3:2,r1:0)
+; CHECK-DAG: [[REG1:p([0-3])]] = dfcmp.uo(r3:2,r1:0)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_ole_d(double %val, double %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp ole double %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = dfcmp.ge(r1:0,r3:2)
+; CHECK-DAG: [[REG1:p([0-3])]] = dfcmp.uo(r1:0,r3:2)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_oge_d(double %val, double %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp oge double %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
+
+;
+; CHECK-DAG: [[REG0:p([0-3])]] = dfcmp.gt(r3:2,r1:0)
+; CHECK-DAG: [[REG1:p([0-3])]] = dfcmp.uo(r3:2,r1:0)
+; CHECK: [[REG2:p([0-3])]] = and([[REG0]],![[REG1]])
+; CHECK: r0 = mux([[REG2]],#1,#0)
+;
+define i32 @compare_olt_d(double %val, double %val2) local_unnamed_addr #0 {
+entry:
+  %cmpinf = fcmp olt double %val, %val2
+  %0 = zext i1 %cmpinf to i32
+  ret i32 %0
+}
+
diff --git a/llvm/test/CodeGen/Hexagon/fixed-spill-mutable.ll b/llvm/test/CodeGen/Hexagon/fixed-spill-mutable.ll
index f99b448cc1a7..aa8766661a24 100644
--- a/llvm/test/CodeGen/Hexagon/fixed-spill-mutable.ll
+++ b/llvm/test/CodeGen/Hexagon/fixed-spill-mutable.ll
@@ -12,10 +12,11 @@
 ; The problem is that the load will execute before the store, clobbering the
 ; pair r17:16.
 ;
-; Check that the store and the load are not in the same packet.
+
+; Validate that store executes before load.
 ; CHECK: memd{{.*}} = r17:16
-; CHECK: }
 ; CHECK: r17:16 = memd
+; CHECK: } :mem_noshuf
 ; CHECK-LABEL: LBB0_1:
 
 target triple = "hexagon"
diff --git a/llvm/test/CodeGen/Hexagon/fp16-promote.ll b/llvm/test/CodeGen/Hexagon/fp16-promote.ll
new file mode 100644
index 000000000000..1ef0a133ce30
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/fp16-promote.ll
@@ -0,0 +1,44 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=hexagon  < %s | FileCheck %s
+
+define half @freeze_half_undef() nounwind {
+; CHECK-LABEL: freeze_half_undef:
+; CHECK:       // %bb.0:
+; CHECK-NEXT:    {
+; CHECK-NEXT:     call __truncsfhf2
+; CHECK-NEXT:     r0 = #0
+; CHECK-NEXT:     allocframe(#0)
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
+; CHECK-NEXT:     call __extendhfsf2
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
+; CHECK-NEXT:     call __truncsfhf2
+; CHECK-NEXT:     r0 = sfadd(r0,r0)
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
+; CHECK-NEXT:     r31:30 = dealloc_return(r30):raw
+; CHECK-NEXT:    }
+  %y1 = freeze half undef
+  %t1 = fadd half %y1, %y1
+  ret half %t1
+}
+
+define half @freeze_half_poison(half %maybe.poison) {
+; CHECK-LABEL: freeze_half_poison:
+; CHECK:  // %bb.0:
+; CHECK:    {
+; CHECK-NEXT:     call __extendhfsf2
+; CHECK-NEXT:     allocframe(r29,#0):raw
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
+; CHECK-NEXT:     call __truncsfhf2
+; CHECK-NEXT:     r0 = sfadd(r0,r0)
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
+; CHECK-NEXT:     r31:30 = dealloc_return(r30):raw
+; CHECK-NEXT:    }
+  %y1 = freeze half %maybe.poison
+  %t1 = fadd half %y1, %y1
+  ret half %t1
+}
diff --git a/llvm/test/CodeGen/Hexagon/hwloop-dist-check.mir b/llvm/test/CodeGen/Hexagon/hwloop-dist-check.mir
new file mode 100644
index 000000000000..9f8c14a31430
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/hwloop-dist-check.mir
@@ -0,0 +1,277 @@
+# RUN: llc --mtriple=hexagon -run-pass=hwloops %s -o - | FileCheck %s
+
+# CHECK-LABEL: name: f
+# CHECK: [[R1:%[0-9]+]]:predregs = C2_cmpgti [[R0:%[0-9]+]], 0
+# CHECK: [[R3:%[0-9]+]]:intregs = C2_muxir [[R1:%[0-9]+]], [[R2:%[0-9]+]], 1
+# CHECK-LABEL: name: g
+# CHECK: [[R1:%[0-9]+]]:predregs = C2_cmpgti [[R0:%[0-9]+]], 0
+# CHECK: [[R3:%[0-9]+]]:intregs = C2_muxir [[R1:%[0-9]+]], [[R2:%[0-9]+]], 1
+--- |
+  @a = dso_local global [255 x ptr] zeroinitializer, align 8
+
+  ; Function Attrs: minsize nofree norecurse nosync nounwind optsize memory(write, argmem: none, inaccessiblemem: none)
+  define dso_local void @f(i32 noundef %m) local_unnamed_addr #0 {
+  entry:
+    %cond = tail call i32 @llvm.smax.i32(i32 %m, i32 2)
+    %0 = add nsw i32 %cond, -4
+    %1 = shl i32 %cond, 3
+    %cgep = getelementptr i8, ptr @a, i32 %1
+    %cgep36 = bitcast ptr @a to ptr
+    br label %do.body
+
+  do.body:                                          ; preds = %do.body, %entry
+    %lsr.iv1 = phi ptr [ %cgep4, %do.body ], [ %cgep, %entry ]
+    %lsr.iv = phi i32 [ %lsr.iv.next, %do.body ], [ %0, %entry ]
+    %sh.0 = phi i32 [ 256, %entry ], [ %shr, %do.body ]
+    %shr = lshr i32 %sh.0, 1
+    %cgep5 = getelementptr inbounds [255 x ptr], ptr %cgep36, i32 0, i32 %shr
+    store ptr %lsr.iv1, ptr %cgep5, align 4, !tbaa !5
+    %lsr.iv.next = add nsw i32 %lsr.iv, 4
+    %cmp1 = icmp samesign ult i32 %lsr.iv.next, 1073741836
+    %cgep4 = getelementptr i8, ptr %lsr.iv1, i32 32
+    br i1 %cmp1, label %do.body, label %do.end, !llvm.loop !9
+
+  do.end:                                           ; preds = %do.body
+    ret void
+  }
+
+  ; Function Attrs: minsize nofree norecurse nosync nounwind optsize memory(write, argmem: none, inaccessiblemem: none)
+  define dso_local void @g(i32 noundef %m) local_unnamed_addr #0 {
+  entry:
+    %0 = add i32 %m, -4
+    %1 = shl i32 %m, 3
+    %cgep = getelementptr i8, ptr @a, i32 %1
+    %cgep36 = bitcast ptr @a to ptr
+    br label %do.body
+
+  do.body:                                          ; preds = %do.body, %entry
+    %lsr.iv1 = phi ptr [ %cgep4, %do.body ], [ %cgep, %entry ]
+    %lsr.iv = phi i32 [ %lsr.iv.next, %do.body ], [ %0, %entry ]
+    %sh.0 = phi i32 [ 256, %entry ], [ %shr, %do.body ]
+    %shr = lshr i32 %sh.0, 1
+    %cgep5 = getelementptr inbounds [255 x ptr], ptr %cgep36, i32 0, i32 %shr
+    store ptr %lsr.iv1, ptr %cgep5, align 4, !tbaa !5
+    %lsr.iv.next = add i32 %lsr.iv, 4
+    %cmp = icmp slt i32 %lsr.iv.next, 1073741836
+    %cgep4 = getelementptr i8, ptr %lsr.iv1, i32 32
+    br i1 %cmp, label %do.body, label %do.end, !llvm.loop !11
+
+  do.end:                                           ; preds = %do.body
+    ret void
+  }
+
+  ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
+  declare i32 @llvm.smax.i32(i32, i32) #1
+
+  !llvm.module.flags = !{!0, !1, !2, !3}
+  !0 = !{i32 1, !"wchar_size", i32 4}
+  !1 = !{i32 8, !"PIC Level", i32 2}
+  !2 = !{i32 7, !"PIE Level", i32 2}
+  !3 = !{i32 7, !"frame-pointer", i32 2}
+  !5 = !{!6, !6, i64 0}
+  !6 = !{!"any pointer", !7, i64 0}
+  !7 = !{!"omnipotent char", !8, i64 0}
+  !8 = !{!"Simple C/C++ TBAA"}
+  !9 = distinct !{!9, !10}
+  !10 = !{!"llvm.loop.mustprogress"}
+  !11 = distinct !{!11, !10}
+
+...
+---
+name:            f
+alignment:       4
+exposesReturnsTwice: false
+legalized:       false
+regBankSelected: false
+selected:        false
+failedISel:      false
+tracksRegLiveness: true
+hasWinCFI:       false
+noPhis:          false
+isSSA:           true
+noVRegs:         false
+hasFakeUses:     false
+callsEHReturn:   false
+callsUnwindInit: false
+hasEHScopes:     false
+hasEHFunclets:   false
+isOutlined:      false
+debugInstrRef:   false
+failsVerification: false
+tracksDebugUserValues: false
+registers:
+  - { id: 0, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 1, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 2, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 3, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 4, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 5, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 6, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 7, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 8, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 9, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 10, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 11, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 12, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 13, class: predregs, preferred-register: '', flags: [  ] }
+  - { id: 14, class: predregs, preferred-register: '', flags: [  ] }
+  - { id: 15, class: intregs, preferred-register: '', flags: [  ] }
+liveins:
+  - { reg: '$r0', virtual-reg: '%9' }
+frameInfo:
+  isFrameAddressTaken: false
+  isReturnAddressTaken: false
+  hasStackMap:     false
+  hasPatchPoint:   false
+  stackSize:       0
+  offsetAdjustment: 0
+  maxAlignment:    1
+  adjustsStack:    false
+  hasCalls:        false
+  stackProtector:  ''
+  functionContext: ''
+  maxCallFrameSize: 4294967295
+  cvBytesOfCalleeSavedRegisters: 0
+  hasOpaqueSPAdjustment: false
+  hasVAStart:      false
+  hasMustTailInVarArgFunc: false
+  hasTailCall:     false
+  isCalleeSavedInfoValid: false
+  localFrameSize:  0
+  savePoint:       ''
+  restorePoint:    ''
+fixedStack:      []
+stack:           []
+entry_values:    []
+callSites:       []
+debugValueSubstitutions: []
+constants:       []
+machineFunctionInfo: {}
+body:             |
+  bb.0.entry:
+    successors: %bb.1(0x80000000)
+    liveins: $r0
+
+    %9:intregs = COPY $r0
+    %11:intregs = A2_tfrsi 2
+    %12:intregs = A2_max %9, %11
+    %0:intregs = nsw A2_addi %12, -4
+    %1:intregs = S4_addi_asl_ri @a, %12, 3
+    %2:intregs = A2_tfrsi @a
+    %10:intregs = A2_tfrsi 256
+
+  bb.1.do.body:
+    successors: %bb.1(0x7c000000), %bb.2(0x04000000)
+
+    %3:intregs = PHI %1, %bb.0, %8, %bb.1
+    %4:intregs = PHI %0, %bb.0, %7, %bb.1
+    %5:intregs = PHI %10, %bb.0, %15, %bb.1
+    %15:intregs = S2_extractu %5, 8, 1
+    S4_storeri_rr %2, %15, 2, %3 :: (store (s32) into %ir.cgep5, !tbaa !5)
+    %7:intregs = nsw A2_addi %4, 4
+    %13:predregs = C2_cmpgtui %7, 1073741835
+    %8:intregs = A2_addi %3, 32
+    J2_jumpf %13, %bb.1, implicit-def dead $pc
+    J2_jump %bb.2, implicit-def dead $pc
+
+  bb.2.do.end:
+    PS_jmpret $r31, implicit-def dead $pc
+
+...
+---
+name:            g
+alignment:       4
+exposesReturnsTwice: false
+legalized:       false
+regBankSelected: false
+selected:        false
+failedISel:      false
+tracksRegLiveness: true
+hasWinCFI:       false
+noPhis:          false
+isSSA:           true
+noVRegs:         false
+hasFakeUses:     false
+callsEHReturn:   false
+callsUnwindInit: false
+hasEHScopes:     false
+hasEHFunclets:   false
+isOutlined:      false
+debugInstrRef:   false
+failsVerification: false
+tracksDebugUserValues: false
+registers:
+  - { id: 0, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 1, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 2, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 3, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 4, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 5, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 6, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 7, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 8, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 9, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 10, class: intregs, preferred-register: '', flags: [  ] }
+  - { id: 11, class: predregs, preferred-register: '', flags: [  ] }
+  - { id: 12, class: predregs, preferred-register: '', flags: [  ] }
+  - { id: 13, class: intregs, preferred-register: '', flags: [  ] }
+liveins:
+  - { reg: '$r0', virtual-reg: '%9' }
+frameInfo:
+  isFrameAddressTaken: false
+  isReturnAddressTaken: false
+  hasStackMap:     false
+  hasPatchPoint:   false
+  stackSize:       0
+  offsetAdjustment: 0
+  maxAlignment:    1
+  adjustsStack:    false
+  hasCalls:        false
+  stackProtector:  ''
+  functionContext: ''
+  maxCallFrameSize: 4294967295
+  cvBytesOfCalleeSavedRegisters: 0
+  hasOpaqueSPAdjustment: false
+  hasVAStart:      false
+  hasMustTailInVarArgFunc: false
+  hasTailCall:     false
+  isCalleeSavedInfoValid: false
+  localFrameSize:  0
+  savePoint:       ''
+  restorePoint:    ''
+fixedStack:      []
+stack:           []
+entry_values:    []
+callSites:       []
+debugValueSubstitutions: []
+constants:       []
+machineFunctionInfo: {}
+body:             |
+  bb.0.entry:
+    successors: %bb.1(0x80000000)
+    liveins: $r0
+
+    %9:intregs = COPY $r0
+    %0:intregs = A2_addi %9, -4
+    %1:intregs = S4_addi_asl_ri @a, %9, 3
+    %2:intregs = A2_tfrsi @a
+    %10:intregs = A2_tfrsi 256
+
+  bb.1.do.body:
+    successors: %bb.1(0x7c000000), %bb.2(0x04000000)
+
+    %3:intregs = PHI %1, %bb.0, %8, %bb.1
+    %4:intregs = PHI %0, %bb.0, %7, %bb.1
+    %5:intregs = PHI %10, %bb.0, %13, %bb.1
+    %13:intregs = S2_extractu %5, 8, 1
+    S4_storeri_rr %2, %13, 2, %3 :: (store (s32) into %ir.cgep5, !tbaa !5)
+    %7:intregs = A2_addi %4, 4
+    %11:predregs = C2_cmpgti %7, 1073741835
+    %8:intregs = A2_addi %3, 32
+    J2_jumpf %11, %bb.1, implicit-def dead $pc
+    J2_jump %bb.2, implicit-def dead $pc
+
+  bb.2.do.end:
+    PS_jmpret $r31, implicit-def dead $pc
+
+...
diff --git a/llvm/test/CodeGen/Hexagon/isel-memory-vNi1.ll b/llvm/test/CodeGen/Hexagon/isel-memory-vNi1.ll
index e1b848c0d247..23c919df0555 100644
--- a/llvm/test/CodeGen/Hexagon/isel-memory-vNi1.ll
+++ b/llvm/test/CodeGen/Hexagon/isel-memory-vNi1.ll
@@ -8,12 +8,15 @@ define i64 @f0(ptr %a0, <8 x i8> %a1) #0 {
 ; CHECK-NEXT:     r0 = memub(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5:4 = combine(#0,#0)
+; CHECK-NEXT:     r1 = #0
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = r0
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     r5:4 = vsplatb(r1)
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
 ; CHECK-NEXT:     r1:0 = vmux(p0,r3:2,r5:4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -114,7 +117,10 @@ define void @f4(ptr %a0, i64 %a1) #0 {
 ; CHECK-LABEL: f4:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5:4 = combine(#0,#0)
+; CHECK-NEXT:     r1 = #0
+; CHECK-NEXT:    }
+; CHECK-NEXT:    {
+; CHECK-NEXT:     r5:4 = vsplatb(r1)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r3:2,r5:4)
@@ -123,10 +129,10 @@ define void @f4(ptr %a0, i64 %a1) #0 {
 ; CHECK-NEXT:     p0 = not(p0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r1 = p0
+; CHECK-NEXT:     r2 = p0
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     memb(r0+#0) = r1
+; CHECK-NEXT:     memb(r0+#0) = r2
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     jumpr r31
@@ -173,64 +179,64 @@ define void @f6(ptr %a0, i16 %a1) #0 {
 ; CHECK-LABEL: f6:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r2 = extractu(r1,#8,#8)
+; CHECK-NEXT:     r2 = #255
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r3 = #255
+; CHECK-NEXT:     r3 = extractu(r1,#8,#8)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     p1 = !bitsclr(r1,r3)
+; CHECK-NEXT:     p1 = !bitsclr(r1,r2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     p0 = cmp.eq(r2,#0)
+; CHECK-NEXT:     p0 = cmp.eq(r3,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     if (p0) r2 = #0
+; CHECK-NEXT:     if (p0) r3 = #0
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     r1 = mux(p1,#8,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r3 = mux(p1,#2,#0)
+; CHECK-NEXT:     r2 = mux(p1,#2,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 = setbit(r1,#2)
+; CHECK-NEXT:     if (!p0) r3 = #128
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = setbit(r3,#0)
+; CHECK-NEXT:     r4 = mux(p0,#0,#32)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     if (!p0) r2 = #128
+; CHECK-NEXT:     r5 = setbit(r1,#2)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = mux(p0,#0,#32)
+; CHECK-NEXT:     r6 = setbit(r2,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     if (!p1) r5 = add(r1,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     if (!p1) r6 = add(r3,#0)
+; CHECK-NEXT:      r1 = setbit(r3,#6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r1 = setbit(r2,#6)
+; CHECK-NEXT:     if (!p1) r6 = add(r2,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r3 = setbit(r4,#4)
+; CHECK-NEXT:     r2 = setbit(r4,#4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 = or(r6,r5)
+; CHECK-NEXT:     if (!p0) r4 = add(r2,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     if (!p0) r2 = add(r1,#0)
+; CHECK-NEXT:     if (!p0) r3 = add(r1,#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     if (!p0) r4 = add(r3,#0)
+; CHECK-NEXT:     r2 = or(r6,r5)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 |= or(r4,r2)
+; CHECK-NEXT:     r2 |= or(r4,r3)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     memb(r0+#0) = r5
+; CHECK-NEXT:     memb(r0+#0) = r2
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     jumpr r31
diff --git a/llvm/test/CodeGen/Hexagon/isel/extract-subvec.ll b/llvm/test/CodeGen/Hexagon/isel/extract-subvec.ll
new file mode 100644
index 000000000000..f7262eacabe8
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/isel/extract-subvec.ll
@@ -0,0 +1,34 @@
+; Check if extract_subvectors is handled properly in Hexagon backend when the
+; the source vector is a vector-pair and result vector is not hvx vector size.
+; https://github.com/llvm/llvm-project/issues/128775
+;
+; Example of such a case:
+;    ...
+;    t2: v64i32,ch = CopyFromReg t0, Register:v64i32 %0
+;    t17: v2i32 = extract_subvector t2, Constant:i32<4>
+;    ...
+
+; RUN: llc -mtriple=hexagon -mattr="hvx-length128b" < %s | FileCheck %s
+
+; CHECK-LABEL: extract_subvec:
+; CHECK: r29 = and(r29,#-128)
+; CHECK: [[R1:r([0-9]+)]] = add(r29,#0)
+; CHECK: vmem([[R1]]+#0) = v0
+; CHECK-DAG: r[[R4:[0-9]+]] = memw([[R1]]+#0)
+; CHECK-DAG: r[[R5:[0-9]+]] = memw([[R1]]+#4)
+; CHECK-DAG: r[[R6:[0-9]+]] = memw([[R1]]+#8)
+; CHECK-DAG: r[[R7:[0-9]+]] = memw([[R1]]+#12)
+; CHECK-DAG: r[[R8:[0-9]+]] = memw([[R1]]+#16)
+; CHECK-DAG: r[[R9:[0-9]+]] = memw([[R1]]+#20)
+; CHECK-DAG: r[[R2:[0-9]+]] = memw([[R1]]+#24)
+; CHECK-DAG: r[[R3:[0-9]+]] = memw([[R1]]+#28)
+; CHECK-DAG: memd(r0+#0) = r[[R5]]:[[R4]]
+; CHECK-DAG: memd(r0+#8) = r[[R7]]:[[R6]]
+; CHECK-DAG: memd(r0+#16) = r[[R9]]:[[R8]]
+; CHECK-DAG: memw(r0+#24) = r[[R2]]
+define void @extract_subvec(<56 x i32> %val, ptr %buf) {
+entry:
+  %split = shufflevector <56 x i32> %val, <56 x i32> zeroinitializer, <7 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6>
+  store <7 x i32> %split, ptr %buf, align 32
+  ret void
+}
diff --git a/llvm/test/CodeGen/Hexagon/isel/logical.ll b/llvm/test/CodeGen/Hexagon/isel/logical.ll
index 7f9c178c4241..669d01dcd6ad 100644
--- a/llvm/test/CodeGen/Hexagon/isel/logical.ll
+++ b/llvm/test/CodeGen/Hexagon/isel/logical.ll
@@ -1399,10 +1399,10 @@ define <8 x i8> @f39(<8 x i8> %a0, <8 x i8> %a1) #1 {
 ; CHECK-LABEL: f39:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = ##16843009
+; CHECK-NEXT:     r4 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 = ##16843009
+; CHECK-NEXT:     r5:4 = vsplatb(r4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r5:4)
@@ -1431,10 +1431,10 @@ define <8 x i8> @f40(<8 x i8> %a0, <8 x i8> %a1) #1 {
 ; CHECK-LABEL: f40:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = ##16843009
+; CHECK-NEXT:     r4 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 = ##16843009
+; CHECK-NEXT:     r5:4 = vsplatb(r4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r5:4)
@@ -1463,10 +1463,10 @@ define <8 x i8> @f41(<8 x i8> %a0, <8 x i8> %a1) #1 {
 ; CHECK-LABEL: f41:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = ##16843009
+; CHECK-NEXT:     r4 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 = ##16843009
+; CHECK-NEXT:     r5:4 = vsplatb(r4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r5:4)
@@ -1495,10 +1495,10 @@ define <8 x i8> @f42(<8 x i8> %a0, <8 x i8> %a1) #1 {
 ; CHECK-LABEL: f42:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = ##16843009
+; CHECK-NEXT:     r4 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 = ##16843009
+; CHECK-NEXT:     r5:4 = vsplatb(r4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r5:4)
@@ -1528,10 +1528,10 @@ define <8 x i8> @f43(<8 x i8> %a0, <8 x i8> %a1) #1 {
 ; CHECK-LABEL: f43:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r4 = ##16843009
+; CHECK-NEXT:     r4 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r5 = ##16843009
+; CHECK-NEXT:     r5:4 = vsplatb(r4)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r5:4)
@@ -1561,10 +1561,10 @@ define <8 x i8> @f44(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f44:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
@@ -1598,10 +1598,10 @@ define <8 x i8> @f45(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f45:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
@@ -1635,10 +1635,10 @@ define <8 x i8> @f46(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f46:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
@@ -1672,10 +1672,10 @@ define <8 x i8> @f47(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f47:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
@@ -1709,10 +1709,10 @@ define <8 x i8> @f48(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f48:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
@@ -1750,10 +1750,10 @@ define <8 x i8> @f49(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f49:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
@@ -1788,10 +1788,10 @@ define <8 x i8> @f50(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f50:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
@@ -1826,10 +1826,10 @@ define <8 x i8> @f51(<8 x i8> %a0, <8 x i8> %a1, <8 x i8> %a2) #1 {
 ; CHECK-LABEL: f51:
 ; CHECK:       // %bb.0: // %b0
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r6 = ##16843009
+; CHECK-NEXT:     r6 = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r7 = ##16843009
+; CHECK-NEXT:     r7:6 = vsplatb(r6)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = vcmpb.eq(r1:0,r7:6)
diff --git a/llvm/test/CodeGen/Hexagon/isel/pfalse-v4i1.ll b/llvm/test/CodeGen/Hexagon/isel/pfalse-v4i1.ll
new file mode 100644
index 000000000000..c0904b8b4fdd
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/isel/pfalse-v4i1.ll
@@ -0,0 +1,29 @@
+; RUN: llc -march=hexagon -debug-only=isel 2>&1 < %s - | FileCheck %s
+
+; CHECK: [[R0:%[0-9]+]]:intregs = A2_tfrsi 0
+; CHECK-NEXT: predregs = C2_tfrrp killed [[R0]]:intregs
+
+define fastcc i16 @test(ptr %0, { <4 x i32>, <4 x i1> } %1, <4 x i1> %2) {
+Entry:
+  %3 = alloca [16 x i8], i32 0, align 16
+  %4 = alloca [16 x i8], i32 0, align 16
+  store <4 x i32> <i32 1, i32 2, i32 3, i32 4>, ptr %4, align 16
+  store <4 x i32> <i32 5, i32 6, i32 7, i32 8>, ptr %3, align 16
+  %5 = load <4 x i32>, ptr %4, align 16
+  %6 = load <4 x i32>, ptr %3, align 16
+  %7 = call { <4 x i32>, <4 x i1> } @llvm.sadd.with.overflow.v4i32(<4 x i32> %5, <4 x i32> %6)
+  %8 = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> %2)
+  br i1 %8, label %OverflowFail, label %OverflowOk
+
+OverflowFail:                                     ; preds = %Entry
+  store volatile i32 0, ptr null, align 4
+    unreachable
+
+OverflowOk:                                       ; preds = %Entry
+  %9 = extractvalue { <4 x i32>, <4 x i1> } %7, 0
+    store <4 x i32> %9, ptr %0, align 16
+      ret i16 0
+      }
+
+declare { <4 x i32>, <4 x i1> } @llvm.sadd.with.overflow.v4i32(<4 x i32>, <4 x i32>) #0
+declare i1 @llvm.vector.reduce.or.v4i1(<4 x i1>) #0
diff --git a/llvm/test/CodeGen/Hexagon/isel/select-i1.ll b/llvm/test/CodeGen/Hexagon/isel/select-i1.ll
index 193b354bc5a8..eb77850f0a5a 100644
--- a/llvm/test/CodeGen/Hexagon/isel/select-i1.ll
+++ b/llvm/test/CodeGen/Hexagon/isel/select-i1.ll
@@ -124,11 +124,9 @@ define void @f4(ptr %a0, ptr %a1, ptr %a2, ptr %a3) {
 ; CHECK-NEXT:     r0 = memub(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r2 = memub(r2+#0)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = r0
 ; CHECK-NEXT:     p1 = r1
+; CHECK-NEXT:     r2 = memub(r2+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p2 = r2
@@ -160,11 +158,9 @@ define void @f5(ptr %a0, ptr %a1, ptr %a2, ptr %a3) {
 ; CHECK-NEXT:     r0 = memub(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r2 = memub(r2+#0)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = r0
 ; CHECK-NEXT:     p1 = r1
+; CHECK-NEXT:     r2 = memub(r2+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p2 = r2
@@ -196,11 +192,9 @@ define void @f6(ptr %a0, ptr %a1, ptr %a2, ptr %a3) {
 ; CHECK-NEXT:     r0 = memub(r0+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     r2 = memub(r2+#0)
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
 ; CHECK-NEXT:     p0 = r0
 ; CHECK-NEXT:     p1 = r1
+; CHECK-NEXT:     r2 = memub(r2+#0)
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     p2 = r2
diff --git a/llvm/test/CodeGen/Hexagon/postinc-baseoffset.mir b/llvm/test/CodeGen/Hexagon/postinc-baseoffset.mir
index fa07febcbf5a..172a28b8e64e 100644
--- a/llvm/test/CodeGen/Hexagon/postinc-baseoffset.mir
+++ b/llvm/test/CodeGen/Hexagon/postinc-baseoffset.mir
@@ -2,11 +2,11 @@
 
 # Check that we don't packetize these two instructions together. It happened
 # earlier because "offset" in the post-increment instruction was taken to be 8.
+# If they are packetized together, make sure "mem_noshuf" attribute is set.
 
 # CHECK: memw(r0+#0) = #-1
-# CHECK: }
-# CHECK: {
 # CHECK: r1 = memw(r0++#8)
+# CHECK: :mem_noshuf
 
 --- |
   define void @fred(ptr %a) { ret void }
diff --git a/llvm/test/CodeGen/Hexagon/setmemrefs.ll b/llvm/test/CodeGen/Hexagon/setmemrefs.ll
index 85f46af7e56a..13b7b955cb62 100644
--- a/llvm/test/CodeGen/Hexagon/setmemrefs.ll
+++ b/llvm/test/CodeGen/Hexagon/setmemrefs.ll
@@ -1,4 +1,4 @@
-; RUN: llc -mtriple=hexagon < %s | FileCheck %s
+; RUN: llc -mtriple=hexagon -mcpu=hexagonv60 < %s | FileCheck %s
 
 ; This test checks to see if, after lowering the two loads below, we set up the
 ; memrefs of the resulting load MIs correctly, so that they are packetized
diff --git a/llvm/test/CodeGen/Hexagon/swp-phi-start.ll b/llvm/test/CodeGen/Hexagon/swp-phi-start.ll
index 52c258656ec2..6c2b08d83b1c 100644
--- a/llvm/test/CodeGen/Hexagon/swp-phi-start.ll
+++ b/llvm/test/CodeGen/Hexagon/swp-phi-start.ll
@@ -5,8 +5,9 @@
 ; the same stage.
 
 ; CHECK-DAG: [[REG3:(r[0-9]+)]] = add([[REG1:(r[0-9]+)]],#-1)
-; CHECK-DAG: [[REG2:(r[0-9]+)]] = add([[REG1]],#-1)
-; CHECK-DAG: loop0(.LBB0_[[LOOP:.]],[[REG3]])
+; CHECK-DAG: [[REG2:(r[0-9]+)]] = add([[REG4:(r[0-9]+)]],#-1)
+; CHECK-DAG: loop0(.LBB0_[[LOOP:.]],[[REG2]])
+; CHECK-NOT: = [[REG3]]
 ; CHECK-NOT: = [[REG2]]
 ; CHECK: .LBB0_[[LOOP]]:
 ; CHECK: }{{[ \t]*}}:endloop
diff --git a/llvm/test/CodeGen/Hexagon/swp-ws-live-intervals-issue128714.mir b/llvm/test/CodeGen/Hexagon/swp-ws-live-intervals-issue128714.mir
new file mode 100644
index 000000000000..ef52ff11af9c
--- /dev/null
+++ b/llvm/test/CodeGen/Hexagon/swp-ws-live-intervals-issue128714.mir
@@ -0,0 +1,157 @@
+# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
+# RUN: llc --mtriple=hexagon %s -run-pass=pipeliner -o -| FileCheck %s
+
+--- |
+  define void @test_swp_ws_live_intervals(i32 %.pre) {
+  entry:
+    %cgep9 = bitcast ptr null to ptr
+    br label %for.body147
+
+  for.body147:                                      ; preds = %for.body170, %entry
+    %add11.i526 = or i32 %.pre, 1
+    br label %for.body158
+
+  for.body158:                                      ; preds = %for.body158, %for.body147
+    %lsr.iv = phi i32 [ %lsr.iv.next, %for.body158 ], [ -1, %for.body147 ]
+    %add11.i536602603 = phi i32 [ %add11.i526, %for.body147 ], [ 0, %for.body158 ]
+    %and8.i534 = and i32 %add11.i536602603, 1
+    %cgep7 = getelementptr [64 x i32], ptr %cgep9, i32 0, i32 %and8.i534
+    store i32 0, ptr %cgep7, align 4
+    %lsr.iv.next = add nsw i32 %lsr.iv, 1
+    %cmp157.3 = icmp ult i32 %lsr.iv.next, 510
+    br i1 %cmp157.3, label %for.body158, label %for.body170
+
+  for.body170:                                      ; preds = %for.body170, %for.body158
+    %lsr.iv3 = phi ptr [ %cgep6, %for.body170 ], [ inttoptr (i32 4 to ptr), %for.body158 ]
+    %lsr.iv1 = phi i32 [ %lsr.iv.next2, %for.body170 ], [ -1, %for.body158 ]
+    %add11.i556606607 = phi i32 [ 0, %for.body170 ], [ 1, %for.body158 ]
+    %cgep5 = getelementptr i8, ptr %lsr.iv3, i32 -4
+    store i32 0, ptr %cgep5, align 8
+    %sub.i547.1 = add i32 %add11.i556606607, 1
+    %and.i548.1 = and i32 %sub.i547.1, 1
+    %cgep8 = getelementptr [64 x i32], ptr %cgep9, i32 0, i32 %and.i548.1
+    %0 = load i32, ptr %cgep8, align 4
+    store i32 %0, ptr %lsr.iv3, align 4
+    %lsr.iv.next2 = add nsw i32 %lsr.iv1, 1
+    %cmp169.1 = icmp ult i32 %lsr.iv.next2, 254
+    %cgep6 = getelementptr i8, ptr %lsr.iv3, i32 2
+    br i1 %cmp169.1, label %for.body170, label %for.body147
+  }
+
+...
+---
+name:            test_swp_ws_live_intervals
+tracksRegLiveness: true
+body:             |
+  ; CHECK-LABEL: name: test_swp_ws_live_intervals
+  ; CHECK: bb.0.entry:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT:   liveins: $r0
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:intregs = COPY $r0
+  ; CHECK-NEXT:   [[S2_setbit_i:%[0-9]+]]:intregs = S2_setbit_i [[COPY]], 0
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.1:
+  ; CHECK-NEXT:   successors: %bb.5(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.5:
+  ; CHECK-NEXT:   successors: %bb.6(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[A2_andir:%[0-9]+]]:intregs = A2_andir [[S2_setbit_i]], 1
+  ; CHECK-NEXT:   [[S2_asl_i_r:%[0-9]+]]:intregs = S2_asl_i_r [[A2_andir]], 2
+  ; CHECK-NEXT:   [[A2_tfrsi:%[0-9]+]]:intregs = A2_tfrsi 1
+  ; CHECK-NEXT:   [[A2_tfrsi1:%[0-9]+]]:intregs = A2_tfrsi 4
+  ; CHECK-NEXT:   [[A2_tfrsi2:%[0-9]+]]:intregs = A2_tfrsi 0
+  ; CHECK-NEXT:   J2_loop0i %bb.6, 510, implicit-def $lc0, implicit-def $sa0, implicit-def $usr
+  ; CHECK-NEXT:   J2_jump %bb.6, implicit-def $pc
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.6:
+  ; CHECK-NEXT:   successors: %bb.6(0x7c000000), %bb.7(0x04000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:intregs = PHI [[A2_tfrsi2]], %bb.5, %24, %bb.6
+  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:intregs = PHI [[S2_asl_i_r]], %bb.5, %23, %bb.6
+  ; CHECK-NEXT:   S4_storeiri_io [[PHI1]], 0, 0 :: (store (s32) into %ir.cgep7)
+  ; CHECK-NEXT:   [[A2_andir1:%[0-9]+]]:intregs = A2_andir [[PHI]], 1
+  ; CHECK-NEXT:   [[A2_tfrsi3:%[0-9]+]]:intregs = A2_tfrsi 1
+  ; CHECK-NEXT:   [[A2_tfrsi4:%[0-9]+]]:intregs = A2_tfrsi 4
+  ; CHECK-NEXT:   [[S2_asl_i_r1:%[0-9]+]]:intregs = S2_asl_i_r [[A2_andir1]], 2
+  ; CHECK-NEXT:   [[A2_tfrsi5:%[0-9]+]]:intregs = A2_tfrsi 0
+  ; CHECK-NEXT:   ENDLOOP0 %bb.6, implicit-def $pc, implicit-def $lc0, implicit $sa0, implicit $lc0
+  ; CHECK-NEXT:   J2_jump %bb.7, implicit-def $pc
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.7:
+  ; CHECK-NEXT:   successors: %bb.3(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:intregs = PHI [[S2_asl_i_r1]], %bb.6
+  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:intregs = PHI [[A2_tfrsi3]], %bb.6
+  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:intregs = PHI [[A2_tfrsi4]], %bb.6
+  ; CHECK-NEXT:   S4_storeiri_io [[PHI2]], 0, 0 :: (store unknown-size into %ir.cgep7, align 4)
+  ; CHECK-NEXT:   J2_jump %bb.3, implicit-def $pc
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.3:
+  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   J2_loop0i %bb.4, 255, implicit-def $lc0, implicit-def $sa0, implicit-def $usr
+  ; CHECK-NEXT:   J2_jump %bb.4, implicit-def $pc
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.4:
+  ; CHECK-NEXT:   successors: %bb.4(0x7c000000), %bb.1(0x04000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:intregs = PHI [[PHI4]], %bb.3, %9, %bb.4
+  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:intregs = PHI [[PHI3]], %bb.3, %11, %bb.4
+  ; CHECK-NEXT:   [[A2_tfrsi6:%[0-9]+]]:intregs = A2_tfrsi 0
+  ; CHECK-NEXT:   S2_storeri_io [[PHI5]], -4, [[A2_tfrsi6]] :: (store (s32) into %ir.cgep5, align 8)
+  ; CHECK-NEXT:   [[A2_addi:%[0-9]+]]:intregs = A2_addi [[PHI6]], 1
+  ; CHECK-NEXT:   [[S2_insert:%[0-9]+]]:intregs = S2_insert [[PHI2]], [[A2_addi]], 1, 2
+  ; CHECK-NEXT:   [[L2_loadri_io:%[0-9]+]]:intregs = L2_loadri_io [[S2_insert]], 0 :: (load (s32) from %ir.cgep8)
+  ; CHECK-NEXT:   S2_storeri_io [[PHI5]], 0, [[L2_loadri_io]] :: (store (s32) into %ir.lsr.iv3)
+  ; CHECK-NEXT:   [[A2_addi1:%[0-9]+]]:intregs = A2_addi [[PHI5]], 2
+  ; CHECK-NEXT:   ENDLOOP0 %bb.4, implicit-def $pc, implicit-def $lc0, implicit $sa0, implicit $lc0
+  ; CHECK-NEXT:   J2_jump %bb.1, implicit-def dead $pc
+  bb.0.entry:
+    successors: %bb.1(0x80000000)
+    liveins: $r0
+
+    %0:intregs = COPY $r0
+    %1:intregs = S2_setbit_i %0, 0
+
+  bb.1:
+    successors: %bb.2(0x80000000)
+
+    J2_loop0i %bb.2, 511, implicit-def $lc0, implicit-def $sa0, implicit-def $usr
+
+  bb.2:
+    successors: %bb.2(0x7c000000), %bb.3(0x04000000)
+
+    %2:intregs = PHI %1, %bb.1, %3, %bb.2
+    %4:intregs = A2_andir %2, 1
+    %5:intregs = S2_asl_i_r %4, 2
+    S4_storeiri_io %5, 0, 0 :: (store (s32) into %ir.cgep7)
+    %6:intregs = A2_tfrsi 1
+    %7:intregs = A2_tfrsi 4
+    %3:intregs = A2_tfrsi 0
+    ENDLOOP0 %bb.2, implicit-def $pc, implicit-def $lc0, implicit $sa0, implicit $lc0
+    J2_jump %bb.3, implicit-def dead $pc
+
+  bb.3:
+    successors: %bb.4(0x80000000)
+
+    J2_loop0i %bb.4, 255, implicit-def $lc0, implicit-def $sa0, implicit-def $usr
+    J2_jump %bb.4, implicit-def $pc
+
+  bb.4:
+    successors: %bb.4(0x7c000000), %bb.1(0x04000000)
+
+    %8:intregs = PHI %7, %bb.3, %9, %bb.4
+    %10:intregs = PHI %6, %bb.3, %11, %bb.4
+    %11:intregs = A2_tfrsi 0
+    S2_storeri_io %8, -4, %11 :: (store (s32) into %ir.cgep5, align 8)
+    %12:intregs = A2_addi %10, 1
+    %13:intregs = S2_insert %5, %12, 1, 2
+    %14:intregs = L2_loadri_io %13, 0 :: (load (s32) from %ir.cgep8)
+    S2_storeri_io %8, 0, %14 :: (store (s32) into %ir.lsr.iv3)
+    %9:intregs = A2_addi %8, 2
+    ENDLOOP0 %bb.4, implicit-def $pc, implicit-def $lc0, implicit $sa0, implicit $lc0
+    J2_jump %bb.1, implicit-def dead $pc
+
+...
diff --git a/llvm/test/CodeGen/LoongArch/lasx/inline-asm-operand-modifier.ll b/llvm/test/CodeGen/LoongArch/lasx/inline-asm-operand-modifier.ll
index 201e34c8b5ae..8b25a6525381 100644
--- a/llvm/test/CodeGen/LoongArch/lasx/inline-asm-operand-modifier.ll
+++ b/llvm/test/CodeGen/LoongArch/lasx/inline-asm-operand-modifier.ll
@@ -12,3 +12,43 @@ entry:
   %0 = tail call <4 x i64> asm sideeffect "xvldi ${0:u}, 1", "=f"()
   ret void
 }
+
+define void @test_u_2xi64() nounwind {
+; CHECK-LABEL: test_u_2xi64:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    #APP
+; CHECK-NEXT:    xvldi $xr0, 1
+; CHECK-NEXT:    #NO_APP
+; CHECK-NEXT:    ret
+entry:
+  %0 = tail call <2 x i64> asm sideeffect "xvldi ${0:u}, 1", "=f"()
+  ret void
+}
+
+define void @test_w_4xi64() nounwind {
+; CHECK-LABEL: test_w_4xi64:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    #APP
+; CHECK-NEXT:    vldi $vr0, 1
+; CHECK-NEXT:    #NO_APP
+; CHECK-NEXT:    ret
+entry:
+  %0 = tail call <4 x i64> asm sideeffect "vldi ${0:w}, 1", "=f"()
+  ret void
+}
+
+define void @m128i_to_m256i(ptr %out, ptr %in) nounwind {
+; CHECK-LABEL: m128i_to_m256i:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    vld $vr0, $a1, 0
+; CHECK-NEXT:    xvrepli.b $xr1, 0
+; CHECK-NEXT:    #APP
+; CHECK-NEXT:    xvpermi.q $xr1, $xr0, 32
+; CHECK-NEXT:    #NO_APP
+; CHECK-NEXT:    xvst $xr1, $a0, 0
+; CHECK-NEXT:    ret
+  %v = load <2 x i64>, ptr %in
+  %x = call <4 x i64> asm sideeffect "xvpermi.q ${0:u}, ${1:u}, 32", "=f,f,0"(<2 x i64> %v, <4 x i64> zeroinitializer)
+  store <4 x i64> %x, ptr %out
+  ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/prefetchi.ll b/llvm/test/CodeGen/LoongArch/prefetchi.ll
new file mode 100644
index 000000000000..a00f6f816186
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/prefetchi.ll
@@ -0,0 +1,33 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
+; RUN: llc --mtriple=loongarch32 < %s | FileCheck %s --check-prefix=LA32
+; RUN: llc --mtriple=loongarch64 < %s | FileCheck %s --check-prefix=LA64
+
+declare void @llvm.prefetch(ptr, i32, i32, i32) nounwind
+
+define dso_local void @prefetch_no_offset(ptr %ptr) nounwind {
+; LA32-LABEL: prefetch_no_offset:
+; LA32:       # %bb.0: # %entry
+; LA32-NEXT:    ret
+;
+; LA64-LABEL: prefetch_no_offset:
+; LA64:       # %bb.0: # %entry
+; LA64-NEXT:    ret
+entry:
+  tail call void @llvm.prefetch(ptr %ptr, i32 0, i32 3, i32 0)
+  ret void
+}
+
+
+define dso_local void @prefetch_with_offset(ptr %ptr) nounwind {
+; LA32-LABEL: prefetch_with_offset:
+; LA32:       # %bb.0: # %entry
+; LA32-NEXT:    ret
+;
+; LA64-LABEL: prefetch_with_offset:
+; LA64:       # %bb.0: # %entry
+; LA64-NEXT:    ret
+entry:
+  %addr = getelementptr i8, ptr %ptr, i64 200
+  tail call void @llvm.prefetch(ptr %addr, i32 0, i32 3, i32 0)
+  ret void
+}
diff --git a/llvm/test/CodeGen/PowerPC/f128-conv.ll b/llvm/test/CodeGen/PowerPC/f128-conv.ll
index d8eed1fb4092..1a51ca64177a 100644
--- a/llvm/test/CodeGen/PowerPC/f128-conv.ll
+++ b/llvm/test/CodeGen/PowerPC/f128-conv.ll
@@ -10,11 +10,11 @@
 @umem = global [5 x i64] [i64 560, i64 100, i64 34, i64 2, i64 5], align 8
 @swMem = global [5 x i32] [i32 5, i32 2, i32 3, i32 4, i32 0], align 4
 @uwMem = global [5 x i32] [i32 5, i32 2, i32 3, i32 4, i32 0], align 4
-@uhwMem = local_unnamed_addr global [5 x i16] [i16 5, i16 2, i16 3, i16 4, i16 0], align 2
-@ubMem = local_unnamed_addr global [5 x i8] c"\05\02\03\04\00", align 1
+@uhwMem = global [5 x i16] [i16 5, i16 2, i16 3, i16 4, i16 0], align 2
+@ubMem = global [5 x i8] c"\05\02\03\04\00", align 1
 
 ; Function Attrs: norecurse nounwind
-define void @sdwConv2qp(ptr nocapture %a, i64 %b) {
+define void @sdwConv2qp(ptr nocapture %a, i64 %b) nounwind {
 ; CHECK-LABEL: sdwConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mtvsrd v2, r4
@@ -25,9 +25,6 @@ define void @sdwConv2qp(ptr nocapture %a, i64 %b) {
 ; CHECK-P8-LABEL: sdwConv2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -50,13 +47,10 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @sdwConv2qp_01(ptr nocapture %a, i128 %b) {
+define void @sdwConv2qp_01(ptr nocapture %a, i128 %b) nounwind {
 ; CHECK-LABEL: sdwConv2qp_01:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mflr r0
-; CHECK-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-NEXT:    .cfi_offset lr, 16
-; CHECK-NEXT:    .cfi_offset r30, -16
 ; CHECK-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-NEXT:    stdu r1, -48(r1)
 ; CHECK-NEXT:    mr r30, r3
@@ -75,9 +69,6 @@ define void @sdwConv2qp_01(ptr nocapture %a, i128 %b) {
 ; CHECK-P8-LABEL: sdwConv2qp_01:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -101,7 +92,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @sdwConv2qp_02(ptr nocapture %a) {
+define void @sdwConv2qp_02(ptr nocapture %a) nounwind {
 ; CHECK-LABEL: sdwConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC0@toc@ha
@@ -114,9 +105,6 @@ define void @sdwConv2qp_02(ptr nocapture %a) {
 ; CHECK-P8-LABEL: sdwConv2qp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -134,16 +122,16 @@ define void @sdwConv2qp_02(ptr nocapture %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i64, ptr getelementptr inbounds
+  %i = load i64, ptr getelementptr inbounds
                         ([5 x i64], ptr @mem, i64 0, i64 2), align 8
-  %conv = sitofp i64 %0 to fp128
+  %conv = sitofp i64 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @sdwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) {
+define void @sdwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) nounwind {
 ; CHECK-LABEL: sdwConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxsd v2, 0(r4)
@@ -154,9 +142,6 @@ define void @sdwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-LABEL: sdwConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -172,15 +157,15 @@ define void @sdwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i64, ptr %b, align 8
-  %conv = sitofp i64 %0 to fp128
+  %i = load i64, ptr %b, align 8
+  %conv = sitofp i64 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @sdwConv2qp_04(ptr nocapture %a, i1 %b) {
+define void @sdwConv2qp_04(ptr nocapture %a, i1 %b) nounwind {
 ; CHECK-LABEL: sdwConv2qp_04:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    andi. r4, r4, 1
@@ -195,9 +180,6 @@ define void @sdwConv2qp_04(ptr nocapture %a, i1 %b) {
 ; CHECK-P8-LABEL: sdwConv2qp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -223,7 +205,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @udwConv2qp(ptr nocapture %a, i64 %b) {
+define void @udwConv2qp(ptr nocapture %a, i64 %b) nounwind {
 ; CHECK-LABEL: udwConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mtvsrd v2, r4
@@ -234,9 +216,6 @@ define void @udwConv2qp(ptr nocapture %a, i64 %b) {
 ; CHECK-P8-LABEL: udwConv2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -259,13 +238,10 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @udwConv2qp_01(ptr nocapture %a, i128 %b) {
+define void @udwConv2qp_01(ptr nocapture %a, i128 %b) nounwind {
 ; CHECK-LABEL: udwConv2qp_01:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mflr r0
-; CHECK-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-NEXT:    .cfi_offset lr, 16
-; CHECK-NEXT:    .cfi_offset r30, -16
 ; CHECK-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-NEXT:    stdu r1, -48(r1)
 ; CHECK-NEXT:    mr r30, r3
@@ -284,9 +260,6 @@ define void @udwConv2qp_01(ptr nocapture %a, i128 %b) {
 ; CHECK-P8-LABEL: udwConv2qp_01:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -310,7 +283,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @udwConv2qp_02(ptr nocapture %a) {
+define void @udwConv2qp_02(ptr nocapture %a) nounwind {
 ; CHECK-LABEL: udwConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC1@toc@ha
@@ -323,9 +296,6 @@ define void @udwConv2qp_02(ptr nocapture %a) {
 ; CHECK-P8-LABEL: udwConv2qp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -343,16 +313,16 @@ define void @udwConv2qp_02(ptr nocapture %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i64, ptr getelementptr inbounds
+  %i = load i64, ptr getelementptr inbounds
                         ([5 x i64], ptr @umem, i64 0, i64 4), align 8
-  %conv = uitofp i64 %0 to fp128
+  %conv = uitofp i64 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @udwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) {
+define void @udwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) nounwind {
 ; CHECK-LABEL: udwConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxsd v2, 0(r4)
@@ -363,9 +333,6 @@ define void @udwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-LABEL: udwConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -381,15 +348,15 @@ define void @udwConv2qp_03(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i64, ptr %b, align 8
-  %conv = uitofp i64 %0 to fp128
+  %i = load i64, ptr %b, align 8
+  %conv = uitofp i64 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @udwConv2qp_04(ptr nocapture %a, i1 %b) {
+define void @udwConv2qp_04(ptr nocapture %a, i1 %b) nounwind {
 ; CHECK-LABEL: udwConv2qp_04:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    clrlwi r4, r4, 31
@@ -401,9 +368,6 @@ define void @udwConv2qp_04(ptr nocapture %a, i1 %b) {
 ; CHECK-P8-LABEL: udwConv2qp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -439,9 +403,6 @@ define ptr @sdwConv2qp_testXForm(ptr returned %sink,
 ; CHECK-P8-LABEL: sdwConv2qp_testXForm:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -459,11 +420,11 @@ define ptr @sdwConv2qp_testXForm(ptr returned %sink,
 ; CHECK-P8-NEXT:    ld r30, -16(r1) # 8-byte Folded Reload
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
-                                    ptr nocapture readonly %a) {
+                                 ptr nocapture readonly %a) nounwind {
 entry:
   %add.ptr = getelementptr inbounds i8, ptr %a, i64 73333
-  %0 = load i64, ptr %add.ptr, align 8
-  %conv = sitofp i64 %0 to fp128
+  %i = load i64, ptr %add.ptr, align 8
+  %conv = sitofp i64 %i to fp128
   store fp128 %conv, ptr %sink, align 16
   ret ptr %sink
 
@@ -483,9 +444,6 @@ define ptr @udwConv2qp_testXForm(ptr returned %sink,
 ; CHECK-P8-LABEL: udwConv2qp_testXForm:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -503,18 +461,18 @@ define ptr @udwConv2qp_testXForm(ptr returned %sink,
 ; CHECK-P8-NEXT:    ld r30, -16(r1) # 8-byte Folded Reload
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
-                                    ptr nocapture readonly %a) {
+                                 ptr nocapture readonly %a) nounwind {
 entry:
   %add.ptr = getelementptr inbounds i8, ptr %a, i64 73333
-  %0 = load i64, ptr %add.ptr, align 8
-  %conv = uitofp i64 %0 to fp128
+  %i = load i64, ptr %add.ptr, align 8
+  %conv = uitofp i64 %i to fp128
   store fp128 %conv, ptr %sink, align 16
   ret ptr %sink
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @swConv2qp(ptr nocapture %a, i32 signext %b) {
+define void @swConv2qp(ptr nocapture %a, i32 signext %b) nounwind {
 ; CHECK-LABEL: swConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mtvsrwa v2, r4
@@ -525,9 +483,6 @@ define void @swConv2qp(ptr nocapture %a, i32 signext %b) {
 ; CHECK-P8-LABEL: swConv2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -550,7 +505,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @swConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
+define void @swConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) nounwind {
 ; CHECK-LABEL: swConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxsiwax v2, 0, r4
@@ -561,9 +516,6 @@ define void @swConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-LABEL: swConv2qp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -579,15 +531,15 @@ define void @swConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i32, ptr %b, align 4
-  %conv = sitofp i32 %0 to fp128
+  %i = load i32, ptr %b, align 4
+  %conv = sitofp i32 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @swConv2qp_03(ptr nocapture %a) {
+define void @swConv2qp_03(ptr nocapture %a) nounwind {
 ; CHECK-LABEL: swConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC2@toc@ha
@@ -601,9 +553,6 @@ define void @swConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-LABEL: swConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -621,16 +570,16 @@ define void @swConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i32, ptr getelementptr inbounds
+  %i = load i32, ptr getelementptr inbounds
                         ([5 x i32], ptr @swMem, i64 0, i64 3), align 4
-  %conv = sitofp i32 %0 to fp128
+  %conv = sitofp i32 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @uwConv2qp(ptr nocapture %a, i32 zeroext %b) {
+define void @uwConv2qp(ptr nocapture %a, i32 zeroext %b) nounwind {
 ; CHECK-LABEL: uwConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mtvsrwz v2, r4
@@ -641,9 +590,6 @@ define void @uwConv2qp(ptr nocapture %a, i32 zeroext %b) {
 ; CHECK-P8-LABEL: uwConv2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -666,7 +612,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @uwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
+define void @uwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) nounwind {
 ; CHECK-LABEL: uwConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxsiwzx v2, 0, r4
@@ -677,9 +623,6 @@ define void @uwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-LABEL: uwConv2qp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -695,15 +638,15 @@ define void @uwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i32, ptr %b, align 4
-  %conv = uitofp i32 %0 to fp128
+  %i = load i32, ptr %b, align 4
+  %conv = uitofp i32 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @uwConv2qp_03(ptr nocapture %a) {
+define void @uwConv2qp_03(ptr nocapture %a) nounwind {
 ; CHECK-LABEL: uwConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC3@toc@ha
@@ -717,9 +660,6 @@ define void @uwConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-LABEL: uwConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -737,9 +677,9 @@ define void @uwConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i32, ptr getelementptr inbounds
+  %i = load i32, ptr getelementptr inbounds
                         ([5 x i32], ptr @uwMem, i64 0, i64 3), align 4
-  %conv = uitofp i32 %0 to fp128
+  %conv = uitofp i32 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
@@ -759,9 +699,6 @@ define void @uwConv2qp_04(ptr nocapture %a,
 ; CHECK-P8-LABEL: uwConv2qp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -778,10 +715,10 @@ define void @uwConv2qp_04(ptr nocapture %a,
 ; CHECK-P8-NEXT:    ld r30, -16(r1) # 8-byte Folded Reload
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
-                          i32 zeroext %b, ptr nocapture readonly %c) {
+                          i32 zeroext %b, ptr nocapture readonly %c) nounwind {
 entry:
-  %0 = load i32, ptr %c, align 4
-  %add = add i32 %0, %b
+  %i = load i32, ptr %c, align 4
+  %add = add i32 %i, %b
   %conv = uitofp i32 %add to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
@@ -789,7 +726,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @uhwConv2qp(ptr nocapture %a, i16 zeroext %b) {
+define void @uhwConv2qp(ptr nocapture %a, i16 zeroext %b) nounwind {
 ; CHECK-LABEL: uhwConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mtvsrwz v2, r4
@@ -800,9 +737,6 @@ define void @uhwConv2qp(ptr nocapture %a, i16 zeroext %b) {
 ; CHECK-P8-LABEL: uhwConv2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -826,7 +760,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @uhwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
+define void @uhwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) nounwind {
 ; CHECK-LABEL: uhwConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxsihzx v2, 0, r4
@@ -837,9 +771,6 @@ define void @uhwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-LABEL: uhwConv2qp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -855,15 +786,15 @@ define void @uhwConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i16, ptr %b, align 2
-  %conv = uitofp i16 %0 to fp128
+  %i = load i16, ptr %b, align 2
+  %conv = uitofp i16 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @uhwConv2qp_03(ptr nocapture %a) {
+define void @uhwConv2qp_03(ptr nocapture %a) nounwind {
 ; CHECK-LABEL: uhwConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC4@toc@ha
@@ -877,9 +808,6 @@ define void @uhwConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-LABEL: uhwConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -897,9 +825,9 @@ define void @uhwConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i16, ptr getelementptr inbounds
+  %i = load i16, ptr getelementptr inbounds
                         ([5 x i16], ptr @uhwMem, i64 0, i64 3), align 2
-  %conv = uitofp i16 %0 to fp128
+  %conv = uitofp i16 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
@@ -919,9 +847,6 @@ define void @uhwConv2qp_04(ptr nocapture %a, i16 zeroext %b,
 ; CHECK-P8-LABEL: uhwConv2qp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -938,11 +863,11 @@ define void @uhwConv2qp_04(ptr nocapture %a, i16 zeroext %b,
 ; CHECK-P8-NEXT:    ld r30, -16(r1) # 8-byte Folded Reload
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
-                           ptr nocapture readonly %c) {
+                           ptr nocapture readonly %c) nounwind {
 entry:
   %conv = zext i16 %b to i32
-  %0 = load i16, ptr %c, align 2
-  %conv1 = zext i16 %0 to i32
+  %i = load i16, ptr %c, align 2
+  %conv1 = zext i16 %i to i32
   %add = add nuw nsw i32 %conv1, %conv
   %conv2 = sitofp i32 %add to fp128
   store fp128 %conv2, ptr %a, align 16
@@ -951,7 +876,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @ubConv2qp(ptr nocapture %a, i8 zeroext %b) {
+define void @ubConv2qp(ptr nocapture %a, i8 zeroext %b) nounwind {
 ; CHECK-LABEL: ubConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mtvsrwz v2, r4
@@ -962,9 +887,6 @@ define void @ubConv2qp(ptr nocapture %a, i8 zeroext %b) {
 ; CHECK-P8-LABEL: ubConv2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -987,7 +909,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @ubConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
+define void @ubConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) nounwind {
 ; CHECK-LABEL: ubConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxsibzx v2, 0, r4
@@ -998,9 +920,6 @@ define void @ubConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-LABEL: ubConv2qp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -1016,15 +935,15 @@ define void @ubConv2qp_02(ptr nocapture %a, ptr nocapture readonly %b) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i8, ptr %b, align 1
-  %conv = uitofp i8 %0 to fp128
+  %i = load i8, ptr %b, align 1
+  %conv = uitofp i8 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
 }
 
 ; Function Attrs: norecurse nounwind
-define void @ubConv2qp_03(ptr nocapture %a) {
+define void @ubConv2qp_03(ptr nocapture %a) nounwind {
 ; CHECK-LABEL: ubConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC5@toc@ha
@@ -1038,9 +957,6 @@ define void @ubConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-LABEL: ubConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -1058,9 +974,9 @@ define void @ubConv2qp_03(ptr nocapture %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load i8, ptr getelementptr inbounds
+  %i = load i8, ptr getelementptr inbounds
                       ([5 x i8], ptr @ubMem, i64 0, i64 2), align 1
-  %conv = uitofp i8 %0 to fp128
+  %conv = uitofp i8 %i to fp128
   store fp128 %conv, ptr %a, align 16
   ret void
 
@@ -1080,9 +996,6 @@ define void @ubConv2qp_04(ptr nocapture %a, i8 zeroext %b,
 ; CHECK-P8-LABEL: ubConv2qp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -1099,11 +1012,11 @@ define void @ubConv2qp_04(ptr nocapture %a, i8 zeroext %b,
 ; CHECK-P8-NEXT:    ld r30, -16(r1) # 8-byte Folded Reload
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
-                          ptr nocapture readonly %c) {
+                          ptr nocapture readonly %c) nounwind {
 entry:
   %conv = zext i8 %b to i32
-  %0 = load i8, ptr %c, align 1
-  %conv1 = zext i8 %0 to i32
+  %i = load i8, ptr %c, align 1
+  %conv1 = zext i8 %i to i32
   %add = add nuw nsw i32 %conv1, %conv
   %conv2 = sitofp i32 %add to fp128
   store fp128 %conv2, ptr %a, align 16
@@ -1121,7 +1034,7 @@ entry:
 @f128global = global fp128 0xL300000000000000040089CA8F5C28F5C, align 16
 
 ; Function Attrs: norecurse nounwind readonly
-define double @qpConv2dp(ptr nocapture readonly %a) {
+define double @qpConv2dp(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: qpConv2dp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxv v2, 0(r3)
@@ -1134,8 +1047,6 @@ define double @qpConv2dp(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lxvd2x vs0, 0, r3
 ; CHECK-P8-NEXT:    xxswapd v2, vs0
 ; CHECK-P8-NEXT:    bl __trunckfdf2
@@ -1145,13 +1056,13 @@ define double @qpConv2dp(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr %a, align 16
-  %conv = fptrunc fp128 %0 to double
+  %i = load fp128, ptr %a, align 16
+  %conv = fptrunc fp128 %i to double
   ret double %conv
 }
 
 ; Function Attrs: norecurse nounwind
-define void @qpConv2dp_02(ptr nocapture %res) {
+define void @qpConv2dp_02(ptr nocapture %res) nounwind {
 ; CHECK-LABEL: qpConv2dp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC6@toc@ha
@@ -1164,9 +1075,6 @@ define void @qpConv2dp_02(ptr nocapture %res) {
 ; CHECK-P8-LABEL: qpConv2dp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -1184,14 +1092,14 @@ define void @qpConv2dp_02(ptr nocapture %res) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr @f128global, align 16
-  %conv = fptrunc fp128 %0 to double
+  %i = load fp128, ptr @f128global, align 16
+  %conv = fptrunc fp128 %i to double
   store double %conv, ptr %res, align 8
   ret void
 }
 
 ; Function Attrs: norecurse nounwind
-define void @qpConv2dp_03(ptr nocapture %res, i32 signext %idx) {
+define void @qpConv2dp_03(ptr nocapture %res, i32 signext %idx) nounwind {
 ; CHECK-LABEL: qpConv2dp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r5, r2, .LC7@toc@ha
@@ -1205,10 +1113,6 @@ define void @qpConv2dp_03(ptr nocapture %res, i32 signext %idx) {
 ; CHECK-P8-LABEL: qpConv2dp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 64
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r29, -24
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r29, -24(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -64(r1)
@@ -1230,8 +1134,8 @@ define void @qpConv2dp_03(ptr nocapture %res, i32 signext %idx) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr @f128Array, align 16
-  %conv = fptrunc fp128 %0 to double
+  %i = load fp128, ptr @f128Array, align 16
+  %conv = fptrunc fp128 %i to double
   %idxprom = sext i32 %idx to i64
   %arrayidx = getelementptr inbounds double, ptr %res, i64 %idxprom
   store double %conv, ptr %arrayidx, align 8
@@ -1239,7 +1143,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @qpConv2dp_04(ptr nocapture readonly %a, ptr nocapture readonly %b, ptr nocapture %res) {
+define void @qpConv2dp_04(ptr nocapture readonly %a, ptr nocapture readonly %b,
 ; CHECK-LABEL: qpConv2dp_04:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxv v2, 0(r3)
@@ -1252,9 +1156,6 @@ define void @qpConv2dp_04(ptr nocapture readonly %a, ptr nocapture readonly %b,
 ; CHECK-P8-LABEL: qpConv2dp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -1273,10 +1174,11 @@ define void @qpConv2dp_04(ptr nocapture readonly %a, ptr nocapture readonly %b,
 ; CHECK-P8-NEXT:    ld r30, -16(r1) # 8-byte Folded Reload
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
+                          ptr nocapture %res) nounwind {
 entry:
-  %0 = load fp128, ptr %a, align 16
+  %i = load fp128, ptr %a, align 16
   %1 = load fp128, ptr %b, align 16
-  %add = fadd fp128 %0, %1
+  %add = fadd fp128 %i, %1
   %conv = fptrunc fp128 %add to double
   store double %conv, ptr %res, align 8
   ret void
@@ -1285,7 +1187,7 @@ entry:
 ;  Convert QP to SP
 
 ; Function Attrs: norecurse nounwind readonly
-define float @qpConv2sp(ptr nocapture readonly %a) {
+define float @qpConv2sp(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: qpConv2sp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxv v2, 0(r3)
@@ -1298,8 +1200,6 @@ define float @qpConv2sp(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lxvd2x vs0, 0, r3
 ; CHECK-P8-NEXT:    xxswapd v2, vs0
 ; CHECK-P8-NEXT:    bl __trunckfsf2
@@ -1309,13 +1209,13 @@ define float @qpConv2sp(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr %a, align 16
-  %conv = fptrunc fp128 %0 to float
+  %i = load fp128, ptr %a, align 16
+  %conv = fptrunc fp128 %i to float
   ret float %conv
 }
 
 ; Function Attrs: norecurse nounwind
-define void @qpConv2sp_02(ptr nocapture %res) {
+define void @qpConv2sp_02(ptr nocapture %res) nounwind {
 ; CHECK-LABEL: qpConv2sp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r4, r2, .LC6@toc@ha
@@ -1329,9 +1229,6 @@ define void @qpConv2sp_02(ptr nocapture %res) {
 ; CHECK-P8-LABEL: qpConv2sp_02:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    mr r30, r3
@@ -1349,14 +1246,14 @@ define void @qpConv2sp_02(ptr nocapture %res) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr @f128global, align 16
-  %conv = fptrunc fp128 %0 to float
+  %i = load fp128, ptr @f128global, align 16
+  %conv = fptrunc fp128 %i to float
   store float %conv, ptr %res, align 4
   ret void
 }
 
 ; Function Attrs: norecurse nounwind
-define void @qpConv2sp_03(ptr nocapture %res, i32 signext %idx) {
+define void @qpConv2sp_03(ptr nocapture %res, i32 signext %idx) nounwind {
 ; CHECK-LABEL: qpConv2sp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    addis r5, r2, .LC7@toc@ha
@@ -1371,10 +1268,6 @@ define void @qpConv2sp_03(ptr nocapture %res, i32 signext %idx) {
 ; CHECK-P8-LABEL: qpConv2sp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 64
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r29, -24
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r29, -24(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -64(r1)
@@ -1397,8 +1290,8 @@ define void @qpConv2sp_03(ptr nocapture %res, i32 signext %idx) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr getelementptr inbounds ([4 x fp128], ptr @f128Array, i64 0, i64 3), align 16
-  %conv = fptrunc fp128 %0 to float
+  %i = load fp128, ptr getelementptr inbounds ([4 x fp128], ptr @f128Array, i64 0, i64 3), align 16
+  %conv = fptrunc fp128 %i to float
   %idxprom = sext i32 %idx to i64
   %arrayidx = getelementptr inbounds float, ptr %res, i64 %idxprom
   store float %conv, ptr %arrayidx, align 4
@@ -1406,7 +1299,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @qpConv2sp_04(ptr nocapture readonly %a, ptr nocapture readonly %b, ptr nocapture %res) {
+define void @qpConv2sp_04(ptr nocapture readonly %a, ptr nocapture readonly %b,
 ; CHECK-LABEL: qpConv2sp_04:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxv v2, 0(r3)
@@ -1420,9 +1313,6 @@ define void @qpConv2sp_04(ptr nocapture readonly %a, ptr nocapture readonly %b,
 ; CHECK-P8-LABEL: qpConv2sp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -1441,19 +1331,78 @@ define void @qpConv2sp_04(ptr nocapture readonly %a, ptr nocapture readonly %b,
 ; CHECK-P8-NEXT:    ld r30, -16(r1) # 8-byte Folded Reload
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
+                          ptr nocapture %res) nounwind {
 entry:
-  %0 = load fp128, ptr %a, align 16
+  %i = load fp128, ptr %a, align 16
   %1 = load fp128, ptr %b, align 16
-  %add = fadd fp128 %0, %1
+  %add = fadd fp128 %i, %1
   %conv = fptrunc fp128 %add to float
   store float %conv, ptr %res, align 4
   ret void
 }
 
+define half @trunc(fp128 %a) nounwind {
+; CHECK-LABEL: trunc:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    mflr r0
+; CHECK-NEXT:    stdu r1, -32(r1)
+; CHECK-NEXT:    std r0, 48(r1)
+; CHECK-NEXT:    bl __trunckfhf2
+; CHECK-NEXT:    nop
+; CHECK-NEXT:    clrlwi r3, r3, 16
+; CHECK-NEXT:    mtfprwz f0, r3
+; CHECK-NEXT:    xscvhpdp f1, f0
+; CHECK-NEXT:    addi r1, r1, 32
+; CHECK-NEXT:    ld r0, 16(r1)
+; CHECK-NEXT:    mtlr r0
+; CHECK-NEXT:    blr
+;
+; CHECK-P8-LABEL: trunc:
+; CHECK-P8:       # %bb.0: # %entry
+; CHECK-P8-NEXT:    mflr r0
+; CHECK-P8-NEXT:    stdu r1, -32(r1)
+; CHECK-P8-NEXT:    std r0, 48(r1)
+; CHECK-P8-NEXT:    bl __trunckfhf2
+; CHECK-P8-NEXT:    nop
+; CHECK-P8-NEXT:    clrldi r3, r3, 48
+; CHECK-P8-NEXT:    bl __gnu_h2f_ieee
+; CHECK-P8-NEXT:    nop
+; CHECK-P8-NEXT:    addi r1, r1, 32
+; CHECK-P8-NEXT:    ld r0, 16(r1)
+; CHECK-P8-NEXT:    mtlr r0
+; CHECK-P8-NEXT:    blr
+entry:
+  %i = fptrunc fp128 %a to half
+  ret half %i
+}
+
+define fp128 @ext(half %a) nounwind {
+; CHECK-LABEL: ext:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    xscpsgndp v2, f1, f1
+; CHECK-NEXT:    xscvdpqp v2, v2
+; CHECK-NEXT:    blr
+;
+; CHECK-P8-LABEL: ext:
+; CHECK-P8:       # %bb.0: # %entry
+; CHECK-P8-NEXT:    mflr r0
+; CHECK-P8-NEXT:    stdu r1, -32(r1)
+; CHECK-P8-NEXT:    std r0, 48(r1)
+; CHECK-P8-NEXT:    bl __extendsfkf2
+; CHECK-P8-NEXT:    nop
+; CHECK-P8-NEXT:    addi r1, r1, 32
+; CHECK-P8-NEXT:    ld r0, 16(r1)
+; CHECK-P8-NEXT:    mtlr r0
+; CHECK-P8-NEXT:    blr
+entry:
+  %i = fpext half %a to fp128
+  ret fp128 %i
+}
+
 @f128Glob = common global fp128 0xL00000000000000000000000000000000, align 16
 
 ; Function Attrs: norecurse nounwind readnone
-define fp128 @dpConv2qp(double %a) {
+define fp128 @dpConv2qp(double %a) nounwind {
 ; CHECK-LABEL: dpConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscpsgndp v2, f1, f1
@@ -1465,8 +1414,6 @@ define fp128 @dpConv2qp(double %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    bl __extenddfkf2
 ; CHECK-P8-NEXT:    nop
 ; CHECK-P8-NEXT:    addi r1, r1, 32
@@ -1479,7 +1426,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @dpConv2qp_02(ptr nocapture readonly %a) {
+define void @dpConv2qp_02(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: dpConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxsd v2, 0(r3)
@@ -1494,8 +1441,6 @@ define void @dpConv2qp_02(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lfd f1, 0(r3)
 ; CHECK-P8-NEXT:    bl __extenddfkf2
 ; CHECK-P8-NEXT:    nop
@@ -1508,14 +1453,14 @@ define void @dpConv2qp_02(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load double, ptr %a, align 8
-  %conv = fpext double %0 to fp128
+  %i = load double, ptr %a, align 8
+  %conv = fpext double %i to fp128
   store fp128 %conv, ptr @f128Glob, align 16
   ret void
 }
 
 ; Function Attrs: norecurse nounwind
-define void @dpConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) {
+define void @dpConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) nounwind {
 ; CHECK-LABEL: dpConv2qp_02b:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    sldi r4, r4, 3
@@ -1530,10 +1475,8 @@ define void @dpConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) {
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
-; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    sldi r4, r4, 3
+; CHECK-P8-NEXT:    std r0, 48(r1)
 ; CHECK-P8-NEXT:    lfdx f1, r3, r4
 ; CHECK-P8-NEXT:    bl __extenddfkf2
 ; CHECK-P8-NEXT:    nop
@@ -1548,14 +1491,14 @@ define void @dpConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) {
 entry:
   %idxprom = sext i32 %idx to i64
   %arrayidx = getelementptr inbounds double, ptr %a, i64 %idxprom
-  %0 = load double, ptr %arrayidx, align 8
-  %conv = fpext double %0 to fp128
+  %i = load double, ptr %arrayidx, align 8
+  %conv = fpext double %i to fp128
   store fp128 %conv, ptr @f128Glob, align 16
   ret void
 }
 
 ; Function Attrs: norecurse nounwind
-define void @dpConv2qp_03(ptr nocapture %res, i32 signext %idx, double %a) {
+define void @dpConv2qp_03(ptr nocapture %res, i32 signext %idx, double %a) nounwind {
 ; CHECK-LABEL: dpConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscpsgndp v2, f1, f1
@@ -1567,10 +1510,6 @@ define void @dpConv2qp_03(ptr nocapture %res, i32 signext %idx, double %a) {
 ; CHECK-P8-LABEL: dpConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 64
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r29, -24
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r29, -24(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -64(r1)
@@ -1597,7 +1536,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @dpConv2qp_04(double %a, ptr nocapture %res) {
+define void @dpConv2qp_04(double %a, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: dpConv2qp_04:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscpsgndp v2, f1, f1
@@ -1608,9 +1547,6 @@ define void @dpConv2qp_04(double %a, ptr nocapture %res) {
 ; CHECK-P8-LABEL: dpConv2qp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -1631,7 +1567,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind readnone
-define fp128 @spConv2qp(float %a) {
+define fp128 @spConv2qp(float %a) nounwind {
 ; CHECK-LABEL: spConv2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscpsgndp v2, f1, f1
@@ -1643,8 +1579,6 @@ define fp128 @spConv2qp(float %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    bl __extendsfkf2
 ; CHECK-P8-NEXT:    nop
 ; CHECK-P8-NEXT:    addi r1, r1, 32
@@ -1657,7 +1591,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @spConv2qp_02(ptr nocapture readonly %a) {
+define void @spConv2qp_02(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: spConv2qp_02:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxssp v2, 0(r3)
@@ -1672,8 +1606,6 @@ define void @spConv2qp_02(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lfs f1, 0(r3)
 ; CHECK-P8-NEXT:    bl __extendsfkf2
 ; CHECK-P8-NEXT:    nop
@@ -1686,14 +1618,14 @@ define void @spConv2qp_02(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load float, ptr %a, align 4
-  %conv = fpext float %0 to fp128
+  %i = load float, ptr %a, align 4
+  %conv = fpext float %i to fp128
   store fp128 %conv, ptr @f128Glob, align 16
   ret void
 }
 
 ; Function Attrs: norecurse nounwind
-define void @spConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) {
+define void @spConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) nounwind {
 ; CHECK-LABEL: spConv2qp_02b:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    sldi r4, r4, 2
@@ -1708,10 +1640,8 @@ define void @spConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) {
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
-; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    sldi r4, r4, 2
+; CHECK-P8-NEXT:    std r0, 48(r1)
 ; CHECK-P8-NEXT:    lfsx f1, r3, r4
 ; CHECK-P8-NEXT:    bl __extendsfkf2
 ; CHECK-P8-NEXT:    nop
@@ -1726,14 +1656,14 @@ define void @spConv2qp_02b(ptr nocapture readonly %a, i32 signext %idx) {
 entry:
   %idxprom = sext i32 %idx to i64
   %arrayidx = getelementptr inbounds float, ptr %a, i64 %idxprom
-  %0 = load float, ptr %arrayidx, align 4
-  %conv = fpext float %0 to fp128
+  %i = load float, ptr %arrayidx, align 4
+  %conv = fpext float %i to fp128
   store fp128 %conv, ptr @f128Glob, align 16
   ret void
 }
 
 ; Function Attrs: norecurse nounwind
-define void @spConv2qp_03(ptr nocapture %res, i32 signext %idx, float %a) {
+define void @spConv2qp_03(ptr nocapture %res, i32 signext %idx, float %a) nounwind {
 ; CHECK-LABEL: spConv2qp_03:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscpsgndp v2, f1, f1
@@ -1745,10 +1675,6 @@ define void @spConv2qp_03(ptr nocapture %res, i32 signext %idx, float %a) {
 ; CHECK-P8-LABEL: spConv2qp_03:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 64
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r29, -24
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r29, -24(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -64(r1)
@@ -1775,7 +1701,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @spConv2qp_04(float %a, ptr nocapture %res) {
+define void @spConv2qp_04(float %a, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: spConv2qp_04:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscpsgndp v2, f1, f1
@@ -1786,9 +1712,6 @@ define void @spConv2qp_04(float %a, ptr nocapture %res) {
 ; CHECK-P8-LABEL: spConv2qp_04:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    std r0, 64(r1)
@@ -1810,7 +1733,7 @@ entry:
 
 
 ; Function Attrs: norecurse nounwind
-define void @cvdp2sw2qp(double %val, ptr nocapture %res) {
+define void @cvdp2sw2qp(double %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvdp2sw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpsxws v2, f1
@@ -1822,9 +1745,6 @@ define void @cvdp2sw2qp(double %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvdp2sw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpsxws f0, f1
@@ -1849,7 +1769,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @cvdp2sdw2qp(double %val, ptr nocapture %res) {
+define void @cvdp2sdw2qp(double %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvdp2sdw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpsxds v2, f1
@@ -1860,9 +1780,6 @@ define void @cvdp2sdw2qp(double %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvdp2sdw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpsxds f0, f1
@@ -1886,7 +1803,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @cvsp2sw2qp(float %val, ptr nocapture %res) {
+define void @cvsp2sw2qp(float %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvsp2sw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpsxws v2, f1
@@ -1898,9 +1815,6 @@ define void @cvsp2sw2qp(float %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvsp2sw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpsxws f0, f1
@@ -1925,7 +1839,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @cvsp2sdw2qp(float %val, ptr nocapture %res) {
+define void @cvsp2sdw2qp(float %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvsp2sdw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpsxds v2, f1
@@ -1936,9 +1850,6 @@ define void @cvsp2sdw2qp(float %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvsp2sdw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpsxds f0, f1
@@ -1962,7 +1873,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @cvdp2uw2qp(double %val, ptr nocapture %res) {
+define void @cvdp2uw2qp(double %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvdp2uw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpuxws f0, f1
@@ -1974,9 +1885,6 @@ define void @cvdp2uw2qp(double %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvdp2uw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpuxws f0, f1
@@ -2000,7 +1908,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @cvdp2udw2qp(double %val, ptr nocapture %res) {
+define void @cvdp2udw2qp(double %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvdp2udw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpuxds v2, f1
@@ -2011,9 +1919,6 @@ define void @cvdp2udw2qp(double %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvdp2udw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpuxds f0, f1
@@ -2037,7 +1942,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @cvsp2uw2qp(float %val, ptr nocapture %res) {
+define void @cvsp2uw2qp(float %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvsp2uw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpuxws f0, f1
@@ -2049,9 +1954,6 @@ define void @cvsp2uw2qp(float %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvsp2uw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpuxws f0, f1
@@ -2075,7 +1977,7 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind
-define void @cvsp2udw2qp(float %val, ptr nocapture %res) {
+define void @cvsp2udw2qp(float %val, ptr nocapture %res) nounwind {
 ; CHECK-LABEL: cvsp2udw2qp:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    xscvdpuxds v2, f1
@@ -2086,9 +1988,6 @@ define void @cvsp2udw2qp(float %val, ptr nocapture %res) {
 ; CHECK-P8-LABEL: cvsp2udw2qp:
 ; CHECK-P8:       # %bb.0: # %entry
 ; CHECK-P8-NEXT:    mflr r0
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 48
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
-; CHECK-P8-NEXT:    .cfi_offset r30, -16
 ; CHECK-P8-NEXT:    std r30, -16(r1) # 8-byte Folded Spill
 ; CHECK-P8-NEXT:    stdu r1, -48(r1)
 ; CHECK-P8-NEXT:    xscvdpuxds f0, f1
@@ -2112,14 +2011,12 @@ entry:
 }
 
 ; Function Attrs: norecurse nounwind readonly
-define i128 @qpConv2i128(ptr nocapture readonly %a) {
+define i128 @qpConv2i128(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: qpConv2i128:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mflr r0
 ; CHECK-NEXT:    stdu r1, -32(r1)
 ; CHECK-NEXT:    std r0, 48(r1)
-; CHECK-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-NEXT:    .cfi_offset lr, 16
 ; CHECK-NEXT:    lxv v2, 0(r3)
 ; CHECK-NEXT:    bl __fixkfti
 ; CHECK-NEXT:    nop
@@ -2133,8 +2030,6 @@ define i128 @qpConv2i128(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lxvd2x vs0, 0, r3
 ; CHECK-P8-NEXT:    xxswapd v2, vs0
 ; CHECK-P8-NEXT:    bl __fixkfti
@@ -2144,20 +2039,18 @@ define i128 @qpConv2i128(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr %a, align 16
-  %conv = fptosi fp128 %0 to i128
+  %i = load fp128, ptr %a, align 16
+  %conv = fptosi fp128 %i to i128
   ret i128 %conv
 }
 
 ; Function Attrs: norecurse nounwind readonly
-define i128 @qpConv2ui128(ptr nocapture readonly %a) {
+define i128 @qpConv2ui128(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: qpConv2ui128:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    mflr r0
 ; CHECK-NEXT:    stdu r1, -32(r1)
 ; CHECK-NEXT:    std r0, 48(r1)
-; CHECK-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-NEXT:    .cfi_offset lr, 16
 ; CHECK-NEXT:    lxv v2, 0(r3)
 ; CHECK-NEXT:    bl __fixunskfti
 ; CHECK-NEXT:    nop
@@ -2171,8 +2064,6 @@ define i128 @qpConv2ui128(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lxvd2x vs0, 0, r3
 ; CHECK-P8-NEXT:    xxswapd v2, vs0
 ; CHECK-P8-NEXT:    bl __fixunskfti
@@ -2182,13 +2073,13 @@ define i128 @qpConv2ui128(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr %a, align 16
-  %conv = fptoui fp128 %0 to i128
+  %i = load fp128, ptr %a, align 16
+  %conv = fptoui fp128 %i to i128
   ret i128 %conv
 }
 
 ; Function Attrs: norecurse nounwind readonly
-define i1 @qpConv2ui1(ptr nocapture readonly %a) {
+define i1 @qpConv2ui1(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: qpConv2ui1:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxv v2, 0(r3)
@@ -2201,8 +2092,6 @@ define i1 @qpConv2ui1(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lxvd2x vs0, 0, r3
 ; CHECK-P8-NEXT:    xxswapd v2, vs0
 ; CHECK-P8-NEXT:    bl __fixkfsi
@@ -2212,13 +2101,13 @@ define i1 @qpConv2ui1(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr %a, align 16
-  %conv = fptoui fp128 %0 to i1
+  %i = load fp128, ptr %a, align 16
+  %conv = fptoui fp128 %i to i1
   ret i1 %conv
 }
 
 ; Function Attrs: norecurse nounwind readonly
-define i1 @qpConv2si1(ptr nocapture readonly %a) {
+define i1 @qpConv2si1(ptr nocapture readonly %a) nounwind {
 ; CHECK-LABEL: qpConv2si1:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    lxv v2, 0(r3)
@@ -2231,8 +2120,6 @@ define i1 @qpConv2si1(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mflr r0
 ; CHECK-P8-NEXT:    stdu r1, -32(r1)
 ; CHECK-P8-NEXT:    std r0, 48(r1)
-; CHECK-P8-NEXT:    .cfi_def_cfa_offset 32
-; CHECK-P8-NEXT:    .cfi_offset lr, 16
 ; CHECK-P8-NEXT:    lxvd2x vs0, 0, r3
 ; CHECK-P8-NEXT:    xxswapd v2, vs0
 ; CHECK-P8-NEXT:    bl __fixkfsi
@@ -2242,7 +2129,7 @@ define i1 @qpConv2si1(ptr nocapture readonly %a) {
 ; CHECK-P8-NEXT:    mtlr r0
 ; CHECK-P8-NEXT:    blr
 entry:
-  %0 = load fp128, ptr %a, align 16
-  %conv = fptosi fp128 %0 to i1
+  %i = load fp128, ptr %a, align 16
+  %conv = fptosi fp128 %i to i1
   ret i1 %conv
 }
diff --git a/llvm/test/CodeGen/PowerPC/fp128-libcalls.ll b/llvm/test/CodeGen/PowerPC/fp128-libcalls.ll
index 9d875c854e32..e348d40a4cc7 100644
--- a/llvm/test/CodeGen/PowerPC/fp128-libcalls.ll
+++ b/llvm/test/CodeGen/PowerPC/fp128-libcalls.ll
@@ -30,6 +30,15 @@ define fp128 @divkf3(fp128 %a, fp128 %b) {
   ret fp128 %1
 }
 
+
+define fp128 @extendsfkf2_f16(half %a) {
+; CHECK-LABEL: extendsfkf2_f16:
+; CHECK: __extendsfkf2
+entry:
+  %i = fpext half %a to fp128
+  ret fp128 %i
+}
+
 define fp128 @extendsfkf2(float %a) {
 ; CHECK-LABEL: extendsfkf2:
 ; CHECK: __extendsfkf2
@@ -44,6 +53,14 @@ define fp128 @extenddfkf2(double %a) {
   ret fp128 %1
 }
 
+define half @trunctfhf2(fp128 %a) {
+; CHECK-LABEL: trunctfhf2:
+; CHECK: __trunckfhf2
+entry:
+  %i = fptrunc fp128 %a to half
+  ret half %i
+}
+
 define float @trunckfsf2(fp128 %a) {
 ; CHECK-LABEL: trunckfsf2:
 ; CHECK: __trunckfsf2
diff --git a/llvm/test/CodeGen/PowerPC/global-merge-llvm-metadata.ll b/llvm/test/CodeGen/PowerPC/global-merge-llvm-metadata.ll
new file mode 100644
index 000000000000..7db092e13afe
--- /dev/null
+++ b/llvm/test/CodeGen/PowerPC/global-merge-llvm-metadata.ll
@@ -0,0 +1,9 @@
+; RUN: llc -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s
+
+@index = global i32 0, align 4
+@.str = private unnamed_addr constant [1 x i8] zeroinitializer, section "llvm.metadata"
+@.str.1 = private unnamed_addr constant [7 x i8] c"test.c\00", section "llvm.metadata" 
+@llvm.global.annotations = appending global [1 x { ptr, ptr, ptr, i32, ptr }] [{ ptr, ptr, ptr, i32, ptr } { ptr @index, ptr @.str, ptr @.str.1, i32 1, ptr null }], section "llvm.metadata"
+
+; CHECK-NOT: .set
+; CHECK-NOT: _MergedGlobals
diff --git a/llvm/test/CodeGen/SystemZ/args-12.ll b/llvm/test/CodeGen/SystemZ/args-12.ll
index f8954eee550f..472672bbfd5c 100644
--- a/llvm/test/CodeGen/SystemZ/args-12.ll
+++ b/llvm/test/CodeGen/SystemZ/args-12.ll
@@ -2,7 +2,7 @@
 ; Test the handling of i128 argument values
 ;
 ; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z13 | FileCheck %s
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 declare void @bar(i64, i64, i64, i64, i128,
                   i64, i64, i64, i64, i128)
diff --git a/llvm/test/CodeGen/SystemZ/args-13.ll b/llvm/test/CodeGen/SystemZ/args-13.ll
index d9e986cbb6a4..29a718901e81 100644
--- a/llvm/test/CodeGen/SystemZ/args-13.ll
+++ b/llvm/test/CodeGen/SystemZ/args-13.ll
@@ -2,7 +2,7 @@
 ; Test incoming i128 arguments.
 ;
 ; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z13 | FileCheck %s
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Do some arithmetic so that we can see the register being used.
 define void @f1(ptr %r2, i16 %r3, i32 %r4, i64 %r5, i128 %r6) {
diff --git a/llvm/test/CodeGen/SystemZ/bitop-intrinsics.ll b/llvm/test/CodeGen/SystemZ/bitop-intrinsics.ll
index f5b0aaa243a7..bbd9be463a01 100644
--- a/llvm/test/CodeGen/SystemZ/bitop-intrinsics.ll
+++ b/llvm/test/CodeGen/SystemZ/bitop-intrinsics.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test bit deposit / extract intrinsics
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 declare i64 @llvm.s390.bdepg(i64, i64)
 declare i64 @llvm.s390.bextg(i64, i64)
diff --git a/llvm/test/CodeGen/SystemZ/int-abs-03.ll b/llvm/test/CodeGen/SystemZ/int-abs-03.ll
index 238b2431c9b3..2a8969c27fbc 100644
--- a/llvm/test/CodeGen/SystemZ/int-abs-03.ll
+++ b/llvm/test/CodeGen/SystemZ/int-abs-03.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test 128-bit absolute value in vector registers on arch15
+; Test 128-bit absolute value in vector registers on z17
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 define i128 @f1(i128 %src) {
 ; CHECK-LABEL: f1:
diff --git a/llvm/test/CodeGen/SystemZ/int-add-19.ll b/llvm/test/CodeGen/SystemZ/int-add-19.ll
index a9bce2c827ff..f5ef08b4514f 100644
--- a/llvm/test/CodeGen/SystemZ/int-add-19.ll
+++ b/llvm/test/CodeGen/SystemZ/int-add-19.ll
@@ -2,7 +2,7 @@
 ; Test 128-bit addition in vector registers on z13 and later
 ;
 ; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z13 | FileCheck %s
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 define i128 @f1(i128 %a, i128 %b) {
 ; CHECK-LABEL: f1:
diff --git a/llvm/test/CodeGen/SystemZ/int-cmp-64.ll b/llvm/test/CodeGen/SystemZ/int-cmp-64.ll
index be212ef2a721..821a57bf30bc 100644
--- a/llvm/test/CodeGen/SystemZ/int-cmp-64.ll
+++ b/llvm/test/CodeGen/SystemZ/int-cmp-64.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test 128-bit comparisons in vector registers on arch15
+; Test 128-bit comparisons in vector registers on z17
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 -verify-machineinstrs | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 -verify-machineinstrs | FileCheck %s
 
 ; Equality comparison.
 define i64 @f1(i128 %value1, i128 %value2, i64 %a, i64 %b) {
diff --git a/llvm/test/CodeGen/SystemZ/int-conv-15.ll b/llvm/test/CodeGen/SystemZ/int-conv-15.ll
index bea0bb889031..0d8ee75b10b8 100644
--- a/llvm/test/CodeGen/SystemZ/int-conv-15.ll
+++ b/llvm/test/CodeGen/SystemZ/int-conv-15.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test 128-bit arithmetic in vector registers on arch15
+; Test 128-bit arithmetic in vector registers on z17
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Sign extension from i64.
 define i128 @f1(i64 %a) {
diff --git a/llvm/test/CodeGen/SystemZ/int-div-08.ll b/llvm/test/CodeGen/SystemZ/int-div-08.ll
index a3723c125797..5838d4913c86 100644
--- a/llvm/test/CodeGen/SystemZ/int-div-08.ll
+++ b/llvm/test/CodeGen/SystemZ/int-div-08.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test 128-bit division and remainder in vector registers on arch15
+; Test 128-bit division and remainder in vector registers on z17
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Divide signed.
 define i128 @f1(i128 %a, i128 %b) {
diff --git a/llvm/test/CodeGen/SystemZ/int-max-02.ll b/llvm/test/CodeGen/SystemZ/int-max-02.ll
index bd5e9593e25e..5f5188c66065 100644
--- a/llvm/test/CodeGen/SystemZ/int-max-02.ll
+++ b/llvm/test/CodeGen/SystemZ/int-max-02.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test i128 maximum on arch15.
+; Test i128 maximum on z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Test with slt.
 define i128 @f1(i128 %val1, i128 %val2) {
diff --git a/llvm/test/CodeGen/SystemZ/int-min-02.ll b/llvm/test/CodeGen/SystemZ/int-min-02.ll
index e4cdd25fbc00..3066af924fb8 100644
--- a/llvm/test/CodeGen/SystemZ/int-min-02.ll
+++ b/llvm/test/CodeGen/SystemZ/int-min-02.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test i128 minimum on arch15.
+; Test i128 minimum on z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Test with slt.
 define i128 @f1(i128 %val1, i128 %val2) {
diff --git a/llvm/test/CodeGen/SystemZ/int-mul-14.ll b/llvm/test/CodeGen/SystemZ/int-mul-14.ll
index e7e0889634d1..6678e90f3bfa 100644
--- a/llvm/test/CodeGen/SystemZ/int-mul-14.ll
+++ b/llvm/test/CodeGen/SystemZ/int-mul-14.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test 128-bit multiplication in vector registers on arch15
+; Test 128-bit multiplication in vector registers on z17
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Multiplication.
 define i128 @f1(i128 %a, i128 %b) {
diff --git a/llvm/test/CodeGen/SystemZ/int-mul-15.ll b/llvm/test/CodeGen/SystemZ/int-mul-15.ll
index a4a0faa0cb0c..b7d41412d9c5 100644
--- a/llvm/test/CodeGen/SystemZ/int-mul-15.ll
+++ b/llvm/test/CodeGen/SystemZ/int-mul-15.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 3
-; Test high-part i64->i128 multiplications on arch15.
+; Test high-part i64->i128 multiplications on z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Check zero-extended multiplication in which only the high part is used.
 define i64 @f1(i64 %dummy, i64 %a, i64 %b) {
diff --git a/llvm/test/CodeGen/SystemZ/int-mul-16.ll b/llvm/test/CodeGen/SystemZ/int-mul-16.ll
index d84ca93e3b12..772c419dfc8e 100644
--- a/llvm/test/CodeGen/SystemZ/int-mul-16.ll
+++ b/llvm/test/CodeGen/SystemZ/int-mul-16.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test high-part i128->i256 multiplications on arch15.
+; Test high-part i128->i256 multiplications on z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Multiply high signed.
 define i128 @f1(i128 %a, i128 %b) {
diff --git a/llvm/test/CodeGen/SystemZ/int-neg-04.ll b/llvm/test/CodeGen/SystemZ/int-neg-04.ll
index 05b7b397e735..a6da2db7d14b 100644
--- a/llvm/test/CodeGen/SystemZ/int-neg-04.ll
+++ b/llvm/test/CodeGen/SystemZ/int-neg-04.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test 128-bit negation in vector registers on arch15
+; Test 128-bit negation in vector registers on z17
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 define i128 @f1(i128 %src) {
 ; CHECK-LABEL: f1:
diff --git a/llvm/test/CodeGen/SystemZ/int-sub-12.ll b/llvm/test/CodeGen/SystemZ/int-sub-12.ll
index 8f7d816d5cbd..44d2adfb41dc 100644
--- a/llvm/test/CodeGen/SystemZ/int-sub-12.ll
+++ b/llvm/test/CodeGen/SystemZ/int-sub-12.ll
@@ -2,7 +2,7 @@
 ; Test 128-bit subtraction in vector registers on z13 and later
 ;
 ; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z13 | FileCheck %s
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 define i128 @f1(i128 %a, i128 %b) {
 ; CHECK-LABEL: f1:
diff --git a/llvm/test/CodeGen/SystemZ/llxa-01.ll b/llvm/test/CodeGen/SystemZ/llxa-01.ll
index 19bc6ef31a28..2c57556dc9ee 100644
--- a/llvm/test/CodeGen/SystemZ/llxa-01.ll
+++ b/llvm/test/CodeGen/SystemZ/llxa-01.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD LOGICAL INDEXED ADDRESS byte instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; DO NOT USE: LLXAB with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/llxa-02.ll b/llvm/test/CodeGen/SystemZ/llxa-02.ll
index 0ca2527dcb25..e2cd929a0bc9 100644
--- a/llvm/test/CodeGen/SystemZ/llxa-02.ll
+++ b/llvm/test/CodeGen/SystemZ/llxa-02.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD LOGICAL INDEXED ADDRESS halfword instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LLXAH with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/llxa-03.ll b/llvm/test/CodeGen/SystemZ/llxa-03.ll
index b6c940678518..b5c91b1d7e60 100644
--- a/llvm/test/CodeGen/SystemZ/llxa-03.ll
+++ b/llvm/test/CodeGen/SystemZ/llxa-03.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD LOGICAL INDEXED ADDRESS word instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LLXAF with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/llxa-04.ll b/llvm/test/CodeGen/SystemZ/llxa-04.ll
index 9c5cd2f54bc6..186892dd755a 100644
--- a/llvm/test/CodeGen/SystemZ/llxa-04.ll
+++ b/llvm/test/CodeGen/SystemZ/llxa-04.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD LOGICAL INDEXED ADDRESS doubleword instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LLXAG with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/llxa-05.ll b/llvm/test/CodeGen/SystemZ/llxa-05.ll
index eba400f6d256..1e5880de57d5 100644
--- a/llvm/test/CodeGen/SystemZ/llxa-05.ll
+++ b/llvm/test/CodeGen/SystemZ/llxa-05.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD LOGICAL INDEXED ADDRESS quadword instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LLXAQ with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/lxa-01.ll b/llvm/test/CodeGen/SystemZ/lxa-01.ll
index fb3edeaaeb38..8bba6f78f503 100644
--- a/llvm/test/CodeGen/SystemZ/lxa-01.ll
+++ b/llvm/test/CodeGen/SystemZ/lxa-01.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD INDEXED ADDRESS byte instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; DO NOT USE: LXAB with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/lxa-02.ll b/llvm/test/CodeGen/SystemZ/lxa-02.ll
index 64816fa24838..c233bf7d28a5 100644
--- a/llvm/test/CodeGen/SystemZ/lxa-02.ll
+++ b/llvm/test/CodeGen/SystemZ/lxa-02.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD INDEXED ADDRESS halfword instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LXAH with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/lxa-03.ll b/llvm/test/CodeGen/SystemZ/lxa-03.ll
index e73d43a48ebd..43e9b4d14d6c 100644
--- a/llvm/test/CodeGen/SystemZ/lxa-03.ll
+++ b/llvm/test/CodeGen/SystemZ/lxa-03.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD INDEXED ADDRESS word instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LXAF with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/lxa-04.ll b/llvm/test/CodeGen/SystemZ/lxa-04.ll
index 7b6764cf22fa..96af585547e3 100644
--- a/llvm/test/CodeGen/SystemZ/lxa-04.ll
+++ b/llvm/test/CodeGen/SystemZ/lxa-04.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD INDEXED ADDRESS doubleword instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LXAG with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/lxa-05.ll b/llvm/test/CodeGen/SystemZ/lxa-05.ll
index 0a45cba0b3f8..4f0b4e838f15 100644
--- a/llvm/test/CodeGen/SystemZ/lxa-05.ll
+++ b/llvm/test/CodeGen/SystemZ/lxa-05.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of LOAD INDEXED ADDRESS quadword instructions.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; LXAQ with base and index.
 define dso_local ptr @f0(ptr %ptr, i32 %idx) {
diff --git a/llvm/test/CodeGen/SystemZ/scalar-ctlz-03.ll b/llvm/test/CodeGen/SystemZ/scalar-ctlz-03.ll
index f18ee2418383..3dbd18fb8cc6 100644
--- a/llvm/test/CodeGen/SystemZ/scalar-ctlz-03.ll
+++ b/llvm/test/CodeGen/SystemZ/scalar-ctlz-03.ll
@@ -1,5 +1,5 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 ;
 ; FIXME: two consecutive immediate adds not fused in i16/i8 functions.
 
diff --git a/llvm/test/CodeGen/SystemZ/scalar-ctlz-04.ll b/llvm/test/CodeGen/SystemZ/scalar-ctlz-04.ll
index bb50e6f417c4..10d28d571bb9 100644
--- a/llvm/test/CodeGen/SystemZ/scalar-ctlz-04.ll
+++ b/llvm/test/CodeGen/SystemZ/scalar-ctlz-04.ll
@@ -1,5 +1,5 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 declare i128 @llvm.ctlz.i128(i128, i1)
 
diff --git a/llvm/test/CodeGen/SystemZ/scalar-cttz-03.ll b/llvm/test/CodeGen/SystemZ/scalar-cttz-03.ll
index 2f3a72160ae2..e1237280ae23 100644
--- a/llvm/test/CodeGen/SystemZ/scalar-cttz-03.ll
+++ b/llvm/test/CodeGen/SystemZ/scalar-cttz-03.ll
@@ -1,5 +1,5 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 declare i64 @llvm.cttz.i64(i64, i1)
 declare i32 @llvm.cttz.i32(i32, i1)
diff --git a/llvm/test/CodeGen/SystemZ/scalar-cttz-04.ll b/llvm/test/CodeGen/SystemZ/scalar-cttz-04.ll
index f440871fd4ff..fdfebef1a1e1 100644
--- a/llvm/test/CodeGen/SystemZ/scalar-cttz-04.ll
+++ b/llvm/test/CodeGen/SystemZ/scalar-cttz-04.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test 128-bit arithmetic in vector registers on arch15
+; Test 128-bit arithmetic in vector registers on z17
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 declare i128 @llvm.cttz.i128(i128, i1)
 
diff --git a/llvm/test/CodeGen/SystemZ/vec-cmp-09.ll b/llvm/test/CodeGen/SystemZ/vec-cmp-09.ll
index 3f6c86e685ea..cb8850e58c58 100644
--- a/llvm/test/CodeGen/SystemZ/vec-cmp-09.ll
+++ b/llvm/test/CodeGen/SystemZ/vec-cmp-09.ll
@@ -1,6 +1,6 @@
-; Test usage of VBLEND on arch15.
+; Test usage of VBLEND on z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 define <16 x i8> @f1(<16 x i8> %val1, <16 x i8> %val2, <16 x i8> %val3) {
 ; CHECK-LABEL: f1:
diff --git a/llvm/test/CodeGen/SystemZ/vec-div-03.ll b/llvm/test/CodeGen/SystemZ/vec-div-03.ll
index 96b161948e39..1c2a702baf1a 100644
--- a/llvm/test/CodeGen/SystemZ/vec-div-03.ll
+++ b/llvm/test/CodeGen/SystemZ/vec-div-03.ll
@@ -1,6 +1,6 @@
-; Test vector division on arch15.
+; Test vector division on z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Test a v4i32 signed division.
 define <4 x i32> @f1(<4 x i32> %dummy, <4 x i32> %val1, <4 x i32> %val2) {
diff --git a/llvm/test/CodeGen/SystemZ/vec-eval.ll b/llvm/test/CodeGen/SystemZ/vec-eval.ll
index 262ab0ea8bb2..bcdedcd3a407 100644
--- a/llvm/test/CodeGen/SystemZ/vec-eval.ll
+++ b/llvm/test/CodeGen/SystemZ/vec-eval.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
 ; Test use of VECTOR EVALUATE for combined boolean operations.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 define <16 x i8> @eval0(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval0:
@@ -279,8 +279,8 @@ entry:
 define <16 x i8> @eval24(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval24:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v26, %v28, %v24, 2
+; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v0, %v24, %v1, 47
 ; CHECK-NEXT:    br %r14
 entry:
@@ -376,8 +376,8 @@ entry:
 define <16 x i8> @eval30(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval30:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v26, %v28, %v24, 2
+; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v0, %v24, %v1, 47
 ; CHECK-NEXT:    br %r14
 entry:
@@ -596,8 +596,8 @@ entry:
 define <16 x i8> @eval45(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval45:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vo %v0, %v28, %v24
 ; CHECK-NEXT:    veval %v1, %v26, %v24, %v28, 1
+; CHECK-NEXT:    vo %v0, %v28, %v24
 ; CHECK-NEXT:    veval %v1, %v1, %v24, %v26, 47
 ; CHECK-NEXT:    veval %v24, %v1, %v26, %v0, 47
 ; CHECK-NEXT:    br %r14
@@ -617,8 +617,8 @@ entry:
 define <16 x i8> @eval46(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval46:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v26, %v28, %v24, 8
+; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v0, %v24, %v1, 47
 ; CHECK-NEXT:    br %r14
 entry:
@@ -722,8 +722,8 @@ entry:
 define <16 x i8> @eval54(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval54:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vn %v1, %v28, %v24
 ; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 2
+; CHECK-NEXT:    vn %v1, %v28, %v24
 ; CHECK-NEXT:    veval %v24, %v0, %v26, %v1, 47
 ; CHECK-NEXT:    br %r14
 entry:
@@ -770,8 +770,8 @@ entry:
 define <16 x i8> @eval57(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval57:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vo %v0, %v28, %v26
 ; CHECK-NEXT:    veval %v1, %v26, %v24, %v28, 1
+; CHECK-NEXT:    vo %v0, %v28, %v26
 ; CHECK-NEXT:    veval %v1, %v1, %v26, %v24, 47
 ; CHECK-NEXT:    veval %v24, %v1, %v24, %v0, 47
 ; CHECK-NEXT:    br %r14
@@ -1060,8 +1060,8 @@ define <16 x i8> @eval77(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval77:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    vgbm %v0, 65535
-; CHECK-NEXT:    vn %v1, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v24, %v0, %v26, 40
+; CHECK-NEXT:    vn %v1, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v28, %v0, %v1, 7
 ; CHECK-NEXT:    veval %v24, %v0, %v24, %v26, 47
 ; CHECK-NEXT:    br %r14
@@ -1540,10 +1540,10 @@ define <16 x i8> @eval109(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval109:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    vgbm %v0, 65535
-; CHECK-NEXT:    vn %v2, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v24, %v0, %v26, 40
-; CHECK-NEXT:    vo %v1, %v28, %v24
+; CHECK-NEXT:    vn %v2, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v28, %v0, %v2, 7
+; CHECK-NEXT:    vo %v1, %v28, %v24
 ; CHECK-NEXT:    veval %v0, %v0, %v24, %v26, 47
 ; CHECK-NEXT:    veval %v24, %v0, %v26, %v1, 47
 ; CHECK-NEXT:    br %r14
@@ -1621,8 +1621,8 @@ define <16 x i8> @eval113(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval113:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    vgbm %v0, 65535
-; CHECK-NEXT:    vn %v1, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v24, %v0, %v26, 40
+; CHECK-NEXT:    vn %v1, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v28, %v0, %v1, 7
 ; CHECK-NEXT:    veval %v24, %v0, %v26, %v24, 47
 ; CHECK-NEXT:    br %r14
@@ -1731,8 +1731,8 @@ define <16 x i8> @eval120(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    vno %v0, %v24, %v24
 ; CHECK-NEXT:    veval %v0, %v0, %v28, %v26, 2
-; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v0, %v26, %v24, 47
+; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v0, %v24, %v1, 47
 ; CHECK-NEXT:    br %r14
 entry:
@@ -1753,10 +1753,10 @@ define <16 x i8> @eval121(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval121:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    vgbm %v0, 65535
-; CHECK-NEXT:    vn %v2, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v24, %v0, %v26, 40
-; CHECK-NEXT:    vo %v1, %v28, %v26
+; CHECK-NEXT:    vn %v2, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v28, %v0, %v2, 7
+; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v0, %v26, %v24, 47
 ; CHECK-NEXT:    veval %v24, %v0, %v24, %v1, 47
 ; CHECK-NEXT:    br %r14
@@ -1802,8 +1802,8 @@ define <16 x i8> @eval123(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    vno %v0, %v24, %v24
 ; CHECK-NEXT:    veval %v0, %v0, %v28, %v26, 2
-; CHECK-NEXT:    voc %v1, %v26, %v28
 ; CHECK-NEXT:    veval %v0, %v0, %v26, %v24, 47
+; CHECK-NEXT:    voc %v1, %v26, %v28
 ; CHECK-NEXT:    veval %v24, %v0, %v1, %v24, 31
 ; CHECK-NEXT:    br %r14
 entry:
@@ -2084,8 +2084,8 @@ entry:
 define <16 x i8> @eval141(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval141:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vo %v0, %v26, %v24
 ; CHECK-NEXT:    veval %v1, %v26, %v24, %v28, 1
+; CHECK-NEXT:    vo %v0, %v26, %v24
 ; CHECK-NEXT:    veval %v1, %v1, %v24, %v26, 47
 ; CHECK-NEXT:    veval %v24, %v1, %v0, %v28, 143
 ; CHECK-NEXT:    br %r14
@@ -2105,8 +2105,8 @@ entry:
 define <16 x i8> @eval142(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval142:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v26, %v24, %v28, 127
+; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v24, %v1, %v0, 174
 ; CHECK-NEXT:    br %r14
 entry:
@@ -2253,8 +2253,8 @@ entry:
 define <16 x i8> @eval151(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval151:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vx %v0, %v28, %v26
 ; CHECK-NEXT:    veval %v1, %v24, %v28, %v26, 2
+; CHECK-NEXT:    vx %v0, %v28, %v26
 ; CHECK-NEXT:    veval %v1, %v1, %v26, %v24, 31
 ; CHECK-NEXT:    veval %v24, %v1, %v0, %v24, 143
 ; CHECK-NEXT:    br %r14
@@ -2289,8 +2289,8 @@ entry:
 define <16 x i8> @eval153(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval153:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 111
+; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v2, %v26, %v24, %v28, 1
 ; CHECK-NEXT:    veval %v24, %v2, %v0, %v1, 239
 ; CHECK-NEXT:    br %r14
@@ -2309,8 +2309,8 @@ entry:
 define <16 x i8> @eval154(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval154:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 111
+; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v2, %v24, %v26, %v28, 2
 ; CHECK-NEXT:    veval %v24, %v2, %v0, %v1, 239
 ; CHECK-NEXT:    br %r14
@@ -2330,9 +2330,9 @@ entry:
 define <16 x i8> @eval155(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval155:
 ; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 111
 ; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    vn %v2, %v26, %v24
-; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 111
 ; CHECK-NEXT:    veval %v24, %v2, %v0, %v1, 239
 ; CHECK-NEXT:    br %r14
 entry:
@@ -2365,8 +2365,8 @@ entry:
 define <16 x i8> @eval157(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval157:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vx %v0, %v28, %v26
 ; CHECK-NEXT:    veval %v1, %v26, %v24, %v28, 1
+; CHECK-NEXT:    vx %v0, %v28, %v26
 ; CHECK-NEXT:    veval %v1, %v1, %v24, %v26, 47
 ; CHECK-NEXT:    veval %v24, %v1, %v0, %v24, 143
 ; CHECK-NEXT:    br %r14
@@ -2386,8 +2386,8 @@ entry:
 define <16 x i8> @eval158(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval158:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 111
+; CHECK-NEXT:    vn %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v24, %v1, %v0, 174
 ; CHECK-NEXT:    br %r14
 entry:
@@ -2685,8 +2685,8 @@ entry:
 define <16 x i8> @eval178(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval178:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vn %v1, %v26, %v24
 ; CHECK-NEXT:    veval %v0, %v26, %v28, %v24, 138
+; CHECK-NEXT:    vn %v1, %v26, %v24
 ; CHECK-NEXT:    veval %v24, %v0, %v1, %v28, 47
 ; CHECK-NEXT:    br %r14
 entry:
@@ -2778,8 +2778,8 @@ entry:
 define <16 x i8> @eval183(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval183:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    voc %v0, %v26, %v28
 ; CHECK-NEXT:    veval %v1, %v24, %v28, %v26, 2
+; CHECK-NEXT:    voc %v0, %v26, %v28
 ; CHECK-NEXT:    veval %v1, %v1, %v26, %v24, 31
 ; CHECK-NEXT:    veval %v24, %v1, %v0, %v24, 47
 ; CHECK-NEXT:    br %r14
@@ -2884,8 +2884,8 @@ entry:
 define <16 x i8> @eval189(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval189:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    voc %v0, %v26, %v28
 ; CHECK-NEXT:    veval %v1, %v26, %v24, %v28, 1
+; CHECK-NEXT:    voc %v0, %v26, %v28
 ; CHECK-NEXT:    veval %v1, %v1, %v24, %v26, 47
 ; CHECK-NEXT:    veval %v24, %v1, %v0, %v24, 47
 ; CHECK-NEXT:    br %r14
@@ -3480,8 +3480,8 @@ define <16 x i8> @eval228(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval228:
 ; CHECK:       # %bb.0: # %entry
 ; CHECK-NEXT:    vno %v0, %v26, %v26
-; CHECK-NEXT:    vo %v1, %v28, %v24
 ; CHECK-NEXT:    veval %v2, %v24, %v28, %v26, 2
+; CHECK-NEXT:    vo %v1, %v28, %v24
 ; CHECK-NEXT:    veval %v0, %v2, %v0, %v24, 47
 ; CHECK-NEXT:    veval %v24, %v0, %v26, %v1, 47
 ; CHECK-NEXT:    br %r14
@@ -3564,8 +3564,8 @@ entry:
 define <16 x i8> @eval232(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval232:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 31
+; CHECK-NEXT:    vo %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v24, %v1, %v0, 174
 ; CHECK-NEXT:    br %r14
 entry:
@@ -3582,8 +3582,8 @@ entry:
 define <16 x i8> @eval233(<16 x i8> %src1, <16 x i8> %src2, <16 x i8> %src3) {
 ; CHECK-LABEL: eval233:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    vx %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v0, %v24, %v28, %v26, 31
+; CHECK-NEXT:    vx %v1, %v28, %v26
 ; CHECK-NEXT:    veval %v24, %v24, %v1, %v0, 174
 ; CHECK-NEXT:    br %r14
 entry:
diff --git a/llvm/test/CodeGen/SystemZ/vec-intrinsics-05.ll b/llvm/test/CodeGen/SystemZ/vec-intrinsics-05.ll
index e750f1e3e7b4..5bbabdd2d56f 100644
--- a/llvm/test/CodeGen/SystemZ/vec-intrinsics-05.ll
+++ b/llvm/test/CodeGen/SystemZ/vec-intrinsics-05.ll
@@ -1,7 +1,7 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
-; Test vector intrinsics added with arch15.
+; Test vector intrinsics added with z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 declare <16 x i8> @llvm.s390.vgemb(<8 x i16>)
 declare <8 x i16> @llvm.s390.vgemh(<16 x i8>)
diff --git a/llvm/test/CodeGen/SystemZ/vec-mul-06.ll b/llvm/test/CodeGen/SystemZ/vec-mul-06.ll
index 22b1b5de62c5..3850a8f60eb1 100644
--- a/llvm/test/CodeGen/SystemZ/vec-mul-06.ll
+++ b/llvm/test/CodeGen/SystemZ/vec-mul-06.ll
@@ -1,6 +1,6 @@
-; Test vector multiplication on arch15.
+; Test vector multiplication on z17.
 ;
-; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=arch15 | FileCheck %s
+; RUN: llc < %s -mtriple=s390x-linux-gnu -mcpu=z17 | FileCheck %s
 
 ; Test a v2i64 multiplication.
 define <2 x i64> @f1(<2 x i64> %dummy, <2 x i64> %val1, <2 x i64> %val2) {
diff --git a/llvm/test/CodeGen/Thumb2/constant-islands-no-split.mir b/llvm/test/CodeGen/Thumb2/constant-islands-no-split.mir
deleted file mode 100644
index 9283ef14ca6c..000000000000
--- a/llvm/test/CodeGen/Thumb2/constant-islands-no-split.mir
+++ /dev/null
@@ -1,165 +0,0 @@
-# RUN: llc -mtriple=thumbv7-linux-gnueabihf -run-pass=arm-cp-islands -arm-constant-island-max-iteration=1 %s -o - | FileCheck %s
---- |
-  ; ModuleID = 'constant-islands-new-island.ll'
-  source_filename = "constant-islands-new-island.ll"
-  target datalayout = "e-m:e-p:32:32-Fi8-i64:64-v128:64:128-a:0:32-n32-S64"
-  target triple = "thumbv7-unknown-linux-gnueabihf"
-  
-  define void @test(i1 %tst) {
-  entry:
-    %0 = call i32 @llvm.arm.space(i32 2000, i32 undef)
-    br label %smallbb
-  
-  smallbb:                                          ; preds = %entry
-    br i1 %tst, label %true, label %false
-  
-  true:                                             ; preds = %false, %smallbb
-    %val = phi float [ 1.234500e+04, %smallbb ], [ undef, %false ]
-    %1 = call i32 @llvm.arm.space(i32 2000, i32 undef)
-    call void @bar(float %val)
-    ret void
-  
-  false:                                            ; preds = %smallbb
-    br label %true
-  }
-  
-  declare void @bar(float)
-  
-  ; Function Attrs: nounwind
-  declare i32 @llvm.arm.space(i32 immarg, i32) #0
-  
-  attributes #0 = { nounwind }
-
-...
----
-name:            test
-alignment:       2
-exposesReturnsTwice: false
-legalized:       false
-regBankSelected: false
-selected:        false
-failedISel:      false
-tracksRegLiveness: true
-hasWinCFI:       false
-noPhis:          true
-isSSA:           false
-noVRegs:         true
-hasFakeUses:     false
-callsEHReturn:   false
-callsUnwindInit: false
-hasEHCatchret:   false
-hasEHScopes:     false
-hasEHFunclets:   false
-isOutlined:      false
-debugInstrRef:   false
-failsVerification: false
-tracksDebugUserValues: false
-registers:       []
-liveins:
-  - { reg: '$r0', virtual-reg: '' }
-frameInfo:
-  isFrameAddressTaken: false
-  isReturnAddressTaken: false
-  hasStackMap:     false
-  hasPatchPoint:   false
-  stackSize:       16
-  offsetAdjustment: 0
-  maxAlignment:    4
-  adjustsStack:    true
-  hasCalls:        true
-  stackProtector:  ''
-  functionContext: ''
-  maxCallFrameSize: 0
-  cvBytesOfCalleeSavedRegisters: 0
-  hasOpaqueSPAdjustment: false
-  hasVAStart:      false
-  hasMustTailInVarArgFunc: false
-  hasTailCall:     false
-  isCalleeSavedInfoValid: true
-  localFrameSize:  0
-  savePoint:       ''
-  restorePoint:    ''
-fixedStack:      []
-stack:
-  - { id: 0, name: '', type: spill-slot, offset: -12, size: 4, alignment: 4, 
-      stack-id: default, callee-saved-register: '', callee-saved-restored: true, 
-      debug-info-variable: '', debug-info-expression: '', debug-info-location: '' }
-  - { id: 1, name: '', type: spill-slot, offset: -16, size: 4, alignment: 4, 
-      stack-id: default, callee-saved-register: '', callee-saved-restored: true, 
-      debug-info-variable: '', debug-info-expression: '', debug-info-location: '' }
-  - { id: 2, name: '', type: spill-slot, offset: -4, size: 4, alignment: 4, 
-      stack-id: default, callee-saved-register: '$lr', callee-saved-restored: false, 
-      debug-info-variable: '', debug-info-expression: '', debug-info-location: '' }
-  - { id: 3, name: '', type: spill-slot, offset: -8, size: 4, alignment: 4, 
-      stack-id: default, callee-saved-register: '$r7', callee-saved-restored: true, 
-      debug-info-variable: '', debug-info-expression: '', debug-info-location: '' }
-entry_values:    []
-callSites:       []
-debugValueSubstitutions: []
-constants:
-  - id:              0
-    value:           'float 1.234500e+04'
-    alignment:       4
-    isTargetSpecific: false
-machineFunctionInfo:
-  isLRSpilled:     true
-body:             |
-  bb.0.entry:
-    successors: %bb.1(0x80000000)
-    liveins: $r0, $r7, $lr
-  
-    frame-setup tPUSH 14 /* CC::al */, $noreg, killed $r7, killed $lr, implicit-def $sp, implicit $sp
-    frame-setup CFI_INSTRUCTION def_cfa_offset 8
-    frame-setup CFI_INSTRUCTION offset $lr, -4
-    frame-setup CFI_INSTRUCTION offset $r7, -8
-    $sp = frame-setup tSUBspi $sp, 2, 14 /* CC::al */, $noreg
-    frame-setup CFI_INSTRUCTION def_cfa_offset 16
-    tSTRspi killed $r0, $sp, 1, 14 /* CC::al */, $noreg :: (store (s32) into %stack.0)
-    renamable $r0 = IMPLICIT_DEF
-    dead renamable $r0 = SPACE 2000, killed renamable $r0
-    t2B %bb.1, 14 /* CC::al */, $noreg
-  
-  bb.1.smallbb:
-    successors: %bb.2(0x40000000), %bb.3(0x40000000)
-  
-    $r0 = tLDRspi $sp, 1, 14 /* CC::al */, $noreg :: (load (s32) from %stack.0)
-    renamable $s0 = VLDRS %const.0, 0, 14 /* CC::al */, $noreg :: (load (s32) from constant-pool)
-    renamable $r0, dead $cpsr = tLSLri renamable $r0, 31, 14 /* CC::al */, $noreg
-    tCMPi8 killed renamable $r0, 0, 14 /* CC::al */, $noreg, implicit-def $cpsr
-    VSTRS killed $s0, $sp, 0, 14 /* CC::al */, $noreg :: (store (s32) into %stack.1)
-    t2Bcc %bb.3, 0 /* CC::eq */, killed $cpsr
-    t2B %bb.2, 14 /* CC::al */, $noreg
-  
-  bb.2.true:
-    $s0 = VLDRS $sp, 0, 14 /* CC::al */, $noreg :: (load (s32) from %stack.1)
-    renamable $r0 = IMPLICIT_DEF
-    dead renamable $r0 = SPACE 2000, killed renamable $r0
-    tBL 14 /* CC::al */, $noreg, @bar, csr_aapcs, implicit-def dead $lr, implicit $sp, implicit killed $s0, implicit-def $sp
-    $sp = frame-destroy tADDspi $sp, 2, 14 /* CC::al */, $noreg
-    frame-destroy tPOP_RET 14 /* CC::al */, $noreg, def $r7, def $pc
-  
-  bb.3.false:
-    successors: %bb.2(0x80000000)
-  
-    renamable $s0 = IMPLICIT_DEF
-    t2B %bb.2, 14 /* CC::al */, $noreg
-
-...
-# Check that smallbb is not split by the constant islands pass.  Previously,
-# smallbb was split due to incorrect calculation of MinNoSplitDisp.
-#
-# CHECK:       bb.1.smallbb:
-# CHECK-NEXT:    successors: %bb.3(0x40000000), %bb.4(0x40000000)
-# CHECK-NEXT:  {{^  $}}
-# CHECK-NEXT:    $r0 = tLDRspi $sp, 1, 14 /* CC::al */, $noreg :: (load (s32) from %stack.0)
-# CHECK-NEXT:    renamable $s0 = VLDRS %const.1, 0, 14 /* CC::al */, $noreg :: (load (s32) from constant-pool)
-# CHECK-NEXT:    renamable $r0, dead $cpsr = tLSLri renamable $r0, 31, 14 /* CC::al */, $noreg
-# CHECK-NEXT:    tCMPi8 killed renamable $r0, 0, 14 /* CC::al */, $noreg, implicit-def $cpsr
-# CHECK-NEXT:    VSTRS killed $s0, $sp, 0, 14 /* CC::al */, $noreg :: (store (s32) into %stack.1)
-# CHECK-NEXT:    t2Bcc %bb.4, 0 /* CC::eq */, killed $cpsr
-# CHECK-NEXT:    tB %bb.3, 14 /* CC::al */, $noreg
-# CHECK-NEXT:  {{^  $}}
-# CHECK-NEXT:  bb.2 (align 4):
-# CHECK-NEXT:    successors:
-# CHECK-NEXT:  {{^  $}}
-# CHECK-NEXT:    CONSTPOOL_ENTRY 1, %const.0, 4
diff --git a/llvm/test/CodeGen/X86/avx10_2_512bf16-intrinsics.ll b/llvm/test/CodeGen/X86/avx10_2_512bf16-intrinsics.ll
index da17b995afed..cbac76e9de27 100644
--- a/llvm/test/CodeGen/X86/avx10_2_512bf16-intrinsics.ll
+++ b/llvm/test/CodeGen/X86/avx10_2_512bf16-intrinsics.ll
@@ -164,7 +164,7 @@ define <32 x bfloat>@test_int_x86_avx512_mask_getexp_bf16_512(<32 x bfloat> %x0,
 ; X64-LABEL: test_int_x86_avx512_mask_getexp_bf16_512:
 ; X64:       # %bb.0:
 ; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
-; X64-NEXT:    vgetexpbf16 %zmm0, %zmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x42,0xc0]
+; X64-NEXT:    vgetexpbf16 %zmm0, %zmm0 # encoding: [0x62,0xf6,0x7c,0x48,0x42,0xc0]
 ; X64-NEXT:    vmovdqu16 %zmm0, %zmm1 {%k1} # encoding: [0x62,0xf1,0xff,0x49,0x6f,0xc8]
 ; X64-NEXT:    vaddbf16 %zmm0, %zmm1, %zmm0 # encoding: [0x62,0xf5,0x75,0x48,0x58,0xc0]
 ; X64-NEXT:    retq # encoding: [0xc3]
@@ -172,7 +172,7 @@ define <32 x bfloat>@test_int_x86_avx512_mask_getexp_bf16_512(<32 x bfloat> %x0,
 ; X86-LABEL: test_int_x86_avx512_mask_getexp_bf16_512:
 ; X86:       # %bb.0:
 ; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
-; X86-NEXT:    vgetexpbf16 %zmm0, %zmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x42,0xc0]
+; X86-NEXT:    vgetexpbf16 %zmm0, %zmm0 # encoding: [0x62,0xf6,0x7c,0x48,0x42,0xc0]
 ; X86-NEXT:    vmovdqu16 %zmm0, %zmm1 {%k1} # encoding: [0x62,0xf1,0xff,0x49,0x6f,0xc8]
 ; X86-NEXT:    vaddbf16 %zmm0, %zmm1, %zmm0 # encoding: [0x62,0xf5,0x75,0x48,0x58,0xc0]
 ; X86-NEXT:    retl # encoding: [0xc3]
diff --git a/llvm/test/CodeGen/X86/avx10_2_512ni-intrinsics.ll b/llvm/test/CodeGen/X86/avx10_2_512ni-intrinsics.ll
index 07e86cb01e13..b2e7caa15944 100644
--- a/llvm/test/CodeGen/X86/avx10_2_512ni-intrinsics.ll
+++ b/llvm/test/CodeGen/X86/avx10_2_512ni-intrinsics.ll
@@ -422,3 +422,14 @@ define { <32 x i16>, <32 x i16>, <32 x i16> } @test_mm512_mask_mpsadbw(<64 x i8>
 }
 
 declare <32 x i16> @llvm.x86.avx10.vmpsadbw.512(<64 x i8>, <64 x i8>, i8)
+
+; Regression test
+
+define <8 x float> @avx_dp_ps(<8 x float> %a, <8 x float> %b) {
+; CHECK-LABEL: avx_dp_ps:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    vdpps $255, %ymm1, %ymm0, %ymm0 # encoding: [0xc4,0xe3,0x7d,0x40,0xc1,0xff]
+; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
+  %r = tail call <8 x float> @llvm.x86.avx.dp.ps.256(<8 x float> %a, <8 x float> %b, i8 -1)
+  ret <8 x float> %r
+}
diff --git a/llvm/test/CodeGen/X86/avx10_2bf16-intrinsics.ll b/llvm/test/CodeGen/X86/avx10_2bf16-intrinsics.ll
index 06875dbe7cd2..ba32b2adc799 100644
--- a/llvm/test/CodeGen/X86/avx10_2bf16-intrinsics.ll
+++ b/llvm/test/CodeGen/X86/avx10_2bf16-intrinsics.ll
@@ -333,7 +333,7 @@ declare <16 x bfloat> @llvm.x86.avx10.mask.getexp.bf16.256(<16 x bfloat>, <16 x
 define <8 x bfloat>@test_int_x86_avx512_getexp_bf16_128(<8 x bfloat> %x0) {
 ; CHECK-LABEL: test_int_x86_avx512_getexp_bf16_128:
 ; CHECK:       # %bb.0:
-; CHECK-NEXT:    vgetexpbf16 %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x42,0xc0]
+; CHECK-NEXT:    vgetexpbf16 %xmm0, %xmm0 # encoding: [0x62,0xf6,0x7c,0x08,0x42,0xc0]
 ; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
   %res = call <8 x bfloat> @llvm.x86.avx10.mask.getexp.bf16.128(<8 x bfloat> %x0, <8 x bfloat> zeroinitializer, i8 -1)
   ret <8 x bfloat> %res
@@ -343,14 +343,14 @@ define <8 x bfloat>@test_int_x86_avx512_mask_getexp_bf16_128(<8 x bfloat> %x0, <
 ; X64-LABEL: test_int_x86_avx512_mask_getexp_bf16_128:
 ; X64:       # %bb.0:
 ; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
-; X64-NEXT:    vgetexpbf16 %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x42,0xc8]
+; X64-NEXT:    vgetexpbf16 %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf6,0x7c,0x09,0x42,0xc8]
 ; X64-NEXT:    vmovaps %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc1]
 ; X64-NEXT:    retq # encoding: [0xc3]
 ;
 ; X86-LABEL: test_int_x86_avx512_mask_getexp_bf16_128:
 ; X86:       # %bb.0:
 ; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
-; X86-NEXT:    vgetexpbf16 %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x42,0xc8]
+; X86-NEXT:    vgetexpbf16 %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf6,0x7c,0x09,0x42,0xc8]
 ; X86-NEXT:    vmovaps %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc1]
 ; X86-NEXT:    retl # encoding: [0xc3]
   %res = call <8 x bfloat> @llvm.x86.avx10.mask.getexp.bf16.128(<8 x bfloat> %x0, <8 x bfloat> %x1, i8 %x2)
@@ -361,13 +361,13 @@ define <8 x bfloat>@test_int_x86_avx512_maskz_getexp_bf16_128(<8 x bfloat> %x0,
 ; X64-LABEL: test_int_x86_avx512_maskz_getexp_bf16_128:
 ; X64:       # %bb.0:
 ; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
-; X64-NEXT:    vgetexpbf16 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7d,0x89,0x42,0xc0]
+; X64-NEXT:    vgetexpbf16 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf6,0x7c,0x89,0x42,0xc0]
 ; X64-NEXT:    retq # encoding: [0xc3]
 ;
 ; X86-LABEL: test_int_x86_avx512_maskz_getexp_bf16_128:
 ; X86:       # %bb.0:
 ; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
-; X86-NEXT:    vgetexpbf16 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7d,0x89,0x42,0xc0]
+; X86-NEXT:    vgetexpbf16 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf6,0x7c,0x89,0x42,0xc0]
 ; X86-NEXT:    retl # encoding: [0xc3]
   %res = call <8 x bfloat> @llvm.x86.avx10.mask.getexp.bf16.128(<8 x bfloat> %x0, <8 x bfloat> zeroinitializer, i8 %x2)
   ret <8 x bfloat> %res
@@ -376,7 +376,7 @@ define <8 x bfloat>@test_int_x86_avx512_maskz_getexp_bf16_128(<8 x bfloat> %x0,
 define <16 x bfloat>@test_int_x86_avx512_getexp_bf16_256(<16 x bfloat> %x0) {
 ; CHECK-LABEL: test_int_x86_avx512_getexp_bf16_256:
 ; CHECK:       # %bb.0:
-; CHECK-NEXT:    vgetexpbf16 %ymm0, %ymm0 # encoding: [0x62,0xf5,0x7d,0x28,0x42,0xc0]
+; CHECK-NEXT:    vgetexpbf16 %ymm0, %ymm0 # encoding: [0x62,0xf6,0x7c,0x28,0x42,0xc0]
 ; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
   %res = call <16 x bfloat> @llvm.x86.avx10.mask.getexp.bf16.256(<16 x bfloat> %x0, <16 x bfloat> zeroinitializer, i16 -1)
   ret <16 x bfloat> %res
@@ -386,14 +386,14 @@ define <16 x bfloat>@test_int_x86_avx512_mask_getexp_bf16_256(<16 x bfloat> %x0,
 ; X64-LABEL: test_int_x86_avx512_mask_getexp_bf16_256:
 ; X64:       # %bb.0:
 ; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
-; X64-NEXT:    vgetexpbf16 %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x42,0xc8]
+; X64-NEXT:    vgetexpbf16 %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf6,0x7c,0x29,0x42,0xc8]
 ; X64-NEXT:    vmovaps %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc1]
 ; X64-NEXT:    retq # encoding: [0xc3]
 ;
 ; X86-LABEL: test_int_x86_avx512_mask_getexp_bf16_256:
 ; X86:       # %bb.0:
 ; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
-; X86-NEXT:    vgetexpbf16 %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x42,0xc8]
+; X86-NEXT:    vgetexpbf16 %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf6,0x7c,0x29,0x42,0xc8]
 ; X86-NEXT:    vmovaps %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc1]
 ; X86-NEXT:    retl # encoding: [0xc3]
   %res = call <16 x bfloat> @llvm.x86.avx10.mask.getexp.bf16.256(<16 x bfloat> %x0, <16 x bfloat> %x1, i16 %x2)
@@ -404,13 +404,13 @@ define <16 x bfloat>@test_int_x86_avx512_maskz_getexp_bf16_256(<16 x bfloat> %x0
 ; X64-LABEL: test_int_x86_avx512_maskz_getexp_bf16_256:
 ; X64:       # %bb.0:
 ; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
-; X64-NEXT:    vgetexpbf16 %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf5,0x7d,0xa9,0x42,0xc0]
+; X64-NEXT:    vgetexpbf16 %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf6,0x7c,0xa9,0x42,0xc0]
 ; X64-NEXT:    retq # encoding: [0xc3]
 ;
 ; X86-LABEL: test_int_x86_avx512_maskz_getexp_bf16_256:
 ; X86:       # %bb.0:
 ; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
-; X86-NEXT:    vgetexpbf16 %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf5,0x7d,0xa9,0x42,0xc0]
+; X86-NEXT:    vgetexpbf16 %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf6,0x7c,0xa9,0x42,0xc0]
 ; X86-NEXT:    retl # encoding: [0xc3]
   %res = call <16 x bfloat> @llvm.x86.avx10.mask.getexp.bf16.256(<16 x bfloat> %x0, <16 x bfloat> zeroinitializer, i16 %x2)
   ret <16 x bfloat> %res
diff --git a/llvm/test/CodeGen/X86/base-pointer-and-cmpxchg.ll b/llvm/test/CodeGen/X86/base-pointer-and-cmpxchg.ll
index 498be7c9e114..5e8da5818fe9 100644
--- a/llvm/test/CodeGen/X86/base-pointer-and-cmpxchg.ll
+++ b/llvm/test/CodeGen/X86/base-pointer-and-cmpxchg.ll
@@ -49,5 +49,39 @@ tail call void asm sideeffect "nop", "~{rax},~{rcx},~{rdx},~{rsi},~{rdi},~{rbp},
   store i32 %n, ptr %idx
   ret i1 %res
 }
+
+; If we compare-and-exchange a frame variable, we additionally need to rewrite
+; the memory operand to use the SAVE_rbx instead of rbx, which already contains
+; the input operand.
+;
+; CHECK-LABEL: cmp_and_swap16_frame:
+; Check that we actually use rbx.
+; gnux32 use the 32bit variant of the registers.
+; USE_BASE_64: movq %rsp, %rbx
+; USE_BASE_32: movl %esp, %ebx
+; Here we drop the inline assembly because the frame pointer is used anyway. So
+; rbx is not spilled to the stack but goes into a (hopefully numbered) register.
+; USE_BASE: movq %rbx, [[SAVE_rbx:%r[0-9]+]]
+;
+; USE_BASE: movq {{[^ ]+}}, %rbx
+; The use of the frame variable expands to N(%rbx) or N(%ebx). But we've just
+; overwritten that with the input operand. We need to use SAVE_rbx instead.
+; USE_BASE_64-NEXT: cmpxchg16b {{[0-9]*}}([[SAVE_rbx]])
+; USE_BASE_32-NEXT: cmpxchg16b {{[0-9]*}}([[SAVE_rbx]]d)
+; USE_BASE-NEXT: movq [[SAVE_rbx]], %rbx
+;
+; DONT_USE_BASE-NOT: movq %rsp, %rbx
+; DONT_USE_BASE-NOT: movl %esp, %ebx
+; DONT_USE_BASE: cmpxchg
+define i1 @cmp_and_swap16_frame(i128 %a, i128 %b, i32 %n) {
+  %local = alloca i128, align 16
+  %dummy = alloca i32, i32 %n
+  %cmp = cmpxchg ptr %local, i128 %a, i128 %b seq_cst seq_cst
+  %res = extractvalue { i128, i1 } %cmp, 1
+  %idx = getelementptr i32, ptr %dummy, i32 5
+  store i32 %n, ptr %idx
+  ret i1 %res
+}
+
 !llvm.module.flags = !{!0}
 !0 = !{i32 2, !"override-stack-alignment", i32 32}
diff --git a/llvm/test/CodeGen/X86/pr134607.ll b/llvm/test/CodeGen/X86/pr134607.ll
new file mode 100644
index 000000000000..5e824c22e5a2
--- /dev/null
+++ b/llvm/test/CodeGen/X86/pr134607.ll
@@ -0,0 +1,20 @@
+; RUN: llc < %s -mtriple=i386-unknown-unknown -mattr=+sse -O3 | FileCheck %s --check-prefixes=X86
+; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=-sse2,+sse -O3 | FileCheck %s --check-prefixes=X64-SSE1
+; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+sse2,+sse -O3 | FileCheck %s --check-prefixes=X64-SSE2
+
+define void @store_v2f32_constant(ptr %v) {
+; X86-LABEL: store_v2f32_constant:
+; X86:       # %bb.0:
+; X86-NEXT:    movl 4(%esp), %eax
+; X86-NEXT:    movaps {{\.?LCPI[0-9]+_[0-9]+}}, %xmm0
+
+; X64-SSE1-LABEL: store_v2f32_constant:
+; X64-SSE1:       # %bb.0:
+; X64-SSE1-NEXT:    movaps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
+
+; X64-SSE2-LABEL: store_v2f32_constant:
+; X64-SSE2:       # %bb.0:
+; X64-SSE2-NEXT:    movsd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
+  store <2 x float> <float 2.560000e+02, float 5.120000e+02>, ptr %v, align 4
+  ret void
+}
diff --git a/llvm/test/CodeGen/X86/pr138982.ll b/llvm/test/CodeGen/X86/pr138982.ll
new file mode 100644
index 000000000000..32346d823a9f
--- /dev/null
+++ b/llvm/test/CodeGen/X86/pr138982.ll
@@ -0,0 +1,23 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
+; RUN: llc < %s -mtriple=x86_64 -mattr=+fma | FileCheck %s
+
+define <4 x float> @pr138982(<4 x float> %in_vec) {
+; CHECK-LABEL: pr138982:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    vxorps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm1
+; CHECK-NEXT:    vrcpps %xmm0, %xmm2
+; CHECK-NEXT:    vrcpps %xmm1, %xmm1
+; CHECK-NEXT:    vxorps %xmm3, %xmm3, %xmm3
+; CHECK-NEXT:    vcmpneqps %xmm0, %xmm3, %xmm0
+; CHECK-NEXT:    vbroadcastss {{.*#+}} xmm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
+; CHECK-NEXT:    vblendvps %xmm0, %xmm1, %xmm4, %xmm0
+; CHECK-NEXT:    vfnmadd231ps {{.*#+}} xmm0 = -(xmm3 * xmm2) + xmm0
+; CHECK-NEXT:    retq
+entry:
+  %fneg = fneg <4 x float> %in_vec
+  %rcp = tail call <4 x float> @llvm.x86.sse.rcp.ps(<4 x float> %fneg)
+  %cmp = fcmp une <4 x float> zeroinitializer, %in_vec
+  %sel = select <4 x i1> %cmp, <4 x float> %rcp, <4 x float> splat (float 1.000000e+00)
+  %fma = call nsz <4 x float> @llvm.fma.v4f32(<4 x float> %rcp, <4 x float> zeroinitializer, <4 x float> %sel)
+  ret <4 x float> %fma
+}
diff --git a/llvm/test/CodeGen/X86/tail-dup-computed-goto.mir b/llvm/test/CodeGen/X86/tail-dup-computed-goto.mir
new file mode 100644
index 000000000000..17de405928d3
--- /dev/null
+++ b/llvm/test/CodeGen/X86/tail-dup-computed-goto.mir
@@ -0,0 +1,366 @@
+# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
+# RUN: llc -mtriple=x86_64-unknown-linux-gnu -run-pass=early-tailduplication -tail-dup-pred-size=1 -tail-dup-succ-size=1 %s -o - | FileCheck %s
+# Check that only the computed goto is not be restrict by tail-dup-pred-size and tail-dup-succ-size.
+--- |
+  @computed_goto.dispatch = constant [5 x ptr] [ptr null, ptr blockaddress(@computed_goto, %bb1), ptr blockaddress(@computed_goto, %bb2), ptr blockaddress(@computed_goto, %bb3), ptr blockaddress(@computed_goto, %bb4)]
+  declare i64 @f0()
+  declare i64 @f1()
+  declare i64 @f2()
+  declare i64 @f3()
+  declare i64 @f4()
+  declare i64 @f5()
+  define void @computed_goto() {
+    start:
+      ret void
+    bb1:
+      ret void
+    bb2:
+      ret void
+    bb3:
+      ret void
+    bb4:
+      ret void
+  }
+  define void @jump_table() { ret void }
+  define void @jump_table_pic() { ret void }
+...
+---
+name:            computed_goto
+tracksRegLiveness: true
+body:             |
+  ; CHECK-LABEL: name: computed_goto
+  ; CHECK: bb.0:
+  ; CHECK-NEXT:   successors: %bb.1(0x20000000), %bb.2(0x20000000), %bb.3(0x20000000), %bb.4(0x20000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f0, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gr64_nosp = COPY $rax
+  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gr64_nosp = COPY [[COPY]]
+  ; CHECK-NEXT:   JMP64m $noreg, 8, [[COPY]], @computed_goto.dispatch, $noreg
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.1.bb1 (ir-block-address-taken %ir-block.bb1):
+  ; CHECK-NEXT:   successors: %bb.1(0x20000000), %bb.2(0x20000000), %bb.3(0x20000000), %bb.4(0x20000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f1, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gr64_nosp = COPY $rax
+  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gr64_nosp = COPY [[COPY2]]
+  ; CHECK-NEXT:   JMP64m $noreg, 8, [[COPY2]], @computed_goto.dispatch, $noreg
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.2.bb2 (ir-block-address-taken %ir-block.bb2):
+  ; CHECK-NEXT:   successors: %bb.1(0x20000000), %bb.2(0x20000000), %bb.3(0x20000000), %bb.4(0x20000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f2, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:gr64_nosp = COPY $rax
+  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:gr64_nosp = COPY [[COPY4]]
+  ; CHECK-NEXT:   JMP64m $noreg, 8, [[COPY4]], @computed_goto.dispatch, $noreg
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.3.bb3 (ir-block-address-taken %ir-block.bb3):
+  ; CHECK-NEXT:   successors: %bb.1(0x20000000), %bb.2(0x20000000), %bb.3(0x20000000), %bb.4(0x20000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f3, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:gr64_nosp = COPY $rax
+  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:gr64_nosp = COPY [[COPY6]]
+  ; CHECK-NEXT:   JMP64m $noreg, 8, [[COPY6]], @computed_goto.dispatch, $noreg
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.4.bb4 (ir-block-address-taken %ir-block.bb4):
+  ; CHECK-NEXT:   successors: %bb.1(0x20000000), %bb.2(0x20000000), %bb.3(0x20000000), %bb.4(0x20000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f4, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:gr64_nosp = COPY $rax
+  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:gr64_nosp = COPY [[COPY8]]
+  ; CHECK-NEXT:   JMP64m $noreg, 8, [[COPY8]], @computed_goto.dispatch, $noreg
+  bb.0:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f0, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %1:gr64 = COPY $rax
+    JMP_1 %bb.5
+
+  bb.1.bb1 (ir-block-address-taken %ir-block.bb1):
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f1, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %3:gr64 = COPY $rax
+    JMP_1 %bb.5
+
+  bb.2.bb2 (ir-block-address-taken %ir-block.bb2):
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f2, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %5:gr64 = COPY $rax
+    JMP_1 %bb.5
+
+  bb.3.bb3 (ir-block-address-taken %ir-block.bb3):
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f3, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %7:gr64 = COPY $rax
+    JMP_1 %bb.5
+
+  bb.4.bb4 (ir-block-address-taken %ir-block.bb4):
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f4, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %9:gr64 = COPY $rax
+
+  bb.5:
+    successors: %bb.1, %bb.2, %bb.3, %bb.4
+
+    %10:gr64_nosp = PHI %1, %bb.0, %9, %bb.4, %7, %bb.3, %5, %bb.2, %3, %bb.1
+    JMP64m $noreg, 8, %10, @computed_goto.dispatch, $noreg
+
+...
+---
+name:            jump_table
+tracksRegLiveness: true
+jumpTable:
+  kind:            block-address
+  entries:
+    - id:              0
+      blocks:          [ '%bb.3', '%bb.4', '%bb.5', '%bb.6', '%bb.7' ]
+body:             |
+  ; CHECK-LABEL: name: jump_table
+  ; CHECK: bb.0:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f0, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.1:
+  ; CHECK-NEXT:   successors: %bb.3(0x1999999a), %bb.4(0x1999999a), %bb.5(0x1999999a), %bb.6(0x1999999a), %bb.7(0x1999999a)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:gr64 = PHI [[COPY]], %bb.0, %2, %bb.7, %3, %bb.6, %4, %bb.5, %5, %bb.4, %6, %bb.3
+  ; CHECK-NEXT:   [[DEC64r:%[0-9]+]]:gr64_nosp = DEC64r [[PHI]], implicit-def dead $eflags
+  ; CHECK-NEXT:   JMP64m $noreg, 8, [[DEC64r]], %jump-table.0, $noreg :: (load (s64) from jump-table)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.3:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f1, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.4:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f2, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.5:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f3, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.6:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f4, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.7:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f5, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  bb.0:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f0, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %1:gr64 = COPY $rax
+
+  bb.1:
+    %2:gr64 = PHI %1, %bb.0, %3, %bb.7, %4, %bb.6, %5, %bb.5, %6, %bb.4, %7, %bb.3
+    %8:gr64_nosp = DEC64r %2, implicit-def dead $eflags
+
+  bb.2:
+    successors: %bb.3(0x1999999a), %bb.4(0x1999999a), %bb.5(0x1999999a), %bb.6(0x1999999a), %bb.7(0x1999999a)
+
+    JMP64m $noreg, 8, %8, %jump-table.0, $noreg :: (load (s64) from jump-table)
+
+  bb.3:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f1, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %7:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.4:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f2, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %6:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.5:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f3, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %5:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.6:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f4, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %4:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.7:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f5, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %3:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+...
+---
+name:            jump_table_pic
+tracksRegLiveness: true
+jumpTable:
+  kind:            block-address
+  entries:
+    - id:              0
+      blocks:          [ '%bb.3', '%bb.4', '%bb.5', '%bb.6', '%bb.7' ]
+body:             |
+  ; CHECK-LABEL: name: jump_table_pic
+  ; CHECK: bb.0:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f0, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.1:
+  ; CHECK-NEXT:   successors: %bb.3(0x1999999a), %bb.4(0x1999999a), %bb.5(0x1999999a), %bb.6(0x1999999a), %bb.7(0x1999999a)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:gr64 = PHI [[COPY]], %bb.0, %2, %bb.7, %3, %bb.6, %4, %bb.5, %5, %bb.4, %6, %bb.3
+  ; CHECK-NEXT:   [[DEC64r:%[0-9]+]]:gr64_nosp = DEC64r [[PHI]], implicit-def dead $eflags
+  ; CHECK-NEXT:   [[LEA64r:%[0-9]+]]:gr64 = LEA64r $rip, 1, $noreg, %jump-table.0, $noreg
+  ; CHECK-NEXT:   [[MOVSX64rm32_:%[0-9]+]]:gr64 = MOVSX64rm32 [[DEC64r]], 4, [[DEC64r]], 0, $noreg :: (load (s32) from jump-table)
+  ; CHECK-NEXT:   [[ADD64rr:%[0-9]+]]:gr64 = ADD64rr [[LEA64r]], [[MOVSX64rm32_]], implicit-def dead $eflags
+  ; CHECK-NEXT:   JMP64r [[ADD64rr]]
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.3:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f1, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.4:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f2, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.5:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f3, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.6:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f4, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT: bb.7:
+  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
+  ; CHECK-NEXT: {{  $}}
+  ; CHECK-NEXT:   ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   CALL64pcrel32 target-flags(x86-plt) @f5, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+  ; CHECK-NEXT:   ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:gr64 = COPY $rax
+  ; CHECK-NEXT:   JMP_1 %bb.1
+  bb.0:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f0, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %1:gr64 = COPY $rax
+
+  bb.1:
+    %2:gr64 = PHI %1, %bb.0, %3, %bb.7, %4, %bb.6, %5, %bb.5, %6, %bb.4, %7, %bb.3
+    %8:gr64_nosp = DEC64r %2, implicit-def dead $eflags
+
+  bb.2:
+    successors: %bb.3(0x1999999a), %bb.4(0x1999999a), %bb.5(0x1999999a), %bb.6(0x1999999a), %bb.7(0x1999999a)
+    %9:gr64 = LEA64r $rip, 1, $noreg, %jump-table.0, $noreg
+    %10:gr64 = MOVSX64rm32 %8, 4, %8, 0, $noreg :: (load (s32) from jump-table)
+    %11:gr64 = ADD64rr %9, %10, implicit-def dead $eflags
+    JMP64r %11
+
+  bb.3:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f1, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %7:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.4:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f2, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %6:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.5:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f3, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %5:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.6:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f4, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %4:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+  bb.7:
+    ADJCALLSTACKDOWN64 0, 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    CALL64pcrel32 target-flags(x86-plt) @f5, csr_64, implicit $rsp, implicit $ssp, implicit-def $rsp, implicit-def $ssp, implicit-def $rax
+    ADJCALLSTACKUP64 0, 0, implicit-def dead $rsp, implicit-def dead $eflags, implicit-def dead $ssp, implicit $rsp, implicit $ssp
+    %3:gr64 = COPY $rax
+    JMP_1 %bb.1
+
+...
diff --git a/llvm/test/CodeGen/X86/vselect-constants.ll b/llvm/test/CodeGen/X86/vselect-constants.ll
index 901f7e4a00eb..34bda718db8f 100644
--- a/llvm/test/CodeGen/X86/vselect-constants.ll
+++ b/llvm/test/CodeGen/X86/vselect-constants.ll
@@ -302,3 +302,21 @@ define i32 @wrong_min_signbits(<2 x i16> %x) {
   %t1 = bitcast <2 x i16> %sel to i32
   ret i32 %t1
 }
+
+define i32 @pr129181() {
+; SSE-LABEL: pr129181:
+; SSE:       # %bb.0: # %entry
+; SSE-NEXT:    xorl %eax, %eax
+; SSE-NEXT:    retq
+;
+; AVX-LABEL: pr129181:
+; AVX:       # %bb.0: # %entry
+; AVX-NEXT:    xorl %eax, %eax
+; AVX-NEXT:    retq
+entry:
+  %x = insertelement <4 x i32> zeroinitializer, i32 0, i32 0
+  %cmp = icmp ult <4 x i32> %x, splat (i32 1)
+  %sel = select <4 x i1> %cmp, <4 x i32> zeroinitializer, <4 x i32> <i32 0, i32 0, i32 1, i32 poison>
+  %reduce = tail call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sel)
+  ret i32 %reduce
+}
diff --git a/llvm/test/CodeGen/X86/win32-eh.ll b/llvm/test/CodeGen/X86/win32-eh.ll
index 82dc4beaf972..d3d19ede546d 100644
--- a/llvm/test/CodeGen/X86/win32-eh.ll
+++ b/llvm/test/CodeGen/X86/win32-eh.ll
@@ -1,4 +1,5 @@
 ; RUN: llc -mtriple=i686-pc-windows-msvc < %s | FileCheck %s
+; RUN: llc -mtriple=i686-pc-windows-msvc -filetype=obj < %s -o %t
 
 declare void @may_throw_or_crash()
 declare i32 @_except_handler3(...)
@@ -208,6 +209,14 @@ catch:
 ; CHECK-NEXT:  .long   0
 ; CHECK-NEXT:  .long   1
 
+; CHECK-LABEL: inlineasm:
+; CHECK: .safeseh my_handler
+define i32 @inlineasm() {
+entry:
+  call void asm sideeffect ".safeseh my_handler", "~{dirflag},~{fpsr},~{flags}"()
+  ret i32 0
+}
+
 ; CHECK-LABEL: ___ehhandler$use_CxxFrameHandler3:
 ; CHECK: movl $L__ehtable$use_CxxFrameHandler3, %eax
 ; CHECK-NEXT: jmp  ___CxxFrameHandler3 # TAILCALL
diff --git a/llvm/test/MC/AMDGPU/gfx950_asm_features.s b/llvm/test/MC/AMDGPU/gfx950_asm_features.s
index 389b17296c04..7bc47914f40b 100644
--- a/llvm/test/MC/AMDGPU/gfx950_asm_features.s
+++ b/llvm/test/MC/AMDGPU/gfx950_asm_features.s
@@ -1,11 +1,10 @@
 // RUN: llvm-mc -triple=amdgcn -mcpu=gfx950 -show-encoding %s | FileCheck --check-prefix=GFX950 --strict-whitespace %s
-// xUN: not llvm-mc -triple=amdgcn -mcpu=gfx940 %s 2>&1 | FileCheck --check-prefixes=NOT-GFX950,GFX940 --implicit-check-not=error: %s
-// xUN: not llvm-mc -triple=amdgcn -mcpu=gfx90a %s 2>&1 | FileCheck --check-prefixes=NOT-GFX950,GFX90A --implicit-check-not=error: %s
-// xUN: not llvm-mc -triple=amdgcn -mcpu=gfx1010 %s 2>&1 | FileCheck --check-prefixes=NOT-GFX950,GFX10 --implicit-check-not=error: %s
+// RUN: not llvm-mc -triple=amdgcn -mcpu=gfx942 %s 2>&1 | FileCheck --check-prefixes=NOT-GFX950 --implicit-check-not=error: %s
+// RUN: not llvm-mc -triple=amdgcn -mcpu=gfx90a %s 2>&1 | FileCheck --check-prefixes=NOT-GFX950 --implicit-check-not=error: %s
+// RUN: not llvm-mc -triple=amdgcn -mcpu=gfx1010 %s 2>&1 | FileCheck --check-prefixes=NOT-GFX950 --implicit-check-not=error: %s
 
 // NOT-GFX950: :[[@LINE+2]]:{{[0-9]+}}: error: instruction not supported on this GPU
 // GFX950: global_load_lds_dwordx3 v[2:3], off     ; encoding: [0x00,0x80,0xf8,0xdd,0x02,0x00,0x7f,0x00]
-
 global_load_lds_dwordx3 v[2:3], off
 
 // NOT-GFX950: :[[@LINE+2]]:{{[0-9]+}}: error:
diff --git a/llvm/test/MC/Disassembler/SystemZ/insns-arch15.txt b/llvm/test/MC/Disassembler/SystemZ/insns-z17.txt
similarity index 99%
rename from llvm/test/MC/Disassembler/SystemZ/insns-arch15.txt
rename to llvm/test/MC/Disassembler/SystemZ/insns-z17.txt
index 93274e665980..c5a30b072d99 100644
--- a/llvm/test/MC/Disassembler/SystemZ/insns-arch15.txt
+++ b/llvm/test/MC/Disassembler/SystemZ/insns-z17.txt
@@ -1,5 +1,5 @@
-# Test arch15 instructions that don't have PC-relative operands.
-# RUN: llvm-mc --disassemble %s -triple=s390x-linux-gnu -mcpu=arch15 \
+# Test z17 instructions that don't have PC-relative operands.
+# RUN: llvm-mc --disassemble %s -triple=s390x-linux-gnu -mcpu=z17 \
 # RUN:   | FileCheck %s
 
 # CHECK: bdepg %r0, %r0, %r0
diff --git a/llvm/test/MC/Disassembler/X86/avx10.2-bf16-32.txt b/llvm/test/MC/Disassembler/X86/avx10.2-bf16-32.txt
index a32e55e20e6b..0db70d290e56 100644
--- a/llvm/test/MC/Disassembler/X86/avx10.2-bf16-32.txt
+++ b/llvm/test/MC/Disassembler/X86/avx10.2-bf16-32.txt
@@ -1719,111 +1719,111 @@
 
 # ATT:   vgetexpbf16 %xmm3, %xmm2
 # INTEL: vgetexpbf16 xmm2, xmm3
-0x62,0xf5,0x7d,0x08,0x42,0xd3
+0x62,0xf6,0x7c,0x08,0x42,0xd3
 
 # ATT:   vgetexpbf16 %xmm3, %xmm2 {%k7}
 # INTEL: vgetexpbf16 xmm2 {k7}, xmm3
-0x62,0xf5,0x7d,0x0f,0x42,0xd3
+0x62,0xf6,0x7c,0x0f,0x42,0xd3
 
 # ATT:   vgetexpbf16 %xmm3, %xmm2 {%k7} {z}
 # INTEL: vgetexpbf16 xmm2 {k7} {z}, xmm3
-0x62,0xf5,0x7d,0x8f,0x42,0xd3
+0x62,0xf6,0x7c,0x8f,0x42,0xd3
 
 # ATT:   vgetexpbf16 %zmm3, %zmm2
 # INTEL: vgetexpbf16 zmm2, zmm3
-0x62,0xf5,0x7d,0x48,0x42,0xd3
+0x62,0xf6,0x7c,0x48,0x42,0xd3
 
 # ATT:   vgetexpbf16 %zmm3, %zmm2 {%k7}
 # INTEL: vgetexpbf16 zmm2 {k7}, zmm3
-0x62,0xf5,0x7d,0x4f,0x42,0xd3
+0x62,0xf6,0x7c,0x4f,0x42,0xd3
 
 # ATT:   vgetexpbf16 %zmm3, %zmm2 {%k7} {z}
 # INTEL: vgetexpbf16 zmm2 {k7} {z}, zmm3
-0x62,0xf5,0x7d,0xcf,0x42,0xd3
+0x62,0xf6,0x7c,0xcf,0x42,0xd3
 
 # ATT:   vgetexpbf16 %ymm3, %ymm2
 # INTEL: vgetexpbf16 ymm2, ymm3
-0x62,0xf5,0x7d,0x28,0x42,0xd3
+0x62,0xf6,0x7c,0x28,0x42,0xd3
 
 # ATT:   vgetexpbf16 %ymm3, %ymm2 {%k7}
 # INTEL: vgetexpbf16 ymm2 {k7}, ymm3
-0x62,0xf5,0x7d,0x2f,0x42,0xd3
+0x62,0xf6,0x7c,0x2f,0x42,0xd3
 
 # ATT:   vgetexpbf16 %ymm3, %ymm2 {%k7} {z}
 # INTEL: vgetexpbf16 ymm2 {k7} {z}, ymm3
-0x62,0xf5,0x7d,0xaf,0x42,0xd3
+0x62,0xf6,0x7c,0xaf,0x42,0xd3
 
 # ATT:   vgetexpbf16  268435456(%esp,%esi,8), %xmm2
 # INTEL: vgetexpbf16 xmm2, xmmword ptr [esp + 8*esi + 268435456]
-0x62,0xf5,0x7d,0x08,0x42,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf6,0x7c,0x08,0x42,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vgetexpbf16  291(%edi,%eax,4), %xmm2 {%k7}
 # INTEL: vgetexpbf16 xmm2 {k7}, xmmword ptr [edi + 4*eax + 291]
-0x62,0xf5,0x7d,0x0f,0x42,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf6,0x7c,0x0f,0x42,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vgetexpbf16  (%eax){1to8}, %xmm2
 # INTEL: vgetexpbf16 xmm2, word ptr [eax]{1to8}
-0x62,0xf5,0x7d,0x18,0x42,0x10
+0x62,0xf6,0x7c,0x18,0x42,0x10
 
 # ATT:   vgetexpbf16  -512(,%ebp,2), %xmm2
 # INTEL: vgetexpbf16 xmm2, xmmword ptr [2*ebp - 512]
-0x62,0xf5,0x7d,0x08,0x42,0x14,0x6d,0x00,0xfe,0xff,0xff
+0x62,0xf6,0x7c,0x08,0x42,0x14,0x6d,0x00,0xfe,0xff,0xff
 
 # ATT:   vgetexpbf16  2032(%ecx), %xmm2 {%k7} {z}
 # INTEL: vgetexpbf16 xmm2 {k7} {z}, xmmword ptr [ecx + 2032]
-0x62,0xf5,0x7d,0x8f,0x42,0x51,0x7f
+0x62,0xf6,0x7c,0x8f,0x42,0x51,0x7f
 
 # ATT:   vgetexpbf16  -256(%edx){1to8}, %xmm2 {%k7} {z}
 # INTEL: vgetexpbf16 xmm2 {k7} {z}, word ptr [edx - 256]{1to8}
-0x62,0xf5,0x7d,0x9f,0x42,0x52,0x80
+0x62,0xf6,0x7c,0x9f,0x42,0x52,0x80
 
 # ATT:   vgetexpbf16  268435456(%esp,%esi,8), %ymm2
 # INTEL: vgetexpbf16 ymm2, ymmword ptr [esp + 8*esi + 268435456]
-0x62,0xf5,0x7d,0x28,0x42,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf6,0x7c,0x28,0x42,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vgetexpbf16  291(%edi,%eax,4), %ymm2 {%k7}
 # INTEL: vgetexpbf16 ymm2 {k7}, ymmword ptr [edi + 4*eax + 291]
-0x62,0xf5,0x7d,0x2f,0x42,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf6,0x7c,0x2f,0x42,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vgetexpbf16  (%eax){1to16}, %ymm2
 # INTEL: vgetexpbf16 ymm2, word ptr [eax]{1to16}
-0x62,0xf5,0x7d,0x38,0x42,0x10
+0x62,0xf6,0x7c,0x38,0x42,0x10
 
 # ATT:   vgetexpbf16  -1024(,%ebp,2), %ymm2
 # INTEL: vgetexpbf16 ymm2, ymmword ptr [2*ebp - 1024]
-0x62,0xf5,0x7d,0x28,0x42,0x14,0x6d,0x00,0xfc,0xff,0xff
+0x62,0xf6,0x7c,0x28,0x42,0x14,0x6d,0x00,0xfc,0xff,0xff
 
 # ATT:   vgetexpbf16  4064(%ecx), %ymm2 {%k7} {z}
 # INTEL: vgetexpbf16 ymm2 {k7} {z}, ymmword ptr [ecx + 4064]
-0x62,0xf5,0x7d,0xaf,0x42,0x51,0x7f
+0x62,0xf6,0x7c,0xaf,0x42,0x51,0x7f
 
 # ATT:   vgetexpbf16  -256(%edx){1to16}, %ymm2 {%k7} {z}
 # INTEL: vgetexpbf16 ymm2 {k7} {z}, word ptr [edx - 256]{1to16}
-0x62,0xf5,0x7d,0xbf,0x42,0x52,0x80
+0x62,0xf6,0x7c,0xbf,0x42,0x52,0x80
 
 # ATT:   vgetexpbf16  268435456(%esp,%esi,8), %zmm2
 # INTEL: vgetexpbf16 zmm2, zmmword ptr [esp + 8*esi + 268435456]
-0x62,0xf5,0x7d,0x48,0x42,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf6,0x7c,0x48,0x42,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vgetexpbf16  291(%edi,%eax,4), %zmm2 {%k7}
 # INTEL: vgetexpbf16 zmm2 {k7}, zmmword ptr [edi + 4*eax + 291]
-0x62,0xf5,0x7d,0x4f,0x42,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf6,0x7c,0x4f,0x42,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vgetexpbf16  (%eax){1to32}, %zmm2
 # INTEL: vgetexpbf16 zmm2, word ptr [eax]{1to32}
-0x62,0xf5,0x7d,0x58,0x42,0x10
+0x62,0xf6,0x7c,0x58,0x42,0x10
 
 # ATT:   vgetexpbf16  -2048(,%ebp,2), %zmm2
 # INTEL: vgetexpbf16 zmm2, zmmword ptr [2*ebp - 2048]
-0x62,0xf5,0x7d,0x48,0x42,0x14,0x6d,0x00,0xf8,0xff,0xff
+0x62,0xf6,0x7c,0x48,0x42,0x14,0x6d,0x00,0xf8,0xff,0xff
 
 # ATT:   vgetexpbf16  8128(%ecx), %zmm2 {%k7} {z}
 # INTEL: vgetexpbf16 zmm2 {k7} {z}, zmmword ptr [ecx + 8128]
-0x62,0xf5,0x7d,0xcf,0x42,0x51,0x7f
+0x62,0xf6,0x7c,0xcf,0x42,0x51,0x7f
 
 # ATT:   vgetexpbf16  -256(%edx){1to32}, %zmm2 {%k7} {z}
 # INTEL: vgetexpbf16 zmm2 {k7} {z}, word ptr [edx - 256]{1to32}
-0x62,0xf5,0x7d,0xdf,0x42,0x52,0x80
+0x62,0xf6,0x7c,0xdf,0x42,0x52,0x80
 
 # ATT:   vgetmantbf16 $123, %zmm3, %zmm2
 # INTEL: vgetmantbf16 zmm2, zmm3, 123
diff --git a/llvm/test/MC/Disassembler/X86/avx10.2-bf16-64.txt b/llvm/test/MC/Disassembler/X86/avx10.2-bf16-64.txt
index 1319c5cbd036..197415e5ba32 100644
--- a/llvm/test/MC/Disassembler/X86/avx10.2-bf16-64.txt
+++ b/llvm/test/MC/Disassembler/X86/avx10.2-bf16-64.txt
@@ -1719,111 +1719,111 @@
 
 # ATT:   vgetexpbf16 %xmm23, %xmm22
 # INTEL: vgetexpbf16 xmm22, xmm23
-0x62,0xa5,0x7d,0x08,0x42,0xf7
+0x62,0xa6,0x7c,0x08,0x42,0xf7
 
 # ATT:   vgetexpbf16 %xmm23, %xmm22 {%k7}
 # INTEL: vgetexpbf16 xmm22 {k7}, xmm23
-0x62,0xa5,0x7d,0x0f,0x42,0xf7
+0x62,0xa6,0x7c,0x0f,0x42,0xf7
 
 # ATT:   vgetexpbf16 %xmm23, %xmm22 {%k7} {z}
 # INTEL: vgetexpbf16 xmm22 {k7} {z}, xmm23
-0x62,0xa5,0x7d,0x8f,0x42,0xf7
+0x62,0xa6,0x7c,0x8f,0x42,0xf7
 
 # ATT:   vgetexpbf16 %zmm23, %zmm22
 # INTEL: vgetexpbf16 zmm22, zmm23
-0x62,0xa5,0x7d,0x48,0x42,0xf7
+0x62,0xa6,0x7c,0x48,0x42,0xf7
 
 # ATT:   vgetexpbf16 %zmm23, %zmm22 {%k7}
 # INTEL: vgetexpbf16 zmm22 {k7}, zmm23
-0x62,0xa5,0x7d,0x4f,0x42,0xf7
+0x62,0xa6,0x7c,0x4f,0x42,0xf7
 
 # ATT:   vgetexpbf16 %zmm23, %zmm22 {%k7} {z}
 # INTEL: vgetexpbf16 zmm22 {k7} {z}, zmm23
-0x62,0xa5,0x7d,0xcf,0x42,0xf7
+0x62,0xa6,0x7c,0xcf,0x42,0xf7
 
 # ATT:   vgetexpbf16 %ymm23, %ymm22
 # INTEL: vgetexpbf16 ymm22, ymm23
-0x62,0xa5,0x7d,0x28,0x42,0xf7
+0x62,0xa6,0x7c,0x28,0x42,0xf7
 
 # ATT:   vgetexpbf16 %ymm23, %ymm22 {%k7}
 # INTEL: vgetexpbf16 ymm22 {k7}, ymm23
-0x62,0xa5,0x7d,0x2f,0x42,0xf7
+0x62,0xa6,0x7c,0x2f,0x42,0xf7
 
 # ATT:   vgetexpbf16 %ymm23, %ymm22 {%k7} {z}
 # INTEL: vgetexpbf16 ymm22 {k7} {z}, ymm23
-0x62,0xa5,0x7d,0xaf,0x42,0xf7
+0x62,0xa6,0x7c,0xaf,0x42,0xf7
 
 # ATT:   vgetexpbf16  268435456(%rbp,%r14,8), %xmm22
 # INTEL: vgetexpbf16 xmm22, xmmword ptr [rbp + 8*r14 + 268435456]
-0x62,0xa5,0x7d,0x08,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa6,0x7c,0x08,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vgetexpbf16  291(%r8,%rax,4), %xmm22 {%k7}
 # INTEL: vgetexpbf16 xmm22 {k7}, xmmword ptr [r8 + 4*rax + 291]
-0x62,0xc5,0x7d,0x0f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc6,0x7c,0x0f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vgetexpbf16  (%rip){1to8}, %xmm22
 # INTEL: vgetexpbf16 xmm22, word ptr [rip]{1to8}
-0x62,0xe5,0x7d,0x18,0x42,0x35,0x00,0x00,0x00,0x00
+0x62,0xe6,0x7c,0x18,0x42,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vgetexpbf16  -512(,%rbp,2), %xmm22
 # INTEL: vgetexpbf16 xmm22, xmmword ptr [2*rbp - 512]
-0x62,0xe5,0x7d,0x08,0x42,0x34,0x6d,0x00,0xfe,0xff,0xff
+0x62,0xe6,0x7c,0x08,0x42,0x34,0x6d,0x00,0xfe,0xff,0xff
 
 # ATT:   vgetexpbf16  2032(%rcx), %xmm22 {%k7} {z}
 # INTEL: vgetexpbf16 xmm22 {k7} {z}, xmmword ptr [rcx + 2032]
-0x62,0xe5,0x7d,0x8f,0x42,0x71,0x7f
+0x62,0xe6,0x7c,0x8f,0x42,0x71,0x7f
 
 # ATT:   vgetexpbf16  -256(%rdx){1to8}, %xmm22 {%k7} {z}
 # INTEL: vgetexpbf16 xmm22 {k7} {z}, word ptr [rdx - 256]{1to8}
-0x62,0xe5,0x7d,0x9f,0x42,0x72,0x80
+0x62,0xe6,0x7c,0x9f,0x42,0x72,0x80
 
 # ATT:   vgetexpbf16  268435456(%rbp,%r14,8), %ymm22
 # INTEL: vgetexpbf16 ymm22, ymmword ptr [rbp + 8*r14 + 268435456]
-0x62,0xa5,0x7d,0x28,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa6,0x7c,0x28,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vgetexpbf16  291(%r8,%rax,4), %ymm22 {%k7}
 # INTEL: vgetexpbf16 ymm22 {k7}, ymmword ptr [r8 + 4*rax + 291]
-0x62,0xc5,0x7d,0x2f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc6,0x7c,0x2f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vgetexpbf16  (%rip){1to16}, %ymm22
 # INTEL: vgetexpbf16 ymm22, word ptr [rip]{1to16}
-0x62,0xe5,0x7d,0x38,0x42,0x35,0x00,0x00,0x00,0x00
+0x62,0xe6,0x7c,0x38,0x42,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vgetexpbf16  -1024(,%rbp,2), %ymm22
 # INTEL: vgetexpbf16 ymm22, ymmword ptr [2*rbp - 1024]
-0x62,0xe5,0x7d,0x28,0x42,0x34,0x6d,0x00,0xfc,0xff,0xff
+0x62,0xe6,0x7c,0x28,0x42,0x34,0x6d,0x00,0xfc,0xff,0xff
 
 # ATT:   vgetexpbf16  4064(%rcx), %ymm22 {%k7} {z}
 # INTEL: vgetexpbf16 ymm22 {k7} {z}, ymmword ptr [rcx + 4064]
-0x62,0xe5,0x7d,0xaf,0x42,0x71,0x7f
+0x62,0xe6,0x7c,0xaf,0x42,0x71,0x7f
 
 # ATT:   vgetexpbf16  -256(%rdx){1to16}, %ymm22 {%k7} {z}
 # INTEL: vgetexpbf16 ymm22 {k7} {z}, word ptr [rdx - 256]{1to16}
-0x62,0xe5,0x7d,0xbf,0x42,0x72,0x80
+0x62,0xe6,0x7c,0xbf,0x42,0x72,0x80
 
 # ATT:   vgetexpbf16  268435456(%rbp,%r14,8), %zmm22
 # INTEL: vgetexpbf16 zmm22, zmmword ptr [rbp + 8*r14 + 268435456]
-0x62,0xa5,0x7d,0x48,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa6,0x7c,0x48,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vgetexpbf16  291(%r8,%rax,4), %zmm22 {%k7}
 # INTEL: vgetexpbf16 zmm22 {k7}, zmmword ptr [r8 + 4*rax + 291]
-0x62,0xc5,0x7d,0x4f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc6,0x7c,0x4f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vgetexpbf16  (%rip){1to32}, %zmm22
 # INTEL: vgetexpbf16 zmm22, word ptr [rip]{1to32}
-0x62,0xe5,0x7d,0x58,0x42,0x35,0x00,0x00,0x00,0x00
+0x62,0xe6,0x7c,0x58,0x42,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vgetexpbf16  -2048(,%rbp,2), %zmm22
 # INTEL: vgetexpbf16 zmm22, zmmword ptr [2*rbp - 2048]
-0x62,0xe5,0x7d,0x48,0x42,0x34,0x6d,0x00,0xf8,0xff,0xff
+0x62,0xe6,0x7c,0x48,0x42,0x34,0x6d,0x00,0xf8,0xff,0xff
 
 # ATT:   vgetexpbf16  8128(%rcx), %zmm22 {%k7} {z}
 # INTEL: vgetexpbf16 zmm22 {k7} {z}, zmmword ptr [rcx + 8128]
-0x62,0xe5,0x7d,0xcf,0x42,0x71,0x7f
+0x62,0xe6,0x7c,0xcf,0x42,0x71,0x7f
 
 # ATT:   vgetexpbf16  -256(%rdx){1to32}, %zmm22 {%k7} {z}
 # INTEL: vgetexpbf16 zmm22 {k7} {z}, word ptr [rdx - 256]{1to32}
-0x62,0xe5,0x7d,0xdf,0x42,0x72,0x80
+0x62,0xe6,0x7c,0xdf,0x42,0x72,0x80
 
 # ATT:   vgetmantbf16 $123, %zmm23, %zmm22
 # INTEL: vgetmantbf16 zmm22, zmm23, 123
diff --git a/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-32.txt b/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-32.txt
index e7adacbbf88c..ecdc75979e8d 100644
--- a/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-32.txt
+++ b/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-32.txt
@@ -3,193 +3,193 @@
 
 # ATT:   vcomxsd %xmm3, %xmm2
 # INTEL: vcomxsd xmm2, xmm3
-0x62,0xf1,0xfe,0x08,0x2f,0xd3
+0x62,0xf1,0xff,0x08,0x2f,0xd3
 
 # ATT:   vcomxsd {sae}, %xmm3, %xmm2
 # INTEL: vcomxsd xmm2, xmm3, {sae}
-0x62,0xf1,0xfe,0x18,0x2f,0xd3
+0x62,0xf1,0xff,0x18,0x2f,0xd3
 
 # ATT:   vcomxsd  268435456(%esp,%esi,8), %xmm2
 # INTEL: vcomxsd xmm2, qword ptr [esp + 8*esi + 268435456]
-0x62,0xf1,0xfe,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf1,0xff,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vcomxsd  291(%edi,%eax,4), %xmm2
 # INTEL: vcomxsd xmm2, qword ptr [edi + 4*eax + 291]
-0x62,0xf1,0xfe,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf1,0xff,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vcomxsd  (%eax), %xmm2
 # INTEL: vcomxsd xmm2, qword ptr [eax]
-0x62,0xf1,0xfe,0x08,0x2f,0x10
+0x62,0xf1,0xff,0x08,0x2f,0x10
 
 # ATT:   vcomxsd  -256(,%ebp,2), %xmm2
 # INTEL: vcomxsd xmm2, qword ptr [2*ebp - 256]
-0x62,0xf1,0xfe,0x08,0x2f,0x14,0x6d,0x00,0xff,0xff,0xff
+0x62,0xf1,0xff,0x08,0x2f,0x14,0x6d,0x00,0xff,0xff,0xff
 
 # ATT:   vcomxsd  1016(%ecx), %xmm2
 # INTEL: vcomxsd xmm2, qword ptr [ecx + 1016]
-0x62,0xf1,0xfe,0x08,0x2f,0x51,0x7f
+0x62,0xf1,0xff,0x08,0x2f,0x51,0x7f
 
 # ATT:   vcomxsd  -1024(%edx), %xmm2
 # INTEL: vcomxsd xmm2, qword ptr [edx - 1024]
-0x62,0xf1,0xfe,0x08,0x2f,0x52,0x80
+0x62,0xf1,0xff,0x08,0x2f,0x52,0x80
 
 # ATT:   vcomxsh %xmm3, %xmm2
 # INTEL: vcomxsh xmm2, xmm3
-0x62,0xf5,0x7f,0x08,0x2f,0xd3
+0x62,0xf5,0x7e,0x08,0x2f,0xd3
 
 # ATT:   vcomxsh {sae}, %xmm3, %xmm2
 # INTEL: vcomxsh xmm2, xmm3, {sae}
-0x62,0xf5,0x7f,0x18,0x2f,0xd3
+0x62,0xf5,0x7e,0x18,0x2f,0xd3
 
 # ATT:   vcomxsh  268435456(%esp,%esi,8), %xmm2
 # INTEL: vcomxsh xmm2, word ptr [esp + 8*esi + 268435456]
-0x62,0xf5,0x7f,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf5,0x7e,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vcomxsh  291(%edi,%eax,4), %xmm2
 # INTEL: vcomxsh xmm2, word ptr [edi + 4*eax + 291]
-0x62,0xf5,0x7f,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf5,0x7e,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vcomxsh  (%eax), %xmm2
 # INTEL: vcomxsh xmm2, word ptr [eax]
-0x62,0xf5,0x7f,0x08,0x2f,0x10
+0x62,0xf5,0x7e,0x08,0x2f,0x10
 
 # ATT:   vcomxsh  -64(,%ebp,2), %xmm2
 # INTEL: vcomxsh xmm2, word ptr [2*ebp - 64]
-0x62,0xf5,0x7f,0x08,0x2f,0x14,0x6d,0xc0,0xff,0xff,0xff
+0x62,0xf5,0x7e,0x08,0x2f,0x14,0x6d,0xc0,0xff,0xff,0xff
 
 # ATT:   vcomxsh  254(%ecx), %xmm2
 # INTEL: vcomxsh xmm2, word ptr [ecx + 254]
-0x62,0xf5,0x7f,0x08,0x2f,0x51,0x7f
+0x62,0xf5,0x7e,0x08,0x2f,0x51,0x7f
 
 # ATT:   vcomxsh  -256(%edx), %xmm2
 # INTEL: vcomxsh xmm2, word ptr [edx - 256]
-0x62,0xf5,0x7f,0x08,0x2f,0x52,0x80
+0x62,0xf5,0x7e,0x08,0x2f,0x52,0x80
 
 # ATT:   vcomxss %xmm3, %xmm2
 # INTEL: vcomxss xmm2, xmm3
-0x62,0xf1,0x7f,0x08,0x2f,0xd3
+0x62,0xf1,0x7e,0x08,0x2f,0xd3
 
 # ATT:   vcomxss {sae}, %xmm3, %xmm2
 # INTEL: vcomxss xmm2, xmm3, {sae}
-0x62,0xf1,0x7f,0x18,0x2f,0xd3
+0x62,0xf1,0x7e,0x18,0x2f,0xd3
 
 # ATT:   vcomxss  268435456(%esp,%esi,8), %xmm2
 # INTEL: vcomxss xmm2, dword ptr [esp + 8*esi + 268435456]
-0x62,0xf1,0x7f,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf1,0x7e,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vcomxss  291(%edi,%eax,4), %xmm2
 # INTEL: vcomxss xmm2, dword ptr [edi + 4*eax + 291]
-0x62,0xf1,0x7f,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf1,0x7e,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vcomxss  (%eax), %xmm2
 # INTEL: vcomxss xmm2, dword ptr [eax]
-0x62,0xf1,0x7f,0x08,0x2f,0x10
+0x62,0xf1,0x7e,0x08,0x2f,0x10
 
 # ATT:   vcomxss  -128(,%ebp,2), %xmm2
 # INTEL: vcomxss xmm2, dword ptr [2*ebp - 128]
-0x62,0xf1,0x7f,0x08,0x2f,0x14,0x6d,0x80,0xff,0xff,0xff
+0x62,0xf1,0x7e,0x08,0x2f,0x14,0x6d,0x80,0xff,0xff,0xff
 
 # ATT:   vcomxss  508(%ecx), %xmm2
 # INTEL: vcomxss xmm2, dword ptr [ecx + 508]
-0x62,0xf1,0x7f,0x08,0x2f,0x51,0x7f
+0x62,0xf1,0x7e,0x08,0x2f,0x51,0x7f
 
 # ATT:   vcomxss  -512(%edx), %xmm2
 # INTEL: vcomxss xmm2, dword ptr [edx - 512]
-0x62,0xf1,0x7f,0x08,0x2f,0x52,0x80
+0x62,0xf1,0x7e,0x08,0x2f,0x52,0x80
 
 # ATT:   vucomxsd %xmm3, %xmm2
 # INTEL: vucomxsd xmm2, xmm3
-0x62,0xf1,0xfe,0x08,0x2e,0xd3
+0x62,0xf1,0xff,0x08,0x2e,0xd3
 
 # ATT:   vucomxsd {sae}, %xmm3, %xmm2
 # INTEL: vucomxsd xmm2, xmm3, {sae}
-0x62,0xf1,0xfe,0x18,0x2e,0xd3
+0x62,0xf1,0xff,0x18,0x2e,0xd3
 
 # ATT:   vucomxsd  268435456(%esp,%esi,8), %xmm2
 # INTEL: vucomxsd xmm2, qword ptr [esp + 8*esi + 268435456]
-0x62,0xf1,0xfe,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf1,0xff,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vucomxsd  291(%edi,%eax,4), %xmm2
 # INTEL: vucomxsd xmm2, qword ptr [edi + 4*eax + 291]
-0x62,0xf1,0xfe,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf1,0xff,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vucomxsd  (%eax), %xmm2
 # INTEL: vucomxsd xmm2, qword ptr [eax]
-0x62,0xf1,0xfe,0x08,0x2e,0x10
+0x62,0xf1,0xff,0x08,0x2e,0x10
 
 # ATT:   vucomxsd  -256(,%ebp,2), %xmm2
 # INTEL: vucomxsd xmm2, qword ptr [2*ebp - 256]
-0x62,0xf1,0xfe,0x08,0x2e,0x14,0x6d,0x00,0xff,0xff,0xff
+0x62,0xf1,0xff,0x08,0x2e,0x14,0x6d,0x00,0xff,0xff,0xff
 
 # ATT:   vucomxsd  1016(%ecx), %xmm2
 # INTEL: vucomxsd xmm2, qword ptr [ecx + 1016]
-0x62,0xf1,0xfe,0x08,0x2e,0x51,0x7f
+0x62,0xf1,0xff,0x08,0x2e,0x51,0x7f
 
 # ATT:   vucomxsd  -1024(%edx), %xmm2
 # INTEL: vucomxsd xmm2, qword ptr [edx - 1024]
-0x62,0xf1,0xfe,0x08,0x2e,0x52,0x80
+0x62,0xf1,0xff,0x08,0x2e,0x52,0x80
 
 # ATT:   vucomxsh %xmm3, %xmm2
 # INTEL: vucomxsh xmm2, xmm3
-0x62,0xf5,0x7f,0x08,0x2e,0xd3
+0x62,0xf5,0x7e,0x08,0x2e,0xd3
 
 # ATT:   vucomxsh {sae}, %xmm3, %xmm2
 # INTEL: vucomxsh xmm2, xmm3, {sae}
-0x62,0xf5,0x7f,0x18,0x2e,0xd3
+0x62,0xf5,0x7e,0x18,0x2e,0xd3
 
 # ATT:   vucomxsh  268435456(%esp,%esi,8), %xmm2
 # INTEL: vucomxsh xmm2, word ptr [esp + 8*esi + 268435456]
-0x62,0xf5,0x7f,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf5,0x7e,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vucomxsh  291(%edi,%eax,4), %xmm2
 # INTEL: vucomxsh xmm2, word ptr [edi + 4*eax + 291]
-0x62,0xf5,0x7f,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf5,0x7e,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vucomxsh  (%eax), %xmm2
 # INTEL: vucomxsh xmm2, word ptr [eax]
-0x62,0xf5,0x7f,0x08,0x2e,0x10
+0x62,0xf5,0x7e,0x08,0x2e,0x10
 
 # ATT:   vucomxsh  -64(,%ebp,2), %xmm2
 # INTEL: vucomxsh xmm2, word ptr [2*ebp - 64]
-0x62,0xf5,0x7f,0x08,0x2e,0x14,0x6d,0xc0,0xff,0xff,0xff
+0x62,0xf5,0x7e,0x08,0x2e,0x14,0x6d,0xc0,0xff,0xff,0xff
 
 # ATT:   vucomxsh  254(%ecx), %xmm2
 # INTEL: vucomxsh xmm2, word ptr [ecx + 254]
-0x62,0xf5,0x7f,0x08,0x2e,0x51,0x7f
+0x62,0xf5,0x7e,0x08,0x2e,0x51,0x7f
 
 # ATT:   vucomxsh  -256(%edx), %xmm2
 # INTEL: vucomxsh xmm2, word ptr [edx - 256]
-0x62,0xf5,0x7f,0x08,0x2e,0x52,0x80
+0x62,0xf5,0x7e,0x08,0x2e,0x52,0x80
 
 # ATT:   vucomxss %xmm3, %xmm2
 # INTEL: vucomxss xmm2, xmm3
-0x62,0xf1,0x7f,0x08,0x2e,0xd3
+0x62,0xf1,0x7e,0x08,0x2e,0xd3
 
 # ATT:   vucomxss {sae}, %xmm3, %xmm2
 # INTEL: vucomxss xmm2, xmm3, {sae}
-0x62,0xf1,0x7f,0x18,0x2e,0xd3
+0x62,0xf1,0x7e,0x18,0x2e,0xd3
 
 # ATT:   vucomxss  268435456(%esp,%esi,8), %xmm2
 # INTEL: vucomxss xmm2, dword ptr [esp + 8*esi + 268435456]
-0x62,0xf1,0x7f,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10
+0x62,0xf1,0x7e,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10
 
 # ATT:   vucomxss  291(%edi,%eax,4), %xmm2
 # INTEL: vucomxss xmm2, dword ptr [edi + 4*eax + 291]
-0x62,0xf1,0x7f,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00
+0x62,0xf1,0x7e,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00
 
 # ATT:   vucomxss  (%eax), %xmm2
 # INTEL: vucomxss xmm2, dword ptr [eax]
-0x62,0xf1,0x7f,0x08,0x2e,0x10
+0x62,0xf1,0x7e,0x08,0x2e,0x10
 
 # ATT:   vucomxss  -128(,%ebp,2), %xmm2
 # INTEL: vucomxss xmm2, dword ptr [2*ebp - 128]
-0x62,0xf1,0x7f,0x08,0x2e,0x14,0x6d,0x80,0xff,0xff,0xff
+0x62,0xf1,0x7e,0x08,0x2e,0x14,0x6d,0x80,0xff,0xff,0xff
 
 # ATT:   vucomxss  508(%ecx), %xmm2
 # INTEL: vucomxss xmm2, dword ptr [ecx + 508]
-0x62,0xf1,0x7f,0x08,0x2e,0x51,0x7f
+0x62,0xf1,0x7e,0x08,0x2e,0x51,0x7f
 
 # ATT:   vucomxss  -512(%edx), %xmm2
 # INTEL: vucomxss xmm2, dword ptr [edx - 512]
-0x62,0xf1,0x7f,0x08,0x2e,0x52,0x80
+0x62,0xf1,0x7e,0x08,0x2e,0x52,0x80
 
diff --git a/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-64.txt b/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-64.txt
index ea580fe8d508..e01e762d12aa 100644
--- a/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-64.txt
+++ b/llvm/test/MC/Disassembler/X86/avx10.2-com-ef-64.txt
@@ -3,193 +3,193 @@
 
 # ATT:   vcomxsd %xmm23, %xmm22
 # INTEL: vcomxsd xmm22, xmm23
-0x62,0xa1,0xfe,0x08,0x2f,0xf7
+0x62,0xa1,0xff,0x08,0x2f,0xf7
 
 # ATT:   vcomxsd {sae}, %xmm23, %xmm22
 # INTEL: vcomxsd xmm22, xmm23, {sae}
-0x62,0xa1,0xfe,0x18,0x2f,0xf7
+0x62,0xa1,0xff,0x18,0x2f,0xf7
 
 # ATT:   vcomxsd  268435456(%rbp,%r14,8), %xmm22
 # INTEL: vcomxsd xmm22, qword ptr [rbp + 8*r14 + 268435456]
-0x62,0xa1,0xfe,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa1,0xff,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vcomxsd  291(%r8,%rax,4), %xmm22
 # INTEL: vcomxsd xmm22, qword ptr [r8 + 4*rax + 291]
-0x62,0xc1,0xfe,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc1,0xff,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vcomxsd  (%rip), %xmm22
 # INTEL: vcomxsd xmm22, qword ptr [rip]
-0x62,0xe1,0xfe,0x08,0x2f,0x35,0x00,0x00,0x00,0x00
+0x62,0xe1,0xff,0x08,0x2f,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vcomxsd  -256(,%rbp,2), %xmm22
 # INTEL: vcomxsd xmm22, qword ptr [2*rbp - 256]
-0x62,0xe1,0xfe,0x08,0x2f,0x34,0x6d,0x00,0xff,0xff,0xff
+0x62,0xe1,0xff,0x08,0x2f,0x34,0x6d,0x00,0xff,0xff,0xff
 
 # ATT:   vcomxsd  1016(%rcx), %xmm22
 # INTEL: vcomxsd xmm22, qword ptr [rcx + 1016]
-0x62,0xe1,0xfe,0x08,0x2f,0x71,0x7f
+0x62,0xe1,0xff,0x08,0x2f,0x71,0x7f
 
 # ATT:   vcomxsd  -1024(%rdx), %xmm22
 # INTEL: vcomxsd xmm22, qword ptr [rdx - 1024]
-0x62,0xe1,0xfe,0x08,0x2f,0x72,0x80
+0x62,0xe1,0xff,0x08,0x2f,0x72,0x80
 
 # ATT:   vcomxsh %xmm23, %xmm22
 # INTEL: vcomxsh xmm22, xmm23
-0x62,0xa5,0x7f,0x08,0x2f,0xf7
+0x62,0xa5,0x7e,0x08,0x2f,0xf7
 
 # ATT:   vcomxsh {sae}, %xmm23, %xmm22
 # INTEL: vcomxsh xmm22, xmm23, {sae}
-0x62,0xa5,0x7f,0x18,0x2f,0xf7
+0x62,0xa5,0x7e,0x18,0x2f,0xf7
 
 # ATT:   vcomxsh  268435456(%rbp,%r14,8), %xmm22
 # INTEL: vcomxsh xmm22, word ptr [rbp + 8*r14 + 268435456]
-0x62,0xa5,0x7f,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa5,0x7e,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vcomxsh  291(%r8,%rax,4), %xmm22
 # INTEL: vcomxsh xmm22, word ptr [r8 + 4*rax + 291]
-0x62,0xc5,0x7f,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc5,0x7e,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vcomxsh  (%rip), %xmm22
 # INTEL: vcomxsh xmm22, word ptr [rip]
-0x62,0xe5,0x7f,0x08,0x2f,0x35,0x00,0x00,0x00,0x00
+0x62,0xe5,0x7e,0x08,0x2f,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vcomxsh  -64(,%rbp,2), %xmm22
 # INTEL: vcomxsh xmm22, word ptr [2*rbp - 64]
-0x62,0xe5,0x7f,0x08,0x2f,0x34,0x6d,0xc0,0xff,0xff,0xff
+0x62,0xe5,0x7e,0x08,0x2f,0x34,0x6d,0xc0,0xff,0xff,0xff
 
 # ATT:   vcomxsh  254(%rcx), %xmm22
 # INTEL: vcomxsh xmm22, word ptr [rcx + 254]
-0x62,0xe5,0x7f,0x08,0x2f,0x71,0x7f
+0x62,0xe5,0x7e,0x08,0x2f,0x71,0x7f
 
 # ATT:   vcomxsh  -256(%rdx), %xmm22
 # INTEL: vcomxsh xmm22, word ptr [rdx - 256]
-0x62,0xe5,0x7f,0x08,0x2f,0x72,0x80
+0x62,0xe5,0x7e,0x08,0x2f,0x72,0x80
 
 # ATT:   vcomxss %xmm23, %xmm22
 # INTEL: vcomxss xmm22, xmm23
-0x62,0xa1,0x7f,0x08,0x2f,0xf7
+0x62,0xa1,0x7e,0x08,0x2f,0xf7
 
 # ATT:   vcomxss {sae}, %xmm23, %xmm22
 # INTEL: vcomxss xmm22, xmm23, {sae}
-0x62,0xa1,0x7f,0x18,0x2f,0xf7
+0x62,0xa1,0x7e,0x18,0x2f,0xf7
 
 # ATT:   vcomxss  268435456(%rbp,%r14,8), %xmm22
 # INTEL: vcomxss xmm22, dword ptr [rbp + 8*r14 + 268435456]
-0x62,0xa1,0x7f,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa1,0x7e,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vcomxss  291(%r8,%rax,4), %xmm22
 # INTEL: vcomxss xmm22, dword ptr [r8 + 4*rax + 291]
-0x62,0xc1,0x7f,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc1,0x7e,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vcomxss  (%rip), %xmm22
 # INTEL: vcomxss xmm22, dword ptr [rip]
-0x62,0xe1,0x7f,0x08,0x2f,0x35,0x00,0x00,0x00,0x00
+0x62,0xe1,0x7e,0x08,0x2f,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vcomxss  -128(,%rbp,2), %xmm22
 # INTEL: vcomxss xmm22, dword ptr [2*rbp - 128]
-0x62,0xe1,0x7f,0x08,0x2f,0x34,0x6d,0x80,0xff,0xff,0xff
+0x62,0xe1,0x7e,0x08,0x2f,0x34,0x6d,0x80,0xff,0xff,0xff
 
 # ATT:   vcomxss  508(%rcx), %xmm22
 # INTEL: vcomxss xmm22, dword ptr [rcx + 508]
-0x62,0xe1,0x7f,0x08,0x2f,0x71,0x7f
+0x62,0xe1,0x7e,0x08,0x2f,0x71,0x7f
 
 # ATT:   vcomxss  -512(%rdx), %xmm22
 # INTEL: vcomxss xmm22, dword ptr [rdx - 512]
-0x62,0xe1,0x7f,0x08,0x2f,0x72,0x80
+0x62,0xe1,0x7e,0x08,0x2f,0x72,0x80
 
 # ATT:   vucomxsd %xmm23, %xmm22
 # INTEL: vucomxsd xmm22, xmm23
-0x62,0xa1,0xfe,0x08,0x2e,0xf7
+0x62,0xa1,0xff,0x08,0x2e,0xf7
 
 # ATT:   vucomxsd {sae}, %xmm23, %xmm22
 # INTEL: vucomxsd xmm22, xmm23, {sae}
-0x62,0xa1,0xfe,0x18,0x2e,0xf7
+0x62,0xa1,0xff,0x18,0x2e,0xf7
 
 # ATT:   vucomxsd  268435456(%rbp,%r14,8), %xmm22
 # INTEL: vucomxsd xmm22, qword ptr [rbp + 8*r14 + 268435456]
-0x62,0xa1,0xfe,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa1,0xff,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vucomxsd  291(%r8,%rax,4), %xmm22
 # INTEL: vucomxsd xmm22, qword ptr [r8 + 4*rax + 291]
-0x62,0xc1,0xfe,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc1,0xff,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vucomxsd  (%rip), %xmm22
 # INTEL: vucomxsd xmm22, qword ptr [rip]
-0x62,0xe1,0xfe,0x08,0x2e,0x35,0x00,0x00,0x00,0x00
+0x62,0xe1,0xff,0x08,0x2e,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vucomxsd  -256(,%rbp,2), %xmm22
 # INTEL: vucomxsd xmm22, qword ptr [2*rbp - 256]
-0x62,0xe1,0xfe,0x08,0x2e,0x34,0x6d,0x00,0xff,0xff,0xff
+0x62,0xe1,0xff,0x08,0x2e,0x34,0x6d,0x00,0xff,0xff,0xff
 
 # ATT:   vucomxsd  1016(%rcx), %xmm22
 # INTEL: vucomxsd xmm22, qword ptr [rcx + 1016]
-0x62,0xe1,0xfe,0x08,0x2e,0x71,0x7f
+0x62,0xe1,0xff,0x08,0x2e,0x71,0x7f
 
 # ATT:   vucomxsd  -1024(%rdx), %xmm22
 # INTEL: vucomxsd xmm22, qword ptr [rdx - 1024]
-0x62,0xe1,0xfe,0x08,0x2e,0x72,0x80
+0x62,0xe1,0xff,0x08,0x2e,0x72,0x80
 
 # ATT:   vucomxsh %xmm23, %xmm22
 # INTEL: vucomxsh xmm22, xmm23
-0x62,0xa5,0x7f,0x08,0x2e,0xf7
+0x62,0xa5,0x7e,0x08,0x2e,0xf7
 
 # ATT:   vucomxsh {sae}, %xmm23, %xmm22
 # INTEL: vucomxsh xmm22, xmm23, {sae}
-0x62,0xa5,0x7f,0x18,0x2e,0xf7
+0x62,0xa5,0x7e,0x18,0x2e,0xf7
 
 # ATT:   vucomxsh  268435456(%rbp,%r14,8), %xmm22
 # INTEL: vucomxsh xmm22, word ptr [rbp + 8*r14 + 268435456]
-0x62,0xa5,0x7f,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa5,0x7e,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vucomxsh  291(%r8,%rax,4), %xmm22
 # INTEL: vucomxsh xmm22, word ptr [r8 + 4*rax + 291]
-0x62,0xc5,0x7f,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc5,0x7e,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vucomxsh  (%rip), %xmm22
 # INTEL: vucomxsh xmm22, word ptr [rip]
-0x62,0xe5,0x7f,0x08,0x2e,0x35,0x00,0x00,0x00,0x00
+0x62,0xe5,0x7e,0x08,0x2e,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vucomxsh  -64(,%rbp,2), %xmm22
 # INTEL: vucomxsh xmm22, word ptr [2*rbp - 64]
-0x62,0xe5,0x7f,0x08,0x2e,0x34,0x6d,0xc0,0xff,0xff,0xff
+0x62,0xe5,0x7e,0x08,0x2e,0x34,0x6d,0xc0,0xff,0xff,0xff
 
 # ATT:   vucomxsh  254(%rcx), %xmm22
 # INTEL: vucomxsh xmm22, word ptr [rcx + 254]
-0x62,0xe5,0x7f,0x08,0x2e,0x71,0x7f
+0x62,0xe5,0x7e,0x08,0x2e,0x71,0x7f
 
 # ATT:   vucomxsh  -256(%rdx), %xmm22
 # INTEL: vucomxsh xmm22, word ptr [rdx - 256]
-0x62,0xe5,0x7f,0x08,0x2e,0x72,0x80
+0x62,0xe5,0x7e,0x08,0x2e,0x72,0x80
 
 # ATT:   vucomxss %xmm23, %xmm22
 # INTEL: vucomxss xmm22, xmm23
-0x62,0xa1,0x7f,0x08,0x2e,0xf7
+0x62,0xa1,0x7e,0x08,0x2e,0xf7
 
 # ATT:   vucomxss {sae}, %xmm23, %xmm22
 # INTEL: vucomxss xmm22, xmm23, {sae}
-0x62,0xa1,0x7f,0x18,0x2e,0xf7
+0x62,0xa1,0x7e,0x18,0x2e,0xf7
 
 # ATT:   vucomxss  268435456(%rbp,%r14,8), %xmm22
 # INTEL: vucomxss xmm22, dword ptr [rbp + 8*r14 + 268435456]
-0x62,0xa1,0x7f,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10
+0x62,0xa1,0x7e,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10
 
 # ATT:   vucomxss  291(%r8,%rax,4), %xmm22
 # INTEL: vucomxss xmm22, dword ptr [r8 + 4*rax + 291]
-0x62,0xc1,0x7f,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00
+0x62,0xc1,0x7e,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00
 
 # ATT:   vucomxss  (%rip), %xmm22
 # INTEL: vucomxss xmm22, dword ptr [rip]
-0x62,0xe1,0x7f,0x08,0x2e,0x35,0x00,0x00,0x00,0x00
+0x62,0xe1,0x7e,0x08,0x2e,0x35,0x00,0x00,0x00,0x00
 
 # ATT:   vucomxss  -128(,%rbp,2), %xmm22
 # INTEL: vucomxss xmm22, dword ptr [2*rbp - 128]
-0x62,0xe1,0x7f,0x08,0x2e,0x34,0x6d,0x80,0xff,0xff,0xff
+0x62,0xe1,0x7e,0x08,0x2e,0x34,0x6d,0x80,0xff,0xff,0xff
 
 # ATT:   vucomxss  508(%rcx), %xmm22
 # INTEL: vucomxss xmm22, dword ptr [rcx + 508]
-0x62,0xe1,0x7f,0x08,0x2e,0x71,0x7f
+0x62,0xe1,0x7e,0x08,0x2e,0x71,0x7f
 
 # ATT:   vucomxss  -512(%rdx), %xmm22
 # INTEL: vucomxss xmm22, dword ptr [rdx - 512]
-0x62,0xe1,0x7f,0x08,0x2e,0x72,0x80
+0x62,0xe1,0x7e,0x08,0x2e,0x72,0x80
 
diff --git a/llvm/test/MC/Hexagon/align-leb128.s b/llvm/test/MC/Hexagon/align-leb128.s
new file mode 100644
index 000000000000..77018f011431
--- /dev/null
+++ b/llvm/test/MC/Hexagon/align-leb128.s
@@ -0,0 +1,18 @@
+# RUN: llvm-mc -triple=hexagon -filetype=obj %s | llvm-readelf -x .data - \
+# RUN:   | FileCheck %s --match-full-lines
+
+# Illustrate the case when padding packets across labels also breaks leb128
+# relocations. This happens because .align padding is inserted once at the
+# very end of the section layout.
+L1:
+  nop
+L2:
+.size L1, L2-L1
+.align 16
+  nop
+.data
+.word L2-L1
+.uleb128 L2-L1
+
+# CHECK: Hex dump of section '.data':
+# CHECK-NEXT: 0x00000000 04000000 04 .....
diff --git a/llvm/test/MC/Hexagon/align.s b/llvm/test/MC/Hexagon/align.s
index 9c2978df7137..e17d09cfd8c9 100644
--- a/llvm/test/MC/Hexagon/align.s
+++ b/llvm/test/MC/Hexagon/align.s
@@ -58,3 +58,16 @@ r0 = vextract(v0, r0)
   r1 = sub (##1, r1) }
 .align 16
 { r0 = sub (#1, r0) }
+
+# Don't search backwards to pad packets beyond a label:
+{ r1 = add(r1, r0) }
+# CHECK-NEXT: { r1 = add(r1,r0)
+# CHECK-NOT:  nop
+
+post_label:
+.align 16
+# CHECK-LABEL: post_label
+# CHECK-NEXT: { nop
+# CHECK-NEXT:   nop }
+# CHECK-NEXT: { r1 = sub(#1,r1) }
+{ r1 = sub(#1, r1) }
diff --git a/llvm/test/MC/Hexagon/arch-support.s b/llvm/test/MC/Hexagon/arch-support.s
index 99364cc93691..eb362a7db3ca 100644
--- a/llvm/test/MC/Hexagon/arch-support.s
+++ b/llvm/test/MC/Hexagon/arch-support.s
@@ -11,6 +11,9 @@
 # RUN: llvm-mc -triple=hexagon -mv75 -filetype=obj %s | llvm-readelf -h - | FileCheck --check-prefix=CHECK-V75 %s
 # RUN: llvm-mc -triple=hexagon -mv79 -filetype=obj %s | llvm-readelf -h - | FileCheck --check-prefix=CHECK-V79 %s
 
+## Check which arch version llvm-mc sets when the user does not provide one.
+# RUN: llvm-mc -triple=hexagon -filetype=obj %s | llvm-readelf -h - | FileCheck --check-prefix=CHECK-DEFAULT %s
+
 # RUN: llvm-mc -triple=hexagon -mv5 -filetype=obj %s | llvm-objdump --disassemble - | FileCheck --check-prefix=CHECK-OBJDUMP %s
 # RUN: llvm-mc -triple=hexagon -mv55 -filetype=obj %s | llvm-objdump --disassemble - | FileCheck --check-prefix=CHECK-OBJDUMP %s
 # RUN: llvm-mc -triple=hexagon -mv60 -filetype=obj %s | llvm-objdump --disassemble - | FileCheck --check-prefix=CHECK-OBJDUMP %s
@@ -38,5 +41,6 @@ r1 = r1
 # CHECK-V73: Flags:{{.*}}0x73
 # CHECK-V75: Flags:{{.*}}0x75
 # CHECK-V79: Flags:{{.*}}0x79
+# CHECK-DEFAULT: Flags:{{.*}}0x68
 
 # CHECK-OBJDUMP: { r1 = r1 }
diff --git a/llvm/test/MC/Hexagon/hexagon_attributes.s b/llvm/test/MC/Hexagon/hexagon_attributes.s
index 4cd5223cd220..8d96993eee99 100644
--- a/llvm/test/MC/Hexagon/hexagon_attributes.s
+++ b/llvm/test/MC/Hexagon/hexagon_attributes.s
@@ -5,8 +5,11 @@ r3:2=cround(r1:0,#0x0)       // v67, audio
 v3:0.w=vrmpyz(v0.b,r0.b)     // hvxv73, zreg
 v1:0.sf=vadd(v0.bf,v0.bf)    // hvxv73, hvx-ieee-fp
 
-// RUN: llvm-mc --mattr=+v67,+hvxv73,+hvx-qfloat,+hvx-ieee-fp,+zreg,+audio %s \
-// RUN:   -triple=hexagon -filetype=obj --hexagon-add-build-attributes -o %t.o
+// Note that the CPU version should be set with `--mcpu` and not with attributes
+// because attributes are additive.
+// RUN: llvm-mc -triple=hexagon --mcpu=hexagonv67 \
+// RUN:   --mattr=+hvxv73,+hvx-qfloat,+hvx-ieee-fp,+zreg,+audio %s \
+// RUN:   -filetype=obj --hexagon-add-build-attributes -o %t.o
 
 // RUN: llvm-readelf -A %t.o | \
 // RUN:   FileCheck %s --match-full-lines --implicit-check-not={{.}} --check-prefix=READELF
@@ -15,8 +18,9 @@ v1:0.sf=vadd(v0.bf,v0.bf)    // hvxv73, hvx-ieee-fp
 /// without manually passing in features when an attribute section is present.
 // RUN: llvm-objdump -d %t.o | FileCheck %s --check-prefix=OBJDUMP
 
-// RUN: llvm-mc --mattr=+v67,+hvxv73,+hvx-qfloat,+hvx-ieee-fp,+zreg,+audio %s \
-// RUN:   -triple=hexagon -filetype=asm --hexagon-add-build-attributes | \
+// RUN: llvm-mc -triple=hexagon --mcpu=hexagonv67 \
+// RUN:   --mattr=+hvxv73,+hvx-qfloat,+hvx-ieee-fp,+zreg,+audio %s \
+// RUN:   -filetype=asm --hexagon-add-build-attributes | \
 // RUN:     FileCheck %s --match-full-lines --implicit-check-not={{.}} --check-prefix=ASM
 
 //      READELF: BuildAttributes {
diff --git a/llvm/test/MC/LoongArch/Relocations/relocation-specifier.s b/llvm/test/MC/LoongArch/Relocations/relocation-specifier.s
new file mode 100644
index 000000000000..d0898aaab92f
--- /dev/null
+++ b/llvm/test/MC/LoongArch/Relocations/relocation-specifier.s
@@ -0,0 +1,26 @@
+# RUN: llvm-mc --filetype=obj --triple=loongarch32 %s -o %t-la32
+# RUN: llvm-readelf -rs %t-la32 | FileCheck %s --check-prefixes=CHECK,RELOC32
+# RUN: llvm-mc --filetype=obj --triple=loongarch64 %s -o %t-la64
+# RUN: llvm-readelf -rs %t-la64 | FileCheck %s --check-prefixes=CHECK,RELOC64
+
+## This test is similar to test/MC/CSKY/relocation-specifier.s.
+
+# RELOC32: '.rela.data'
+# RELOC32: R_LARCH_32 00000000 .data + 0
+
+# RELOC64: '.rela.data'
+# RELOC64: R_LARCH_32 0000000000000000 .data + 0
+
+# CHECK: TLS GLOBAL DEFAULT UND gd
+# CHECK: TLS GLOBAL DEFAULT UND ld
+# CHECK: TLS GLOBAL DEFAULT UND ie
+# CHECK: TLS GLOBAL DEFAULT UND le
+
+pcalau12i $t1, %gd_pc_hi20(gd)
+pcalau12i $t1, %ld_pc_hi20(ld)
+pcalau12i $t1, %ie_pc_hi20(ie)
+lu12i.w $t1, %le_hi20_r(le)
+
+.data
+local:
+.long local
diff --git a/llvm/test/MC/LoongArch/Relocations/relocations.s b/llvm/test/MC/LoongArch/Relocations/relocations.s
index 091dce200b7d..f91a941295d9 100644
--- a/llvm/test/MC/LoongArch/Relocations/relocations.s
+++ b/llvm/test/MC/LoongArch/Relocations/relocations.s
@@ -308,3 +308,33 @@ pcaddi $t1, %desc_pcrel_20(foo)
 # RELOC: R_LARCH_TLS_DESC_PCREL20_S2 foo 0x0
 # INSTR: pcaddi $t1, %desc_pcrel_20(foo)
 # FIXUP: fixup A - offset: 0, value: %desc_pcrel_20(foo), kind: FK_NONE
+
+fld.s $ft1, $a0, %pc_lo12(foo)
+# RELOC: R_LARCH_PCALA_LO12 foo 0x0
+# INSTR: fld.s $ft1, $a0, %pc_lo12(foo)
+# FIXUP: fixup A - offset: 0, value: %pc_lo12(foo), kind: FK_NONE
+
+fst.d $ft1, $a0, %pc_lo12(foo)
+# RELOC: R_LARCH_PCALA_LO12 foo 0x0
+# INSTR: fst.d $ft1, $a0, %pc_lo12(foo)
+# FIXUP: fixup A - offset: 0, value: %pc_lo12(foo), kind: FK_NONE
+
+vld $vr9, $a0, %pc_lo12(foo)
+# RELOC: R_LARCH_PCALA_LO12 foo 0x0
+# INSTR: vld $vr9, $a0, %pc_lo12(foo)
+# FIXUP: fixup A - offset: 0, value: %pc_lo12(foo), kind: FK_NONE
+
+vst $vr9, $a0, %pc_lo12(foo)
+# RELOC: R_LARCH_PCALA_LO12 foo 0x0
+# INSTR: vst $vr9, $a0, %pc_lo12(foo)
+# FIXUP: fixup A - offset: 0, value: %pc_lo12(foo), kind: FK_NONE
+
+xvld $xr9, $a0, %pc_lo12(foo)
+# RELOC: R_LARCH_PCALA_LO12 foo 0x0
+# INSTR: xvld $xr9, $a0, %pc_lo12(foo)
+# FIXUP: fixup A - offset: 0, value: %pc_lo12(foo), kind: FK_NONE
+
+xvst $xr9, $a0, %pc_lo12(foo)
+# RELOC: R_LARCH_PCALA_LO12 foo 0x0
+# INSTR: xvst $xr9, $a0, %pc_lo12(foo)
+# FIXUP: fixup A - offset: 0, value: %pc_lo12(foo), kind: FK_NONE
diff --git a/llvm/test/MC/LoongArch/lasx/invalid-imm.s b/llvm/test/MC/LoongArch/lasx/invalid-imm.s
index 6f64a6f87802..adfd35367d7b 100644
--- a/llvm/test/MC/LoongArch/lasx/invalid-imm.s
+++ b/llvm/test/MC/LoongArch/lasx/invalid-imm.s
@@ -1167,22 +1167,22 @@ xvldrepl.h $xr0, $a0, 2048
 
 ## simm12
 xvldrepl.b $xr0, $a0, -2049
-# CHECK: :[[#@LINE-1]]:23: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:23: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 xvldrepl.b $xr0, $a0, 2048
-# CHECK: :[[#@LINE-1]]:23: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:23: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 xvst $xr0, $a0, -2049
-# CHECK: :[[#@LINE-1]]:17: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:17: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 xvst $xr0, $a0, 2048
-# CHECK: :[[#@LINE-1]]:17: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:17: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 xvld $xr0, $a0, -2049
-# CHECK: :[[#@LINE-1]]:17: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:17: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 xvld $xr0, $a0, 2048
-# CHECK: :[[#@LINE-1]]:17: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:17: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 ## simm13
 xvldi $xr0, -4097
diff --git a/llvm/test/MC/LoongArch/lsx/invalid-imm.s b/llvm/test/MC/LoongArch/lsx/invalid-imm.s
index c3f9aaa08281..61fbac06794c 100644
--- a/llvm/test/MC/LoongArch/lsx/invalid-imm.s
+++ b/llvm/test/MC/LoongArch/lsx/invalid-imm.s
@@ -1167,22 +1167,22 @@ vldrepl.h $vr0, $a0, 2048
 
 ## simm12
 vldrepl.b $vr0, $a0, -2049
-# CHECK: :[[#@LINE-1]]:22: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:22: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 vldrepl.b $vr0, $a0, 2048
-# CHECK: :[[#@LINE-1]]:22: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:22: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 vst $vr0, $a0, -2049
-# CHECK: :[[#@LINE-1]]:16: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:16: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 vst $vr0, $a0, 2048
-# CHECK: :[[#@LINE-1]]:16: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:16: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 vld $vr0, $a0, -2049
-# CHECK: :[[#@LINE-1]]:16: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:16: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 vld $vr0, $a0, 2048
-# CHECK: :[[#@LINE-1]]:16: error: immediate must be an integer in the range [-2048, 2047]
+# CHECK: :[[#@LINE-1]]:16: error: operand must be a symbol with modifier (e.g. %pc_lo12) or an integer in the range [-2048, 2047]
 
 ## simm13
 vldi $vr0, -4097
diff --git a/llvm/test/MC/SystemZ/insn-bad-arch15.s b/llvm/test/MC/SystemZ/insn-bad-z17.s
similarity index 98%
rename from llvm/test/MC/SystemZ/insn-bad-arch15.s
rename to llvm/test/MC/SystemZ/insn-bad-z17.s
index 915efbc94230..02e26220490f 100644
--- a/llvm/test/MC/SystemZ/insn-bad-arch15.s
+++ b/llvm/test/MC/SystemZ/insn-bad-z17.s
@@ -1,5 +1,5 @@
-# For arch15 only.
-# RUN: not llvm-mc -triple s390x-linux-gnu -mcpu=arch15 < %s 2> %t
+# For z17 only.
+# RUN: not llvm-mc -triple s390x-linux-gnu -mcpu=z17 < %s 2> %t
 # RUN: FileCheck < %t %s
 
 #CHECK: error: invalid use of indexed addressing
diff --git a/llvm/test/MC/SystemZ/insn-good-arch15.s b/llvm/test/MC/SystemZ/insn-good-z17.s
similarity index 99%
rename from llvm/test/MC/SystemZ/insn-good-arch15.s
rename to llvm/test/MC/SystemZ/insn-good-z17.s
index 46ff13db0b54..96f27137e482 100644
--- a/llvm/test/MC/SystemZ/insn-good-arch15.s
+++ b/llvm/test/MC/SystemZ/insn-good-z17.s
@@ -1,5 +1,5 @@
-# For arch15 and above.
-# RUN: llvm-mc -triple s390x-linux-gnu -mcpu=arch15 -show-encoding %s \
+# For z17 and above.
+# RUN: llvm-mc -triple s390x-linux-gnu -mcpu=z17 -show-encoding %s \
 # RUN:   | FileCheck %s
 
 #CHECK: bdepg	%r0, %r0, %r0           # encoding: [0xb9,0x6d,0x00,0x00]
diff --git a/llvm/test/MC/X86/avx10.2-bf16-32-att.s b/llvm/test/MC/X86/avx10.2-bf16-32-att.s
index e1e82623d838..88433d7a3411 100644
--- a/llvm/test/MC/X86/avx10.2-bf16-32-att.s
+++ b/llvm/test/MC/X86/avx10.2-bf16-32-att.s
@@ -1717,111 +1717,111 @@
           vfpclassbf16  $123, -256(%edx){1to32}, %k5 {%k7}
 
 // CHECK: vgetexpbf16 %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x08,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x08,0x42,0xd3]
           vgetexpbf16 %xmm3, %xmm2
 
 // CHECK: vgetexpbf16 %xmm3, %xmm2 {%k7}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x0f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x0f,0x42,0xd3]
           vgetexpbf16 %xmm3, %xmm2 {%k7}
 
 // CHECK: vgetexpbf16 %xmm3, %xmm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x8f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x8f,0x42,0xd3]
           vgetexpbf16 %xmm3, %xmm2 {%k7} {z}
 
 // CHECK: vgetexpbf16 %zmm3, %zmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x48,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x48,0x42,0xd3]
           vgetexpbf16 %zmm3, %zmm2
 
 // CHECK: vgetexpbf16 %zmm3, %zmm2 {%k7}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x4f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x4f,0x42,0xd3]
           vgetexpbf16 %zmm3, %zmm2 {%k7}
 
 // CHECK: vgetexpbf16 %zmm3, %zmm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xcf,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xcf,0x42,0xd3]
           vgetexpbf16 %zmm3, %zmm2 {%k7} {z}
 
 // CHECK: vgetexpbf16 %ymm3, %ymm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x28,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x28,0x42,0xd3]
           vgetexpbf16 %ymm3, %ymm2
 
 // CHECK: vgetexpbf16 %ymm3, %ymm2 {%k7}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x2f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x2f,0x42,0xd3]
           vgetexpbf16 %ymm3, %ymm2 {%k7}
 
 // CHECK: vgetexpbf16 %ymm3, %ymm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xaf,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xaf,0x42,0xd3]
           vgetexpbf16 %ymm3, %ymm2 {%k7} {z}
 
 // CHECK: vgetexpbf16  268435456(%esp,%esi,8), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x08,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x08,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
           vgetexpbf16  268435456(%esp,%esi,8), %xmm2
 
 // CHECK: vgetexpbf16  291(%edi,%eax,4), %xmm2 {%k7}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x0f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x0f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
           vgetexpbf16  291(%edi,%eax,4), %xmm2 {%k7}
 
 // CHECK: vgetexpbf16  (%eax){1to8}, %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x18,0x42,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x18,0x42,0x10]
           vgetexpbf16  (%eax){1to8}, %xmm2
 
 // CHECK: vgetexpbf16  -512(,%ebp,2), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x08,0x42,0x14,0x6d,0x00,0xfe,0xff,0xff]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x08,0x42,0x14,0x6d,0x00,0xfe,0xff,0xff]
           vgetexpbf16  -512(,%ebp,2), %xmm2
 
 // CHECK: vgetexpbf16  2032(%ecx), %xmm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x8f,0x42,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x8f,0x42,0x51,0x7f]
           vgetexpbf16  2032(%ecx), %xmm2 {%k7} {z}
 
 // CHECK: vgetexpbf16  -256(%edx){1to8}, %xmm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x9f,0x42,0x52,0x80]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x9f,0x42,0x52,0x80]
           vgetexpbf16  -256(%edx){1to8}, %xmm2 {%k7} {z}
 
 // CHECK: vgetexpbf16  268435456(%esp,%esi,8), %ymm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x28,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x28,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
           vgetexpbf16  268435456(%esp,%esi,8), %ymm2
 
 // CHECK: vgetexpbf16  291(%edi,%eax,4), %ymm2 {%k7}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x2f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x2f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
           vgetexpbf16  291(%edi,%eax,4), %ymm2 {%k7}
 
 // CHECK: vgetexpbf16  (%eax){1to16}, %ymm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x38,0x42,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x38,0x42,0x10]
           vgetexpbf16  (%eax){1to16}, %ymm2
 
 // CHECK: vgetexpbf16  -1024(,%ebp,2), %ymm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x28,0x42,0x14,0x6d,0x00,0xfc,0xff,0xff]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x28,0x42,0x14,0x6d,0x00,0xfc,0xff,0xff]
           vgetexpbf16  -1024(,%ebp,2), %ymm2
 
 // CHECK: vgetexpbf16  4064(%ecx), %ymm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xaf,0x42,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xaf,0x42,0x51,0x7f]
           vgetexpbf16  4064(%ecx), %ymm2 {%k7} {z}
 
 // CHECK: vgetexpbf16  -256(%edx){1to16}, %ymm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xbf,0x42,0x52,0x80]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xbf,0x42,0x52,0x80]
           vgetexpbf16  -256(%edx){1to16}, %ymm2 {%k7} {z}
 
 // CHECK: vgetexpbf16  268435456(%esp,%esi,8), %zmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x48,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x48,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
           vgetexpbf16  268435456(%esp,%esi,8), %zmm2
 
 // CHECK: vgetexpbf16  291(%edi,%eax,4), %zmm2 {%k7}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x4f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x4f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
           vgetexpbf16  291(%edi,%eax,4), %zmm2 {%k7}
 
 // CHECK: vgetexpbf16  (%eax){1to32}, %zmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x58,0x42,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x58,0x42,0x10]
           vgetexpbf16  (%eax){1to32}, %zmm2
 
 // CHECK: vgetexpbf16  -2048(,%ebp,2), %zmm2
-// CHECK: encoding: [0x62,0xf5,0x7d,0x48,0x42,0x14,0x6d,0x00,0xf8,0xff,0xff]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x48,0x42,0x14,0x6d,0x00,0xf8,0xff,0xff]
           vgetexpbf16  -2048(,%ebp,2), %zmm2
 
 // CHECK: vgetexpbf16  8128(%ecx), %zmm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xcf,0x42,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xcf,0x42,0x51,0x7f]
           vgetexpbf16  8128(%ecx), %zmm2 {%k7} {z}
 
 // CHECK: vgetexpbf16  -256(%edx){1to32}, %zmm2 {%k7} {z}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xdf,0x42,0x52,0x80]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xdf,0x42,0x52,0x80]
           vgetexpbf16  -256(%edx){1to32}, %zmm2 {%k7} {z}
 
 // CHECK: vgetmantbf16 $123, %zmm3, %zmm2
diff --git a/llvm/test/MC/X86/avx10.2-bf16-32-intel.s b/llvm/test/MC/X86/avx10.2-bf16-32-intel.s
index d2e9440ba9c3..7e1d0c305336 100644
--- a/llvm/test/MC/X86/avx10.2-bf16-32-intel.s
+++ b/llvm/test/MC/X86/avx10.2-bf16-32-intel.s
@@ -1717,111 +1717,111 @@
           vfpclassbf16 k5 {k7}, word ptr [edx - 256]{1to32}, 123
 
 // CHECK: vgetexpbf16 xmm2, xmm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0x08,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x08,0x42,0xd3]
           vgetexpbf16 xmm2, xmm3
 
 // CHECK: vgetexpbf16 xmm2 {k7}, xmm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0x0f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x0f,0x42,0xd3]
           vgetexpbf16 xmm2 {k7}, xmm3
 
 // CHECK: vgetexpbf16 xmm2 {k7} {z}, xmm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0x8f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x8f,0x42,0xd3]
           vgetexpbf16 xmm2 {k7} {z}, xmm3
 
 // CHECK: vgetexpbf16 zmm2, zmm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0x48,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x48,0x42,0xd3]
           vgetexpbf16 zmm2, zmm3
 
 // CHECK: vgetexpbf16 zmm2 {k7}, zmm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0x4f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x4f,0x42,0xd3]
           vgetexpbf16 zmm2 {k7}, zmm3
 
 // CHECK: vgetexpbf16 zmm2 {k7} {z}, zmm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0xcf,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xcf,0x42,0xd3]
           vgetexpbf16 zmm2 {k7} {z}, zmm3
 
 // CHECK: vgetexpbf16 ymm2, ymm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0x28,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x28,0x42,0xd3]
           vgetexpbf16 ymm2, ymm3
 
 // CHECK: vgetexpbf16 ymm2 {k7}, ymm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0x2f,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x2f,0x42,0xd3]
           vgetexpbf16 ymm2 {k7}, ymm3
 
 // CHECK: vgetexpbf16 ymm2 {k7} {z}, ymm3
-// CHECK: encoding: [0x62,0xf5,0x7d,0xaf,0x42,0xd3]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xaf,0x42,0xd3]
           vgetexpbf16 ymm2 {k7} {z}, ymm3
 
 // CHECK: vgetexpbf16 xmm2, xmmword ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x08,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x08,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
           vgetexpbf16 xmm2, xmmword ptr [esp + 8*esi + 268435456]
 
 // CHECK: vgetexpbf16 xmm2 {k7}, xmmword ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x0f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x0f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
           vgetexpbf16 xmm2 {k7}, xmmword ptr [edi + 4*eax + 291]
 
 // CHECK: vgetexpbf16 xmm2, word ptr [eax]{1to8}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x18,0x42,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x18,0x42,0x10]
           vgetexpbf16 xmm2, word ptr [eax]{1to8}
 
 // CHECK: vgetexpbf16 xmm2, xmmword ptr [2*ebp - 512]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x08,0x42,0x14,0x6d,0x00,0xfe,0xff,0xff]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x08,0x42,0x14,0x6d,0x00,0xfe,0xff,0xff]
           vgetexpbf16 xmm2, xmmword ptr [2*ebp - 512]
 
 // CHECK: vgetexpbf16 xmm2 {k7} {z}, xmmword ptr [ecx + 2032]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x8f,0x42,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x8f,0x42,0x51,0x7f]
           vgetexpbf16 xmm2 {k7} {z}, xmmword ptr [ecx + 2032]
 
 // CHECK: vgetexpbf16 xmm2 {k7} {z}, word ptr [edx - 256]{1to8}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x9f,0x42,0x52,0x80]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x9f,0x42,0x52,0x80]
           vgetexpbf16 xmm2 {k7} {z}, word ptr [edx - 256]{1to8}
 
 // CHECK: vgetexpbf16 ymm2, ymmword ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x28,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x28,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
           vgetexpbf16 ymm2, ymmword ptr [esp + 8*esi + 268435456]
 
 // CHECK: vgetexpbf16 ymm2 {k7}, ymmword ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x2f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x2f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
           vgetexpbf16 ymm2 {k7}, ymmword ptr [edi + 4*eax + 291]
 
 // CHECK: vgetexpbf16 ymm2, word ptr [eax]{1to16}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x38,0x42,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x38,0x42,0x10]
           vgetexpbf16 ymm2, word ptr [eax]{1to16}
 
 // CHECK: vgetexpbf16 ymm2, ymmword ptr [2*ebp - 1024]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x28,0x42,0x14,0x6d,0x00,0xfc,0xff,0xff]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x28,0x42,0x14,0x6d,0x00,0xfc,0xff,0xff]
           vgetexpbf16 ymm2, ymmword ptr [2*ebp - 1024]
 
 // CHECK: vgetexpbf16 ymm2 {k7} {z}, ymmword ptr [ecx + 4064]
-// CHECK: encoding: [0x62,0xf5,0x7d,0xaf,0x42,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xaf,0x42,0x51,0x7f]
           vgetexpbf16 ymm2 {k7} {z}, ymmword ptr [ecx + 4064]
 
 // CHECK: vgetexpbf16 ymm2 {k7} {z}, word ptr [edx - 256]{1to16}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xbf,0x42,0x52,0x80]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xbf,0x42,0x52,0x80]
           vgetexpbf16 ymm2 {k7} {z}, word ptr [edx - 256]{1to16}
 
 // CHECK: vgetexpbf16 zmm2, zmmword ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x48,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x48,0x42,0x94,0xf4,0x00,0x00,0x00,0x10]
           vgetexpbf16 zmm2, zmmword ptr [esp + 8*esi + 268435456]
 
 // CHECK: vgetexpbf16 zmm2 {k7}, zmmword ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x4f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x4f,0x42,0x94,0x87,0x23,0x01,0x00,0x00]
           vgetexpbf16 zmm2 {k7}, zmmword ptr [edi + 4*eax + 291]
 
 // CHECK: vgetexpbf16 zmm2, word ptr [eax]{1to32}
-// CHECK: encoding: [0x62,0xf5,0x7d,0x58,0x42,0x10]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x58,0x42,0x10]
           vgetexpbf16 zmm2, word ptr [eax]{1to32}
 
 // CHECK: vgetexpbf16 zmm2, zmmword ptr [2*ebp - 2048]
-// CHECK: encoding: [0x62,0xf5,0x7d,0x48,0x42,0x14,0x6d,0x00,0xf8,0xff,0xff]
+// CHECK: encoding: [0x62,0xf6,0x7c,0x48,0x42,0x14,0x6d,0x00,0xf8,0xff,0xff]
           vgetexpbf16 zmm2, zmmword ptr [2*ebp - 2048]
 
 // CHECK: vgetexpbf16 zmm2 {k7} {z}, zmmword ptr [ecx + 8128]
-// CHECK: encoding: [0x62,0xf5,0x7d,0xcf,0x42,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xcf,0x42,0x51,0x7f]
           vgetexpbf16 zmm2 {k7} {z}, zmmword ptr [ecx + 8128]
 
 // CHECK: vgetexpbf16 zmm2 {k7} {z}, word ptr [edx - 256]{1to32}
-// CHECK: encoding: [0x62,0xf5,0x7d,0xdf,0x42,0x52,0x80]
+// CHECK: encoding: [0x62,0xf6,0x7c,0xdf,0x42,0x52,0x80]
           vgetexpbf16 zmm2 {k7} {z}, word ptr [edx - 256]{1to32}
 
 // CHECK: vgetmantbf16 zmm2, zmm3, 123
diff --git a/llvm/test/MC/X86/avx10.2-bf16-64-att.s b/llvm/test/MC/X86/avx10.2-bf16-64-att.s
index 67d6f3a531df..0eb10fbf6d86 100644
--- a/llvm/test/MC/X86/avx10.2-bf16-64-att.s
+++ b/llvm/test/MC/X86/avx10.2-bf16-64-att.s
@@ -1717,111 +1717,111 @@
           vfpclassbf16  $123, -256(%rdx){1to32}, %k5 {%k7}
 
 // CHECK: vgetexpbf16 %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7d,0x08,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x08,0x42,0xf7]
           vgetexpbf16 %xmm23, %xmm22
 
 // CHECK: vgetexpbf16 %xmm23, %xmm22 {%k7}
-// CHECK: encoding: [0x62,0xa5,0x7d,0x0f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x0f,0x42,0xf7]
           vgetexpbf16 %xmm23, %xmm22 {%k7}
 
 // CHECK: vgetexpbf16 %xmm23, %xmm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xa5,0x7d,0x8f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x8f,0x42,0xf7]
           vgetexpbf16 %xmm23, %xmm22 {%k7} {z}
 
 // CHECK: vgetexpbf16 %zmm23, %zmm22
-// CHECK: encoding: [0x62,0xa5,0x7d,0x48,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x48,0x42,0xf7]
           vgetexpbf16 %zmm23, %zmm22
 
 // CHECK: vgetexpbf16 %zmm23, %zmm22 {%k7}
-// CHECK: encoding: [0x62,0xa5,0x7d,0x4f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x4f,0x42,0xf7]
           vgetexpbf16 %zmm23, %zmm22 {%k7}
 
 // CHECK: vgetexpbf16 %zmm23, %zmm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xa5,0x7d,0xcf,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0xcf,0x42,0xf7]
           vgetexpbf16 %zmm23, %zmm22 {%k7} {z}
 
 // CHECK: vgetexpbf16 %ymm23, %ymm22
-// CHECK: encoding: [0x62,0xa5,0x7d,0x28,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x28,0x42,0xf7]
           vgetexpbf16 %ymm23, %ymm22
 
 // CHECK: vgetexpbf16 %ymm23, %ymm22 {%k7}
-// CHECK: encoding: [0x62,0xa5,0x7d,0x2f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x2f,0x42,0xf7]
           vgetexpbf16 %ymm23, %ymm22 {%k7}
 
 // CHECK: vgetexpbf16 %ymm23, %ymm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xa5,0x7d,0xaf,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0xaf,0x42,0xf7]
           vgetexpbf16 %ymm23, %ymm22 {%k7} {z}
 
 // CHECK: vgetexpbf16  268435456(%rbp,%r14,8), %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7d,0x08,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x08,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vgetexpbf16  268435456(%rbp,%r14,8), %xmm22
 
 // CHECK: vgetexpbf16  291(%r8,%rax,4), %xmm22 {%k7}
-// CHECK: encoding: [0x62,0xc5,0x7d,0x0f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc6,0x7c,0x0f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
           vgetexpbf16  291(%r8,%rax,4), %xmm22 {%k7}
 
 // CHECK: vgetexpbf16  (%rip){1to8}, %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7d,0x18,0x42,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x18,0x42,0x35,0x00,0x00,0x00,0x00]
           vgetexpbf16  (%rip){1to8}, %xmm22
 
 // CHECK: vgetexpbf16  -512(,%rbp,2), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7d,0x08,0x42,0x34,0x6d,0x00,0xfe,0xff,0xff]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x08,0x42,0x34,0x6d,0x00,0xfe,0xff,0xff]
           vgetexpbf16  -512(,%rbp,2), %xmm22
 
 // CHECK: vgetexpbf16  2032(%rcx), %xmm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xe5,0x7d,0x8f,0x42,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x8f,0x42,0x71,0x7f]
           vgetexpbf16  2032(%rcx), %xmm22 {%k7} {z}
 
 // CHECK: vgetexpbf16  -256(%rdx){1to8}, %xmm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xe5,0x7d,0x9f,0x42,0x72,0x80]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x9f,0x42,0x72,0x80]
           vgetexpbf16  -256(%rdx){1to8}, %xmm22 {%k7} {z}
 
 // CHECK: vgetexpbf16  268435456(%rbp,%r14,8), %ymm22
-// CHECK: encoding: [0x62,0xa5,0x7d,0x28,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x28,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vgetexpbf16  268435456(%rbp,%r14,8), %ymm22
 
 // CHECK: vgetexpbf16  291(%r8,%rax,4), %ymm22 {%k7}
-// CHECK: encoding: [0x62,0xc5,0x7d,0x2f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc6,0x7c,0x2f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
           vgetexpbf16  291(%r8,%rax,4), %ymm22 {%k7}
 
 // CHECK: vgetexpbf16  (%rip){1to16}, %ymm22
-// CHECK: encoding: [0x62,0xe5,0x7d,0x38,0x42,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x38,0x42,0x35,0x00,0x00,0x00,0x00]
           vgetexpbf16  (%rip){1to16}, %ymm22
 
 // CHECK: vgetexpbf16  -1024(,%rbp,2), %ymm22
-// CHECK: encoding: [0x62,0xe5,0x7d,0x28,0x42,0x34,0x6d,0x00,0xfc,0xff,0xff]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x28,0x42,0x34,0x6d,0x00,0xfc,0xff,0xff]
           vgetexpbf16  -1024(,%rbp,2), %ymm22
 
 // CHECK: vgetexpbf16  4064(%rcx), %ymm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xe5,0x7d,0xaf,0x42,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xaf,0x42,0x71,0x7f]
           vgetexpbf16  4064(%rcx), %ymm22 {%k7} {z}
 
 // CHECK: vgetexpbf16  -256(%rdx){1to16}, %ymm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xe5,0x7d,0xbf,0x42,0x72,0x80]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xbf,0x42,0x72,0x80]
           vgetexpbf16  -256(%rdx){1to16}, %ymm22 {%k7} {z}
 
 // CHECK: vgetexpbf16  268435456(%rbp,%r14,8), %zmm22
-// CHECK: encoding: [0x62,0xa5,0x7d,0x48,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x48,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vgetexpbf16  268435456(%rbp,%r14,8), %zmm22
 
 // CHECK: vgetexpbf16  291(%r8,%rax,4), %zmm22 {%k7}
-// CHECK: encoding: [0x62,0xc5,0x7d,0x4f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc6,0x7c,0x4f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
           vgetexpbf16  291(%r8,%rax,4), %zmm22 {%k7}
 
 // CHECK: vgetexpbf16  (%rip){1to32}, %zmm22
-// CHECK: encoding: [0x62,0xe5,0x7d,0x58,0x42,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x58,0x42,0x35,0x00,0x00,0x00,0x00]
           vgetexpbf16  (%rip){1to32}, %zmm22
 
 // CHECK: vgetexpbf16  -2048(,%rbp,2), %zmm22
-// CHECK: encoding: [0x62,0xe5,0x7d,0x48,0x42,0x34,0x6d,0x00,0xf8,0xff,0xff]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x48,0x42,0x34,0x6d,0x00,0xf8,0xff,0xff]
           vgetexpbf16  -2048(,%rbp,2), %zmm22
 
 // CHECK: vgetexpbf16  8128(%rcx), %zmm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xe5,0x7d,0xcf,0x42,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xcf,0x42,0x71,0x7f]
           vgetexpbf16  8128(%rcx), %zmm22 {%k7} {z}
 
 // CHECK: vgetexpbf16  -256(%rdx){1to32}, %zmm22 {%k7} {z}
-// CHECK: encoding: [0x62,0xe5,0x7d,0xdf,0x42,0x72,0x80]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xdf,0x42,0x72,0x80]
           vgetexpbf16  -256(%rdx){1to32}, %zmm22 {%k7} {z}
 
 // CHECK: vgetmantbf16 $123, %zmm23, %zmm22
diff --git a/llvm/test/MC/X86/avx10.2-bf16-64-intel.s b/llvm/test/MC/X86/avx10.2-bf16-64-intel.s
index d1727c586e24..b0787a60c714 100644
--- a/llvm/test/MC/X86/avx10.2-bf16-64-intel.s
+++ b/llvm/test/MC/X86/avx10.2-bf16-64-intel.s
@@ -1717,111 +1717,111 @@
           vfpclassbf16 k5 {k7}, word ptr [rdx - 256]{1to32}, 123
 
 // CHECK: vgetexpbf16 xmm22, xmm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0x08,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x08,0x42,0xf7]
           vgetexpbf16 xmm22, xmm23
 
 // CHECK: vgetexpbf16 xmm22 {k7}, xmm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0x0f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x0f,0x42,0xf7]
           vgetexpbf16 xmm22 {k7}, xmm23
 
 // CHECK: vgetexpbf16 xmm22 {k7} {z}, xmm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0x8f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x8f,0x42,0xf7]
           vgetexpbf16 xmm22 {k7} {z}, xmm23
 
 // CHECK: vgetexpbf16 zmm22, zmm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0x48,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x48,0x42,0xf7]
           vgetexpbf16 zmm22, zmm23
 
 // CHECK: vgetexpbf16 zmm22 {k7}, zmm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0x4f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x4f,0x42,0xf7]
           vgetexpbf16 zmm22 {k7}, zmm23
 
 // CHECK: vgetexpbf16 zmm22 {k7} {z}, zmm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0xcf,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0xcf,0x42,0xf7]
           vgetexpbf16 zmm22 {k7} {z}, zmm23
 
 // CHECK: vgetexpbf16 ymm22, ymm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0x28,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x28,0x42,0xf7]
           vgetexpbf16 ymm22, ymm23
 
 // CHECK: vgetexpbf16 ymm22 {k7}, ymm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0x2f,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x2f,0x42,0xf7]
           vgetexpbf16 ymm22 {k7}, ymm23
 
 // CHECK: vgetexpbf16 ymm22 {k7} {z}, ymm23
-// CHECK: encoding: [0x62,0xa5,0x7d,0xaf,0x42,0xf7]
+// CHECK: encoding: [0x62,0xa6,0x7c,0xaf,0x42,0xf7]
           vgetexpbf16 ymm22 {k7} {z}, ymm23
 
 // CHECK: vgetexpbf16 xmm22, xmmword ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa5,0x7d,0x08,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x08,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vgetexpbf16 xmm22, xmmword ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vgetexpbf16 xmm22 {k7}, xmmword ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc5,0x7d,0x0f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc6,0x7c,0x0f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
           vgetexpbf16 xmm22 {k7}, xmmword ptr [r8 + 4*rax + 291]
 
 // CHECK: vgetexpbf16 xmm22, word ptr [rip]{1to8}
-// CHECK: encoding: [0x62,0xe5,0x7d,0x18,0x42,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x18,0x42,0x35,0x00,0x00,0x00,0x00]
           vgetexpbf16 xmm22, word ptr [rip]{1to8}
 
 // CHECK: vgetexpbf16 xmm22, xmmword ptr [2*rbp - 512]
-// CHECK: encoding: [0x62,0xe5,0x7d,0x08,0x42,0x34,0x6d,0x00,0xfe,0xff,0xff]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x08,0x42,0x34,0x6d,0x00,0xfe,0xff,0xff]
           vgetexpbf16 xmm22, xmmword ptr [2*rbp - 512]
 
 // CHECK: vgetexpbf16 xmm22 {k7} {z}, xmmword ptr [rcx + 2032]
-// CHECK: encoding: [0x62,0xe5,0x7d,0x8f,0x42,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x8f,0x42,0x71,0x7f]
           vgetexpbf16 xmm22 {k7} {z}, xmmword ptr [rcx + 2032]
 
 // CHECK: vgetexpbf16 xmm22 {k7} {z}, word ptr [rdx - 256]{1to8}
-// CHECK: encoding: [0x62,0xe5,0x7d,0x9f,0x42,0x72,0x80]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x9f,0x42,0x72,0x80]
           vgetexpbf16 xmm22 {k7} {z}, word ptr [rdx - 256]{1to8}
 
 // CHECK: vgetexpbf16 ymm22, ymmword ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa5,0x7d,0x28,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x28,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vgetexpbf16 ymm22, ymmword ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vgetexpbf16 ymm22 {k7}, ymmword ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc5,0x7d,0x2f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc6,0x7c,0x2f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
           vgetexpbf16 ymm22 {k7}, ymmword ptr [r8 + 4*rax + 291]
 
 // CHECK: vgetexpbf16 ymm22, word ptr [rip]{1to16}
-// CHECK: encoding: [0x62,0xe5,0x7d,0x38,0x42,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x38,0x42,0x35,0x00,0x00,0x00,0x00]
           vgetexpbf16 ymm22, word ptr [rip]{1to16}
 
 // CHECK: vgetexpbf16 ymm22, ymmword ptr [2*rbp - 1024]
-// CHECK: encoding: [0x62,0xe5,0x7d,0x28,0x42,0x34,0x6d,0x00,0xfc,0xff,0xff]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x28,0x42,0x34,0x6d,0x00,0xfc,0xff,0xff]
           vgetexpbf16 ymm22, ymmword ptr [2*rbp - 1024]
 
 // CHECK: vgetexpbf16 ymm22 {k7} {z}, ymmword ptr [rcx + 4064]
-// CHECK: encoding: [0x62,0xe5,0x7d,0xaf,0x42,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xaf,0x42,0x71,0x7f]
           vgetexpbf16 ymm22 {k7} {z}, ymmword ptr [rcx + 4064]
 
 // CHECK: vgetexpbf16 ymm22 {k7} {z}, word ptr [rdx - 256]{1to16}
-// CHECK: encoding: [0x62,0xe5,0x7d,0xbf,0x42,0x72,0x80]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xbf,0x42,0x72,0x80]
           vgetexpbf16 ymm22 {k7} {z}, word ptr [rdx - 256]{1to16}
 
 // CHECK: vgetexpbf16 zmm22, zmmword ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa5,0x7d,0x48,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa6,0x7c,0x48,0x42,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vgetexpbf16 zmm22, zmmword ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vgetexpbf16 zmm22 {k7}, zmmword ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc5,0x7d,0x4f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc6,0x7c,0x4f,0x42,0xb4,0x80,0x23,0x01,0x00,0x00]
           vgetexpbf16 zmm22 {k7}, zmmword ptr [r8 + 4*rax + 291]
 
 // CHECK: vgetexpbf16 zmm22, word ptr [rip]{1to32}
-// CHECK: encoding: [0x62,0xe5,0x7d,0x58,0x42,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x58,0x42,0x35,0x00,0x00,0x00,0x00]
           vgetexpbf16 zmm22, word ptr [rip]{1to32}
 
 // CHECK: vgetexpbf16 zmm22, zmmword ptr [2*rbp - 2048]
-// CHECK: encoding: [0x62,0xe5,0x7d,0x48,0x42,0x34,0x6d,0x00,0xf8,0xff,0xff]
+// CHECK: encoding: [0x62,0xe6,0x7c,0x48,0x42,0x34,0x6d,0x00,0xf8,0xff,0xff]
           vgetexpbf16 zmm22, zmmword ptr [2*rbp - 2048]
 
 // CHECK: vgetexpbf16 zmm22 {k7} {z}, zmmword ptr [rcx + 8128]
-// CHECK: encoding: [0x62,0xe5,0x7d,0xcf,0x42,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xcf,0x42,0x71,0x7f]
           vgetexpbf16 zmm22 {k7} {z}, zmmword ptr [rcx + 8128]
 
 // CHECK: vgetexpbf16 zmm22 {k7} {z}, word ptr [rdx - 256]{1to32}
-// CHECK: encoding: [0x62,0xe5,0x7d,0xdf,0x42,0x72,0x80]
+// CHECK: encoding: [0x62,0xe6,0x7c,0xdf,0x42,0x72,0x80]
           vgetexpbf16 zmm22 {k7} {z}, word ptr [rdx - 256]{1to32}
 
 // CHECK: vgetmantbf16 zmm22, zmm23, 123
diff --git a/llvm/test/MC/X86/avx10.2-com-ef-32-att.s b/llvm/test/MC/X86/avx10.2-com-ef-32-att.s
index 8883bb3d6775..5f91ec8370ef 100644
--- a/llvm/test/MC/X86/avx10.2-com-ef-32-att.s
+++ b/llvm/test/MC/X86/avx10.2-com-ef-32-att.s
@@ -1,194 +1,194 @@
 // RUN: llvm-mc -triple i386 --show-encoding %s | FileCheck %s
 
 // CHECK: vcomxsd %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0xd3]
           vcomxsd %xmm3, %xmm2
 
 // CHECK: vcomxsd {sae}, %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x18,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x18,0x2f,0xd3]
           vcomxsd {sae}, %xmm3, %xmm2
 
 // CHECK: vcomxsd  268435456(%esp,%esi,8), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
           vcomxsd  268435456(%esp,%esi,8), %xmm2
 
 // CHECK: vcomxsd  291(%edi,%eax,4), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
           vcomxsd  291(%edi,%eax,4), %xmm2
 
 // CHECK: vcomxsd  (%eax), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x10]
           vcomxsd  (%eax), %xmm2
 
 // CHECK: vcomxsd  -256(,%ebp,2), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x14,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x14,0x6d,0x00,0xff,0xff,0xff]
           vcomxsd  -256(,%ebp,2), %xmm2
 
 // CHECK: vcomxsd  1016(%ecx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x51,0x7f]
           vcomxsd  1016(%ecx), %xmm2
 
 // CHECK: vcomxsd  -1024(%edx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x52,0x80]
           vcomxsd  -1024(%edx), %xmm2
 
 // CHECK: vcomxsh %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0xd3]
           vcomxsh %xmm3, %xmm2
 
 // CHECK: vcomxsh {sae}, %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x18,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x18,0x2f,0xd3]
           vcomxsh {sae}, %xmm3, %xmm2
 
 // CHECK: vcomxsh  268435456(%esp,%esi,8), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
           vcomxsh  268435456(%esp,%esi,8), %xmm2
 
 // CHECK: vcomxsh  291(%edi,%eax,4), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
           vcomxsh  291(%edi,%eax,4), %xmm2
 
 // CHECK: vcomxsh  (%eax), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x10]
           vcomxsh  (%eax), %xmm2
 
 // CHECK: vcomxsh  -64(,%ebp,2), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x14,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x14,0x6d,0xc0,0xff,0xff,0xff]
           vcomxsh  -64(,%ebp,2), %xmm2
 
 // CHECK: vcomxsh  254(%ecx), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x51,0x7f]
           vcomxsh  254(%ecx), %xmm2
 
 // CHECK: vcomxsh  -256(%edx), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x52,0x80]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x52,0x80]
           vcomxsh  -256(%edx), %xmm2
 
 // CHECK: vcomxss %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0xd3]
           vcomxss %xmm3, %xmm2
 
 // CHECK: vcomxss {sae}, %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x18,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x18,0x2f,0xd3]
           vcomxss {sae}, %xmm3, %xmm2
 
 // CHECK: vcomxss  268435456(%esp,%esi,8), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
           vcomxss  268435456(%esp,%esi,8), %xmm2
 
 // CHECK: vcomxss  291(%edi,%eax,4), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
           vcomxss  291(%edi,%eax,4), %xmm2
 
 // CHECK: vcomxss  (%eax), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x10]
           vcomxss  (%eax), %xmm2
 
 // CHECK: vcomxss  -128(,%ebp,2), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x14,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x14,0x6d,0x80,0xff,0xff,0xff]
           vcomxss  -128(,%ebp,2), %xmm2
 
 // CHECK: vcomxss  508(%ecx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x51,0x7f]
           vcomxss  508(%ecx), %xmm2
 
 // CHECK: vcomxss  -512(%edx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x52,0x80]
           vcomxss  -512(%edx), %xmm2
 
 // CHECK: vucomxsd %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0xd3]
           vucomxsd %xmm3, %xmm2
 
 // CHECK: vucomxsd {sae}, %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x18,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x18,0x2e,0xd3]
           vucomxsd {sae}, %xmm3, %xmm2
 
 // CHECK: vucomxsd  268435456(%esp,%esi,8), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
           vucomxsd  268435456(%esp,%esi,8), %xmm2
 
 // CHECK: vucomxsd  291(%edi,%eax,4), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
           vucomxsd  291(%edi,%eax,4), %xmm2
 
 // CHECK: vucomxsd  (%eax), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x10]
           vucomxsd  (%eax), %xmm2
 
 // CHECK: vucomxsd  -256(,%ebp,2), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x14,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x14,0x6d,0x00,0xff,0xff,0xff]
           vucomxsd  -256(,%ebp,2), %xmm2
 
 // CHECK: vucomxsd  1016(%ecx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x51,0x7f]
           vucomxsd  1016(%ecx), %xmm2
 
 // CHECK: vucomxsd  -1024(%edx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x52,0x80]
           vucomxsd  -1024(%edx), %xmm2
 
 // CHECK: vucomxsh %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0xd3]
           vucomxsh %xmm3, %xmm2
 
 // CHECK: vucomxsh {sae}, %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x18,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x18,0x2e,0xd3]
           vucomxsh {sae}, %xmm3, %xmm2
 
 // CHECK: vucomxsh  268435456(%esp,%esi,8), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
           vucomxsh  268435456(%esp,%esi,8), %xmm2
 
 // CHECK: vucomxsh  291(%edi,%eax,4), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
           vucomxsh  291(%edi,%eax,4), %xmm2
 
 // CHECK: vucomxsh  (%eax), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x10]
           vucomxsh  (%eax), %xmm2
 
 // CHECK: vucomxsh  -64(,%ebp,2), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x14,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x14,0x6d,0xc0,0xff,0xff,0xff]
           vucomxsh  -64(,%ebp,2), %xmm2
 
 // CHECK: vucomxsh  254(%ecx), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x51,0x7f]
           vucomxsh  254(%ecx), %xmm2
 
 // CHECK: vucomxsh  -256(%edx), %xmm2
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x52,0x80]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x52,0x80]
           vucomxsh  -256(%edx), %xmm2
 
 // CHECK: vucomxss %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0xd3]
           vucomxss %xmm3, %xmm2
 
 // CHECK: vucomxss {sae}, %xmm3, %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x18,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x18,0x2e,0xd3]
           vucomxss {sae}, %xmm3, %xmm2
 
 // CHECK: vucomxss  268435456(%esp,%esi,8), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
           vucomxss  268435456(%esp,%esi,8), %xmm2
 
 // CHECK: vucomxss  291(%edi,%eax,4), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
           vucomxss  291(%edi,%eax,4), %xmm2
 
 // CHECK: vucomxss  (%eax), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x10]
           vucomxss  (%eax), %xmm2
 
 // CHECK: vucomxss  -128(,%ebp,2), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x14,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x14,0x6d,0x80,0xff,0xff,0xff]
           vucomxss  -128(,%ebp,2), %xmm2
 
 // CHECK: vucomxss  508(%ecx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x51,0x7f]
           vucomxss  508(%ecx), %xmm2
 
 // CHECK: vucomxss  -512(%edx), %xmm2
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x52,0x80]
           vucomxss  -512(%edx), %xmm2
 
diff --git a/llvm/test/MC/X86/avx10.2-com-ef-32-intel.s b/llvm/test/MC/X86/avx10.2-com-ef-32-intel.s
index 9ff0484db133..7cbd4e9722dd 100644
--- a/llvm/test/MC/X86/avx10.2-com-ef-32-intel.s
+++ b/llvm/test/MC/X86/avx10.2-com-ef-32-intel.s
@@ -1,194 +1,194 @@
 // RUN: llvm-mc -triple i386 -x86-asm-syntax=intel -output-asm-variant=1 --show-encoding %s | FileCheck %s
 
 // CHECK: vcomxsd xmm2, xmm3
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0xd3]
           vcomxsd xmm2, xmm3
 
 // CHECK: vcomxsd xmm2, xmm3, {sae}
-// CHECK: encoding: [0x62,0xf1,0xfe,0x18,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x18,0x2f,0xd3]
           vcomxsd xmm2, xmm3, {sae}
 
 // CHECK: vcomxsd xmm2, qword ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
           vcomxsd xmm2, qword ptr [esp + 8*esi + 268435456]
 
 // CHECK: vcomxsd xmm2, qword ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
           vcomxsd xmm2, qword ptr [edi + 4*eax + 291]
 
 // CHECK: vcomxsd xmm2, qword ptr [eax]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x10]
           vcomxsd xmm2, qword ptr [eax]
 
 // CHECK: vcomxsd xmm2, qword ptr [2*ebp - 256]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x14,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x14,0x6d,0x00,0xff,0xff,0xff]
           vcomxsd xmm2, qword ptr [2*ebp - 256]
 
 // CHECK: vcomxsd xmm2, qword ptr [ecx + 1016]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x51,0x7f]
           vcomxsd xmm2, qword ptr [ecx + 1016]
 
 // CHECK: vcomxsd xmm2, qword ptr [edx - 1024]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2f,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2f,0x52,0x80]
           vcomxsd xmm2, qword ptr [edx - 1024]
 
 // CHECK: vcomxsh xmm2, xmm3
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0xd3]
           vcomxsh xmm2, xmm3
 
 // CHECK: vcomxsh xmm2, xmm3, {sae}
-// CHECK: encoding: [0x62,0xf5,0x7f,0x18,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x18,0x2f,0xd3]
           vcomxsh xmm2, xmm3, {sae}
 
 // CHECK: vcomxsh xmm2, word ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
           vcomxsh xmm2, word ptr [esp + 8*esi + 268435456]
 
 // CHECK: vcomxsh xmm2, word ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
           vcomxsh xmm2, word ptr [edi + 4*eax + 291]
 
 // CHECK: vcomxsh xmm2, word ptr [eax]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x10]
           vcomxsh xmm2, word ptr [eax]
 
 // CHECK: vcomxsh xmm2, word ptr [2*ebp - 64]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x14,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x14,0x6d,0xc0,0xff,0xff,0xff]
           vcomxsh xmm2, word ptr [2*ebp - 64]
 
 // CHECK: vcomxsh xmm2, word ptr [ecx + 254]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x51,0x7f]
           vcomxsh xmm2, word ptr [ecx + 254]
 
 // CHECK: vcomxsh xmm2, word ptr [edx - 256]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2f,0x52,0x80]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2f,0x52,0x80]
           vcomxsh xmm2, word ptr [edx - 256]
 
 // CHECK: vcomxss xmm2, xmm3
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0xd3]
           vcomxss xmm2, xmm3
 
 // CHECK: vcomxss xmm2, xmm3, {sae}
-// CHECK: encoding: [0x62,0xf1,0x7f,0x18,0x2f,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x18,0x2f,0xd3]
           vcomxss xmm2, xmm3, {sae}
 
 // CHECK: vcomxss xmm2, dword ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x94,0xf4,0x00,0x00,0x00,0x10]
           vcomxss xmm2, dword ptr [esp + 8*esi + 268435456]
 
 // CHECK: vcomxss xmm2, dword ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x94,0x87,0x23,0x01,0x00,0x00]
           vcomxss xmm2, dword ptr [edi + 4*eax + 291]
 
 // CHECK: vcomxss xmm2, dword ptr [eax]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x10]
           vcomxss xmm2, dword ptr [eax]
 
 // CHECK: vcomxss xmm2, dword ptr [2*ebp - 128]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x14,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x14,0x6d,0x80,0xff,0xff,0xff]
           vcomxss xmm2, dword ptr [2*ebp - 128]
 
 // CHECK: vcomxss xmm2, dword ptr [ecx + 508]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x51,0x7f]
           vcomxss xmm2, dword ptr [ecx + 508]
 
 // CHECK: vcomxss xmm2, dword ptr [edx - 512]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2f,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2f,0x52,0x80]
           vcomxss xmm2, dword ptr [edx - 512]
 
 // CHECK: vucomxsd xmm2, xmm3
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0xd3]
           vucomxsd xmm2, xmm3
 
 // CHECK: vucomxsd xmm2, xmm3, {sae}
-// CHECK: encoding: [0x62,0xf1,0xfe,0x18,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0xff,0x18,0x2e,0xd3]
           vucomxsd xmm2, xmm3, {sae}
 
 // CHECK: vucomxsd xmm2, qword ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
           vucomxsd xmm2, qword ptr [esp + 8*esi + 268435456]
 
 // CHECK: vucomxsd xmm2, qword ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
           vucomxsd xmm2, qword ptr [edi + 4*eax + 291]
 
 // CHECK: vucomxsd xmm2, qword ptr [eax]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x10]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x10]
           vucomxsd xmm2, qword ptr [eax]
 
 // CHECK: vucomxsd xmm2, qword ptr [2*ebp - 256]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x14,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x14,0x6d,0x00,0xff,0xff,0xff]
           vucomxsd xmm2, qword ptr [2*ebp - 256]
 
 // CHECK: vucomxsd xmm2, qword ptr [ecx + 1016]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x51,0x7f]
           vucomxsd xmm2, qword ptr [ecx + 1016]
 
 // CHECK: vucomxsd xmm2, qword ptr [edx - 1024]
-// CHECK: encoding: [0x62,0xf1,0xfe,0x08,0x2e,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0xff,0x08,0x2e,0x52,0x80]
           vucomxsd xmm2, qword ptr [edx - 1024]
 
 // CHECK: vucomxsh xmm2, xmm3
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0xd3]
           vucomxsh xmm2, xmm3
 
 // CHECK: vucomxsh xmm2, xmm3, {sae}
-// CHECK: encoding: [0x62,0xf5,0x7f,0x18,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x18,0x2e,0xd3]
           vucomxsh xmm2, xmm3, {sae}
 
 // CHECK: vucomxsh xmm2, word ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
           vucomxsh xmm2, word ptr [esp + 8*esi + 268435456]
 
 // CHECK: vucomxsh xmm2, word ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
           vucomxsh xmm2, word ptr [edi + 4*eax + 291]
 
 // CHECK: vucomxsh xmm2, word ptr [eax]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x10]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x10]
           vucomxsh xmm2, word ptr [eax]
 
 // CHECK: vucomxsh xmm2, word ptr [2*ebp - 64]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x14,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x14,0x6d,0xc0,0xff,0xff,0xff]
           vucomxsh xmm2, word ptr [2*ebp - 64]
 
 // CHECK: vucomxsh xmm2, word ptr [ecx + 254]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x51,0x7f]
           vucomxsh xmm2, word ptr [ecx + 254]
 
 // CHECK: vucomxsh xmm2, word ptr [edx - 256]
-// CHECK: encoding: [0x62,0xf5,0x7f,0x08,0x2e,0x52,0x80]
+// CHECK: encoding: [0x62,0xf5,0x7e,0x08,0x2e,0x52,0x80]
           vucomxsh xmm2, word ptr [edx - 256]
 
 // CHECK: vucomxss xmm2, xmm3
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0xd3]
           vucomxss xmm2, xmm3
 
 // CHECK: vucomxss xmm2, xmm3, {sae}
-// CHECK: encoding: [0x62,0xf1,0x7f,0x18,0x2e,0xd3]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x18,0x2e,0xd3]
           vucomxss xmm2, xmm3, {sae}
 
 // CHECK: vucomxss xmm2, dword ptr [esp + 8*esi + 268435456]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x94,0xf4,0x00,0x00,0x00,0x10]
           vucomxss xmm2, dword ptr [esp + 8*esi + 268435456]
 
 // CHECK: vucomxss xmm2, dword ptr [edi + 4*eax + 291]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x94,0x87,0x23,0x01,0x00,0x00]
           vucomxss xmm2, dword ptr [edi + 4*eax + 291]
 
 // CHECK: vucomxss xmm2, dword ptr [eax]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x10]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x10]
           vucomxss xmm2, dword ptr [eax]
 
 // CHECK: vucomxss xmm2, dword ptr [2*ebp - 128]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x14,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x14,0x6d,0x80,0xff,0xff,0xff]
           vucomxss xmm2, dword ptr [2*ebp - 128]
 
 // CHECK: vucomxss xmm2, dword ptr [ecx + 508]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x51,0x7f]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x51,0x7f]
           vucomxss xmm2, dword ptr [ecx + 508]
 
 // CHECK: vucomxss xmm2, dword ptr [edx - 512]
-// CHECK: encoding: [0x62,0xf1,0x7f,0x08,0x2e,0x52,0x80]
+// CHECK: encoding: [0x62,0xf1,0x7e,0x08,0x2e,0x52,0x80]
           vucomxss xmm2, dword ptr [edx - 512]
 
diff --git a/llvm/test/MC/X86/avx10.2-com-ef-64-att.s b/llvm/test/MC/X86/avx10.2-com-ef-64-att.s
index 2f3690537334..832151ab2370 100644
--- a/llvm/test/MC/X86/avx10.2-com-ef-64-att.s
+++ b/llvm/test/MC/X86/avx10.2-com-ef-64-att.s
@@ -1,194 +1,194 @@
 // RUN: llvm-mc -triple x86_64 --show-encoding %s | FileCheck %s
 
 // CHECK: vcomxsd %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2f,0xf7]
           vcomxsd %xmm23, %xmm22
 
 // CHECK: vcomxsd {sae}, %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0xfe,0x18,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x18,0x2f,0xf7]
           vcomxsd {sae}, %xmm23, %xmm22
 
 // CHECK: vcomxsd  268435456(%rbp,%r14,8), %xmm22
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vcomxsd  268435456(%rbp,%r14,8), %xmm22
 
 // CHECK: vcomxsd  291(%r8,%rax,4), %xmm22
-// CHECK: encoding: [0x62,0xc1,0xfe,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0xff,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
           vcomxsd  291(%r8,%rax,4), %xmm22
 
 // CHECK: vcomxsd  (%rip), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
           vcomxsd  (%rip), %xmm22
 
 // CHECK: vcomxsd  -256(,%rbp,2), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x34,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x34,0x6d,0x00,0xff,0xff,0xff]
           vcomxsd  -256(,%rbp,2), %xmm22
 
 // CHECK: vcomxsd  1016(%rcx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x71,0x7f]
           vcomxsd  1016(%rcx), %xmm22
 
 // CHECK: vcomxsd  -1024(%rdx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x72,0x80]
           vcomxsd  -1024(%rdx), %xmm22
 
 // CHECK: vcomxsh %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2f,0xf7]
           vcomxsh %xmm23, %xmm22
 
 // CHECK: vcomxsh {sae}, %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7f,0x18,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x18,0x2f,0xf7]
           vcomxsh {sae}, %xmm23, %xmm22
 
 // CHECK: vcomxsh  268435456(%rbp,%r14,8), %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vcomxsh  268435456(%rbp,%r14,8), %xmm22
 
 // CHECK: vcomxsh  291(%r8,%rax,4), %xmm22
-// CHECK: encoding: [0x62,0xc5,0x7f,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc5,0x7e,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
           vcomxsh  291(%r8,%rax,4), %xmm22
 
 // CHECK: vcomxsh  (%rip), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
           vcomxsh  (%rip), %xmm22
 
 // CHECK: vcomxsh  -64(,%rbp,2), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x34,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x34,0x6d,0xc0,0xff,0xff,0xff]
           vcomxsh  -64(,%rbp,2), %xmm22
 
 // CHECK: vcomxsh  254(%rcx), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x71,0x7f]
           vcomxsh  254(%rcx), %xmm22
 
 // CHECK: vcomxsh  -256(%rdx), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x72,0x80]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x72,0x80]
           vcomxsh  -256(%rdx), %xmm22
 
 // CHECK: vcomxss %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2f,0xf7]
           vcomxss %xmm23, %xmm22
 
 // CHECK: vcomxss {sae}, %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0x7f,0x18,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x18,0x2f,0xf7]
           vcomxss {sae}, %xmm23, %xmm22
 
 // CHECK: vcomxss  268435456(%rbp,%r14,8), %xmm22
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vcomxss  268435456(%rbp,%r14,8), %xmm22
 
 // CHECK: vcomxss  291(%r8,%rax,4), %xmm22
-// CHECK: encoding: [0x62,0xc1,0x7f,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0x7e,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
           vcomxss  291(%r8,%rax,4), %xmm22
 
 // CHECK: vcomxss  (%rip), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
           vcomxss  (%rip), %xmm22
 
 // CHECK: vcomxss  -128(,%rbp,2), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x34,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x34,0x6d,0x80,0xff,0xff,0xff]
           vcomxss  -128(,%rbp,2), %xmm22
 
 // CHECK: vcomxss  508(%rcx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x71,0x7f]
           vcomxss  508(%rcx), %xmm22
 
 // CHECK: vcomxss  -512(%rdx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x72,0x80]
           vcomxss  -512(%rdx), %xmm22
 
 // CHECK: vucomxsd %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2e,0xf7]
           vucomxsd %xmm23, %xmm22
 
 // CHECK: vucomxsd {sae}, %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0xfe,0x18,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x18,0x2e,0xf7]
           vucomxsd {sae}, %xmm23, %xmm22
 
 // CHECK: vucomxsd  268435456(%rbp,%r14,8), %xmm22
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vucomxsd  268435456(%rbp,%r14,8), %xmm22
 
 // CHECK: vucomxsd  291(%r8,%rax,4), %xmm22
-// CHECK: encoding: [0x62,0xc1,0xfe,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0xff,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
           vucomxsd  291(%r8,%rax,4), %xmm22
 
 // CHECK: vucomxsd  (%rip), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
           vucomxsd  (%rip), %xmm22
 
 // CHECK: vucomxsd  -256(,%rbp,2), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x34,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x34,0x6d,0x00,0xff,0xff,0xff]
           vucomxsd  -256(,%rbp,2), %xmm22
 
 // CHECK: vucomxsd  1016(%rcx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x71,0x7f]
           vucomxsd  1016(%rcx), %xmm22
 
 // CHECK: vucomxsd  -1024(%rdx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x72,0x80]
           vucomxsd  -1024(%rdx), %xmm22
 
 // CHECK: vucomxsh %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2e,0xf7]
           vucomxsh %xmm23, %xmm22
 
 // CHECK: vucomxsh {sae}, %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7f,0x18,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x18,0x2e,0xf7]
           vucomxsh {sae}, %xmm23, %xmm22
 
 // CHECK: vucomxsh  268435456(%rbp,%r14,8), %xmm22
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vucomxsh  268435456(%rbp,%r14,8), %xmm22
 
 // CHECK: vucomxsh  291(%r8,%rax,4), %xmm22
-// CHECK: encoding: [0x62,0xc5,0x7f,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc5,0x7e,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
           vucomxsh  291(%r8,%rax,4), %xmm22
 
 // CHECK: vucomxsh  (%rip), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
           vucomxsh  (%rip), %xmm22
 
 // CHECK: vucomxsh  -64(,%rbp,2), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x34,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x34,0x6d,0xc0,0xff,0xff,0xff]
           vucomxsh  -64(,%rbp,2), %xmm22
 
 // CHECK: vucomxsh  254(%rcx), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x71,0x7f]
           vucomxsh  254(%rcx), %xmm22
 
 // CHECK: vucomxsh  -256(%rdx), %xmm22
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x72,0x80]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x72,0x80]
           vucomxsh  -256(%rdx), %xmm22
 
 // CHECK: vucomxss %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2e,0xf7]
           vucomxss %xmm23, %xmm22
 
 // CHECK: vucomxss {sae}, %xmm23, %xmm22
-// CHECK: encoding: [0x62,0xa1,0x7f,0x18,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x18,0x2e,0xf7]
           vucomxss {sae}, %xmm23, %xmm22
 
 // CHECK: vucomxss  268435456(%rbp,%r14,8), %xmm22
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vucomxss  268435456(%rbp,%r14,8), %xmm22
 
 // CHECK: vucomxss  291(%r8,%rax,4), %xmm22
-// CHECK: encoding: [0x62,0xc1,0x7f,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0x7e,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
           vucomxss  291(%r8,%rax,4), %xmm22
 
 // CHECK: vucomxss  (%rip), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
           vucomxss  (%rip), %xmm22
 
 // CHECK: vucomxss  -128(,%rbp,2), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x34,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x34,0x6d,0x80,0xff,0xff,0xff]
           vucomxss  -128(,%rbp,2), %xmm22
 
 // CHECK: vucomxss  508(%rcx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x71,0x7f]
           vucomxss  508(%rcx), %xmm22
 
 // CHECK: vucomxss  -512(%rdx), %xmm22
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x72,0x80]
           vucomxss  -512(%rdx), %xmm22
 
diff --git a/llvm/test/MC/X86/avx10.2-com-ef-64-intel.s b/llvm/test/MC/X86/avx10.2-com-ef-64-intel.s
index 41aaf99270b8..94e3b77984c8 100644
--- a/llvm/test/MC/X86/avx10.2-com-ef-64-intel.s
+++ b/llvm/test/MC/X86/avx10.2-com-ef-64-intel.s
@@ -1,194 +1,194 @@
 // RUN: llvm-mc -triple x86_64 -x86-asm-syntax=intel -output-asm-variant=1 --show-encoding %s | FileCheck %s
 
 // CHECK: vcomxsd xmm22, xmm23
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2f,0xf7]
           vcomxsd xmm22, xmm23
 
 // CHECK: vcomxsd xmm22, xmm23, {sae}
-// CHECK: encoding: [0x62,0xa1,0xfe,0x18,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x18,0x2f,0xf7]
           vcomxsd xmm22, xmm23, {sae}
 
 // CHECK: vcomxsd xmm22, qword ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vcomxsd xmm22, qword ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vcomxsd xmm22, qword ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc1,0xfe,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0xff,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
           vcomxsd xmm22, qword ptr [r8 + 4*rax + 291]
 
 // CHECK: vcomxsd xmm22, qword ptr [rip]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
           vcomxsd xmm22, qword ptr [rip]
 
 // CHECK: vcomxsd xmm22, qword ptr [2*rbp - 256]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x34,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x34,0x6d,0x00,0xff,0xff,0xff]
           vcomxsd xmm22, qword ptr [2*rbp - 256]
 
 // CHECK: vcomxsd xmm22, qword ptr [rcx + 1016]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x71,0x7f]
           vcomxsd xmm22, qword ptr [rcx + 1016]
 
 // CHECK: vcomxsd xmm22, qword ptr [rdx - 1024]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2f,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2f,0x72,0x80]
           vcomxsd xmm22, qword ptr [rdx - 1024]
 
 // CHECK: vcomxsh xmm22, xmm23
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2f,0xf7]
           vcomxsh xmm22, xmm23
 
 // CHECK: vcomxsh xmm22, xmm23, {sae}
-// CHECK: encoding: [0x62,0xa5,0x7f,0x18,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x18,0x2f,0xf7]
           vcomxsh xmm22, xmm23, {sae}
 
 // CHECK: vcomxsh xmm22, word ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vcomxsh xmm22, word ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vcomxsh xmm22, word ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc5,0x7f,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc5,0x7e,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
           vcomxsh xmm22, word ptr [r8 + 4*rax + 291]
 
 // CHECK: vcomxsh xmm22, word ptr [rip]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
           vcomxsh xmm22, word ptr [rip]
 
 // CHECK: vcomxsh xmm22, word ptr [2*rbp - 64]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x34,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x34,0x6d,0xc0,0xff,0xff,0xff]
           vcomxsh xmm22, word ptr [2*rbp - 64]
 
 // CHECK: vcomxsh xmm22, word ptr [rcx + 254]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x71,0x7f]
           vcomxsh xmm22, word ptr [rcx + 254]
 
 // CHECK: vcomxsh xmm22, word ptr [rdx - 256]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2f,0x72,0x80]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2f,0x72,0x80]
           vcomxsh xmm22, word ptr [rdx - 256]
 
 // CHECK: vcomxss xmm22, xmm23
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2f,0xf7]
           vcomxss xmm22, xmm23
 
 // CHECK: vcomxss xmm22, xmm23, {sae}
-// CHECK: encoding: [0x62,0xa1,0x7f,0x18,0x2f,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x18,0x2f,0xf7]
           vcomxss xmm22, xmm23, {sae}
 
 // CHECK: vcomxss xmm22, dword ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2f,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vcomxss xmm22, dword ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vcomxss xmm22, dword ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc1,0x7f,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0x7e,0x08,0x2f,0xb4,0x80,0x23,0x01,0x00,0x00]
           vcomxss xmm22, dword ptr [r8 + 4*rax + 291]
 
 // CHECK: vcomxss xmm22, dword ptr [rip]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x35,0x00,0x00,0x00,0x00]
           vcomxss xmm22, dword ptr [rip]
 
 // CHECK: vcomxss xmm22, dword ptr [2*rbp - 128]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x34,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x34,0x6d,0x80,0xff,0xff,0xff]
           vcomxss xmm22, dword ptr [2*rbp - 128]
 
 // CHECK: vcomxss xmm22, dword ptr [rcx + 508]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x71,0x7f]
           vcomxss xmm22, dword ptr [rcx + 508]
 
 // CHECK: vcomxss xmm22, dword ptr [rdx - 512]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2f,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2f,0x72,0x80]
           vcomxss xmm22, dword ptr [rdx - 512]
 
 // CHECK: vucomxsd xmm22, xmm23
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2e,0xf7]
           vucomxsd xmm22, xmm23
 
 // CHECK: vucomxsd xmm22, xmm23, {sae}
-// CHECK: encoding: [0x62,0xa1,0xfe,0x18,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0xff,0x18,0x2e,0xf7]
           vucomxsd xmm22, xmm23, {sae}
 
 // CHECK: vucomxsd xmm22, qword ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa1,0xfe,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0xff,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vucomxsd xmm22, qword ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vucomxsd xmm22, qword ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc1,0xfe,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0xff,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
           vucomxsd xmm22, qword ptr [r8 + 4*rax + 291]
 
 // CHECK: vucomxsd xmm22, qword ptr [rip]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
           vucomxsd xmm22, qword ptr [rip]
 
 // CHECK: vucomxsd xmm22, qword ptr [2*rbp - 256]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x34,0x6d,0x00,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x34,0x6d,0x00,0xff,0xff,0xff]
           vucomxsd xmm22, qword ptr [2*rbp - 256]
 
 // CHECK: vucomxsd xmm22, qword ptr [rcx + 1016]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x71,0x7f]
           vucomxsd xmm22, qword ptr [rcx + 1016]
 
 // CHECK: vucomxsd xmm22, qword ptr [rdx - 1024]
-// CHECK: encoding: [0x62,0xe1,0xfe,0x08,0x2e,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0xff,0x08,0x2e,0x72,0x80]
           vucomxsd xmm22, qword ptr [rdx - 1024]
 
 // CHECK: vucomxsh xmm22, xmm23
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2e,0xf7]
           vucomxsh xmm22, xmm23
 
 // CHECK: vucomxsh xmm22, xmm23, {sae}
-// CHECK: encoding: [0x62,0xa5,0x7f,0x18,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x18,0x2e,0xf7]
           vucomxsh xmm22, xmm23, {sae}
 
 // CHECK: vucomxsh xmm22, word ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa5,0x7f,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa5,0x7e,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vucomxsh xmm22, word ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vucomxsh xmm22, word ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc5,0x7f,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc5,0x7e,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
           vucomxsh xmm22, word ptr [r8 + 4*rax + 291]
 
 // CHECK: vucomxsh xmm22, word ptr [rip]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
           vucomxsh xmm22, word ptr [rip]
 
 // CHECK: vucomxsh xmm22, word ptr [2*rbp - 64]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x34,0x6d,0xc0,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x34,0x6d,0xc0,0xff,0xff,0xff]
           vucomxsh xmm22, word ptr [2*rbp - 64]
 
 // CHECK: vucomxsh xmm22, word ptr [rcx + 254]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x71,0x7f]
           vucomxsh xmm22, word ptr [rcx + 254]
 
 // CHECK: vucomxsh xmm22, word ptr [rdx - 256]
-// CHECK: encoding: [0x62,0xe5,0x7f,0x08,0x2e,0x72,0x80]
+// CHECK: encoding: [0x62,0xe5,0x7e,0x08,0x2e,0x72,0x80]
           vucomxsh xmm22, word ptr [rdx - 256]
 
 // CHECK: vucomxss xmm22, xmm23
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2e,0xf7]
           vucomxss xmm22, xmm23
 
 // CHECK: vucomxss xmm22, xmm23, {sae}
-// CHECK: encoding: [0x62,0xa1,0x7f,0x18,0x2e,0xf7]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x18,0x2e,0xf7]
           vucomxss xmm22, xmm23, {sae}
 
 // CHECK: vucomxss xmm22, dword ptr [rbp + 8*r14 + 268435456]
-// CHECK: encoding: [0x62,0xa1,0x7f,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
+// CHECK: encoding: [0x62,0xa1,0x7e,0x08,0x2e,0xb4,0xf5,0x00,0x00,0x00,0x10]
           vucomxss xmm22, dword ptr [rbp + 8*r14 + 268435456]
 
 // CHECK: vucomxss xmm22, dword ptr [r8 + 4*rax + 291]
-// CHECK: encoding: [0x62,0xc1,0x7f,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
+// CHECK: encoding: [0x62,0xc1,0x7e,0x08,0x2e,0xb4,0x80,0x23,0x01,0x00,0x00]
           vucomxss xmm22, dword ptr [r8 + 4*rax + 291]
 
 // CHECK: vucomxss xmm22, dword ptr [rip]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x35,0x00,0x00,0x00,0x00]
           vucomxss xmm22, dword ptr [rip]
 
 // CHECK: vucomxss xmm22, dword ptr [2*rbp - 128]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x34,0x6d,0x80,0xff,0xff,0xff]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x34,0x6d,0x80,0xff,0xff,0xff]
           vucomxss xmm22, dword ptr [2*rbp - 128]
 
 // CHECK: vucomxss xmm22, dword ptr [rcx + 508]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x71,0x7f]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x71,0x7f]
           vucomxss xmm22, dword ptr [rcx + 508]
 
 // CHECK: vucomxss xmm22, dword ptr [rdx - 512]
-// CHECK: encoding: [0x62,0xe1,0x7f,0x08,0x2e,0x72,0x80]
+// CHECK: encoding: [0x62,0xe1,0x7e,0x08,0x2e,0x72,0x80]
           vucomxss xmm22, dword ptr [rdx - 512]
 
diff --git a/llvm/test/Transforms/GlobalOpt/malloc-promote-atomic.ll b/llvm/test/Transforms/GlobalOpt/malloc-promote-atomic.ll
new file mode 100644
index 000000000000..0ecdf095efdd
--- /dev/null
+++ b/llvm/test/Transforms/GlobalOpt/malloc-promote-atomic.ll
@@ -0,0 +1,28 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -passes=globalopt -S < %s | FileCheck %s
+
+@g = internal global ptr null, align 8
+
+define void @init() {
+; CHECK-LABEL: define void @init() local_unnamed_addr {
+; CHECK-NEXT:    [[ALLOC:%.*]] = call ptr @malloc(i64 48)
+; CHECK-NEXT:    store atomic ptr [[ALLOC]], ptr @g seq_cst, align 8
+; CHECK-NEXT:    ret void
+;
+  %alloc = call ptr @malloc(i64 48)
+  store atomic ptr %alloc, ptr @g seq_cst, align 8
+  ret void
+}
+
+define i1 @check() {
+; CHECK-LABEL: define i1 @check() local_unnamed_addr {
+; CHECK-NEXT:    [[VAL:%.*]] = load atomic ptr, ptr @g seq_cst, align 8
+; CHECK-NEXT:    [[CMP:%.*]] = icmp eq ptr [[VAL]], null
+; CHECK-NEXT:    ret i1 [[CMP]]
+;
+  %val = load atomic ptr, ptr @g seq_cst, align 8
+  %cmp = icmp eq ptr %val, null
+  ret i1 %cmp
+}
+
+declare ptr @malloc(i64) allockind("alloc,uninitialized") allocsize(0)
diff --git a/llvm/test/Transforms/IndVarSimplify/pr135182.ll b/llvm/test/Transforms/IndVarSimplify/pr135182.ll
new file mode 100644
index 000000000000..1db96872cffc
--- /dev/null
+++ b/llvm/test/Transforms/IndVarSimplify/pr135182.ll
@@ -0,0 +1,27 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -S -passes=indvars < %s | FileCheck %s
+
+target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
+
+define i32 @pr135182() {
+; CHECK-LABEL: define i32 @pr135182() {
+; CHECK-NEXT:  [[ENTRY:.*:]]
+; CHECK-NEXT:    br label %[[FOR_BODY:.*]]
+; CHECK:       [[FOR_BODY]]:
+; CHECK-NEXT:    br i1 false, label %[[FOR_BODY]], label %[[FOR_END:.*]]
+; CHECK:       [[FOR_END]]:
+; CHECK-NEXT:    ret i32 65512
+;
+entry:
+  br label %for.body
+
+for.body:
+  %indvar = phi i16 [ -12, %entry ], [ %indvar.next, %for.body ]
+  %add = add i16 %indvar, %indvar
+  %ext = zext i16 %add to i32
+  %indvar.next = add i16 %indvar, 1
+  br i1 false, label %for.body, label %for.end
+
+for.end:
+  ret i32 %ext
+}
diff --git a/llvm/test/Transforms/InstCombine/and-fcmp.ll b/llvm/test/Transforms/InstCombine/and-fcmp.ll
index c7bbc8ab56f9..ec1b6ad2ea16 100644
--- a/llvm/test/Transforms/InstCombine/and-fcmp.ll
+++ b/llvm/test/Transforms/InstCombine/and-fcmp.ll
@@ -4990,6 +4990,34 @@ define i1 @clang_builtin_isnormal_inf_check_copysign(half %x, half %y) {
   ret i1 %and
 }
 
+define i1 @clang_builtin_isnormal_inf_check_copysign_logical_select(half %x, half %y) {
+; CHECK-LABEL: @clang_builtin_isnormal_inf_check_copysign_logical_select(
+; CHECK-NEXT:    [[COPYSIGN_X:%.*]] = call half @llvm.copysign.f16(half [[X:%.*]], half [[Y:%.*]])
+; CHECK-NEXT:    [[ORD:%.*]] = fcmp ord half [[X]], 0xH0000
+; CHECK-NEXT:    [[CMP:%.*]] = fcmp ueq half [[COPYSIGN_X]], 0xH7C00
+; CHECK-NEXT:    [[AND:%.*]] = select i1 [[ORD]], i1 [[CMP]], i1 false
+; CHECK-NEXT:    ret i1 [[AND]]
+;
+  %copysign.x = call half @llvm.copysign.f16(half %x, half %y)
+  %ord = fcmp ord half %x, 0.0
+  %cmp = fcmp uge half %copysign.x, 0xH7C00
+  %and = select i1 %ord, i1 %cmp, i1 false
+  ret i1 %and
+}
+
+define i1 @clang_builtin_isnormal_inf_check_fabs_nnan_logical_select(half %x) {
+; CHECK-LABEL: @clang_builtin_isnormal_inf_check_fabs_nnan_logical_select(
+; CHECK-NEXT:    [[COPYSIGN_X:%.*]] = call half @llvm.fabs.f16(half [[X:%.*]])
+; CHECK-NEXT:    [[AND:%.*]] = fcmp oeq half [[COPYSIGN_X]], 0xH7C00
+; CHECK-NEXT:    ret i1 [[AND]]
+;
+  %copysign.x = call nnan half @llvm.fabs.f16(half %x)
+  %ord = fcmp ord half %x, 0.0
+  %cmp = fcmp uge half %copysign.x, 0xH7C00
+  %and = select i1 %ord, i1 %cmp, i1 false
+  ret i1 %and
+}
+
 define i1 @isnormal_logical_select_0(half %x) {
 ; CHECK-LABEL: @isnormal_logical_select_0(
 ; CHECK-NEXT:    [[FABS_X:%.*]] = call half @llvm.fabs.f16(half [[X:%.*]])
diff --git a/llvm/test/Transforms/InstCombine/fabs.ll b/llvm/test/Transforms/InstCombine/fabs.ll
index 7b9a672f188c..f449d4b8e6b3 100644
--- a/llvm/test/Transforms/InstCombine/fabs.ll
+++ b/llvm/test/Transforms/InstCombine/fabs.ll
@@ -256,6 +256,19 @@ define double @select_fcmp_ole_zero(double %x) {
 ; CHECK-LABEL: @select_fcmp_ole_zero(
 ; CHECK-NEXT:    [[FABS:%.*]] = call double @llvm.fabs.f64(double [[X:%.*]])
 ; CHECK-NEXT:    ret double [[FABS]]
+;
+  %lezero = fcmp nnan ole double %x, 0.0
+  %negx = fsub double 0.0, %x
+  %fabs = select i1 %lezero, double %negx, double %x
+  ret double %fabs
+}
+
+define double @select_fcmp_ole_zero_no_nnan(double %x) {
+; CHECK-LABEL: @select_fcmp_ole_zero_no_nnan(
+; CHECK-NEXT:    [[LEZERO:%.*]] = fcmp ole double [[X:%.*]], 0.000000e+00
+; CHECK-NEXT:    [[NEGX:%.*]] = fsub double 0.000000e+00, [[X]]
+; CHECK-NEXT:    [[FABS:%.*]] = select i1 [[LEZERO]], double [[NEGX]], double [[X]]
+; CHECK-NEXT:    ret double [[FABS]]
 ;
   %lezero = fcmp ole double %x, 0.0
   %negx = fsub double 0.0, %x
@@ -263,12 +276,34 @@ define double @select_fcmp_ole_zero(double %x) {
   ret double %fabs
 }
 
+define double @select_fcmp_ole_zero_no_nnan_input_nofpclass_nan(double nofpclass(nan) %x) {
+; CHECK-LABEL: @select_fcmp_ole_zero_no_nnan_input_nofpclass_nan(
+; CHECK-NEXT:    [[FABS:%.*]] = call double @llvm.fabs.f64(double [[X:%.*]])
+; CHECK-NEXT:    ret double [[FABS]]
+;
+  %lezero = fcmp ole double %x, 0.0
+  %negx = fsub double 0.0, %x
+  %fabs = select i1 %lezero, double %negx, double %x
+  ret double %fabs
+}
+
+define double @select_fcmp_ole_zero_select_nnan(double %x) {
+; CHECK-LABEL: @select_fcmp_ole_zero_select_nnan(
+; CHECK-NEXT:    [[FABS:%.*]] = call nnan double @llvm.fabs.f64(double [[X:%.*]])
+; CHECK-NEXT:    ret double [[FABS]]
+;
+  %lezero = fcmp ole double %x, 0.0
+  %negx = fsub double 0.0, %x
+  %fabs = select nnan i1 %lezero, double %negx, double %x
+  ret double %fabs
+}
+
 define double @select_fcmp_nnan_ole_zero(double %x) {
 ; CHECK-LABEL: @select_fcmp_nnan_ole_zero(
 ; CHECK-NEXT:    [[FABS:%.*]] = call double @llvm.fabs.f64(double [[X:%.*]])
 ; CHECK-NEXT:    ret double [[FABS]]
 ;
-  %lezero = fcmp ole double %x, 0.0
+  %lezero = fcmp nnan ole double %x, 0.0
   %negx = fsub nnan double 0.0, %x
   %fabs = select i1 %lezero, double %negx, double %x
   ret double %fabs
@@ -279,7 +314,7 @@ define double @select_nnan_fcmp_nnan_ole_zero(double %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call nnan double @llvm.fabs.f64(double [[X:%.*]])
 ; CHECK-NEXT:    ret double [[FABS]]
 ;
-  %lezero = fcmp ole double %x, 0.0
+  %lezero = fcmp nnan ole double %x, 0.0
   %negx = fsub nnan double 0.0, %x
   %fabs = select nnan i1 %lezero, double %negx, double %x
   ret double %fabs
@@ -292,7 +327,7 @@ define double @select_fcmp_nnan_ule_zero(double %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call double @llvm.fabs.f64(double [[X:%.*]])
 ; CHECK-NEXT:    ret double [[FABS]]
 ;
-  %lezero = fcmp ule double %x, 0.0
+  %lezero = fcmp nnan ule double %x, 0.0
   %negx = fsub nnan double 0.0, %x
   %fabs = select i1 %lezero, double %negx, double %x
   ret double %fabs
@@ -320,7 +355,7 @@ define <2 x float> @select_fcmp_nnan_ole_negzero(<2 x float> %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call <2 x float> @llvm.fabs.v2f32(<2 x float> [[X:%.*]])
 ; CHECK-NEXT:    ret <2 x float> [[FABS]]
 ;
-  %lezero = fcmp ole <2 x float> %x, <float -0.0, float -0.0>
+  %lezero = fcmp nnan ole <2 x float> %x, <float -0.0, float -0.0>
   %negx = fsub nnan <2 x float> <float 0.0, float poison>, %x
   %fabs = select <2 x i1> %lezero, <2 x float> %negx, <2 x float> %x
   ret <2 x float> %fabs
@@ -331,7 +366,7 @@ define <2 x float> @select_nnan_fcmp_nnan_ole_negzero(<2 x float> %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call nnan <2 x float> @llvm.fabs.v2f32(<2 x float> [[X:%.*]])
 ; CHECK-NEXT:    ret <2 x float> [[FABS]]
 ;
-  %lezero = fcmp ole <2 x float> %x, <float -0.0, float -0.0>
+  %lezero = fcmp nnan ole <2 x float> %x, <float -0.0, float -0.0>
   %negx = fsub nnan <2 x float> <float 0.0, float poison>, %x
   %fabs = select nnan <2 x i1> %lezero, <2 x float> %negx, <2 x float> %x
   ret <2 x float> %fabs
@@ -344,7 +379,7 @@ define fp128 @select_fcmp_ogt_zero(fp128 %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call fp128 @llvm.fabs.f128(fp128 [[X:%.*]])
 ; CHECK-NEXT:    ret fp128 [[FABS]]
 ;
-  %gtzero = fcmp ogt fp128 %x, zeroinitializer
+  %gtzero = fcmp nnan ogt fp128 %x, zeroinitializer
   %negx = fsub fp128 zeroinitializer, %x
   %fabs = select i1 %gtzero, fp128 %x, fp128 %negx
   ret fp128 %fabs
@@ -382,7 +417,7 @@ define fp128 @select_fcmp_nnan_ogt_zero(fp128 %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call fp128 @llvm.fabs.f128(fp128 [[X:%.*]])
 ; CHECK-NEXT:    ret fp128 [[FABS]]
 ;
-  %gtzero = fcmp ogt fp128 %x, zeroinitializer
+  %gtzero = fcmp nnan ogt fp128 %x, zeroinitializer
   %negx = fsub nnan fp128 zeroinitializer, %x
   %fabs = select i1 %gtzero, fp128 %x, fp128 %negx
   ret fp128 %fabs
@@ -393,7 +428,7 @@ define fp128 @select_nnan_fcmp_nnan_ogt_zero(fp128 %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call nnan fp128 @llvm.fabs.f128(fp128 [[X:%.*]])
 ; CHECK-NEXT:    ret fp128 [[FABS]]
 ;
-  %gtzero = fcmp ogt fp128 %x, zeroinitializer
+  %gtzero = fcmp nnan ogt fp128 %x, zeroinitializer
   %negx = fsub nnan fp128 zeroinitializer, %x
   %fabs = select nnan i1 %gtzero, fp128 %x, fp128 %negx
   ret fp128 %fabs
@@ -406,7 +441,7 @@ define half @select_fcmp_nnan_ogt_negzero(half %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call half @llvm.fabs.f16(half [[X:%.*]])
 ; CHECK-NEXT:    ret half [[FABS]]
 ;
-  %gtzero = fcmp ogt half %x, -0.0
+  %gtzero = fcmp nnan ogt half %x, -0.0
   %negx = fsub nnan half 0.0, %x
   %fabs = select i1 %gtzero, half %x, half %negx
   ret half %fabs
@@ -417,7 +452,7 @@ define half @select_nnan_fcmp_nnan_ogt_negzero(half %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call nnan half @llvm.fabs.f16(half [[X:%.*]])
 ; CHECK-NEXT:    ret half [[FABS]]
 ;
-  %gtzero = fcmp ogt half %x, -0.0
+  %gtzero = fcmp nnan ogt half %x, -0.0
   %negx = fsub nnan half 0.0, %x
   %fabs = select nnan i1 %gtzero, half %x, half %negx
   ret half %fabs
@@ -430,7 +465,7 @@ define half @select_fcmp_nnan_ugt_negzero(half %x) {
 ; CHECK-NEXT:    [[FABS:%.*]] = call half @llvm.fabs.f16(half [[X:%.*]])
 ; CHECK-NEXT:    ret half [[FABS]]
 ;
-  %gtzero = fcmp ugt half %x, -0.0
+  %gtzero = fcmp nnan ugt half %x, -0.0
   %negx = fsub nnan half 0.0, %x
   %fabs = select i1 %gtzero, half %x, half %negx
   ret half %fabs
diff --git a/llvm/test/Transforms/InstCombine/shufflevec-bitcast.ll b/llvm/test/Transforms/InstCombine/shufflevec-bitcast.ll
index f20077243273..877dd1eefbae 100644
--- a/llvm/test/Transforms/InstCombine/shufflevec-bitcast.ll
+++ b/llvm/test/Transforms/InstCombine/shufflevec-bitcast.ll
@@ -235,3 +235,38 @@ define <3 x i4> @shuf_bitcast_wrong_size(<2 x i8> %v, i8 %x) {
   %r = shufflevector <4 x i4> %b, <4 x i4> undef, <3 x i32> <i32 0, i32 1, i32 2>
   ret <3 x i4> %r
 }
+
+; Negative test - chain of bitcasts.
+
+define <16 x i8> @shuf_bitcast_chain(<8 x i32> %v) {
+; CHECK-LABEL: @shuf_bitcast_chain(
+; CHECK-NEXT:    [[S:%.*]] = shufflevector <8 x i32> [[V:%.*]], <8 x i32> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
+; CHECK-NEXT:    [[C:%.*]] = bitcast <4 x i32> [[S]] to <16 x i8>
+; CHECK-NEXT:    ret <16 x i8> [[C]]
+;
+  %s = shufflevector <8 x i32> %v, <8 x i32> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
+  %a = bitcast <4 x i32> %s to <2 x i64>
+  %b = bitcast <2 x i64> %a to i128
+  %c = bitcast i128 %b to <16 x i8>
+  ret <16 x i8> %c
+}
+
+; Same as above, but showing why it's not feasable to implement the reverse
+; fold in VectorCombine (see #136998).
+
+define <4 x i32> @shuf_bitcast_chain_2(<8 x i32> %v) {
+; CHECK-LABEL: @shuf_bitcast_chain_2(
+; CHECK-NEXT:    [[S0:%.*]] = shufflevector <8 x i32> [[V:%.*]], <8 x i32> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
+; CHECK-NEXT:    [[S1:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
+; CHECK-NEXT:    [[R:%.*]] = or <4 x i32> [[S0]], [[S1]]
+; CHECK-NEXT:    ret <4 x i32> [[R]]
+;
+  %s0 = shufflevector <8 x i32> %v, <8 x i32> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
+  %s1 = shufflevector <8 x i32> %v, <8 x i32> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
+  %b0 = bitcast <4 x i32> %s0 to i128
+  %b1 = bitcast <4 x i32> %s1 to i128
+  %c0 = bitcast i128 %b0 to <4 x i32>
+  %c1 = bitcast i128 %b1 to <4 x i32>
+  %r = or <4 x i32> %c0, %c1
+  ret <4 x i32> %r
+}
diff --git a/llvm/test/Transforms/InstSimplify/fcmp.ll b/llvm/test/Transforms/InstSimplify/fcmp.ll
index 64132f5fb7db..0c2be5210a74 100644
--- a/llvm/test/Transforms/InstSimplify/fcmp.ll
+++ b/llvm/test/Transforms/InstSimplify/fcmp.ll
@@ -16,3 +16,20 @@ define i1 @poison2(float %x) {
   %v = fcmp ueq float %x, poison
   ret i1 %v
 }
+
+define i1 @pr130408(x86_fp80 %x) {
+; CHECK-LABEL: @pr130408(
+; CHECK-NEXT:    [[BITS:%.*]] = bitcast x86_fp80 [[X:%.*]] to i80
+; CHECK-NEXT:    [[MASKED:%.*]] = and i80 [[BITS]], -604444463063240877801473
+; CHECK-NEXT:    [[OR:%.*]] = or i80 [[MASKED]], 302194561415509874573312
+; CHECK-NEXT:    [[FP:%.*]] = bitcast i80 [[OR]] to x86_fp80
+; CHECK-NEXT:    [[RES:%.*]] = fcmp uno x86_fp80 [[FP]], 0xK00000000000000000000
+; CHECK-NEXT:    ret i1 [[RES]]
+;
+  %bits = bitcast x86_fp80 %x to i80
+  %masked = and i80 %bits, -604444463063240877801473
+  %or = or i80 %masked, 302194561415509874573312
+  %fp = bitcast i80 %or to x86_fp80
+  %res = fcmp uno x86_fp80 %fp, 0xK00000000000000000000
+  ret i1 %res
+}
diff --git a/llvm/test/Transforms/LoopUnroll/pr131465.ll b/llvm/test/Transforms/LoopUnroll/pr131465.ll
new file mode 100644
index 000000000000..643b020c6c11
--- /dev/null
+++ b/llvm/test/Transforms/LoopUnroll/pr131465.ll
@@ -0,0 +1,43 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -S -passes=loop-unroll -unroll-runtime %s | FileCheck %s
+
+define i32 @pr131465(i1 %x) mustprogress {
+; CHECK-LABEL: define i32 @pr131465(
+; CHECK-SAME: i1 [[X:%.*]]) #[[ATTR0:[0-9]+]] {
+; CHECK-NEXT:  [[ENTRY:.*]]:
+; CHECK-NEXT:    [[INC:%.*]] = zext i1 [[X]] to i32
+; CHECK-NEXT:    br label %[[FOR_BODY:.*]]
+; CHECK:       [[FOR_BODY]]:
+; CHECK-NEXT:    [[INDVAR:%.*]] = phi i32 [ 2, %[[ENTRY]] ], [ [[NEXT_1:%.*]], %[[FOR_BODY_1:.*]] ]
+; CHECK-NEXT:    [[NEXT:%.*]] = add nsw i32 [[INDVAR]], [[INC]]
+; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i32 [[NEXT]], 2
+; CHECK-NEXT:    br i1 [[EXITCOND]], label %[[FOR_END:.*]], label %[[FOR_BODY_1]], !llvm.loop [[LOOP0:![0-9]+]]
+; CHECK:       [[FOR_BODY_1]]:
+; CHECK-NEXT:    [[NEXT_1]] = add nsw i32 [[NEXT]], [[INC]]
+; CHECK-NEXT:    [[EXITCOND_1:%.*]] = icmp eq i32 [[NEXT_1]], 2
+; CHECK-NEXT:    br i1 [[EXITCOND_1]], label %[[FOR_END]], label %[[FOR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
+; CHECK:       [[FOR_END]]:
+; CHECK-NEXT:    ret i32 0
+;
+entry:
+  %inc = zext i1 %x to i32
+  br label %for.body
+
+for.body:
+  %indvar = phi i32 [ 2, %entry ], [ %next, %for.body ]
+  %next = add nsw i32 %indvar, %inc
+  %exitcond = icmp eq i32 %next, 2
+  br i1 %exitcond, label %for.end, label %for.body, !llvm.loop !0
+
+for.end:
+  ret i32 0
+}
+
+; Force runtime unrolling.
+!0 = !{!0, !{!"llvm.loop.unroll.count", i32 2}}
+;.
+; CHECK: [[LOOP0]] = distinct !{[[LOOP0]], [[META1:![0-9]+]]}
+; CHECK: [[META1]] = !{!"llvm.loop.unroll.count", i32 2}
+; CHECK: [[LOOP2]] = distinct !{[[LOOP2]], [[META3:![0-9]+]]}
+; CHECK: [[META3]] = !{!"llvm.loop.unroll.disable"}
+;.
diff --git a/llvm/test/Transforms/LoopVectorize/AArch64/epilog-iv-select-cmp.ll b/llvm/test/Transforms/LoopVectorize/AArch64/epilog-iv-select-cmp.ll
new file mode 100644
index 000000000000..7355aed6ae65
--- /dev/null
+++ b/llvm/test/Transforms/LoopVectorize/AArch64/epilog-iv-select-cmp.ll
@@ -0,0 +1,166 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --check-globals none --version 5
+; RUN: opt -passes=loop-vectorize -mtriple=arm64-apple-macosx -S %s | FileCheck %s
+
+define i8 @select_icmp_var_start(ptr %a, i8 %n, i8 %start) {
+; CHECK-LABEL: define i8 @select_icmp_var_start(
+; CHECK-SAME: ptr [[A:%.*]], i8 [[N:%.*]], i8 [[START:%.*]]) {
+; CHECK-NEXT:  [[ENTRY:.*]]:
+; CHECK-NEXT:    [[TMP0:%.*]] = add i8 [[N]], -1
+; CHECK-NEXT:    [[TMP1:%.*]] = zext i8 [[TMP0]] to i32
+; CHECK-NEXT:    [[TMP2:%.*]] = add nuw nsw i32 [[TMP1]], 1
+; CHECK-NEXT:    [[MIN_ITERS_CHECK1:%.*]] = icmp ult i32 [[TMP2]], 32
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK1]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+; CHECK:       [[VECTOR_PH]]:
+; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i32 [[TMP2]], 32
+; CHECK-NEXT:    [[N_VEC:%.*]] = sub i32 [[TMP2]], [[N_MOD_VF]]
+; CHECK-NEXT:    [[TMP3:%.*]] = trunc i32 [[N_VEC]] to i8
+; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK:       [[VECTOR_BODY]]:
+; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <16 x i8> [ <i8 0, i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <16 x i8> [ splat (i8 -128), %[[VECTOR_PH]] ], [ [[TMP10:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI2:%.*]] = phi <16 x i8> [ splat (i8 -128), %[[VECTOR_PH]] ], [ [[TMP11:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[STEP_ADD:%.*]] = add <16 x i8> [[VEC_IND]], splat (i8 16)
+; CHECK-NEXT:    [[INDEX4:%.*]] = trunc i32 [[INDEX]] to i8
+; CHECK-NEXT:    [[TMP5:%.*]] = add i8 [[INDEX4]], 0
+; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i8, ptr [[A]], i8 [[TMP5]]
+; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds i8, ptr [[TMP8]], i32 0
+; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds i8, ptr [[TMP8]], i32 16
+; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <16 x i8>, ptr [[TMP9]], align 8
+; CHECK-NEXT:    [[WIDE_LOAD3:%.*]] = load <16 x i8>, ptr [[TMP7]], align 8
+; CHECK-NEXT:    [[TMP17:%.*]] = icmp eq <16 x i8> [[WIDE_LOAD]], splat (i8 3)
+; CHECK-NEXT:    [[TMP23:%.*]] = icmp eq <16 x i8> [[WIDE_LOAD3]], splat (i8 3)
+; CHECK-NEXT:    [[TMP10]] = select <16 x i1> [[TMP17]], <16 x i8> [[VEC_IND]], <16 x i8> [[VEC_PHI]]
+; CHECK-NEXT:    [[TMP11]] = select <16 x i1> [[TMP23]], <16 x i8> [[STEP_ADD]], <16 x i8> [[VEC_PHI2]]
+; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i32 [[INDEX]], 32
+; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <16 x i8> [[STEP_ADD]], splat (i8 16)
+; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i32 [[INDEX_NEXT]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
+; CHECK:       [[MIDDLE_BLOCK]]:
+; CHECK-NEXT:    [[RDX_MINMAX:%.*]] = call <16 x i8> @llvm.smax.v16i8(<16 x i8> [[TMP10]], <16 x i8> [[TMP11]])
+; CHECK-NEXT:    [[TMP13:%.*]] = call i8 @llvm.vector.reduce.smax.v16i8(<16 x i8> [[RDX_MINMAX]])
+; CHECK-NEXT:    [[RDX_SELECT_CMP12:%.*]] = icmp ne i8 [[TMP13]], -128
+; CHECK-NEXT:    [[RDX_SELECT:%.*]] = select i1 [[RDX_SELECT_CMP12]], i8 [[TMP13]], i8 [[START]]
+; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i32 [[TMP2]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+; CHECK:       [[SCALAR_PH]]:
+; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i8 [ [[TMP3]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i8 [ [[RDX_SELECT]], %[[MIDDLE_BLOCK]] ], [ [[START]], %[[ENTRY]] ]
+; CHECK-NEXT:    br label %[[LOOP:.*]]
+; CHECK:       [[LOOP]]:
+; CHECK-NEXT:    [[IV1:%.*]] = phi i8 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[RDX:%.*]] = phi i8 [ [[BC_MERGE_RDX]], %[[SCALAR_PH]] ], [ [[SEL:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[GEP1:%.*]] = getelementptr inbounds i8, ptr [[A]], i8 [[IV1]]
+; CHECK-NEXT:    [[L:%.*]] = load i8, ptr [[GEP1]], align 8
+; CHECK-NEXT:    [[C:%.*]] = icmp eq i8 [[L]], 3
+; CHECK-NEXT:    [[SEL]] = select i1 [[C]], i8 [[IV1]], i8 [[RDX]]
+; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i8 [[IV1]], 1
+; CHECK-NEXT:    [[EC:%.*]] = icmp eq i8 [[IV_NEXT]], [[N]]
+; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP3:![0-9]+]]
+; CHECK:       [[EXIT]]:
+; CHECK-NEXT:    [[SEL_LCSSA:%.*]] = phi i8 [ [[SEL]], %[[LOOP]] ], [ [[RDX_SELECT]], %[[MIDDLE_BLOCK]] ]
+; CHECK-NEXT:    ret i8 [[SEL_LCSSA]]
+;
+entry:
+  br label %loop
+
+loop:
+  %iv = phi i8 [ 0, %entry ], [ %iv.next, %loop ]
+  %rdx = phi i8 [ %start, %entry ], [ %sel, %loop ]
+  %gep = getelementptr inbounds i8, ptr %a, i8 %iv
+  %l = load i8, ptr %gep, align 8
+  %c = icmp eq i8 %l, 3
+  %sel = select i1 %c, i8 %iv, i8 %rdx
+  %iv.next = add nuw nsw i8 %iv, 1
+  %ec = icmp eq i8 %iv.next, %n
+  br i1 %ec, label %exit, label %loop
+
+exit:
+  ret i8 %sel
+}
+
+define i32 @select_icmp_var_start_iv_trunc(i32 %N, i32 %start) #0 {
+; CHECK-LABEL: define i32 @select_icmp_var_start_iv_trunc(
+; CHECK-SAME: i32 [[N:%.*]], i32 [[START:%.*]]) #[[ATTR0:[0-9]+]] {
+; CHECK-NEXT:  [[ENTRY:.*]]:
+; CHECK-NEXT:    [[N_POS:%.*]] = icmp sgt i32 [[N]], 0
+; CHECK-NEXT:    call void @llvm.assume(i1 [[N_POS]])
+; CHECK-NEXT:    [[N_EXT:%.*]] = zext i32 [[N]] to i64
+; CHECK-NEXT:    [[TMP0:%.*]] = add nuw nsw i64 [[N_EXT]], 1
+; CHECK-NEXT:    [[MIN_ITERS_CHECK1:%.*]] = icmp ult i64 [[TMP0]], 16
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK1]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+; CHECK:       [[VECTOR_PH]]:
+; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[TMP0]], 16
+; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[TMP0]], [[N_MOD_VF]]
+; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <4 x i32> poison, i32 [[START]], i64 0
+; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <4 x i32> [[BROADCAST_SPLATINSERT]], <4 x i32> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    [[TMP1:%.*]] = icmp eq <4 x i32> [[BROADCAST_SPLAT]], zeroinitializer
+; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK:       [[VECTOR_BODY]]:
+; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i32> [ splat (i32 -2147483648), %[[VECTOR_PH]] ], [ [[TMP3:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI2:%.*]] = phi <4 x i32> [ splat (i32 -2147483648), %[[VECTOR_PH]] ], [ [[TMP4:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI3:%.*]] = phi <4 x i32> [ splat (i32 -2147483648), %[[VECTOR_PH]] ], [ [[TMP5:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI4:%.*]] = phi <4 x i32> [ splat (i32 -2147483648), %[[VECTOR_PH]] ], [ [[TMP6:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <4 x i32> [ <i32 0, i32 1, i32 2, i32 3>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[STEP_ADD:%.*]] = add <4 x i32> [[VEC_IND]], splat (i32 4)
+; CHECK-NEXT:    [[STEP_ADD_2:%.*]] = add <4 x i32> [[STEP_ADD]], splat (i32 4)
+; CHECK-NEXT:    [[STEP_ADD_3:%.*]] = add <4 x i32> [[STEP_ADD_2]], splat (i32 4)
+; CHECK-NEXT:    [[TMP2:%.*]] = extractelement <4 x i1> [[TMP1]], i32 0
+; CHECK-NEXT:    [[TMP3]] = select i1 [[TMP2]], <4 x i32> [[VEC_IND]], <4 x i32> [[VEC_PHI]]
+; CHECK-NEXT:    [[TMP4]] = select i1 [[TMP2]], <4 x i32> [[STEP_ADD]], <4 x i32> [[VEC_PHI2]]
+; CHECK-NEXT:    [[TMP5]] = select i1 [[TMP2]], <4 x i32> [[STEP_ADD_2]], <4 x i32> [[VEC_PHI3]]
+; CHECK-NEXT:    [[TMP6]] = select i1 [[TMP2]], <4 x i32> [[STEP_ADD_3]], <4 x i32> [[VEC_PHI4]]
+; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 16
+; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <4 x i32> [[STEP_ADD_3]], splat (i32 4)
+; CHECK-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[TMP7]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
+; CHECK:       [[MIDDLE_BLOCK]]:
+; CHECK-NEXT:    [[RDX_MINMAX:%.*]] = call <4 x i32> @llvm.smax.v4i32(<4 x i32> [[TMP3]], <4 x i32> [[TMP4]])
+; CHECK-NEXT:    [[RDX_MINMAX5:%.*]] = call <4 x i32> @llvm.smax.v4i32(<4 x i32> [[RDX_MINMAX]], <4 x i32> [[TMP5]])
+; CHECK-NEXT:    [[RDX_MINMAX6:%.*]] = call <4 x i32> @llvm.smax.v4i32(<4 x i32> [[RDX_MINMAX5]], <4 x i32> [[TMP6]])
+; CHECK-NEXT:    [[TMP16:%.*]] = call i32 @llvm.vector.reduce.smax.v4i32(<4 x i32> [[RDX_MINMAX6]])
+; CHECK-NEXT:    [[RDX_SELECT_CMP18:%.*]] = icmp ne i32 [[TMP16]], -2147483648
+; CHECK-NEXT:    [[RDX_SELECT19:%.*]] = select i1 [[RDX_SELECT_CMP18]], i32 [[TMP16]], i32 [[START]]
+; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[TMP0]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+; CHECK:       [[SCALAR_PH]]:
+; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ [[RDX_SELECT19]], %[[MIDDLE_BLOCK]] ], [ [[START]], %[[ENTRY]] ]
+; CHECK-NEXT:    br label %[[LOOP:.*]]
+; CHECK:       [[LOOP]]:
+; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[RED:%.*]] = phi i32 [ [[BC_MERGE_RDX]], %[[SCALAR_PH]] ], [ [[RED_NEXT:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[C:%.*]] = icmp eq i32 [[START]], 0
+; CHECK-NEXT:    [[IV_TRUNC:%.*]] = trunc i64 [[IV]] to i32
+; CHECK-NEXT:    [[RED_NEXT]] = select i1 [[C]], i32 [[IV_TRUNC]], i32 [[RED]]
+; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
+; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV]], [[N_EXT]]
+; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP5:![0-9]+]]
+; CHECK:       [[EXIT]]:
+; CHECK-NEXT:    [[RED_NEXT_LCSSA:%.*]] = phi i32 [ [[RED_NEXT]], %[[LOOP]] ], [ [[RDX_SELECT19]], %[[MIDDLE_BLOCK]] ]
+; CHECK-NEXT:    ret i32 [[RED_NEXT_LCSSA]]
+;
+entry:
+  %N.pos = icmp sgt i32 %N, 0
+  call void @llvm.assume(i1 %N.pos)
+  %N.ext = zext i32 %N to i64
+  br label %loop
+
+loop:
+  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop ]
+  %red = phi i32 [ %start, %entry ], [ %red.next, %loop ]
+  %c = icmp eq i32 %start, 0
+  %iv.trunc = trunc i64 %iv to i32
+  %red.next = select i1 %c, i32 %iv.trunc, i32 %red
+  %iv.next = add i64 %iv, 1
+  %ec = icmp eq i64 %iv, %N.ext
+  br i1 %ec, label %exit, label %loop
+
+exit:
+  ret i32 %red.next
+}
+
+declare void @llvm.assume(i1 noundef)
+
+attributes #0 = { "target-cpu"="apple-m1" }
diff --git a/llvm/test/Transforms/LoopVectorize/X86/cost-model.ll b/llvm/test/Transforms/LoopVectorize/X86/cost-model.ll
index bd28e28ddff9..3718a092d9aa 100644
--- a/llvm/test/Transforms/LoopVectorize/X86/cost-model.ll
+++ b/llvm/test/Transforms/LoopVectorize/X86/cost-model.ll
@@ -1211,6 +1211,130 @@ exit:
   ret i32 %or
 }
 
+; Check if the vplan-based cost model select same VF to the legacy cost model.
+; Reduced from: https://github.com/llvm/llvm-project/issues/115744#issuecomment-2670479463
+define i32 @g(i64 %n) {
+; CHECK-LABEL: @g(
+; CHECK-NEXT:  iter.check:
+; CHECK-NEXT:    [[TMP0:%.*]] = trunc i64 [[N:%.*]] to i32
+; CHECK-NEXT:    [[TMP1:%.*]] = add i32 [[TMP0]], 1
+; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i32 [[TMP1]], 4
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[VEC_EPILOG_SCALAR_PH:%.*]], label [[VECTOR_SCEVCHECK:%.*]]
+; CHECK:       vector.scevcheck:
+; CHECK-NEXT:    [[TMP2:%.*]] = icmp ugt i64 [[N]], 4294967295
+; CHECK-NEXT:    br i1 [[TMP2]], label [[VEC_EPILOG_SCALAR_PH]], label [[VECTOR_MAIN_LOOP_ITER_CHECK:%.*]]
+; CHECK:       vector.main.loop.iter.check:
+; CHECK-NEXT:    [[MIN_ITERS_CHECK1:%.*]] = icmp ult i32 [[TMP1]], 16
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK1]], label [[VEC_EPILOG_PH:%.*]], label [[VECTOR_PH:%.*]]
+; CHECK:       vector.ph:
+; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i32 [[TMP1]], 16
+; CHECK-NEXT:    [[N_VEC:%.*]] = sub i32 [[TMP1]], [[N_MOD_VF]]
+; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <4 x i64> poison, i64 [[N]], i64 0
+; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <4 x i64> [[BROADCAST_SPLATINSERT]], <4 x i64> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
+; CHECK:       vector.body:
+; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <4 x i32> [ <i32 0, i32 1, i32 2, i32 3>, [[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], [[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP15:%.*]], [[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI2:%.*]] = phi <4 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP16:%.*]], [[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI3:%.*]] = phi <4 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP17:%.*]], [[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI4:%.*]] = phi <4 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP18:%.*]], [[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[STEP_ADD:%.*]] = add <4 x i32> [[VEC_IND]], splat (i32 4)
+; CHECK-NEXT:    [[STEP_ADD_2:%.*]] = add <4 x i32> [[STEP_ADD]], splat (i32 4)
+; CHECK-NEXT:    [[STEP_ADD_3:%.*]] = add <4 x i32> [[STEP_ADD_2]], splat (i32 4)
+; CHECK-NEXT:    [[TMP3:%.*]] = zext <4 x i32> [[VEC_IND]] to <4 x i64>
+; CHECK-NEXT:    [[TMP4:%.*]] = zext <4 x i32> [[STEP_ADD]] to <4 x i64>
+; CHECK-NEXT:    [[TMP5:%.*]] = zext <4 x i32> [[STEP_ADD_2]] to <4 x i64>
+; CHECK-NEXT:    [[TMP6:%.*]] = zext <4 x i32> [[STEP_ADD_3]] to <4 x i64>
+; CHECK-NEXT:    [[TMP7:%.*]] = icmp eq <4 x i64> [[BROADCAST_SPLAT]], [[TMP3]]
+; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq <4 x i64> [[BROADCAST_SPLAT]], [[TMP4]]
+; CHECK-NEXT:    [[TMP9:%.*]] = icmp eq <4 x i64> [[BROADCAST_SPLAT]], [[TMP5]]
+; CHECK-NEXT:    [[TMP10:%.*]] = icmp eq <4 x i64> [[BROADCAST_SPLAT]], [[TMP6]]
+; CHECK-NEXT:    [[TMP11:%.*]] = select <4 x i1> [[TMP7]], <4 x i32> zeroinitializer, <4 x i32> splat (i32 2)
+; CHECK-NEXT:    [[TMP12:%.*]] = select <4 x i1> [[TMP8]], <4 x i32> zeroinitializer, <4 x i32> splat (i32 2)
+; CHECK-NEXT:    [[TMP13:%.*]] = select <4 x i1> [[TMP9]], <4 x i32> zeroinitializer, <4 x i32> splat (i32 2)
+; CHECK-NEXT:    [[TMP14:%.*]] = select <4 x i1> [[TMP10]], <4 x i32> zeroinitializer, <4 x i32> splat (i32 2)
+; CHECK-NEXT:    [[TMP15]] = or <4 x i32> [[TMP11]], [[VEC_PHI]]
+; CHECK-NEXT:    [[TMP16]] = or <4 x i32> [[TMP12]], [[VEC_PHI2]]
+; CHECK-NEXT:    [[TMP17]] = or <4 x i32> [[TMP13]], [[VEC_PHI3]]
+; CHECK-NEXT:    [[TMP18]] = or <4 x i32> [[TMP14]], [[VEC_PHI4]]
+; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i32 [[INDEX]], 16
+; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <4 x i32> [[STEP_ADD_3]], splat (i32 4)
+; CHECK-NEXT:    [[TMP19:%.*]] = icmp eq i32 [[INDEX_NEXT]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[TMP19]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP30:![0-9]+]]
+; CHECK:       middle.block:
+; CHECK-NEXT:    [[BIN_RDX:%.*]] = or <4 x i32> [[TMP16]], [[TMP15]]
+; CHECK-NEXT:    [[BIN_RDX5:%.*]] = or <4 x i32> [[TMP17]], [[BIN_RDX]]
+; CHECK-NEXT:    [[BIN_RDX6:%.*]] = or <4 x i32> [[TMP18]], [[BIN_RDX5]]
+; CHECK-NEXT:    [[TMP20:%.*]] = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> [[BIN_RDX6]])
+; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i32 [[TMP1]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[CMP_N]], label [[EXIT:%.*]], label [[VEC_EPILOG_ITER_CHECK:%.*]]
+; CHECK:       vec.epilog.iter.check:
+; CHECK-NEXT:    [[N_VEC_REMAINING:%.*]] = sub i32 [[TMP1]], [[N_VEC]]
+; CHECK-NEXT:    [[MIN_EPILOG_ITERS_CHECK:%.*]] = icmp ult i32 [[N_VEC_REMAINING]], 4
+; CHECK-NEXT:    br i1 [[MIN_EPILOG_ITERS_CHECK]], label [[VEC_EPILOG_SCALAR_PH]], label [[VEC_EPILOG_PH]]
+; CHECK:       vec.epilog.ph:
+; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ [[N_VEC]], [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ [[TMP20]], [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
+; CHECK-NEXT:    [[N_MOD_VF7:%.*]] = urem i32 [[TMP1]], 4
+; CHECK-NEXT:    [[N_VEC8:%.*]] = sub i32 [[TMP1]], [[N_MOD_VF7]]
+; CHECK-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <4 x i32> poison, i32 [[BC_RESUME_VAL]], i64 0
+; CHECK-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <4 x i32> [[DOTSPLATINSERT]], <4 x i32> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    [[INDUCTION:%.*]] = add <4 x i32> [[DOTSPLAT]], <i32 0, i32 1, i32 2, i32 3>
+; CHECK-NEXT:    [[TMP21:%.*]] = insertelement <4 x i32> zeroinitializer, i32 [[BC_MERGE_RDX]], i32 0
+; CHECK-NEXT:    [[BROADCAST_SPLATINSERT13:%.*]] = insertelement <4 x i64> poison, i64 [[N]], i64 0
+; CHECK-NEXT:    [[BROADCAST_SPLAT14:%.*]] = shufflevector <4 x i64> [[BROADCAST_SPLATINSERT13]], <4 x i64> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    br label [[VEC_EPILOG_VECTOR_BODY:%.*]]
+; CHECK:       vec.epilog.vector.body:
+; CHECK-NEXT:    [[INDEX9:%.*]] = phi i32 [ [[BC_RESUME_VAL]], [[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT15:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND10:%.*]] = phi <4 x i32> [ [[INDUCTION]], [[VEC_EPILOG_PH]] ], [ [[VEC_IND_NEXT11:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI12:%.*]] = phi <4 x i32> [ [[TMP21]], [[VEC_EPILOG_PH]] ], [ [[TMP25:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[TMP22:%.*]] = zext <4 x i32> [[VEC_IND10]] to <4 x i64>
+; CHECK-NEXT:    [[TMP23:%.*]] = icmp eq <4 x i64> [[BROADCAST_SPLAT14]], [[TMP22]]
+; CHECK-NEXT:    [[TMP24:%.*]] = select <4 x i1> [[TMP23]], <4 x i32> zeroinitializer, <4 x i32> splat (i32 2)
+; CHECK-NEXT:    [[TMP25]] = or <4 x i32> [[TMP24]], [[VEC_PHI12]]
+; CHECK-NEXT:    [[INDEX_NEXT15]] = add nuw i32 [[INDEX9]], 4
+; CHECK-NEXT:    [[VEC_IND_NEXT11]] = add <4 x i32> [[VEC_IND10]], splat (i32 4)
+; CHECK-NEXT:    [[TMP26:%.*]] = icmp eq i32 [[INDEX_NEXT15]], [[N_VEC8]]
+; CHECK-NEXT:    br i1 [[TMP26]], label [[VEC_EPILOG_MIDDLE_BLOCK:%.*]], label [[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP31:![0-9]+]]
+; CHECK:       vec.epilog.middle.block:
+; CHECK-NEXT:    [[TMP27:%.*]] = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> [[TMP25]])
+; CHECK-NEXT:    [[CMP_N16:%.*]] = icmp eq i32 [[TMP1]], [[N_VEC8]]
+; CHECK-NEXT:    br i1 [[CMP_N16]], label [[EXIT]], label [[VEC_EPILOG_SCALAR_PH]]
+; CHECK:       vec.epilog.scalar.ph:
+; CHECK-NEXT:    [[BC_RESUME_VAL17:%.*]] = phi i32 [ [[N_VEC8]], [[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 0, [[VECTOR_SCEVCHECK]] ], [ 0, [[ITER_CHECK:%.*]] ], [ [[N_VEC]], [[VEC_EPILOG_ITER_CHECK]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX18:%.*]] = phi i32 [ [[TMP27]], [[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 0, [[VECTOR_SCEVCHECK]] ], [ 0, [[ITER_CHECK]] ], [ [[TMP20]], [[VEC_EPILOG_ITER_CHECK]] ]
+; CHECK-NEXT:    br label [[LOOP:%.*]]
+; CHECK:       loop:
+; CHECK-NEXT:    [[IV:%.*]] = phi i32 [ [[BC_RESUME_VAL17]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[IV_NEXT:%.*]], [[LOOP]] ]
+; CHECK-NEXT:    [[SELECT:%.*]] = phi i32 [ [[BC_MERGE_RDX18]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[SELECT_NEXT:%.*]], [[LOOP]] ]
+; CHECK-NEXT:    [[IV_WIDEN:%.*]] = zext i32 [[IV]] to i64
+; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[N]], [[IV_WIDEN]]
+; CHECK-NEXT:    [[SELECT_I:%.*]] = select i1 [[EXITCOND]], i32 0, i32 2
+; CHECK-NEXT:    [[SELECT_NEXT]] = or i32 [[SELECT_I]], [[SELECT]]
+; CHECK-NEXT:    [[IV_NEXT]] = add i32 [[IV]], 1
+; CHECK-NEXT:    br i1 [[EXITCOND]], label [[EXIT]], label [[LOOP]], !llvm.loop [[LOOP32:![0-9]+]]
+; CHECK:       exit:
+; CHECK-NEXT:    [[SELECT_NEXT_LCSSA:%.*]] = phi i32 [ [[SELECT_NEXT]], [[LOOP]] ], [ [[TMP20]], [[MIDDLE_BLOCK]] ], [ [[TMP27]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
+; CHECK-NEXT:    ret i32 [[SELECT_NEXT_LCSSA]]
+;
+entry:
+  br label %loop
+
+loop:
+  %iv = phi i32 [ 0, %entry ], [ %iv.next, %loop ]
+  %select = phi i32 [ 0, %entry ], [ %select.next, %loop ]
+  %iv.widen = zext i32 %iv to i64
+  %exitcond = icmp eq i64 %n, %iv.widen
+  %select.i = select i1 %exitcond, i32 0, i32 2
+  %select.next = or i32 %select.i, %select
+  %iv.next = add i32 %iv, 1
+  br i1 %exitcond, label %exit, label %loop
+
+exit:
+  ret i32 %select.next
+}
+
 declare void @llvm.assume(i1 noundef) #0
 
 attributes #0 = { "target-cpu"="penryn" }
diff --git a/llvm/test/Transforms/LoopVectorize/epilog-iv-select-cmp.ll b/llvm/test/Transforms/LoopVectorize/epilog-iv-select-cmp.ll
index 0db5d70db61d..26719f682167 100644
--- a/llvm/test/Transforms/LoopVectorize/epilog-iv-select-cmp.ll
+++ b/llvm/test/Transforms/LoopVectorize/epilog-iv-select-cmp.ll
@@ -1,23 +1,92 @@
-; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --check-globals none --version 5
 ; RUN: opt -passes=loop-vectorize -force-vector-interleave=1 -force-vector-width=4 -epilogue-vectorization-force-VF=4 -S < %s | FileCheck %s
 
 define i64 @select_icmp_const(ptr %a, i64 %n) {
 ; CHECK-LABEL: define i64 @select_icmp_const(
 ; CHECK-SAME: ptr [[A:%.*]], i64 [[N:%.*]]) {
-; CHECK-NEXT:  [[ENTRY:.*]]:
+; CHECK-NEXT:  [[ITER_CHECK:.*]]:
+; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N]], 4
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH:.*]], label %[[VECTOR_MAIN_LOOP_ITER_CHECK:.*]]
+; CHECK:       [[VECTOR_MAIN_LOOP_ITER_CHECK]]:
+; CHECK-NEXT:    [[MIN_ITERS_CHECK1:%.*]] = icmp ult i64 [[N]], 4
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK1]], label %[[VEC_EPILOG_PH:.*]], label %[[VECTOR_PH:.*]]
+; CHECK:       [[VECTOR_PH]]:
+; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
+; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
+; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK:       [[VECTOR_BODY]]:
+; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ splat (i64 -9223372036854775808), %[[VECTOR_PH]] ], [ [[TMP4:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
+; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds i64, ptr [[A]], i64 [[TMP0]]
+; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds i64, ptr [[TMP1]], i32 0
+; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x i64>, ptr [[TMP2]], align 8
+; CHECK-NEXT:    [[TMP3:%.*]] = icmp eq <4 x i64> [[WIDE_LOAD]], splat (i64 3)
+; CHECK-NEXT:    [[TMP4]] = select <4 x i1> [[TMP3]], <4 x i64> [[VEC_IND]], <4 x i64> [[VEC_PHI]]
+; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
+; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <4 x i64> [[VEC_IND]], splat (i64 4)
+; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[TMP5]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
+; CHECK:       [[MIDDLE_BLOCK]]:
+; CHECK-NEXT:    [[TMP6:%.*]] = call i64 @llvm.vector.reduce.smax.v4i64(<4 x i64> [[TMP4]])
+; CHECK-NEXT:    [[RDX_SELECT_CMP:%.*]] = icmp ne i64 [[TMP6]], -9223372036854775808
+; CHECK-NEXT:    [[RDX_SELECT:%.*]] = select i1 [[RDX_SELECT_CMP]], i64 [[TMP6]], i64 3
+; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[VEC_EPILOG_ITER_CHECK:.*]]
+; CHECK:       [[VEC_EPILOG_ITER_CHECK]]:
+; CHECK-NEXT:    [[N_VEC_REMAINING:%.*]] = sub i64 [[N]], [[N_VEC]]
+; CHECK-NEXT:    [[MIN_EPILOG_ITERS_CHECK:%.*]] = icmp ult i64 [[N_VEC_REMAINING]], 4
+; CHECK-NEXT:    br i1 [[MIN_EPILOG_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH]], label %[[VEC_EPILOG_PH]]
+; CHECK:       [[VEC_EPILOG_PH]]:
+; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX1:%.*]] = phi i64 [ [[RDX_SELECT]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 3, %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
+; CHECK-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[BC_MERGE_RDX1]], 3
+; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = select i1 [[TMP14]], i64 -9223372036854775808, i64 [[BC_MERGE_RDX1]]
+; CHECK-NEXT:    [[N_MOD_VF2:%.*]] = urem i64 [[N]], 4
+; CHECK-NEXT:    [[N_VEC3:%.*]] = sub i64 [[N]], [[N_MOD_VF2]]
+; CHECK-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <4 x i64> poison, i64 [[BC_RESUME_VAL]], i64 0
+; CHECK-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <4 x i64> [[DOTSPLATINSERT]], <4 x i64> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    [[INDUCTION:%.*]] = add <4 x i64> [[DOTSPLAT]], <i64 0, i64 1, i64 2, i64 3>
+; CHECK-NEXT:    [[DOTSPLATINSERT8:%.*]] = insertelement <4 x i64> poison, i64 [[BC_MERGE_RDX]], i64 0
+; CHECK-NEXT:    [[DOTSPLAT9:%.*]] = shufflevector <4 x i64> [[DOTSPLATINSERT8]], <4 x i64> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    br label %[[VEC_EPILOG_VECTOR_BODY:.*]]
+; CHECK:       [[VEC_EPILOG_VECTOR_BODY]]:
+; CHECK-NEXT:    [[INDEX4:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT9:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND5:%.*]] = phi <4 x i64> [ [[INDUCTION]], %[[VEC_EPILOG_PH]] ], [ [[VEC_IND_NEXT6:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI7:%.*]] = phi <4 x i64> [ [[DOTSPLAT9]], %[[VEC_EPILOG_PH]] ], [ [[TMP11:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[TMP7:%.*]] = add i64 [[INDEX4]], 0
+; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i64, ptr [[A]], i64 [[TMP7]]
+; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds i64, ptr [[TMP8]], i32 0
+; CHECK-NEXT:    [[WIDE_LOAD8:%.*]] = load <4 x i64>, ptr [[TMP9]], align 8
+; CHECK-NEXT:    [[TMP10:%.*]] = icmp eq <4 x i64> [[WIDE_LOAD8]], splat (i64 3)
+; CHECK-NEXT:    [[TMP11]] = select <4 x i1> [[TMP10]], <4 x i64> [[VEC_IND5]], <4 x i64> [[VEC_PHI7]]
+; CHECK-NEXT:    [[INDEX_NEXT9]] = add nuw i64 [[INDEX4]], 4
+; CHECK-NEXT:    [[VEC_IND_NEXT6]] = add <4 x i64> [[VEC_IND5]], splat (i64 4)
+; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT9]], [[N_VEC3]]
+; CHECK-NEXT:    br i1 [[TMP12]], label %[[VEC_EPILOG_MIDDLE_BLOCK:.*]], label %[[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP3:![0-9]+]]
+; CHECK:       [[VEC_EPILOG_MIDDLE_BLOCK]]:
+; CHECK-NEXT:    [[TMP13:%.*]] = call i64 @llvm.vector.reduce.smax.v4i64(<4 x i64> [[TMP11]])
+; CHECK-NEXT:    [[RDX_SELECT_CMP10:%.*]] = icmp ne i64 [[TMP13]], -9223372036854775808
+; CHECK-NEXT:    [[RDX_SELECT11:%.*]] = select i1 [[RDX_SELECT_CMP10]], i64 [[TMP13]], i64 3
+; CHECK-NEXT:    [[CMP_N12:%.*]] = icmp eq i64 [[N]], [[N_VEC3]]
+; CHECK-NEXT:    br i1 [[CMP_N12]], label %[[EXIT]], label %[[VEC_EPILOG_SCALAR_PH]]
+; CHECK:       [[VEC_EPILOG_SCALAR_PH]]:
+; CHECK-NEXT:    [[BC_RESUME_VAL15:%.*]] = phi i64 [ [[N_VEC3]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 0, %[[ITER_CHECK]] ], [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX16:%.*]] = phi i64 [ [[RDX_SELECT11]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 3, %[[ITER_CHECK]] ], [ [[RDX_SELECT]], %[[VEC_EPILOG_ITER_CHECK]] ]
 ; CHECK-NEXT:    br label %[[LOOP:.*]]
 ; CHECK:       [[LOOP]]:
-; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ 0, %[[ENTRY]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
-; CHECK-NEXT:    [[RDX:%.*]] = phi i64 [ 3, %[[ENTRY]] ], [ [[SEL:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL15]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[RDX:%.*]] = phi i64 [ [[BC_MERGE_RDX16]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[SEL:%.*]], %[[LOOP]] ]
 ; CHECK-NEXT:    [[GEP:%.*]] = getelementptr inbounds i64, ptr [[A]], i64 [[IV]]
 ; CHECK-NEXT:    [[L:%.*]] = load i64, ptr [[GEP]], align 8
 ; CHECK-NEXT:    [[C:%.*]] = icmp eq i64 [[L]], 3
 ; CHECK-NEXT:    [[SEL]] = select i1 [[C]], i64 [[IV]], i64 [[RDX]]
 ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV]], 1
 ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT:.*]], label %[[LOOP]]
+; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP4:![0-9]+]]
 ; CHECK:       [[EXIT]]:
-; CHECK-NEXT:    [[SEL_LCSSA:%.*]] = phi i64 [ [[SEL]], %[[LOOP]] ]
+; CHECK-NEXT:    [[SEL_LCSSA:%.*]] = phi i64 [ [[SEL]], %[[LOOP]] ], [ [[RDX_SELECT]], %[[MIDDLE_BLOCK]] ], [ [[RDX_SELECT11]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ]
 ; CHECK-NEXT:    ret i64 [[SEL_LCSSA]]
 ;
 entry:
@@ -41,20 +110,89 @@ exit:
 define i64 @select_fcmp_const_fast(ptr %a, i64 %n) {
 ; CHECK-LABEL: define i64 @select_fcmp_const_fast(
 ; CHECK-SAME: ptr [[A:%.*]], i64 [[N:%.*]]) {
-; CHECK-NEXT:  [[ENTRY:.*]]:
+; CHECK-NEXT:  [[ITER_CHECK:.*]]:
+; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N]], 4
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH:.*]], label %[[VECTOR_MAIN_LOOP_ITER_CHECK:.*]]
+; CHECK:       [[VECTOR_MAIN_LOOP_ITER_CHECK]]:
+; CHECK-NEXT:    [[MIN_ITERS_CHECK1:%.*]] = icmp ult i64 [[N]], 4
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK1]], label %[[VEC_EPILOG_PH:.*]], label %[[VECTOR_PH:.*]]
+; CHECK:       [[VECTOR_PH]]:
+; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
+; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
+; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK:       [[VECTOR_BODY]]:
+; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ splat (i64 -9223372036854775808), %[[VECTOR_PH]] ], [ [[TMP4:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
+; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds float, ptr [[A]], i64 [[TMP0]]
+; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, ptr [[TMP1]], i32 0
+; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, ptr [[TMP2]], align 4
+; CHECK-NEXT:    [[TMP3:%.*]] = fcmp fast ueq <4 x float> [[WIDE_LOAD]], splat (float 3.000000e+00)
+; CHECK-NEXT:    [[TMP4]] = select <4 x i1> [[TMP3]], <4 x i64> [[VEC_IND]], <4 x i64> [[VEC_PHI]]
+; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
+; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <4 x i64> [[VEC_IND]], splat (i64 4)
+; CHECK-NEXT:    [[TMP5:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[TMP5]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
+; CHECK:       [[MIDDLE_BLOCK]]:
+; CHECK-NEXT:    [[TMP6:%.*]] = call i64 @llvm.vector.reduce.smax.v4i64(<4 x i64> [[TMP4]])
+; CHECK-NEXT:    [[RDX_SELECT_CMP:%.*]] = icmp ne i64 [[TMP6]], -9223372036854775808
+; CHECK-NEXT:    [[RDX_SELECT:%.*]] = select i1 [[RDX_SELECT_CMP]], i64 [[TMP6]], i64 2
+; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[VEC_EPILOG_ITER_CHECK:.*]]
+; CHECK:       [[VEC_EPILOG_ITER_CHECK]]:
+; CHECK-NEXT:    [[N_VEC_REMAINING:%.*]] = sub i64 [[N]], [[N_VEC]]
+; CHECK-NEXT:    [[MIN_EPILOG_ITERS_CHECK:%.*]] = icmp ult i64 [[N_VEC_REMAINING]], 4
+; CHECK-NEXT:    br i1 [[MIN_EPILOG_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH]], label %[[VEC_EPILOG_PH]]
+; CHECK:       [[VEC_EPILOG_PH]]:
+; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX1:%.*]] = phi i64 [ [[RDX_SELECT]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 2, %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
+; CHECK-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[BC_MERGE_RDX1]], 2
+; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = select i1 [[TMP14]], i64 -9223372036854775808, i64 [[BC_MERGE_RDX1]]
+; CHECK-NEXT:    [[N_MOD_VF2:%.*]] = urem i64 [[N]], 4
+; CHECK-NEXT:    [[N_VEC3:%.*]] = sub i64 [[N]], [[N_MOD_VF2]]
+; CHECK-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <4 x i64> poison, i64 [[BC_RESUME_VAL]], i64 0
+; CHECK-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <4 x i64> [[DOTSPLATINSERT]], <4 x i64> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    [[INDUCTION:%.*]] = add <4 x i64> [[DOTSPLAT]], <i64 0, i64 1, i64 2, i64 3>
+; CHECK-NEXT:    [[DOTSPLATINSERT8:%.*]] = insertelement <4 x i64> poison, i64 [[BC_MERGE_RDX]], i64 0
+; CHECK-NEXT:    [[DOTSPLAT9:%.*]] = shufflevector <4 x i64> [[DOTSPLATINSERT8]], <4 x i64> poison, <4 x i32> zeroinitializer
+; CHECK-NEXT:    br label %[[VEC_EPILOG_VECTOR_BODY:.*]]
+; CHECK:       [[VEC_EPILOG_VECTOR_BODY]]:
+; CHECK-NEXT:    [[INDEX4:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT9:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND5:%.*]] = phi <4 x i64> [ [[INDUCTION]], %[[VEC_EPILOG_PH]] ], [ [[VEC_IND_NEXT6:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI7:%.*]] = phi <4 x i64> [ [[DOTSPLAT9]], %[[VEC_EPILOG_PH]] ], [ [[TMP11:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
+; CHECK-NEXT:    [[TMP7:%.*]] = add i64 [[INDEX4]], 0
+; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds float, ptr [[A]], i64 [[TMP7]]
+; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds float, ptr [[TMP8]], i32 0
+; CHECK-NEXT:    [[WIDE_LOAD8:%.*]] = load <4 x float>, ptr [[TMP9]], align 4
+; CHECK-NEXT:    [[TMP10:%.*]] = fcmp fast ueq <4 x float> [[WIDE_LOAD8]], splat (float 3.000000e+00)
+; CHECK-NEXT:    [[TMP11]] = select <4 x i1> [[TMP10]], <4 x i64> [[VEC_IND5]], <4 x i64> [[VEC_PHI7]]
+; CHECK-NEXT:    [[INDEX_NEXT9]] = add nuw i64 [[INDEX4]], 4
+; CHECK-NEXT:    [[VEC_IND_NEXT6]] = add <4 x i64> [[VEC_IND5]], splat (i64 4)
+; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT9]], [[N_VEC3]]
+; CHECK-NEXT:    br i1 [[TMP12]], label %[[VEC_EPILOG_MIDDLE_BLOCK:.*]], label %[[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
+; CHECK:       [[VEC_EPILOG_MIDDLE_BLOCK]]:
+; CHECK-NEXT:    [[TMP13:%.*]] = call i64 @llvm.vector.reduce.smax.v4i64(<4 x i64> [[TMP11]])
+; CHECK-NEXT:    [[RDX_SELECT_CMP10:%.*]] = icmp ne i64 [[TMP13]], -9223372036854775808
+; CHECK-NEXT:    [[RDX_SELECT11:%.*]] = select i1 [[RDX_SELECT_CMP10]], i64 [[TMP13]], i64 2
+; CHECK-NEXT:    [[CMP_N12:%.*]] = icmp eq i64 [[N]], [[N_VEC3]]
+; CHECK-NEXT:    br i1 [[CMP_N12]], label %[[EXIT]], label %[[VEC_EPILOG_SCALAR_PH]]
+; CHECK:       [[VEC_EPILOG_SCALAR_PH]]:
+; CHECK-NEXT:    [[BC_RESUME_VAL15:%.*]] = phi i64 [ [[N_VEC3]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 0, %[[ITER_CHECK]] ], [ [[N_VEC]], %[[VEC_EPILOG_ITER_CHECK]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX16:%.*]] = phi i64 [ [[RDX_SELECT11]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 2, %[[ITER_CHECK]] ], [ [[RDX_SELECT]], %[[VEC_EPILOG_ITER_CHECK]] ]
 ; CHECK-NEXT:    br label %[[LOOP:.*]]
 ; CHECK:       [[LOOP]]:
-; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ 0, %[[ENTRY]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
-; CHECK-NEXT:    [[RDX:%.*]] = phi i64 [ 2, %[[ENTRY]] ], [ [[SEL:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL15]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[RDX:%.*]] = phi i64 [ [[BC_MERGE_RDX16]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[SEL:%.*]], %[[LOOP]] ]
 ; CHECK-NEXT:    [[GEP:%.*]] = getelementptr inbounds float, ptr [[A]], i64 [[IV]]
 ; CHECK-NEXT:    [[L:%.*]] = load float, ptr [[GEP]], align 4
 ; CHECK-NEXT:    [[C:%.*]] = fcmp fast ueq float [[L]], 3.000000e+00
 ; CHECK-NEXT:    [[SEL]] = select i1 [[C]], i64 [[IV]], i64 [[RDX]]
 ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV]], 1
 ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT:.*]], label %[[LOOP]]
+; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP7:![0-9]+]]
 ; CHECK:       [[EXIT]]:
-; CHECK-NEXT:    [[SEL_LCSSA:%.*]] = phi i64 [ [[SEL]], %[[LOOP]] ]
+; CHECK-NEXT:    [[SEL_LCSSA:%.*]] = phi i64 [ [[SEL]], %[[LOOP]] ], [ [[RDX_SELECT]], %[[MIDDLE_BLOCK]] ], [ [[RDX_SELECT11]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ]
 ; CHECK-NEXT:    ret i64 [[SEL_LCSSA]]
 ;
 entry:
@@ -74,3 +212,74 @@ loop:
 exit:
   ret i64 %sel
 }
+
+define i8 @select_icmp_var_start(ptr %a, i8 %n, i8 %start) {
+; CHECK-LABEL: define i8 @select_icmp_var_start(
+; CHECK-SAME: ptr [[A:%.*]], i8 [[N:%.*]], i8 [[START:%.*]]) {
+; CHECK-NEXT:  [[ENTRY:.*]]:
+; CHECK-NEXT:    [[TMP0:%.*]] = add i8 [[N]], -1
+; CHECK-NEXT:    [[TMP1:%.*]] = zext i8 [[TMP0]] to i32
+; CHECK-NEXT:    [[TMP2:%.*]] = add nuw nsw i32 [[TMP1]], 1
+; CHECK-NEXT:    [[MIN_ITERS_CHECK1:%.*]] = icmp ult i32 [[TMP2]], 4
+; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK1]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+; CHECK:       [[VECTOR_PH]]:
+; CHECK-NEXT:    [[N_MOD_VF2:%.*]] = urem i32 [[TMP2]], 4
+; CHECK-NEXT:    [[N_VEC3:%.*]] = sub i32 [[TMP2]], [[N_MOD_VF2]]
+; CHECK-NEXT:    [[TMP13:%.*]] = trunc i32 [[N_VEC3]] to i8
+; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+; CHECK:       [[VECTOR_BODY]]:
+; CHECK-NEXT:    [[INDEX4:%.*]] = phi i32 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT11:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_IND5:%.*]] = phi <4 x i8> [ <i8 0, i8 1, i8 2, i8 3>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT6:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[VEC_PHI7:%.*]] = phi <4 x i8> [ splat (i8 -128), %[[VECTOR_PH]] ], [ [[TMP17:%.*]], %[[VECTOR_BODY]] ]
+; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = trunc i32 [[INDEX4]] to i8
+; CHECK-NEXT:    [[TMP21:%.*]] = add i8 [[OFFSET_IDX]], 0
+; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds i8, ptr [[A]], i8 [[TMP21]]
+; CHECK-NEXT:    [[TMP15:%.*]] = getelementptr inbounds i8, ptr [[TMP14]], i32 0
+; CHECK-NEXT:    [[WIDE_LOAD10:%.*]] = load <4 x i8>, ptr [[TMP15]], align 8
+; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq <4 x i8> [[WIDE_LOAD10]], splat (i8 3)
+; CHECK-NEXT:    [[TMP17]] = select <4 x i1> [[TMP16]], <4 x i8> [[VEC_IND5]], <4 x i8> [[VEC_PHI7]]
+; CHECK-NEXT:    [[INDEX_NEXT11]] = add nuw i32 [[INDEX4]], 4
+; CHECK-NEXT:    [[VEC_IND_NEXT6]] = add <4 x i8> [[VEC_IND5]], splat (i8 4)
+; CHECK-NEXT:    [[TMP18:%.*]] = icmp eq i32 [[INDEX_NEXT11]], [[N_VEC3]]
+; CHECK-NEXT:    br i1 [[TMP18]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
+; CHECK:       [[MIDDLE_BLOCK]]:
+; CHECK-NEXT:    [[TMP19:%.*]] = call i8 @llvm.vector.reduce.smax.v4i8(<4 x i8> [[TMP17]])
+; CHECK-NEXT:    [[RDX_SELECT_CMP12:%.*]] = icmp ne i8 [[TMP19]], -128
+; CHECK-NEXT:    [[RDX_SELECT13:%.*]] = select i1 [[RDX_SELECT_CMP12]], i8 [[TMP19]], i8 [[START]]
+; CHECK-NEXT:    [[CMP_N14:%.*]] = icmp eq i32 [[TMP2]], [[N_VEC3]]
+; CHECK-NEXT:    br i1 [[CMP_N14]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+; CHECK:       [[SCALAR_PH]]:
+; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i8 [ [[TMP13]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
+; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i8 [ [[RDX_SELECT13]], %[[MIDDLE_BLOCK]] ], [ [[START]], %[[ENTRY]] ]
+; CHECK-NEXT:    br label %[[LOOP:.*]]
+; CHECK:       [[LOOP]]:
+; CHECK-NEXT:    [[IV:%.*]] = phi i8 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[RDX:%.*]] = phi i8 [ [[BC_MERGE_RDX]], %[[SCALAR_PH]] ], [ [[SEL:%.*]], %[[LOOP]] ]
+; CHECK-NEXT:    [[GEP:%.*]] = getelementptr inbounds i8, ptr [[A]], i8 [[IV]]
+; CHECK-NEXT:    [[L:%.*]] = load i8, ptr [[GEP]], align 8
+; CHECK-NEXT:    [[C:%.*]] = icmp eq i8 [[L]], 3
+; CHECK-NEXT:    [[SEL]] = select i1 [[C]], i8 [[IV]], i8 [[RDX]]
+; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i8 [[IV]], 1
+; CHECK-NEXT:    [[EC:%.*]] = icmp eq i8 [[IV_NEXT]], [[N]]
+; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP9:![0-9]+]]
+; CHECK:       [[EXIT]]:
+; CHECK-NEXT:    [[SEL_LCSSA:%.*]] = phi i8 [ [[SEL]], %[[LOOP]] ], [ [[RDX_SELECT13]], %[[MIDDLE_BLOCK]] ]
+; CHECK-NEXT:    ret i8 [[SEL_LCSSA]]
+;
+entry:
+  br label %loop
+
+loop:
+  %iv = phi i8 [ 0, %entry ], [ %iv.next, %loop ]
+  %rdx = phi i8 [ %start, %entry ], [ %sel, %loop ]
+  %gep = getelementptr inbounds i8, ptr %a, i8 %iv
+  %l = load i8, ptr %gep, align 8
+  %c = icmp eq i8 %l, 3
+  %sel = select i1 %c, i8 %iv, i8 %rdx
+  %iv.next = add nuw nsw i8 %iv, 1
+  %ec = icmp eq i8 %iv.next, %n
+  br i1 %ec, label %exit, label %loop
+
+exit:
+  ret i8 %sel
+}
diff --git a/llvm/test/Transforms/MemCpyOpt/fca2memcpy.ll b/llvm/test/Transforms/MemCpyOpt/fca2memcpy.ll
index 61e349e01ed9..7d4557aa331c 100644
--- a/llvm/test/Transforms/MemCpyOpt/fca2memcpy.ll
+++ b/llvm/test/Transforms/MemCpyOpt/fca2memcpy.ll
@@ -51,8 +51,8 @@ define void @destroysrc(ptr %src, ptr %dst) {
 
 define void @destroynoaliassrc(ptr noalias %src, ptr %dst) {
 ; CHECK-LABEL: @destroynoaliassrc(
-; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[DST:%.*]], ptr align 8 [[SRC]], i64 16, i1 false)
-; CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[SRC:%.*]], i8 0, i64 16, i1 false)
+; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[DST:%.*]], ptr align 8 [[SRC:%.*]], i64 16, i1 false)
+; CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[SRC]], i8 0, i64 16, i1 false)
 ; CHECK-NEXT:    ret void
 ;
   %1 = load %S, ptr %src
@@ -79,9 +79,9 @@ define void @copyalias(ptr %src, ptr %dst) {
 ; sure we lift the computation as well if needed and possible.
 define void @addrproducer(ptr %src, ptr %dst) {
 ; CHECK-LABEL: @addrproducer(
-; CHECK-NEXT:    [[DST2:%.*]] = getelementptr [[S:%.*]], ptr [[DST]], i64 1
+; CHECK-NEXT:    [[DST2:%.*]] = getelementptr [[S:%.*]], ptr [[DST:%.*]], i64 1
 ; CHECK-NEXT:    call void @llvm.memmove.p0.p0.i64(ptr align 8 [[DST2]], ptr align 8 [[SRC:%.*]], i64 16, i1 false)
-; CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[DST:%.*]], i8 undef, i64 16, i1 false)
+; CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[DST]], i8 undef, i64 16, i1 false)
 ; CHECK-NEXT:    ret void
 ;
   %1 = load %S, ptr %src
@@ -113,8 +113,8 @@ define void @noaliasaddrproducer(ptr %src, ptr noalias %dst, ptr noalias %dstidp
 ; CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DSTIDPTR:%.*]], align 4
 ; CHECK-NEXT:    [[DSTINDEX:%.*]] = or i32 [[TMP2]], 1
 ; CHECK-NEXT:    [[DST2:%.*]] = getelementptr [[S:%.*]], ptr [[DST:%.*]], i32 [[DSTINDEX]]
-; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[DST2]], ptr align 8 [[SRC]], i64 16, i1 false)
-; CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[SRC:%.*]], i8 undef, i64 16, i1 false)
+; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[DST2]], ptr align 8 [[SRC:%.*]], i64 16, i1 false)
+; CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[SRC]], i8 undef, i64 16, i1 false)
 ; CHECK-NEXT:    ret void
 ;
   %1 = load %S, ptr %src
@@ -130,7 +130,7 @@ define void @throwing_call(ptr noalias %src, ptr %dst) {
 ; CHECK-LABEL: @throwing_call(
 ; CHECK-NEXT:    [[TMP1:%.*]] = load [[S:%.*]], ptr [[SRC:%.*]], align 8
 ; CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[SRC]], i8 0, i64 16, i1 false)
-; CHECK-NEXT:    call void @call() [[ATTR2:#.*]]
+; CHECK-NEXT:    call void @call() #[[ATTR2:[0-9]+]]
 ; CHECK-NEXT:    store [[S]] [[TMP1]], ptr [[DST:%.*]], align 8
 ; CHECK-NEXT:    ret void
 ;
@@ -156,4 +156,30 @@ loop:
   br label %loop
 }
 
+; There are multiple instructions that can clobber the source memory here.
+; We can move the dest write past the store to %ptr.24, but not the memcpy.
+; Make sure we don't perform fca2memcpy conversion in this case.
+define void @multiple_clobbering(ptr %ptr, ptr %ptr.copy) {
+; CHECK-LABEL: @multiple_clobbering(
+; CHECK-NEXT:    [[PTR_8:%.*]] = getelementptr inbounds nuw i8, ptr [[PTR:%.*]], i64 8
+; CHECK-NEXT:    [[PTR_24:%.*]] = getelementptr inbounds nuw i8, ptr [[PTR]], i64 24
+; CHECK-NEXT:    [[PTR_32:%.*]] = getelementptr inbounds nuw i8, ptr [[PTR]], i64 32
+; CHECK-NEXT:    [[PTR_COPY_8:%.*]] = getelementptr inbounds nuw i8, ptr [[PTR_COPY:%.*]], i64 8
+; CHECK-NEXT:    [[STRUCT:%.*]] = load { i32, i64 }, ptr [[PTR_COPY_8]], align 8
+; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr [[PTR_8]], ptr [[PTR_32]], i64 12, i1 false)
+; CHECK-NEXT:    store i64 1, ptr [[PTR_24]], align 8
+; CHECK-NEXT:    store { i32, i64 } [[STRUCT]], ptr [[PTR_32]], align 8
+; CHECK-NEXT:    ret void
+;
+  %ptr.8 = getelementptr inbounds nuw i8, ptr %ptr, i64 8
+  %ptr.24 = getelementptr inbounds nuw i8, ptr %ptr, i64 24
+  %ptr.32 = getelementptr inbounds nuw i8, ptr %ptr, i64 32
+  %ptr.copy.8 = getelementptr inbounds nuw i8, ptr %ptr.copy, i64 8
+  %struct = load { i32, i64 }, ptr %ptr.copy.8, align 8
+  call void @llvm.memcpy.p0.p0.i64(ptr %ptr.8, ptr %ptr.32, i64 12, i1 false)
+  store i64 1, ptr %ptr.24, align 8
+  store { i32, i64 } %struct, ptr %ptr.32, align 8
+  ret void
+}
+
 declare void @call()
diff --git a/llvm/test/Transforms/MemCpyOpt/memcpy-tbaa.ll b/llvm/test/Transforms/MemCpyOpt/memcpy-tbaa.ll
new file mode 100644
index 000000000000..6e446e5ff267
--- /dev/null
+++ b/llvm/test/Transforms/MemCpyOpt/memcpy-tbaa.ll
@@ -0,0 +1,77 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt < %s -passes=memcpyopt,dse -S -verify-memoryssa | FileCheck %s
+
+define void @test() local_unnamed_addr {
+; CHECK-LABEL: define void @test() local_unnamed_addr {
+; CHECK-NEXT:    [[TEST_ARRAY_B:%.*]] = alloca [31 x float], align 4
+; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr float, ptr [[TEST_ARRAY_B]], i64 1
+; CHECK-NEXT:    store float 0x3E6AA51880000000, ptr [[TMP1]], align 4
+; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr float, ptr [[TEST_ARRAY_B]], i64 1
+; CHECK-NEXT:    [[TMP3:%.*]] = load float, ptr [[TMP2]], align 4
+; CHECK-NEXT:    ret void
+;
+  %test_array_a = alloca [31 x float], align 4
+  %test_array_b = alloca [31 x float], align 4
+  %1 = getelementptr float, ptr %test_array_b, i64 1
+  store float 0x3E6AA51880000000, ptr %1, align 4, !tbaa !4
+  call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 4 dereferenceable(124) %test_array_a, ptr noundef nonnull align 4 dereferenceable(124) %test_array_b, i64 124, i1 false)
+  %2 = getelementptr float, ptr %test_array_a, i64 1
+  %3 = load float, ptr %2, align 4, !tbaa !7
+  ret void
+}
+
+%struct.Outer = type { float, double, %struct.Inner }
+%struct.Inner = type { i32, float }
+
+; Function Attrs: nounwind uwtable
+define dso_local float @f() {
+; CHECK-LABEL: define dso_local float @f() {
+; CHECK-NEXT:  [[ENTRY:.*:]]
+; CHECK-NEXT:    [[TEST1:%.*]] = alloca [[STRUCT_OUTER:%.*]], align 8
+; CHECK-NEXT:    [[F:%.*]] = getelementptr inbounds nuw [[STRUCT_OUTER]], ptr [[TEST1]], i32 0, i32 0
+; CHECK-NEXT:    store float 0.000000e+00, ptr [[F]], align 8
+; CHECK-NEXT:    [[F1:%.*]] = getelementptr inbounds nuw [[STRUCT_OUTER]], ptr [[TEST1]], i32 0, i32 0
+; CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[F1]], align 8
+; CHECK-NEXT:    [[ADD:%.*]] = fadd float [[TMP0]], 2.000000e+00
+; CHECK-NEXT:    store float [[ADD]], ptr [[F1]], align 8
+; CHECK-NEXT:    [[F2:%.*]] = getelementptr inbounds nuw [[STRUCT_OUTER]], ptr [[TEST1]], i32 0, i32 0
+; CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[F2]], align 8
+; CHECK-NEXT:    ret float [[TMP1]]
+;
+entry:
+  %test = alloca %struct.Outer, align 8
+  %test1 = alloca %struct.Outer, align 8
+  %f = getelementptr inbounds nuw %struct.Outer, ptr %test1, i32 0, i32 0
+  store float 0.000000e+00, ptr %f, align 8, !tbaa !9
+  %inner_a = getelementptr inbounds nuw %struct.Outer, ptr %test1, i32 0, i32 2
+  %i = getelementptr inbounds nuw %struct.Inner, ptr %inner_a, i32 0, i32 0
+  store i32 0, ptr %i, align 8, !tbaa !17
+  call void @llvm.memcpy.p0.p0.i64(ptr align 8 %test, ptr align 8 %test1, i64 24, i1 false)
+  %f1 = getelementptr inbounds nuw %struct.Outer, ptr %test, i32 0, i32 0
+  %0 = load float, ptr %f1, align 8, !tbaa !9
+  %add = fadd float %0, 2.000000e+00
+  store float %add, ptr %f1, align 8, !tbaa !9
+  %f2 = getelementptr inbounds nuw %struct.Outer, ptr %test, i32 0, i32 0
+  %1 = load float, ptr %f2, align 8, !tbaa !9
+  ret float %1
+}
+
+!1 = !{!"any data access", !2, i64 0}
+!2 = !{!"any access", !3, i64 0}
+!3 = !{!"Flang function root test"}
+!4 = !{!5, !5, i64 0}
+!5 = !{!"allocated data/test_array_a", !6, i64 0}
+!6 = !{!"allocated data", !1, i64 0}
+!7 = !{!8, !8, i64 0}
+!8 = !{!"allocated data/test_array_b", !6, i64 0}
+!9 = !{!10, !11, i64 0}
+!10 = !{!"Outer", !11, i64 0, !14, i64 8, !15, i64 16}
+!11 = !{!"float", !12, i64 0}
+!12 = !{!"omnipotent char", !13, i64 0}
+!13 = !{!"Simple C/C++ TBAA"}
+!14 = !{!"double", !12, i64 0}
+!15 = !{!"Inner", !16, i64 0, !11, i64 4}
+!16 = !{!"int", !12, i64 0}
+!17 = !{!10, !16, i64 16}
+
+
diff --git a/llvm/test/Transforms/MemCpyOpt/stack-move.ll b/llvm/test/Transforms/MemCpyOpt/stack-move.ll
index 6089c0a4d7cf..5ff6f0102120 100644
--- a/llvm/test/Transforms/MemCpyOpt/stack-move.ll
+++ b/llvm/test/Transforms/MemCpyOpt/stack-move.ll
@@ -259,7 +259,7 @@ define void @remove_scoped_noalias() {
 ; CHECK-LABEL: define void @remove_scoped_noalias() {
 ; CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_FOO:%.*]], align 4
 ; CHECK-NEXT:    store [[STRUCT_FOO]] { i32 10, i32 20, i32 30 }, ptr [[SRC]], align 4
-; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]]), !alias.scope !0
+; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]])
 ; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]])
 ; CHECK-NEXT:    ret void
 ;
@@ -283,7 +283,7 @@ define void @remove_alloca_metadata() {
 ; CHECK-LABEL: define void @remove_alloca_metadata() {
 ; CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_FOO:%.*]], align 4
 ; CHECK-NEXT:    store [[STRUCT_FOO]] { i32 10, i32 20, i32 30 }, ptr [[SRC]], align 4
-; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]]), !alias.scope !0
+; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]])
 ; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]])
 ; CHECK-NEXT:    ret void
 ;
@@ -308,7 +308,7 @@ define void @noalias_on_lifetime() {
 ; CHECK-LABEL: define void @noalias_on_lifetime() {
 ; CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_FOO:%.*]], align 4
 ; CHECK-NEXT:    store [[STRUCT_FOO]] { i32 10, i32 20, i32 30 }, ptr [[SRC]], align 4
-; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]]), !alias.scope !0
+; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]])
 ; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]])
 ; CHECK-NEXT:    ret void
 ;
@@ -399,10 +399,10 @@ define void @terminator_lastuse() personality i32 0 {
 ; CHECK-NEXT:    store [[STRUCT_FOO]] { i32 10, i32 20, i32 30 }, ptr [[SRC]], align 4
 ; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @use_nocapture(ptr nocapture [[SRC]])
 ; CHECK-NEXT:    [[RV:%.*]] = invoke i32 @use_nocapture(ptr [[SRC]])
-; CHECK-NEXT:    to label [[SUC:%.*]] unwind label [[UNW:%.*]]
+; CHECK-NEXT:            to label [[SUC:%.*]] unwind label [[UNW:%.*]]
 ; CHECK:       unw:
 ; CHECK-NEXT:    [[LP:%.*]] = landingpad i32
-; CHECK-NEXT:    cleanup
+; CHECK-NEXT:            cleanup
 ; CHECK-NEXT:    resume i32 0
 ; CHECK:       suc:
 ; CHECK-NEXT:    ret void
diff --git a/llvm/test/Transforms/VectorCombine/X86/load-extractelement-scalarization.ll b/llvm/test/Transforms/VectorCombine/X86/load-extractelement-scalarization.ll
index 0acfeccb92ef..d46c8c0de403 100644
--- a/llvm/test/Transforms/VectorCombine/X86/load-extractelement-scalarization.ll
+++ b/llvm/test/Transforms/VectorCombine/X86/load-extractelement-scalarization.ll
@@ -24,3 +24,15 @@ define void @multiple_extract(ptr %p) {
   store i32 %e1, ptr %p1, align 4
   ret void
 }
+
+; infinite loop if we fold an extract that is waiting to be erased
+define void @unused_extract(ptr %p) {
+; CHECK-LABEL: @unused_extract(
+; CHECK-NEXT:    ret void
+;
+  %load = load <4 x float>, ptr %p, align 8
+  %shuffle0 = shufflevector <4 x float> zeroinitializer, <4 x float> %load, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
+  %shuffle1 = shufflevector <4 x float> %shuffle0, <4 x float> zeroinitializer, <4 x i32> <i32 0, i32 4, i32 poison, i32 poison>
+  %extract = extractelement <4 x float> %load, i64 1
+  ret void
+}
diff --git a/llvm/test/Verifier/sme-attributes.ll b/llvm/test/Verifier/sme-attributes.ll
index 4bf5e813daf2..0ae2b9fd91f5 100644
--- a/llvm/test/Verifier/sme-attributes.ll
+++ b/llvm/test/Verifier/sme-attributes.ll
@@ -68,3 +68,6 @@ declare void @zt0_inout_out() "aarch64_inout_zt0" "aarch64_out_zt0";
 
 declare void @zt0_inout_agnostic() "aarch64_inout_zt0" "aarch64_za_state_agnostic";
 ; CHECK: Attributes 'aarch64_new_zt0', 'aarch64_in_zt0', 'aarch64_out_zt0', 'aarch64_inout_zt0', 'aarch64_preserves_zt0' and 'aarch64_za_state_agnostic' are mutually exclusive
+
+declare void @zt0_undef_function() "aarch64_zt0_undef";
+; CHECK: Attribute 'aarch64_zt0_undef' can only be applied to a callsite.
diff --git a/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.generated.expected b/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.generated.expected
index cd135ce9e011..e54510ba8e04 100644
--- a/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.generated.expected
+++ b/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.generated.expected
@@ -76,29 +76,28 @@ attributes #0 = { noredzone nounwind ssp uwtable "frame-pointer"="all" }
 ; CHECK-NEXT:    .cfi_offset r30, -8
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     memw(r29+#4) = #0
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
 ; CHECK-NEXT:     memw(r29+#8) = #0
-; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:     r1 = memw(r29+#8)
-; CHECK-NEXT:     memw(r29+#12) = #2
-; CHECK-NEXT:    }
+; CHECK-EMPTY:
+; CHECK-NEXT:    } :mem_noshuf
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     p0 = cmp.eq(r1,#0)
+; CHECK-NEXT:     memw(r29+#12) = #2
 ; CHECK-NEXT:     memw(r29+#16) = #3
-; CHECK-NEXT:     memw(r29+#20) = #4
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     p0 = cmp.eq(r1,#0)
-; CHECK-NEXT:     if (p0.new) memw(r29+#16) = #3
-; CHECK-NEXT:     if (p0.new) memw(r29+#12) = #2
+; CHECK-NEXT:     memw(r29+#20) = #4
+; CHECK-NEXT:     if (p0) memw(r29+#16) = #3
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     if (p0) memw(r29+#12) = #2
 ; CHECK-NEXT:     if (p0) memw(r29+#20) = #4
-; CHECK-NEXT:     if (p0) memw(r29+#8) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     if (p0) memw(r29+#8) = #1
 ; CHECK-NEXT:     if (!p0) memw(r29+#16) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -117,15 +116,15 @@ attributes #0 = { noredzone nounwind ssp uwtable "frame-pointer"="all" }
 ; CHECK-NEXT:    .cfi_offset r30, -8
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     memw(r29+#4) = #0
-; CHECK-NEXT:     memw(r0+#0) = #1
+; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:     memw(r29+#12) = #2
+; CHECK-NEXT:     memw(r29+#16) = #3
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     memw(r29+#16) = #3
 ; CHECK-NEXT:     memw(r29+#20) = #4
+; CHECK-NEXT:     memw(r0+#0) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    //# InlineAsm Start
 ; CHECK-NEXT:    //# InlineAsm End
diff --git a/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.nogenerated.expected b/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.nogenerated.expected
index 833bf68fc03d..219d6d004fd8 100644
--- a/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.nogenerated.expected
+++ b/llvm/test/tools/UpdateTestChecks/update_llc_test_checks/Inputs/hexagon_generated_funcs.ll.nogenerated.expected
@@ -17,29 +17,28 @@ define dso_local i32 @check_boundaries() #0 {
 ; CHECK-NEXT:    .cfi_offset r30, -8
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     memw(r29+#4) = #0
-; CHECK-NEXT:    }
-; CHECK-NEXT:    {
 ; CHECK-NEXT:     memw(r29+#8) = #0
-; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:     r1 = memw(r29+#8)
-; CHECK-NEXT:     memw(r29+#12) = #2
-; CHECK-NEXT:    }
+; CHECK-EMPTY:
+; CHECK-NEXT:    } :mem_noshuf
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     p0 = cmp.eq(r1,#0)
+; CHECK-NEXT:     memw(r29+#12) = #2
 ; CHECK-NEXT:     memw(r29+#16) = #3
-; CHECK-NEXT:     memw(r29+#20) = #4
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     p0 = cmp.eq(r1,#0)
-; CHECK-NEXT:     if (p0.new) memw(r29+#16) = #3
-; CHECK-NEXT:     if (p0.new) memw(r29+#12) = #2
+; CHECK-NEXT:     memw(r29+#20) = #4
+; CHECK-NEXT:     if (p0) memw(r29+#16) = #3
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     if (p0) memw(r29+#12) = #2
 ; CHECK-NEXT:     if (p0) memw(r29+#20) = #4
-; CHECK-NEXT:     if (p0) memw(r29+#8) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
+; CHECK-NEXT:     if (p0) memw(r29+#8) = #1
 ; CHECK-NEXT:     if (!p0) memw(r29+#16) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
@@ -94,15 +93,15 @@ define dso_local i32 @main() #0 {
 ; CHECK-NEXT:    .cfi_offset r30, -8
 ; CHECK-NEXT:    {
 ; CHECK-NEXT:     memw(r29+#4) = #0
-; CHECK-NEXT:     memw(r0+#0) = #1
+; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     memw(r29+#8) = #1
 ; CHECK-NEXT:     memw(r29+#12) = #2
+; CHECK-NEXT:     memw(r29+#16) = #3
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    {
-; CHECK-NEXT:     memw(r29+#16) = #3
 ; CHECK-NEXT:     memw(r29+#20) = #4
+; CHECK-NEXT:     memw(r0+#0) = #1
 ; CHECK-NEXT:    }
 ; CHECK-NEXT:    //# InlineAsm Start
 ; CHECK-NEXT:    //# InlineAsm End
diff --git a/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_h.yaml b/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_h.yaml
new file mode 100644
index 000000000000..26f3493d6214
--- /dev/null
+++ b/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_h.yaml
@@ -0,0 +1,133 @@
+--- !COFF
+header:
+  Machine:         IMAGE_FILE_MACHINE_I386
+  Characteristics: [ IMAGE_FILE_LINE_NUMS_STRIPPED, IMAGE_FILE_32BIT_MACHINE ]
+sections:
+  - Name:            .text
+    Characteristics: [ IMAGE_SCN_CNT_CODE, IMAGE_SCN_MEM_EXECUTE, IMAGE_SCN_MEM_READ ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            .data
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            .bss
+    Characteristics: [ IMAGE_SCN_CNT_UNINITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            '.idata$2'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '0000000000000000000000000000000000000000'
+    SizeOfRawData:   20
+    Relocations:
+      - VirtualAddress:  0
+        SymbolName:      '.idata$4'
+        Type:            IMAGE_REL_I386_DIR32NB
+      - VirtualAddress:  12
+        SymbolName:      __foo_lib_iname
+        Type:            IMAGE_REL_I386_DIR32NB
+      - VirtualAddress:  16
+        SymbolName:      '.idata$5'
+        Type:            IMAGE_REL_I386_DIR32NB
+  - Name:            '.idata$5'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            '.idata$4'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+symbols:
+  - Name:            .file
+    Value:           0
+    SectionNumber:   -2
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_FILE
+    File:            fake
+  - Name:            hname
+    Value:           0
+    SectionNumber:   6
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            fthunk
+    Value:           0
+    SectionNumber:   5
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            .text
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          0
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            .data
+    Value:           0
+    SectionNumber:   2
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          0
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            .bss
+    Value:           0
+    SectionNumber:   3
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          0
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            '.idata$2'
+    Value:           0
+    SectionNumber:   4
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          20
+      NumberOfRelocations: 3
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            '.idata$4'
+    Value:           0
+    SectionNumber:   6
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            '.idata$5'
+    Value:           0
+    SectionNumber:   5
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            __head_foo_lib
+    Value:           0
+    SectionNumber:   4
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+  - Name:            __foo_lib_iname
+    Value:           0
+    SectionNumber:   0
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+...
diff --git a/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_s00000.yaml b/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_s00000.yaml
new file mode 100644
index 000000000000..f09437fc9925
--- /dev/null
+++ b/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_s00000.yaml
@@ -0,0 +1,116 @@
+--- !COFF
+header:
+  Machine:         IMAGE_FILE_MACHINE_I386
+  Characteristics: [ IMAGE_FILE_LINE_NUMS_STRIPPED, IMAGE_FILE_32BIT_MACHINE ]
+sections:
+  - Name:            .text
+    Characteristics: [ IMAGE_SCN_CNT_CODE, IMAGE_SCN_MEM_EXECUTE, IMAGE_SCN_MEM_READ ]
+    Alignment:       4
+    SectionData:     FF25000000009090
+    SizeOfRawData:   8
+    Relocations:
+      - VirtualAddress:  2
+        SymbolName:      '.idata$5'
+        Type:            IMAGE_REL_I386_DIR32
+  - Name:            .data
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            .bss
+    Characteristics: [ IMAGE_SCN_CNT_UNINITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            '.idata$7'
+    Characteristics: [ IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '00000000'
+    SizeOfRawData:   4
+    Relocations:
+      - VirtualAddress:  0
+        SymbolName:      __head_foo_lib
+        Type:            IMAGE_REL_I386_DIR32NB
+  - Name:            '.idata$5'
+    Characteristics: [ IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '00000000'
+    SizeOfRawData:   4
+    Relocations:
+      - VirtualAddress:  0
+        SymbolName:      '.idata$6'
+        Type:            IMAGE_REL_I386_DIR32NB
+  - Name:            '.idata$4'
+    Characteristics: [ IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '00000000'
+    SizeOfRawData:   4
+    Relocations:
+      - VirtualAddress:  0
+        SymbolName:      '.idata$6'
+        Type:            IMAGE_REL_I386_DIR32NB
+  - Name:            '.idata$6'
+    Characteristics: [ IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       2
+    SectionData:     '010066756E633100'
+    SizeOfRawData:   8
+symbols:
+  - Name:            .text
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            .data
+    Value:           0
+    SectionNumber:   2
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            .bss
+    Value:           0
+    SectionNumber:   3
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            '.idata$7'
+    Value:           0
+    SectionNumber:   4
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            '.idata$5'
+    Value:           0
+    SectionNumber:   5
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            '.idata$4'
+    Value:           0
+    SectionNumber:   6
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            '.idata$6'
+    Value:           0
+    SectionNumber:   7
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            _func1
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+  - Name:            __imp__func1
+    Value:           0
+    SectionNumber:   5
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+  - Name:            __head_foo_lib
+    Value:           0
+    SectionNumber:   0
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+...
diff --git a/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_t.yaml b/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_t.yaml
new file mode 100644
index 000000000000..e4465293bec1
--- /dev/null
+++ b/llvm/test/tools/llvm-dlltool/Inputs/gnu_foo_lib_t.yaml
@@ -0,0 +1,119 @@
+--- !COFF
+header:
+  Machine:         IMAGE_FILE_MACHINE_I386
+  Characteristics: [ IMAGE_FILE_RELOCS_STRIPPED, IMAGE_FILE_LINE_NUMS_STRIPPED, IMAGE_FILE_32BIT_MACHINE ]
+sections:
+  - Name:            .text
+    Characteristics: [ IMAGE_SCN_CNT_CODE, IMAGE_SCN_MEM_EXECUTE, IMAGE_SCN_MEM_READ ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            .data
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            .bss
+    Characteristics: [ IMAGE_SCN_CNT_UNINITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     ''
+  - Name:            '.idata$4'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '00000000'
+    SizeOfRawData:   4
+  - Name:            '.idata$5'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '00000000'
+    SizeOfRawData:   4
+  - Name:            '.idata$7'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     666F6F2E646C6C00
+    SizeOfRawData:   8
+symbols:
+  - Name:            .file
+    Value:           0
+    SectionNumber:   -2
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_FILE
+    File:            fake
+  - Name:            .text
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          0
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            .data
+    Value:           0
+    SectionNumber:   2
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          0
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            .bss
+    Value:           0
+    SectionNumber:   3
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          0
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            '.idata$4'
+    Value:           0
+    SectionNumber:   4
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          4
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            '.idata$5'
+    Value:           0
+    SectionNumber:   5
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          4
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            '.idata$7'
+    Value:           0
+    SectionNumber:   6
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+    SectionDefinition:
+      Length:          8
+      NumberOfRelocations: 0
+      NumberOfLinenumbers: 0
+      CheckSum:        0
+      Number:          0
+  - Name:            __foo_lib_iname
+    Value:           0
+    SectionNumber:   6
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+...
diff --git a/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_1.yaml b/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_1.yaml
new file mode 100644
index 000000000000..f3f669d63bca
--- /dev/null
+++ b/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_1.yaml
@@ -0,0 +1,69 @@
+--- !COFF
+header:
+  Machine:         IMAGE_FILE_MACHINE_AMD64
+  Characteristics: [  ]
+sections:
+  - Name:            '.idata$2'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '0000000000000000000000000000000000000000'
+    SizeOfRawData:   20
+    Relocations:
+      - VirtualAddress:  12
+        SymbolName:      '.idata$6'
+        Type:            IMAGE_REL_AMD64_ADDR32NB
+      - VirtualAddress:  0
+        SymbolName:      '.idata$4'
+        Type:            IMAGE_REL_AMD64_ADDR32NB
+      - VirtualAddress:  16
+        SymbolName:      '.idata$5'
+        Type:            IMAGE_REL_AMD64_ADDR32NB
+  - Name:            '.idata$6'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       2
+    SectionData:     666F6F2E646C6C00
+    SizeOfRawData:   8
+symbols:
+  - Name:            __IMPORT_DESCRIPTOR_foo
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+  - Name:            '.idata$2'
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_SECTION
+  - Name:            '.idata$6'
+    Value:           0
+    SectionNumber:   2
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_STATIC
+  - Name:            '.idata$4'
+    Value:           0
+    SectionNumber:   0
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_SECTION
+  - Name:            '.idata$5'
+    Value:           0
+    SectionNumber:   0
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_SECTION
+  - Name:            __NULL_IMPORT_DESCRIPTOR
+    Value:           0
+    SectionNumber:   0
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+  - Name:            "foo_NULL_THUNK_DATA"
+    Value:           0
+    SectionNumber:   0
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+...
diff --git a/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_2.yaml b/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_2.yaml
new file mode 100644
index 000000000000..26b601fb74c5
--- /dev/null
+++ b/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_2.yaml
@@ -0,0 +1,18 @@
+--- !COFF
+header:
+  Machine:         IMAGE_FILE_MACHINE_AMD64
+  Characteristics: [  ]
+sections:
+  - Name:            '.idata$3'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       4
+    SectionData:     '0000000000000000000000000000000000000000'
+    SizeOfRawData:   20
+symbols:
+  - Name:            __NULL_IMPORT_DESCRIPTOR
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+...
diff --git a/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_3.yaml b/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_3.yaml
new file mode 100644
index 000000000000..68248597cbae
--- /dev/null
+++ b/llvm/test/tools/llvm-dlltool/Inputs/llvm_foo_dll_3.yaml
@@ -0,0 +1,23 @@
+--- !COFF
+header:
+  Machine:         IMAGE_FILE_MACHINE_AMD64
+  Characteristics: [  ]
+sections:
+  - Name:            '.idata$5'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       8
+    SectionData:     '0000000000000000'
+    SizeOfRawData:   8
+  - Name:            '.idata$4'
+    Characteristics: [ IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE ]
+    Alignment:       8
+    SectionData:     '0000000000000000'
+    SizeOfRawData:   8
+symbols:
+  - Name:            "foo_NULL_THUNK_DATA"
+    Value:           0
+    SectionNumber:   1
+    SimpleType:      IMAGE_SYM_TYPE_NULL
+    ComplexType:     IMAGE_SYM_DTYPE_NULL
+    StorageClass:    IMAGE_SYM_CLASS_EXTERNAL
+...
diff --git a/llvm/test/tools/llvm-dlltool/identify.test b/llvm/test/tools/llvm-dlltool/identify.test
new file mode 100644
index 000000000000..eb2792a8e41a
--- /dev/null
+++ b/llvm/test/tools/llvm-dlltool/identify.test
@@ -0,0 +1,69 @@
+Test the -I / --identify option.
+
+Test with both GNU style and LLVM style import libraries; using
+sources from yaml to preserve the checking behaviour even if the
+output of llvm-dlltool itself would change.
+
+RUN: rm -rf %t && mkdir -p %t
+RUN: split-file %s %t
+
+RUN: yaml2obj %S/Inputs/gnu_foo_lib_h.yaml > %t/gnu_foo_lib_h.o
+RUN: yaml2obj %S/Inputs/gnu_foo_lib_s00000.yaml > %t/gnu_foo_lib_s00000.o
+RUN: yaml2obj %S/Inputs/gnu_foo_lib_t.yaml > %t/gnu_foo_lib_t.o
+RUN: llvm-ar rcs %t/gnu.a %t/gnu_foo_lib_h.o %t/gnu_foo_lib_s00000.o %t/gnu_foo_lib_t.o
+
+RUN: yaml2obj %S/Inputs/llvm_foo_dll_1.yaml > %t/llvm_foo_dll_1.o
+RUN: yaml2obj %S/Inputs/llvm_foo_dll_2.yaml > %t/llvm_foo_dll_2.o
+RUN: yaml2obj %S/Inputs/llvm_foo_dll_3.yaml > %t/llvm_foo_dll_3.o
+RUN: llvm-ar rcs %t/llvm.a %t/llvm_foo_dll_1.o %t/llvm_foo_dll_2.o %t/llvm_foo_dll_3.o
+
+
+Check that we can identify the DLL name from a GNU style import library.
+
+RUN: llvm-dlltool -I %t/gnu.a | FileCheck --check-prefix=FOO %s
+RUN: llvm-dlltool --identify %t/gnu.a | count 1
+
+FOO: foo.dll
+
+
+Check that we successfully can identify run while passing the
+--identify-strict option.
+
+RUN: llvm-dlltool -I %t/gnu.a --identify-strict | FileCheck --check-prefix=FOO %s
+
+
+Check that we can identify the DLL name from an LLVM style import library.
+
+RUN: llvm-dlltool -I %t/llvm.a | FileCheck --check-prefix=FOO %s
+RUN: llvm-dlltool -I %t/llvm.a | count 1
+
+
+Check that we can identify the DLL names from an import library that
+contains imports for multiple DLLs.
+
+RUN: llvm-dlltool -m i386:x86-64 -d %t/lib1.def -l %t/lib1.a
+RUN: llvm-dlltool -m i386:x86-64 -d %t/lib2.def -l %t/lib2.a
+RUN: llvm-ar qcsL %t/merged.a %t/lib1.a %t/lib2.a
+
+RUN: llvm-dlltool -I %t/merged.a | FileCheck --check-prefix=MERGED %s
+
+MERGED-DAG: lib1.dll
+MERGED-DAG: lib2.dll
+
+Check that --identify-strict fails this case, when there are multiple
+outputs.
+
+RUN: not llvm-dlltool -I %t/merged.a --identify-strict 2>&1 | FileCheck --check-prefix=ERROR %s
+
+ERROR: contains imports for two or more DLLs
+
+
+#--- lib1.def
+LIBRARY lib1.dll
+EXPORTS
+    func1
+
+#--- lib2.def
+LIBRARY lib2.dll
+EXPORTS
+    func2
diff --git a/llvm/test/tools/llvm-objcopy/MachO/strip-with-encryption-info.test b/llvm/test/tools/llvm-objcopy/MachO/strip-with-encryption-info.test
index 19b06b1ec02c..2b2bd670613d 100644
--- a/llvm/test/tools/llvm-objcopy/MachO/strip-with-encryption-info.test
+++ b/llvm/test/tools/llvm-objcopy/MachO/strip-with-encryption-info.test
@@ -1,13 +1,19 @@
 # RUN: rm -rf %t && mkdir %t
 # RUN: yaml2obj %s -o %t/original
 # RUN: llvm-strip --strip-all %t/original -o %t/stripped
-# RUN: llvm-readobj --macho-segment %t/stripped | FileCheck %s
+# RUN: llvm-readobj --macho-segment --section-headers %t/stripped | FileCheck %s
+
+# CHECK-LABEL: Sections [
+# CHECK:      Index: 0
+# CHECK-NEXT: Name: __text
+# CHECK-NEXT: Segment: __TEXT
+# CHECK:      Offset: 16384
 
 # CHECK-LABEL: Name: __PAGEZERO
-# CHECK:       fileoff: 16384
+# CHECK:       fileoff: 0
 
 # CHECK-LABEL: Name: __TEXT
-# CHECK:       fileoff: 16384
+# CHECK:       fileoff: 0
 
 # The YAML below is the following code
 # int main(int argc, char **argv) { return 0; }
diff --git a/llvm/tools/opt-viewer/optrecord.py b/llvm/tools/opt-viewer/optrecord.py
index 9e2fc7cb553b..8014204a64f4 100644
--- a/llvm/tools/opt-viewer/optrecord.py
+++ b/llvm/tools/opt-viewer/optrecord.py
@@ -64,17 +64,19 @@ class Remark(yaml.YAMLObject):
 
     default_demangler = "c++filt -n"
     demangler_proc = None
+    demangler_lock = Lock()
 
     @classmethod
     def set_demangler(cls, demangler):
         cls.demangler_proc = subprocess.Popen(
             demangler.split(), stdin=subprocess.PIPE, stdout=subprocess.PIPE
         )
-        cls.demangler_lock = Lock()
 
     @classmethod
     def demangle(cls, name):
         with cls.demangler_lock:
+            if not cls.demangler_proc:
+                cls.set_demangler(cls.default_demangler)
             cls.demangler_proc.stdin.write((name + "\n").encode("utf-8"))
             cls.demangler_proc.stdin.flush()
             return cls.demangler_proc.stdout.readline().rstrip().decode("utf-8")
@@ -323,8 +325,6 @@ def get_remarks(input_file, filter_=None):
 def gather_results(filenames, num_jobs, should_print_progress, filter_=None):
     if should_print_progress:
         print("Reading YAML files...")
-    if not Remark.demangler_proc:
-        Remark.set_demangler(Remark.default_demangler)
     remarks = optpmap.pmap(
         get_remarks, filenames, num_jobs, should_print_progress, filter_
     )
diff --git a/llvm/unittests/Target/AArch64/SMEAttributesTest.cpp b/llvm/unittests/Target/AArch64/SMEAttributesTest.cpp
index 3af5e24168c8..f8c77fcba19c 100644
--- a/llvm/unittests/Target/AArch64/SMEAttributesTest.cpp
+++ b/llvm/unittests/Target/AArch64/SMEAttributesTest.cpp
@@ -1,6 +1,7 @@
 #include "Utils/AArch64SMEAttributes.h"
 #include "llvm/AsmParser/Parser.h"
 #include "llvm/IR/Function.h"
+#include "llvm/IR/InstrTypes.h"
 #include "llvm/IR/Module.h"
 #include "llvm/Support/SourceMgr.h"
 
@@ -69,6 +70,15 @@ TEST(SMEAttributes, Constructors) {
   ASSERT_TRUE(SA(*parseIR("declare void @foo() \"aarch64_new_zt0\"")
                       ->getFunction("foo"))
                   .isNewZT0());
+  ASSERT_TRUE(
+      SA(cast<CallBase>((parseIR("declare void @callee()\n"
+                                 "define void @foo() {"
+                                 "call void @callee() \"aarch64_zt0_undef\"\n"
+                                 "ret void\n}")
+                             ->getFunction("foo")
+                             ->begin()
+                             ->front())))
+          .isUndefZT0());
 
   // Invalid combinations.
   EXPECT_DEBUG_DEATH(SA(SA::SM_Enabled | SA::SM_Compatible),
@@ -215,6 +225,18 @@ TEST(SMEAttributes, Basics) {
   ASSERT_FALSE(ZT0_New.hasSharedZAInterface());
   ASSERT_TRUE(ZT0_New.hasPrivateZAInterface());
 
+  SA ZT0_Undef = SA(SA::ZT0_Undef | SA::encodeZT0State(SA::StateValue::New));
+  ASSERT_TRUE(ZT0_Undef.isNewZT0());
+  ASSERT_FALSE(ZT0_Undef.isInZT0());
+  ASSERT_FALSE(ZT0_Undef.isOutZT0());
+  ASSERT_FALSE(ZT0_Undef.isInOutZT0());
+  ASSERT_FALSE(ZT0_Undef.isPreservesZT0());
+  ASSERT_FALSE(ZT0_Undef.sharesZT0());
+  ASSERT_TRUE(ZT0_Undef.hasZT0State());
+  ASSERT_FALSE(ZT0_Undef.hasSharedZAInterface());
+  ASSERT_TRUE(ZT0_Undef.hasPrivateZAInterface());
+  ASSERT_TRUE(ZT0_Undef.isUndefZT0());
+
   ASSERT_FALSE(SA(SA::Normal).isInZT0());
   ASSERT_FALSE(SA(SA::Normal).isOutZT0());
   ASSERT_FALSE(SA(SA::Normal).isInOutZT0());
@@ -285,6 +307,7 @@ TEST(SMEAttributes, Transitions) {
   SA ZT0_Shared = SA(SA::encodeZT0State(SA::StateValue::In));
   SA ZA_ZT0_Shared = SA(SA::encodeZAState(SA::StateValue::In) |
                         SA::encodeZT0State(SA::StateValue::In));
+  SA Undef_ZT0 = SA(SA::ZT0_Undef);
 
   // Shared ZA -> Private ZA Interface
   ASSERT_FALSE(ZA_Shared.requiresDisablingZABeforeCall(Private_ZA));
@@ -295,6 +318,13 @@ TEST(SMEAttributes, Transitions) {
   ASSERT_TRUE(ZT0_Shared.requiresPreservingZT0(Private_ZA));
   ASSERT_TRUE(ZT0_Shared.requiresEnablingZAAfterCall(Private_ZA));
 
+  // Shared Undef ZT0 -> Private ZA Interface
+  // Note: "Undef ZT0" is a callsite attribute that means ZT0 is undefined at
+  // point the of the call.
+  ASSERT_TRUE(ZT0_Shared.requiresDisablingZABeforeCall(Undef_ZT0));
+  ASSERT_FALSE(ZT0_Shared.requiresPreservingZT0(Undef_ZT0));
+  ASSERT_TRUE(ZT0_Shared.requiresEnablingZAAfterCall(Undef_ZT0));
+
   // Shared ZA & ZT0 -> Private ZA Interface
   ASSERT_FALSE(ZA_ZT0_Shared.requiresDisablingZABeforeCall(Private_ZA));
   ASSERT_TRUE(ZA_ZT0_Shared.requiresPreservingZT0(Private_ZA));
diff --git a/llvm/unittests/TargetParser/Host.cpp b/llvm/unittests/TargetParser/Host.cpp
index c5b96e1df904..2a3958151a60 100644
--- a/llvm/unittests/TargetParser/Host.cpp
+++ b/llvm/unittests/TargetParser/Host.cpp
@@ -340,7 +340,7 @@ TEST(getLinuxHostCPUName, s390x) {
 
   // Model Id: 9175
   ExpectedCPUs.push_back("zEC12");
-  ExpectedCPUs.push_back("arch15");
+  ExpectedCPUs.push_back("z17");
 
   // Model Id: 3931
   ExpectedCPUs.push_back("zEC12");
diff --git a/llvm/unittests/TargetParser/RISCVISAInfoTest.cpp b/llvm/unittests/TargetParser/RISCVISAInfoTest.cpp
index 7ebfcf915a7c..5089bc0fd479 100644
--- a/llvm/unittests/TargetParser/RISCVISAInfoTest.cpp
+++ b/llvm/unittests/TargetParser/RISCVISAInfoTest.cpp
@@ -507,6 +507,14 @@ TEST(ParseArchString, RejectsDoubleOrTrailingUnderscore) {
 }
 
 TEST(ParseArchString, RejectsDuplicateExtensionNames) {
+  // Zicsr/Zifencei are allowed to duplicate with "g".
+  ASSERT_THAT_EXPECTED(RISCVISAInfo::parseArchString("rv64g_zicsr", true),
+                       Succeeded());
+  ASSERT_THAT_EXPECTED(RISCVISAInfo::parseArchString("rv64g_zifencei", true),
+                       Succeeded());
+  ASSERT_THAT_EXPECTED(
+      RISCVISAInfo::parseArchString("rv64g_zicsr_zifencei", true), Succeeded());
+
   EXPECT_EQ(toString(RISCVISAInfo::parseArchString("rv64ii", true).takeError()),
             "invalid standard user-level extension 'i'");
   EXPECT_EQ(toString(RISCVISAInfo::parseArchString("rv32ee", true).takeError()),
diff --git a/offload/DeviceRTL/include/Synchronization.h b/offload/DeviceRTL/include/Synchronization.h
index 5a789441b9d3..c510fbf0774c 100644
--- a/offload/DeviceRTL/include/Synchronization.h
+++ b/offload/DeviceRTL/include/Synchronization.h
@@ -61,7 +61,11 @@ V add(Ty *Address, V Val, atomic::OrderingTy Ordering,
 template <typename Ty, typename V = utils::remove_addrspace_t<Ty>>
 V load(Ty *Address, atomic::OrderingTy Ordering,
        MemScopeTy MemScope = MemScopeTy::device) {
+#ifdef __NVPTX__
+  return __scoped_atomic_fetch_add(Address, V(0), Ordering, MemScope);
+#else
   return __scoped_atomic_load_n(Address, Ordering, MemScope);
+#endif
 }
 
 template <typename Ty, typename V = utils::remove_addrspace_t<Ty>>
-- 
2.39.5 (Apple Git-154)

